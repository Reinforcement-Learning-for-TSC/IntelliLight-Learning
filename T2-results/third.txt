Script started on 2021-07-09 00:51:46+10:00 [TERM="xterm-256color" TTY="/dev/pts/0" COLUMNS="80" LINES="24"]

(base) ]0;soup@soup-virtual-machine: ~/IntelliLight[01;32msoup@soup-virtual-machine[00m:[01;34m~/IntelliLight[00m$ co[K[Kcode runexp.py ls[Kcd IntelliLight/ls[Kconda activate venv3.6

(venv3.6) ]0;soup@soup-virtual-machine: ~/IntelliLight[01;32msoup@soup-virtual-machine[00m:[01;34m~/IntelliLight[00m$ conda activate venv3.6[7Pde runexp.py nda activate venv3.6[Kpython runexp.py 

/home/soup/anaconda3/envs/venv3.6/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.

  _np_qint8 = np.dtype([("qint8", np.int8, 1)])

/home/soup/anaconda3/envs/venv3.6/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.

  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])

/home/soup/anaconda3/envs/venv3.6/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.

  _np_qint16 = np.dtype([("qint16", np.int16, 1)])

/home/soup/anaconda3/envs/venv3.6/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.

  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])

/home/soup/anaconda3/envs/venv3.6/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.

  _np_qint32 = np.dtype([("qint32", np.int32, 1)])

/home/soup/anaconda3/envs/venv3.6/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.

  np_resource = np.dtype([("resource", np.ubyte, 1)])

/home/soup/sumo-0.32.0/tools

Using TensorFlow backend.

2021-07-09 00:52:00.211743: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA

__________________________________________________________________________________________________

Layer (type)                    Output Shape         Param #     Connected to                     

==================================================================================================

input_map_feature (InputLayer)  (None, 150, 150, 1)  0                                            

__________________________________________________________________________________________________

conv1 (Conv2D)                  (None, 38, 38, 32)   2048        input_map_feature[0][0]          

__________________________________________________________________________________________________

bn1 (BatchNormalization)        (None, 38, 38, 32)   96          conv1[0][0]                      

__________________________________________________________________________________________________

act1 (Activation)               (None, 38, 38, 32)   0           bn1[0][0]                        

__________________________________________________________________________________________________

max_pooling2d_1 (MaxPooling2D)  (None, 19, 19, 32)   0           act1[0][0]                       

__________________________________________________________________________________________________

dropout_1 (Dropout)             (None, 19, 19, 32)   0           max_pooling2d_1[0][0]            

__________________________________________________________________________________________________

conv2 (Conv2D)                  (None, 10, 10, 16)   8192        dropout_1[0][0]                  

__________________________________________________________________________________________________

bn2 (BatchNormalization)        (None, 10, 10, 16)   48          conv2[0][0]                      

__________________________________________________________________________________________________

act2 (Activation)               (None, 10, 10, 16)   0           bn2[0][0]                        

__________________________________________________________________________________________________

max_pooling2d_2 (MaxPooling2D)  (None, 5, 5, 16)     0           act2[0][0]                       

__________________________________________________________________________________________________

dropout_2 (Dropout)             (None, 5, 5, 16)     0           max_pooling2d_2[0][0]            

__________________________________________________________________________________________________

input_cur_phase (InputLayer)    (None, 1)            0                                            

__________________________________________________________________________________________________

flatten_1 (Flatten)             (None, 400)          0           dropout_2[0][0]                  

__________________________________________________________________________________________________

input_next_phase (InputLayer)   (None, 1)            0                                            

__________________________________________________________________________________________________

input_num_of_vehicles (InputLay (None, 12)           0                                            

__________________________________________________________________________________________________

input_queue_length (InputLayer) (None, 12)           0                                            

__________________________________________________________________________________________________

input_waiting_time (InputLayer) (None, 12)           0                                            

__________________________________________________________________________________________________

all_flatten_feature (Concatenat (None, 438)          0           input_cur_phase[0][0]            

                                                                 flatten_1[0][0]                  

                                                                 input_next_phase[0][0]           

                                                                 input_num_of_vehicles[0][0]      

                                                                 input_queue_length[0][0]         

                                                                 input_waiting_time[0][0]         

__________________________________________________________________________________________________

hidden_shared_1 (Dense)         (None, 20)           8780        all_flatten_feature[0][0]        

__________________________________________________________________________________________________

hidden_separate_branch_0_1 (Den (None, 20)           420         hidden_shared_1[0][0]            

__________________________________________________________________________________________________

hidden_separate_branch_1_1 (Den (None, 20)           420         hidden_shared_1[0][0]            

__________________________________________________________________________________________________

q_values_separate_branch_0 (Den (None, 2)            42          hidden_separate_branch_0_1[0][0] 

__________________________________________________________________________________________________

selector_0 (Selector)           (None, 1)            0           input_cur_phase[0][0]            

__________________________________________________________________________________________________

q_values_separate_branch_1 (Den (None, 2)            42          hidden_separate_branch_1_1[0][0] 

__________________________________________________________________________________________________

selector_1 (Selector)           (None, 1)            0           input_cur_phase[0][0]            

__________________________________________________________________________________________________

multiply_0 (Multiply)           (None, 2)            0           q_values_separate_branch_0[0][0] 

                                                                 selector_0[0][0]                 

__________________________________________________________________________________________________

multiply_1 (Multiply)           (None, 2)            0           q_values_separate_branch_1[0][0] 

                                                                 selector_1[0][0]                 

__________________________________________________________________________________________________

add_1 (Add)                     (None, 2)            0           multiply_0[0][0]                 

                                                                 multiply_1[0][0]                 

==================================================================================================

Total params: 20,088

Trainable params: 19,992

Non-trainable params: 96

__________________________________________________________________________________________________

Could not connect to TraCI server at localhost:47147 [Errno 111] Connection refused

 Retrying in 1 seconds

Loading configuration... done.

***Starting server on port 47147 ***

Loading net-file from '/home/soup/IntelliLight/data/one_run/cross.net.xml'... done (3ms).

Warning: Missing yellow phase in tlLogic 'node0', program '0' for tl-index 5 when switching to phase 1

Loading additional-files from '/home/soup/IntelliLight/data/one_run/cross.tls.switch.xml'... done (5ms).

Loading additional-files from '/home/soup/IntelliLight/data/one_run/cross.car.type.xml'... done (1ms).

Loading done.

Simulation started with time: 0.00

time = 20	action = 0	current_phase = 0	next_phase = 1	reward = -0.096708	array([[-0.97998637, -0.15543285]], dtype=float32)

time = 25	action = 0	current_phase = 0	next_phase = 1	reward = -0.777623	array([[-0.98078287, -0.15541553]], dtype=float32)

time = 30	action = 1	current_phase = 0	next_phase = 1	reward = -2.600898	array([[-0.9753964, -0.1559512]], dtype=float32)

time = 38	action = 0	current_phase = 1	next_phase = 0	reward = -1.414874	array([[1.0384277 , 0.58529764]], dtype=float32)

time = 43	action = 1	current_phase = 1	next_phase = 0	reward = -1.576626	array([[1.059602  , 0.58407116]], dtype=float32)

time = 51	action = 0	current_phase = 0	next_phase = 1	reward = 1.146392	array([[-0.97950673, -0.1691163 ]], dtype=float32)

time = 56	action = 1	current_phase = 0	next_phase = 1	reward = -2.265926	array([[-0.989565  , -0.14012259]], dtype=float32)

time = 64	action = 0	current_phase = 1	next_phase = 0	reward = -1.645498	array([[1.0384967, 0.586825 ]], dtype=float32)

time = 69	action = 1	current_phase = 1	next_phase = 0	reward = -0.725525	array([[1.0307682, 0.6028304]], dtype=float32)

time = 77	action = 0	current_phase = 0	next_phase = 1	reward = 0.388094	array([[-0.9845237 , -0.13275027]], dtype=float32)

time = 82	action = 1	current_phase = 0	next_phase = 1	reward = -1.673133	array([[-0.97565347, -0.17110398]], dtype=float32)

time = 90	action = 0	current_phase = 1	next_phase = 0	reward = -1.844465	array([[1.045959 , 0.5900492]], dtype=float32)

time = 95	action = 1	current_phase = 1	next_phase = 0	reward = -1.134607	array([[1.0597887 , 0.58457124]], dtype=float32)

time = 103	action = 0	current_phase = 0	next_phase = 1	reward = 0.309527	array([[-0.9672671 , -0.17453054]], dtype=float32)

time = 108	action = 1	current_phase = 0	next_phase = 1	reward = -1.736908	array([[-0.98819673, -0.14111215]], dtype=float32)

time = 116	action = 0	current_phase = 1	next_phase = 0	reward = -1.582893	array([[1.033916 , 0.6013948]], dtype=float32)

time = 121	action = 1	current_phase = 1	next_phase = 0	reward = -1.166026	array([[1.0464039 , 0.59575325]], dtype=float32)

time = 129	action = 0	current_phase = 0	next_phase = 1	reward = 0.263370	array([[-0.9840553 , -0.13122958]], dtype=float32)

time = 134	action = 1	current_phase = 0	next_phase = 1	reward = -1.308913	array([[-0.9751476 , -0.17003612]], dtype=float32)

time = 142	action = 0	current_phase = 1	next_phase = 0	reward = -0.526897	array([[1.0736717 , 0.56989276]], dtype=float32)

time = 147	action = 1	current_phase = 1	next_phase = 0	reward = -0.999419	array([[1.0329443 , 0.60056466]], dtype=float32)

time = 155	action = 0	current_phase = 0	next_phase = 1	reward = 0.168933	array([[-0.9746233 , -0.17114969]], dtype=float32)

time = 160	action = 1	current_phase = 0	next_phase = 1	reward = -1.856341	array([[-0.9771784 , -0.14822891]], dtype=float32)

time = 168	action = 0	current_phase = 1	next_phase = 0	reward = -1.208278	array([[1.0382154, 0.5873841]], dtype=float32)

time = 173	action = 1	current_phase = 1	next_phase = 0	reward = -1.544511	array([[1.0570405, 0.5822552]], dtype=float32)

time = 181	action = 0	current_phase = 0	next_phase = 1	reward = -0.937243	array([[-0.9755235 , -0.15358898]], dtype=float32)

time = 186	action = 1	current_phase = 0	next_phase = 1	reward = -2.463486	array([[-0.9912192 , -0.13096346]], dtype=float32)

time = 194	action = 0	current_phase = 1	next_phase = 0	reward = -1.327341	array([[1.0393686, 0.584329 ]], dtype=float32)

time = 199	action = 1	current_phase = 1	next_phase = 0	reward = -1.265181	array([[1.0327425, 0.601804 ]], dtype=float32)

time = 207	action = 0	current_phase = 0	next_phase = 1	reward = -0.399155	array([[-0.98284703, -0.11780998]], dtype=float32)

time = 212	action = 1	current_phase = 0	next_phase = 1	reward = -2.618077	array([[-0.9787686 , -0.16225219]], dtype=float32)

time = 220	action = 0	current_phase = 1	next_phase = 0	reward = -1.637969	array([[1.0386095 , 0.58532476]], dtype=float32)

time = 225	action = 1	current_phase = 1	next_phase = 0	reward = -0.526490	array([[1.0590918 , 0.58513534]], dtype=float32)

time = 233	action = 0	current_phase = 0	next_phase = 1	reward = -0.359261	array([[-0.97647953, -0.17116392]], dtype=float32)

time = 238	action = 1	current_phase = 0	next_phase = 1	reward = -2.692342	array([[-0.98330796, -0.1398815 ]], dtype=float32)

time = 246	action = 0	current_phase = 1	next_phase = 0	reward = -1.815211	array([[1.0312545 , 0.59816724]], dtype=float32)

time = 251	action = 1	current_phase = 1	next_phase = 0	reward = -1.263140	array([[1.0464884 , 0.59531593]], dtype=float32)

time = 259	action = 0	current_phase = 0	next_phase = 1	reward = 0.658564	array([[-0.9873673 , -0.14058769]], dtype=float32)

time = 264	action = 1	current_phase = 0	next_phase = 1	reward = -1.064395	array([[-0.9743883 , -0.17112634]], dtype=float32)

time = 272	action = 0	current_phase = 1	next_phase = 0	reward = -1.309444	array([[1.0748813 , 0.56992203]], dtype=float32)

time = 277	action = 1	current_phase = 1	next_phase = 0	reward = -1.318709	array([[1.0255044 , 0.59941053]], dtype=float32)

time = 285	action = 0	current_phase = 0	next_phase = 1	reward = 0.351236	array([[-1.0066375 , -0.15278782]], dtype=float32)

time = 290	action = 1	current_phase = 0	next_phase = 1	reward = -0.664390	array([[-0.98763025, -0.1409133 ]], dtype=float32)

time = 298	action = 0	current_phase = 1	next_phase = 0	reward = -1.497303	array([[1.0549458, 0.5888805]], dtype=float32)

time = 303	action = 1	current_phase = 1	next_phase = 0	reward = -0.898154	array([[1.0591819, 0.5837575]], dtype=float32)

time = 311	action = 0	current_phase = 0	next_phase = 1	reward = 0.282674	array([[-0.9787345 , -0.18053487]], dtype=float32)

time = 316	action = 1	current_phase = 0	next_phase = 1	reward = -2.011558	array([[-0.9878119 , -0.13859789]], dtype=float32)

time = 324	action = 0	current_phase = 1	next_phase = 0	reward = -0.803495	array([[1.0373039, 0.5882654]], dtype=float32)

time = 329	action = 1	current_phase = 1	next_phase = 0	reward = -1.265001	array([[1.0405403, 0.6034138]], dtype=float32)

time = 337	action = 0	current_phase = 0	next_phase = 1	reward = 0.521309	array([[-0.9880182 , -0.14073543]], dtype=float32)

time = 342	action = 1	current_phase = 0	next_phase = 1	reward = -2.318569	array([[-0.97505224, -0.17054495]], dtype=float32)

time = 350	action = 0	current_phase = 1	next_phase = 0	reward = -1.013377	array([[1.0400665 , 0.58703387]], dtype=float32)

time = 355	action = 1	current_phase = 1	next_phase = 0	reward = -0.889595	array([[1.0679197 , 0.58556366]], dtype=float32)

time = 363	action = 0	current_phase = 0	next_phase = 1	reward = 0.178197	array([[-0.9683685 , -0.17482072]], dtype=float32)

time = 368	action = 1	current_phase = 0	next_phase = 1	reward = -2.489759	array([[-0.98802423, -0.14052515]], dtype=float32)

time = 376	action = 0	current_phase = 1	next_phase = 0	reward = -1.702010	array([[1.0393757 , 0.59875137]], dtype=float32)

time = 381	action = 1	current_phase = 1	next_phase = 0	reward = -1.807516	array([[1.0460922 , 0.59510714]], dtype=float32)

time = 389	action = 0	current_phase = 0	next_phase = 1	reward = -0.575474	array([[-0.9791632 , -0.15354422]], dtype=float32)

time = 394	action = 1	current_phase = 0	next_phase = 1	reward = -1.636992	array([[-0.9789547 , -0.15864117]], dtype=float32)

time = 402	action = 0	current_phase = 1	next_phase = 0	reward = -1.219082	array([[1.074231 , 0.5663551]], dtype=float32)

time = 407	action = 1	current_phase = 1	next_phase = 0	reward = -1.093870	array([[1.0254002, 0.5996361]], dtype=float32)

time = 415	action = 0	current_phase = 0	next_phase = 1	reward = -0.043798	array([[-0.9722034 , -0.16344313]], dtype=float32)

time = 420	action = 1	current_phase = 0	next_phase = 1	reward = -2.130344	array([[-0.97072846, -0.14609706]], dtype=float32)

time = 428	action = 0	current_phase = 1	next_phase = 0	reward = -1.458466	array([[1.0382648, 0.584843 ]], dtype=float32)

time = 433	action = 1	current_phase = 1	next_phase = 0	reward = -1.278082	array([[1.0614318, 0.5833647]], dtype=float32)

time = 441	action = 0	current_phase = 0	next_phase = 1	reward = 0.829507	array([[-0.97626835, -0.15986432]], dtype=float32)

time = 446	action = 1	current_phase = 0	next_phase = 1	reward = -2.197016	array([[-0.9895865 , -0.14184067]], dtype=float32)

time = 454	action = 0	current_phase = 1	next_phase = 0	reward = -1.589359	array([[1.0390049, 0.5880319]], dtype=float32)

time = 459	action = 1	current_phase = 1	next_phase = 0	reward = -0.602276	array([[1.0320994, 0.6028706]], dtype=float32)

time = 467	action = 0	current_phase = 0	next_phase = 1	reward = 0.076850	array([[-0.98744845, -0.13970546]], dtype=float32)

time = 472	action = 1	current_phase = 0	next_phase = 1	reward = -1.525924	array([[-0.9760138 , -0.17102048]], dtype=float32)

time = 480	action = 0	current_phase = 1	next_phase = 0	reward = -1.825488	array([[1.0457237, 0.5891757]], dtype=float32)

time = 485	action = 1	current_phase = 1	next_phase = 0	reward = -1.016533	array([[1.0603135 , 0.58357614]], dtype=float32)

time = 493	action = 0	current_phase = 0	next_phase = 1	reward = 0.313796	array([[-0.9797254 , -0.18015364]], dtype=float32)

time = 498	action = 1	current_phase = 0	next_phase = 1	reward = -1.782687	array([[-0.988092  , -0.14007363]], dtype=float32)

time = 506	action = 0	current_phase = 1	next_phase = 0	reward = -1.586577	array([[1.0397424 , 0.60097283]], dtype=float32)

time = 511	action = 1	current_phase = 1	next_phase = 0	reward = -1.616939	array([[1.0465698 , 0.59461814]], dtype=float32)

time = 519	action = 0	current_phase = 0	next_phase = 1	reward = 0.559931	array([[-0.992411  , -0.13989082]], dtype=float32)

time = 524	action = 1	current_phase = 0	next_phase = 1	reward = -1.310246	array([[-0.97542274, -0.17150095]], dtype=float32)

time = 532	action = 0	current_phase = 1	next_phase = 0	reward = -0.534397	array([[1.0598168, 0.5747534]], dtype=float32)

time = 537	action = 1	current_phase = 1	next_phase = 0	reward = -0.978212	array([[1.035173 , 0.6028352]], dtype=float32)

time = 545	action = 0	current_phase = 0	next_phase = 1	reward = 0.471339	array([[-0.9755661 , -0.17075786]], dtype=float32)

time = 550	action = 1	current_phase = 0	next_phase = 1	reward = -2.111744	array([[-0.9795829 , -0.15569079]], dtype=float32)

time = 558	action = 0	current_phase = 1	next_phase = 0	reward = -1.378155	array([[1.0326284, 0.6007625]], dtype=float32)

time = 563	action = 1	current_phase = 1	next_phase = 0	reward = -0.306135	array([[1.0598521 , 0.58366996]], dtype=float32)

time = 571	action = 0	current_phase = 0	next_phase = 1	reward = 0.422313	array([[-0.98402935, -0.15619072]], dtype=float32)

time = 576	action = 1	current_phase = 0	next_phase = 1	reward = -2.340587	array([[-0.98888105, -0.14076924]], dtype=float32)

time = 584	action = 0	current_phase = 1	next_phase = 0	reward = -1.441663	array([[1.0378727 , 0.58692926]], dtype=float32)

time = 589	action = 1	current_phase = 1	next_phase = 0	reward = -1.081650	array([[1.0317656 , 0.60173404]], dtype=float32)

time = 597	action = 0	current_phase = 0	next_phase = 1	reward = -0.714047	array([[-0.994814  , -0.13355517]], dtype=float32)

time = 602	action = 1	current_phase = 0	next_phase = 1	reward = -2.594061	array([[-0.97885996, -0.16130635]], dtype=float32)

time = 610	action = 0	current_phase = 1	next_phase = 0	reward = -1.503756	array([[1.037443  , 0.58468807]], dtype=float32)

time = 615	action = 1	current_phase = 1	next_phase = 0	reward = -0.811390	array([[1.0597595, 0.5839854]], dtype=float32)

time = 623	action = 0	current_phase = 0	next_phase = 1	reward = -0.143882	array([[-0.978719  , -0.18002963]], dtype=float32)

time = 628	action = 1	current_phase = 0	next_phase = 1	reward = -2.690600	array([[-0.98253876, -0.14007533]], dtype=float32)

time = 636	action = 0	current_phase = 1	next_phase = 0	reward = -1.827355	array([[1.0334624 , 0.59628767]], dtype=float32)

time = 641	action = 1	current_phase = 1	next_phase = 0	reward = -1.361543	array([[1.0461124, 0.5955651]], dtype=float32)

time = 649	action = 0	current_phase = 0	next_phase = 1	reward = 0.675615	array([[-0.97770756, -0.135889  ]], dtype=float32)

time = 654	action = 1	current_phase = 0	next_phase = 1	reward = -1.186104	array([[-0.9749214 , -0.17067878]], dtype=float32)

time = 662	action = 0	current_phase = 1	next_phase = 0	reward = -1.436968	array([[1.0667711, 0.5703914]], dtype=float32)

time = 667	action = 1	current_phase = 1	next_phase = 0	reward = -0.938828	array([[1.0272938, 0.6022692]], dtype=float32)

Terminal occured. Episode end.

Simulation ended at time: 676.00

Reason: TraCI requested termination.

Performance: 

 Duration: 17149ms

 Real time factor: 39.4192

 UPS: 1135.984606

Vehicles: 

 Inserted: 318

 Running: 34

 Waiting: 0



DijkstraRouter answered 318 queries and explored 2.29 edges on average.

DijkstraRouter spent 1ms answering queries (0.00ms on average).

Could not connect to TraCI server at localhost:49181 [Errno 111] Connection refused

 Retrying in 1 seconds

Loading configuration... done.

***Starting server on port 49181 ***

Loading net-file from '/home/soup/IntelliLight/data/one_run/cross.net.xml'... done (6ms).

Warning: Missing yellow phase in tlLogic 'node0', program '0' for tl-index 5 when switching to phase 1

Loading additional-files from '/home/soup/IntelliLight/data/one_run/cross.tls.switch.xml'... done (1ms).

Loading additional-files from '/home/soup/IntelliLight/data/one_run/cross.car.type.xml'... done (1ms).

Loading done.

Simulation started with time: 0.00

time = 20	action = 0	current_phase = 0	next_phase = 1	reward = -0.096708	array([[-0.97998637, -0.15543285]], dtype=float32)

time = 25	action = 0	current_phase = 0	next_phase = 1	reward = -0.777623	array([[-0.98078287, -0.15541553]], dtype=float32)

time = 30	action = 0	current_phase = 0	next_phase = 1	reward = -1.290711	array([[-0.9753964, -0.1559512]], dtype=float32)

time = 35	action = 1	current_phase = 0	next_phase = 1	reward = -2.437328	array([[-0.9839037 , -0.14417462]], dtype=float32)

time = 43	action = 0	current_phase = 1	next_phase = 0	reward = -1.342156	array([[1.0499525 , 0.57240367]], dtype=float32)

time = 48	action = 1	current_phase = 1	next_phase = 0	reward = -1.249396	array([[1.0321229 , 0.60216177]], dtype=float32)

time = 56	action = 0	current_phase = 0	next_phase = 1	reward = 0.104215	array([[-0.9954834 , -0.14777407]], dtype=float32)

time = 61	action = 0	current_phase = 0	next_phase = 1	reward = -1.211859	array([[-0.9789388, -0.1616971]], dtype=float32)

time = 66	action = 1	current_phase = 0	next_phase = 1	reward = -2.368308	array([[-0.99856937, -0.14876443]], dtype=float32)

time = 74	action = 0	current_phase = 1	next_phase = 0	reward = -1.431685	array([[1.0383581 , 0.58537704]], dtype=float32)

time = 79	action = 1	current_phase = 1	next_phase = 0	reward = -0.844323	array([[1.0327301, 0.6004283]], dtype=float32)

time = 87	action = 0	current_phase = 0	next_phase = 1	reward = -0.699967	array([[-1.0000488 , -0.13415515]], dtype=float32)

time = 92	action = 0	current_phase = 0	next_phase = 1	reward = -1.229784	array([[-0.979118  , -0.16167895]], dtype=float32)

time = 97	action = 1	current_phase = 0	next_phase = 1	reward = -2.592219	array([[-0.99854463, -0.14709136]], dtype=float32)

time = 105	action = 0	current_phase = 1	next_phase = 0	reward = -1.459577	array([[1.0389872 , 0.58440936]], dtype=float32)

time = 110	action = 1	current_phase = 1	next_phase = 0	reward = -0.877185	array([[1.0321963, 0.6003723]], dtype=float32)

time = 118	action = 0	current_phase = 0	next_phase = 1	reward = -1.091484	array([[-0.9743582 , -0.13386919]], dtype=float32)

time = 123	action = 0	current_phase = 0	next_phase = 1	reward = -1.208483	array([[-0.9794737 , -0.15883482]], dtype=float32)

time = 128	action = 1	current_phase = 0	next_phase = 1	reward = -1.950244	array([[-0.9972359 , -0.14703068]], dtype=float32)

time = 136	action = 0	current_phase = 1	next_phase = 0	reward = -1.217297	array([[1.0432721 , 0.58921945]], dtype=float32)

time = 141	action = 1	current_phase = 1	next_phase = 0	reward = -1.646101	array([[1.0539237 , 0.58770525]], dtype=float32)

time = 149	action = 0	current_phase = 0	next_phase = 1	reward = -0.872803	array([[-0.9849275, -0.1495746]], dtype=float32)

time = 154	action = 0	current_phase = 0	next_phase = 1	reward = -1.199412	array([[-0.9797229 , -0.16077071]], dtype=float32)

time = 159	action = 1	current_phase = 0	next_phase = 1	reward = -1.535896	array([[-0.9916085 , -0.12833005]], dtype=float32)

time = 167	action = 0	current_phase = 1	next_phase = 0	reward = -1.152971	array([[1.0461948, 0.5835725]], dtype=float32)

time = 172	action = 1	current_phase = 1	next_phase = 0	reward = -1.662859	array([[1.0559887, 0.5836344]], dtype=float32)

time = 180	action = 0	current_phase = 0	next_phase = 1	reward = -1.227042	array([[-0.9823637 , -0.14047104]], dtype=float32)

time = 185	action = 0	current_phase = 0	next_phase = 1	reward = -0.905214	array([[-0.9788744, -0.1598449]], dtype=float32)

time = 190	action = 1	current_phase = 0	next_phase = 1	reward = -1.911154	array([[-0.9863542 , -0.15460753]], dtype=float32)

time = 198	action = 0	current_phase = 1	next_phase = 0	reward = -1.272634	array([[1.0379404, 0.5861744]], dtype=float32)

time = 203	action = 1	current_phase = 1	next_phase = 0	reward = -0.856013	array([[1.0606958, 0.5831741]], dtype=float32)

time = 211	action = 0	current_phase = 0	next_phase = 1	reward = -0.408620	array([[-0.9962716 , -0.16630743]], dtype=float32)

time = 216	action = 0	current_phase = 0	next_phase = 1	reward = -0.640328	array([[-0.9981575 , -0.14283851]], dtype=float32)

time = 221	action = 1	current_phase = 0	next_phase = 1	reward = -2.136866	array([[-0.9842273 , -0.17016655]], dtype=float32)

time = 229	action = 0	current_phase = 1	next_phase = 0	reward = -1.321333	array([[1.0376315 , 0.58756083]], dtype=float32)

time = 234	action = 1	current_phase = 1	next_phase = 0	reward = -0.092584	array([[1.0588655 , 0.58376575]], dtype=float32)

time = 242	action = 0	current_phase = 0	next_phase = 1	reward = 0.153807	array([[-0.9779978 , -0.17908597]], dtype=float32)

time = 247	action = 0	current_phase = 0	next_phase = 1	reward = -0.069486	array([[-0.98855585, -0.1411351 ]], dtype=float32)

time = 252	action = 1	current_phase = 0	next_phase = 1	reward = -2.315088	array([[-0.9766104 , -0.17002043]], dtype=float32)

time = 260	action = 0	current_phase = 1	next_phase = 0	reward = -0.945113	array([[1.0376768, 0.5890284]], dtype=float32)

time = 265	action = 1	current_phase = 1	next_phase = 0	reward = -0.587706	array([[1.0693662, 0.584796 ]], dtype=float32)

time = 273	action = 0	current_phase = 0	next_phase = 1	reward = -0.145384	array([[-0.97555983, -0.16990401]], dtype=float32)

time = 278	action = 0	current_phase = 0	next_phase = 1	reward = -0.066538	array([[-0.98797596, -0.14016497]], dtype=float32)

time = 283	action = 1	current_phase = 0	next_phase = 1	reward = -2.323175	array([[-0.97609746, -0.17024276]], dtype=float32)

time = 291	action = 0	current_phase = 1	next_phase = 0	reward = -1.040204	array([[1.0539829 , 0.58127975]], dtype=float32)

time = 296	action = 1	current_phase = 1	next_phase = 0	reward = -1.364247	array([[1.055852 , 0.5972287]], dtype=float32)

time = 304	action = 0	current_phase = 0	next_phase = 1	reward = 0.490045	array([[-0.975332  , -0.17030814]], dtype=float32)

time = 309	action = 0	current_phase = 0	next_phase = 1	reward = -0.043077	array([[-0.9885398 , -0.14001405]], dtype=float32)

time = 314	action = 1	current_phase = 0	next_phase = 1	reward = -1.356002	array([[-0.9768127 , -0.17065111]], dtype=float32)

time = 322	action = 0	current_phase = 1	next_phase = 0	reward = -0.488386	array([[1.0704409, 0.5702759]], dtype=float32)

time = 327	action = 1	current_phase = 1	next_phase = 0	reward = -1.243896	array([[1.0346801, 0.6013218]], dtype=float32)

time = 335	action = 0	current_phase = 0	next_phase = 1	reward = 0.477547	array([[-0.97127146, -0.16083407]], dtype=float32)

time = 340	action = 0	current_phase = 0	next_phase = 1	reward = -0.016943	array([[-0.991095 , -0.1489963]], dtype=float32)

time = 345	action = 1	current_phase = 0	next_phase = 1	reward = -1.668586	array([[-0.98314905, -0.16279337]], dtype=float32)

time = 353	action = 0	current_phase = 1	next_phase = 0	reward = -0.698089	array([[1.0389271, 0.5874519]], dtype=float32)

time = 358	action = 1	current_phase = 1	next_phase = 0	reward = -0.828084	array([[1.0410298, 0.6033373]], dtype=float32)

time = 366	action = 0	current_phase = 0	next_phase = 1	reward = 0.215298	array([[-0.98469484, -0.1320379 ]], dtype=float32)

time = 371	action = 0	current_phase = 0	next_phase = 1	reward = -0.010564	array([[-0.97558165, -0.1691815 ]], dtype=float32)

time = 376	action = 1	current_phase = 0	next_phase = 1	reward = -2.129344	array([[-0.98943514, -0.13991027]], dtype=float32)

time = 384	action = 0	current_phase = 1	next_phase = 0	reward = -0.807047	array([[1.0383782 , 0.58745116]], dtype=float32)

time = 389	action = 1	current_phase = 1	next_phase = 0	reward = -1.223994	array([[1.0406537 , 0.60356563]], dtype=float32)

time = 397	action = 0	current_phase = 0	next_phase = 1	reward = 0.540809	array([[-0.99595004, -0.12501979]], dtype=float32)

time = 402	action = 0	current_phase = 0	next_phase = 1	reward = 0.017910	array([[-0.97566754, -0.17005557]], dtype=float32)

time = 407	action = 1	current_phase = 0	next_phase = 1	reward = -1.601471	array([[-0.98885965, -0.14105451]], dtype=float32)

time = 415	action = 0	current_phase = 1	next_phase = 0	reward = -1.394908	array([[1.0457942, 0.5892086]], dtype=float32)

time = 420	action = 1	current_phase = 1	next_phase = 0	reward = -0.932358	array([[1.0396274, 0.6036205]], dtype=float32)

time = 428	action = 0	current_phase = 0	next_phase = 1	reward = -0.063145	array([[-0.9870697 , -0.13968658]], dtype=float32)

time = 433	action = 0	current_phase = 0	next_phase = 1	reward = 0.013753	array([[-0.97556645, -0.16955978]], dtype=float32)

time = 438	action = 1	current_phase = 0	next_phase = 1	reward = -1.724826	array([[-0.98937255, -0.13880396]], dtype=float32)

time = 446	action = 0	current_phase = 1	next_phase = 0	reward = -1.640853	array([[1.0325868, 0.6012343]], dtype=float32)

time = 451	action = 1	current_phase = 1	next_phase = 0	reward = -0.962999	array([[1.046295  , 0.59702736]], dtype=float32)

time = 459	action = 0	current_phase = 0	next_phase = 1	reward = -0.052770	array([[-0.9874902 , -0.14125717]], dtype=float32)

time = 464	action = 0	current_phase = 0	next_phase = 1	reward = 0.029140	array([[-0.9763003 , -0.16939673]], dtype=float32)

time = 469	action = 1	current_phase = 0	next_phase = 1	reward = -0.660731	array([[-0.9701353 , -0.15070303]], dtype=float32)

time = 477	action = 0	current_phase = 1	next_phase = 0	reward = -1.188717	array([[1.0566792, 0.5874341]], dtype=float32)

time = 482	action = 1	current_phase = 1	next_phase = 0	reward = -1.051562	array([[1.0542122, 0.5845947]], dtype=float32)

time = 490	action = 0	current_phase = 0	next_phase = 1	reward = 0.257501	array([[-0.98731065, -0.14042774]], dtype=float32)

time = 495	action = 0	current_phase = 0	next_phase = 1	reward = 0.342667	array([[-0.9734826 , -0.16042396]], dtype=float32)

time = 500	action = 1	current_phase = 0	next_phase = 1	reward = -1.333263	array([[-0.98001295, -0.15420446]], dtype=float32)

time = 508	action = 0	current_phase = 1	next_phase = 0	reward = -1.460046	array([[1.0484871, 0.5906626]], dtype=float32)

time = 513	action = 1	current_phase = 1	next_phase = 0	reward = -1.156732	array([[1.060788  , 0.58363795]], dtype=float32)

time = 521	action = 0	current_phase = 0	next_phase = 1	reward = 0.556398	array([[-0.9838151 , -0.15592459]], dtype=float32)

time = 526	action = 0	current_phase = 0	next_phase = 1	reward = 0.064850	array([[-0.987247  , -0.13932261]], dtype=float32)

time = 531	action = 1	current_phase = 0	next_phase = 1	reward = -1.348762	array([[-0.9766704 , -0.17027633]], dtype=float32)

time = 539	action = 0	current_phase = 1	next_phase = 0	reward = -1.527005	array([[1.0487553, 0.5907764]], dtype=float32)

time = 544	action = 1	current_phase = 1	next_phase = 0	reward = -0.698082	array([[1.0607723, 0.5843133]], dtype=float32)

time = 552	action = 0	current_phase = 0	next_phase = 1	reward = 0.024822	array([[-0.975907  , -0.16926655]], dtype=float32)

time = 557	action = 0	current_phase = 0	next_phase = 1	reward = 0.091052	array([[-0.9883196 , -0.14101076]], dtype=float32)

time = 562	action = 1	current_phase = 0	next_phase = 1	reward = -1.617459	array([[-0.97624654, -0.17118245]], dtype=float32)

time = 570	action = 0	current_phase = 1	next_phase = 0	reward = -1.784510	array([[1.0472404 , 0.58834624]], dtype=float32)

time = 575	action = 1	current_phase = 1	next_phase = 0	reward = -1.067811	array([[1.0610963, 0.5832978]], dtype=float32)

time = 583	action = 0	current_phase = 0	next_phase = 1	reward = 0.312839	array([[-0.9678766 , -0.17525303]], dtype=float32)

time = 588	action = 0	current_phase = 0	next_phase = 1	reward = 0.062121	array([[-0.9883657 , -0.14044823]], dtype=float32)

time = 593	action = 1	current_phase = 0	next_phase = 1	reward = -1.792062	array([[-0.97633153, -0.16938886]], dtype=float32)

time = 601	action = 0	current_phase = 1	next_phase = 0	reward = -1.830249	array([[1.0535327 , 0.58139795]], dtype=float32)

time = 606	action = 1	current_phase = 1	next_phase = 0	reward = -0.820355	array([[1.0459799, 0.596485 ]], dtype=float32)

time = 614	action = 0	current_phase = 0	next_phase = 1	reward = 0.043514	array([[-0.9758003 , -0.17073113]], dtype=float32)

time = 619	action = 0	current_phase = 0	next_phase = 1	reward = 0.061808	array([[-0.9887017 , -0.13955104]], dtype=float32)

time = 624	action = 1	current_phase = 0	next_phase = 1	reward = -1.140052	array([[-0.9754903 , -0.17044747]], dtype=float32)

time = 632	action = 0	current_phase = 1	next_phase = 0	reward = -1.380108	array([[1.0719414, 0.5684348]], dtype=float32)

time = 637	action = 1	current_phase = 1	next_phase = 0	reward = -0.992192	array([[1.0253683, 0.6005729]], dtype=float32)

time = 645	action = 0	current_phase = 0	next_phase = 1	reward = 0.056507	array([[-0.97574526, -0.1705738 ]], dtype=float32)

time = 650	action = 0	current_phase = 0	next_phase = 1	reward = 0.323072	array([[-0.98841673, -0.13925597]], dtype=float32)

time = 655	action = 1	current_phase = 0	next_phase = 1	reward = -1.672038	array([[-0.9736769 , -0.16060513]], dtype=float32)

time = 663	action = 0	current_phase = 1	next_phase = 0	reward = -1.593036	array([[1.0394727, 0.5877046]], dtype=float32)

Terminal occured. Episode end.

Simulation ended at time: 669.00

Reason: TraCI requested termination.

Performance: 

 Duration: 16981ms

 Real time factor: 39.397

 UPS: 1102.173017

Vehicles: 

 Inserted: 314

 Running: 30

 Waiting: 0



DijkstraRouter answered 314 queries and explored 2.29 edges on average.

DijkstraRouter spent 0ms answering queries (0.00ms on average).

Could not connect to TraCI server at localhost:50031 [Errno 111] Connection refused

 Retrying in 1 seconds

Loading configuration... done.

***Starting server on port 50031 ***

Loading net-file from '/home/soup/IntelliLight/data/one_run/cross.net.xml'... done (2ms).

Warning: Missing yellow phase in tlLogic 'node0', program '0' for tl-index 5 when switching to phase 1

Loading additional-files from '/home/soup/IntelliLight/data/one_run/cross.tls.switch.xml'... done (0ms).

Loading additional-files from '/home/soup/IntelliLight/data/one_run/cross.car.type.xml'... done (0ms).

Loading done.

Simulation started with time: 0.00

time = 20	action = 0	current_phase = 0	next_phase = 1	reward = -0.096708	array([[-0.97998637, -0.15543285]], dtype=float32)

time = 25	action = 0	current_phase = 0	next_phase = 1	reward = -0.777623	array([[-0.98078287, -0.15541553]], dtype=float32)

time = 30	action = 0	current_phase = 0	next_phase = 1	reward = -1.290711	array([[-0.9753964, -0.1559512]], dtype=float32)

time = 35	action = 1	current_phase = 0	next_phase = 1	reward = -2.437328	array([[-0.9839037 , -0.14417462]], dtype=float32)

time = 43	action = 0	current_phase = 1	next_phase = 0	reward = -1.342156	array([[1.0499525 , 0.57240367]], dtype=float32)

time = 48	action = 1	current_phase = 1	next_phase = 0	reward = -1.249396	array([[1.0321229 , 0.60216177]], dtype=float32)

time = 56	action = 0	current_phase = 0	next_phase = 1	reward = 0.104215	array([[-0.9954834 , -0.14777407]], dtype=float32)

time = 61	action = 0	current_phase = 0	next_phase = 1	reward = -1.211859	array([[-0.9789388, -0.1616971]], dtype=float32)

time = 66	action = 1	current_phase = 0	next_phase = 1	reward = -2.368308	array([[-0.99856937, -0.14876443]], dtype=float32)

time = 74	action = 0	current_phase = 1	next_phase = 0	reward = -1.431685	array([[1.0383581 , 0.58537704]], dtype=float32)

time = 79	action = 1	current_phase = 1	next_phase = 0	reward = -0.844323	array([[1.0327301, 0.6004283]], dtype=float32)

time = 87	action = 0	current_phase = 0	next_phase = 1	reward = -0.699967	array([[-1.0000488 , -0.13415515]], dtype=float32)

time = 92	action = 0	current_phase = 0	next_phase = 1	reward = -1.229784	array([[-0.979118  , -0.16167895]], dtype=float32)

time = 97	action = 1	current_phase = 0	next_phase = 1	reward = -2.592219	array([[-0.99854463, -0.14709136]], dtype=float32)

time = 105	action = 0	current_phase = 1	next_phase = 0	reward = -1.459577	array([[1.0389872 , 0.58440936]], dtype=float32)

time = 110	action = 1	current_phase = 1	next_phase = 0	reward = -0.877185	array([[1.0321963, 0.6003723]], dtype=float32)

time = 118	action = 0	current_phase = 0	next_phase = 1	reward = -1.091484	array([[-0.9743582 , -0.13386919]], dtype=float32)

time = 123	action = 0	current_phase = 0	next_phase = 1	reward = -1.208483	array([[-0.9794737 , -0.15883482]], dtype=float32)

time = 128	action = 1	current_phase = 0	next_phase = 1	reward = -1.950244	array([[-0.9972359 , -0.14703068]], dtype=float32)

time = 136	action = 0	current_phase = 1	next_phase = 0	reward = -1.217297	array([[1.0432721 , 0.58921945]], dtype=float32)

time = 141	action = 1	current_phase = 1	next_phase = 0	reward = -1.646101	array([[1.0539237 , 0.58770525]], dtype=float32)

time = 149	action = 0	current_phase = 0	next_phase = 1	reward = -0.872803	array([[-0.9849275, -0.1495746]], dtype=float32)

time = 154	action = 0	current_phase = 0	next_phase = 1	reward = -1.199412	array([[-0.9797229 , -0.16077071]], dtype=float32)

time = 159	action = 1	current_phase = 0	next_phase = 1	reward = -1.535896	array([[-0.9916085 , -0.12833005]], dtype=float32)

time = 167	action = 0	current_phase = 1	next_phase = 0	reward = -1.152971	array([[1.0461948, 0.5835725]], dtype=float32)

time = 172	action = 1	current_phase = 1	next_phase = 0	reward = -1.662859	array([[1.0559887, 0.5836344]], dtype=float32)

time = 180	action = 0	current_phase = 0	next_phase = 1	reward = -1.227042	array([[-0.9823637 , -0.14047104]], dtype=float32)

time = 185	action = 0	current_phase = 0	next_phase = 1	reward = -0.905214	array([[-0.9788744, -0.1598449]], dtype=float32)

time = 190	action = 1	current_phase = 0	next_phase = 1	reward = -1.911154	array([[-0.9863542 , -0.15460753]], dtype=float32)

time = 198	action = 0	current_phase = 1	next_phase = 0	reward = -1.272634	array([[1.0379404, 0.5861744]], dtype=float32)

time = 203	action = 1	current_phase = 1	next_phase = 0	reward = -0.856013	array([[1.0606958, 0.5831741]], dtype=float32)

time = 211	action = 0	current_phase = 0	next_phase = 1	reward = -0.408620	array([[-0.9962716 , -0.16630743]], dtype=float32)

time = 216	action = 0	current_phase = 0	next_phase = 1	reward = -0.640328	array([[-0.9981575 , -0.14283851]], dtype=float32)

time = 221	action = 1	current_phase = 0	next_phase = 1	reward = -2.136866	array([[-0.9842273 , -0.17016655]], dtype=float32)

time = 229	action = 0	current_phase = 1	next_phase = 0	reward = -1.321333	array([[1.0376315 , 0.58756083]], dtype=float32)

time = 234	action = 1	current_phase = 1	next_phase = 0	reward = -0.092584	array([[1.0588655 , 0.58376575]], dtype=float32)

time = 242	action = 0	current_phase = 0	next_phase = 1	reward = 0.153807	array([[-0.9779978 , -0.17908597]], dtype=float32)

time = 247	action = 0	current_phase = 0	next_phase = 1	reward = -0.069486	array([[-0.98855585, -0.1411351 ]], dtype=float32)

time = 252	action = 1	current_phase = 0	next_phase = 1	reward = -2.315088	array([[-0.9766104 , -0.17002043]], dtype=float32)

time = 260	action = 0	current_phase = 1	next_phase = 0	reward = -0.945113	array([[1.0376768, 0.5890284]], dtype=float32)

time = 265	action = 1	current_phase = 1	next_phase = 0	reward = -0.587706	array([[1.0693662, 0.584796 ]], dtype=float32)

time = 273	action = 0	current_phase = 0	next_phase = 1	reward = -0.145384	array([[-0.97555983, -0.16990401]], dtype=float32)

time = 278	action = 0	current_phase = 0	next_phase = 1	reward = -0.066538	array([[-0.98797596, -0.14016497]], dtype=float32)

time = 283	action = 1	current_phase = 0	next_phase = 1	reward = -2.323175	array([[-0.97609746, -0.17024276]], dtype=float32)

time = 291	action = 0	current_phase = 1	next_phase = 0	reward = -1.040204	array([[1.0539829 , 0.58127975]], dtype=float32)

time = 296	action = 1	current_phase = 1	next_phase = 0	reward = -1.364247	array([[1.055852 , 0.5972287]], dtype=float32)

time = 304	action = 0	current_phase = 0	next_phase = 1	reward = 0.490045	array([[-0.975332  , -0.17030814]], dtype=float32)

time = 309	action = 0	current_phase = 0	next_phase = 1	reward = -0.043077	array([[-0.9885398 , -0.14001405]], dtype=float32)

time = 314	action = 1	current_phase = 0	next_phase = 1	reward = -1.356002	array([[-0.9768127 , -0.17065111]], dtype=float32)

time = 322	action = 0	current_phase = 1	next_phase = 0	reward = -0.488386	array([[1.0704409, 0.5702759]], dtype=float32)

time = 327	action = 1	current_phase = 1	next_phase = 0	reward = -1.243896	array([[1.0346801, 0.6013218]], dtype=float32)

time = 335	action = 0	current_phase = 0	next_phase = 1	reward = 0.477547	array([[-0.97127146, -0.16083407]], dtype=float32)

time = 340	action = 0	current_phase = 0	next_phase = 1	reward = -0.016943	array([[-0.991095 , -0.1489963]], dtype=float32)

time = 345	action = 1	current_phase = 0	next_phase = 1	reward = -1.668586	array([[-0.98314905, -0.16279337]], dtype=float32)

time = 353	action = 0	current_phase = 1	next_phase = 0	reward = -0.698089	array([[1.0389271, 0.5874519]], dtype=float32)

time = 358	action = 1	current_phase = 1	next_phase = 0	reward = -0.828084	array([[1.0410298, 0.6033373]], dtype=float32)

time = 366	action = 0	current_phase = 0	next_phase = 1	reward = 0.215298	array([[-0.98469484, -0.1320379 ]], dtype=float32)

time = 371	action = 0	current_phase = 0	next_phase = 1	reward = -0.010564	array([[-0.97558165, -0.1691815 ]], dtype=float32)

time = 376	action = 1	current_phase = 0	next_phase = 1	reward = -2.129344	array([[-0.98943514, -0.13991027]], dtype=float32)

time = 384	action = 0	current_phase = 1	next_phase = 0	reward = -0.807047	array([[1.0383782 , 0.58745116]], dtype=float32)

time = 389	action = 1	current_phase = 1	next_phase = 0	reward = -1.223994	array([[1.0406537 , 0.60356563]], dtype=float32)

time = 397	action = 0	current_phase = 0	next_phase = 1	reward = 0.540809	array([[-0.99595004, -0.12501979]], dtype=float32)

time = 402	action = 0	current_phase = 0	next_phase = 1	reward = 0.017910	array([[-0.97566754, -0.17005557]], dtype=float32)

time = 407	action = 1	current_phase = 0	next_phase = 1	reward = -1.601471	array([[-0.98885965, -0.14105451]], dtype=float32)

time = 415	action = 0	current_phase = 1	next_phase = 0	reward = -1.394908	array([[1.0457942, 0.5892086]], dtype=float32)

time = 420	action = 1	current_phase = 1	next_phase = 0	reward = -0.932358	array([[1.0396274, 0.6036205]], dtype=float32)

time = 428	action = 0	current_phase = 0	next_phase = 1	reward = -0.063145	array([[-0.9870697 , -0.13968658]], dtype=float32)

time = 433	action = 0	current_phase = 0	next_phase = 1	reward = 0.013753	array([[-0.97556645, -0.16955978]], dtype=float32)

time = 438	action = 1	current_phase = 0	next_phase = 1	reward = -1.724826	array([[-0.98937255, -0.13880396]], dtype=float32)

time = 446	action = 0	current_phase = 1	next_phase = 0	reward = -1.640853	array([[1.0325868, 0.6012343]], dtype=float32)

time = 451	action = 1	current_phase = 1	next_phase = 0	reward = -0.962999	array([[1.046295  , 0.59702736]], dtype=float32)

time = 459	action = 0	current_phase = 0	next_phase = 1	reward = -0.052770	array([[-0.9874902 , -0.14125717]], dtype=float32)

time = 464	action = 0	current_phase = 0	next_phase = 1	reward = 0.029140	array([[-0.9763003 , -0.16939673]], dtype=float32)

time = 469	action = 1	current_phase = 0	next_phase = 1	reward = -0.660731	array([[-0.9701353 , -0.15070303]], dtype=float32)

time = 477	action = 0	current_phase = 1	next_phase = 0	reward = -1.188717	array([[1.0566792, 0.5874341]], dtype=float32)

time = 482	action = 1	current_phase = 1	next_phase = 0	reward = -1.051562	array([[1.0542122, 0.5845947]], dtype=float32)

time = 490	action = 0	current_phase = 0	next_phase = 1	reward = 0.257501	array([[-0.98731065, -0.14042774]], dtype=float32)

time = 495	action = 0	current_phase = 0	next_phase = 1	reward = 0.342667	array([[-0.9734826 , -0.16042396]], dtype=float32)

time = 500	action = 1	current_phase = 0	next_phase = 1	reward = -1.333263	array([[-0.98001295, -0.15420446]], dtype=float32)

time = 508	action = 0	current_phase = 1	next_phase = 0	reward = -1.460046	array([[1.0484871, 0.5906626]], dtype=float32)

time = 513	action = 1	current_phase = 1	next_phase = 0	reward = -1.156732	array([[1.060788  , 0.58363795]], dtype=float32)

time = 521	action = 0	current_phase = 0	next_phase = 1	reward = 0.556398	array([[-0.9838151 , -0.15592459]], dtype=float32)

time = 526	action = 0	current_phase = 0	next_phase = 1	reward = 0.064850	array([[-0.987247  , -0.13932261]], dtype=float32)

time = 531	action = 1	current_phase = 0	next_phase = 1	reward = -1.348762	array([[-0.9766704 , -0.17027633]], dtype=float32)

time = 539	action = 0	current_phase = 1	next_phase = 0	reward = -1.527005	array([[1.0487553, 0.5907764]], dtype=float32)

time = 544	action = 1	current_phase = 1	next_phase = 0	reward = -0.698082	array([[1.0607723, 0.5843133]], dtype=float32)

time = 552	action = 0	current_phase = 0	next_phase = 1	reward = 0.024822	array([[-0.975907  , -0.16926655]], dtype=float32)

time = 557	action = 0	current_phase = 0	next_phase = 1	reward = 0.091052	array([[-0.9883196 , -0.14101076]], dtype=float32)

time = 562	action = 1	current_phase = 0	next_phase = 1	reward = -1.617459	array([[-0.97624654, -0.17118245]], dtype=float32)

time = 570	action = 0	current_phase = 1	next_phase = 0	reward = -1.784510	array([[1.0472404 , 0.58834624]], dtype=float32)

time = 575	action = 1	current_phase = 1	next_phase = 0	reward = -1.067811	array([[1.0610963, 0.5832978]], dtype=float32)

time = 583	action = 0	current_phase = 0	next_phase = 1	reward = 0.312839	array([[-0.9678766 , -0.17525303]], dtype=float32)

time = 588	action = 0	current_phase = 0	next_phase = 1	reward = 0.062121	array([[-0.9883657 , -0.14044823]], dtype=float32)

time = 593	action = 1	current_phase = 0	next_phase = 1	reward = -1.792062	array([[-0.97633153, -0.16938886]], dtype=float32)

time = 601	action = 0	current_phase = 1	next_phase = 0	reward = -1.830249	array([[1.0535327 , 0.58139795]], dtype=float32)

time = 606	action = 1	current_phase = 1	next_phase = 0	reward = -0.820355	array([[1.0459799, 0.596485 ]], dtype=float32)

time = 614	action = 0	current_phase = 0	next_phase = 1	reward = 0.043514	array([[-0.9758003 , -0.17073113]], dtype=float32)

time = 619	action = 0	current_phase = 0	next_phase = 1	reward = 0.061808	array([[-0.9887017 , -0.13955104]], dtype=float32)

time = 624	action = 1	current_phase = 0	next_phase = 1	reward = -1.140052	array([[-0.9754903 , -0.17044747]], dtype=float32)

time = 632	action = 0	current_phase = 1	next_phase = 0	reward = -1.380108	array([[1.0719414, 0.5684348]], dtype=float32)

time = 637	action = 1	current_phase = 1	next_phase = 0	reward = -0.992192	array([[1.0253683, 0.6005729]], dtype=float32)

time = 645	action = 0	current_phase = 0	next_phase = 1	reward = 0.056507	array([[-0.97574526, -0.1705738 ]], dtype=float32)

time = 650	action = 0	current_phase = 0	next_phase = 1	reward = 0.323072	array([[-0.98841673, -0.13925597]], dtype=float32)

time = 655	action = 1	current_phase = 0	next_phase = 1	reward = -1.672038	array([[-0.9736769 , -0.16060513]], dtype=float32)

time = 663	action = 0	current_phase = 1	next_phase = 0	reward = -1.593036	array([[1.0394727, 0.5877046]], dtype=float32)

Terminal occured. Episode end.

Simulation ended at time: 669.00

Reason: TraCI requested termination.

Performance: 

 Duration: 17486ms

 Real time factor: 38.2592

 UPS: 1070.341988

Vehicles: 

 Inserted: 314

 Running: 30

 Waiting: 0



DijkstraRouter answered 314 queries and explored 2.29 edges on average.

DijkstraRouter spent 0ms answering queries (0.00ms on average).

Could not connect to TraCI server at localhost:52669 [Errno 111] Connection refused

 Retrying in 1 seconds

Loading configuration... done.

***Starting server on port 52669 ***

Loading net-file from '/home/soup/IntelliLight/data/one_run/cross.net.xml'... done (3ms).

Warning: Missing yellow phase in tlLogic 'node0', program '0' for tl-index 5 when switching to phase 1

Loading additional-files from '/home/soup/IntelliLight/data/one_run/cross.tls.switch.xml'... done (1ms).

Loading additional-files from '/home/soup/IntelliLight/data/one_run/cross.car.type.xml'... done (1ms).

Loading done.

Simulation started with time: 0.00

time = 20	action = 0	current_phase = 0	next_phase = 1	reward = -0.096708	array([[-0.97998637, -0.15543285]], dtype=float32)

time = 25	action = 0	current_phase = 0	next_phase = 1	reward = -0.777623	array([[-0.98078287, -0.15541553]], dtype=float32)

time = 30	action = 0	current_phase = 0	next_phase = 1	reward = -1.290711	array([[-0.9753964, -0.1559512]], dtype=float32)

time = 35	action = 1	current_phase = 0	next_phase = 1	reward = -2.437328	array([[-0.9839037 , -0.14417462]], dtype=float32)

time = 43	action = 0	current_phase = 1	next_phase = 0	reward = -1.342156	array([[1.0499525 , 0.57240367]], dtype=float32)

time = 48	action = 1	current_phase = 1	next_phase = 0	reward = -1.249396	array([[1.0321229 , 0.60216177]], dtype=float32)

time = 56	action = 0	current_phase = 0	next_phase = 1	reward = 0.104215	array([[-0.9954834 , -0.14777407]], dtype=float32)

time = 61	action = 0	current_phase = 0	next_phase = 1	reward = -1.211859	array([[-0.9789388, -0.1616971]], dtype=float32)

time = 66	action = 1	current_phase = 0	next_phase = 1	reward = -2.368308	array([[-0.99856937, -0.14876443]], dtype=float32)

time = 74	action = 0	current_phase = 1	next_phase = 0	reward = -1.431685	array([[1.0383581 , 0.58537704]], dtype=float32)

time = 79	action = 1	current_phase = 1	next_phase = 0	reward = -0.844323	array([[1.0327301, 0.6004283]], dtype=float32)

time = 87	action = 0	current_phase = 0	next_phase = 1	reward = -0.699967	array([[-1.0000488 , -0.13415515]], dtype=float32)

time = 92	action = 0	current_phase = 0	next_phase = 1	reward = -1.229784	array([[-0.979118  , -0.16167895]], dtype=float32)

time = 97	action = 1	current_phase = 0	next_phase = 1	reward = -2.592219	array([[-0.99854463, -0.14709136]], dtype=float32)

time = 105	action = 0	current_phase = 1	next_phase = 0	reward = -1.459577	array([[1.0389872 , 0.58440936]], dtype=float32)

time = 110	action = 1	current_phase = 1	next_phase = 0	reward = -0.877185	array([[1.0321963, 0.6003723]], dtype=float32)

time = 118	action = 0	current_phase = 0	next_phase = 1	reward = -1.091484	array([[-0.9743582 , -0.13386919]], dtype=float32)

time = 123	action = 0	current_phase = 0	next_phase = 1	reward = -1.208483	array([[-0.9794737 , -0.15883482]], dtype=float32)

time = 128	action = 1	current_phase = 0	next_phase = 1	reward = -1.950244	array([[-0.9972359 , -0.14703068]], dtype=float32)

time = 136	action = 0	current_phase = 1	next_phase = 0	reward = -1.217297	array([[1.0432721 , 0.58921945]], dtype=float32)

time = 141	action = 1	current_phase = 1	next_phase = 0	reward = -1.646101	array([[1.0539237 , 0.58770525]], dtype=float32)

time = 149	action = 0	current_phase = 0	next_phase = 1	reward = -0.872803	array([[-0.9849275, -0.1495746]], dtype=float32)

time = 154	action = 0	current_phase = 0	next_phase = 1	reward = -1.199412	array([[-0.9797229 , -0.16077071]], dtype=float32)

time = 159	action = 1	current_phase = 0	next_phase = 1	reward = -1.535896	array([[-0.9916085 , -0.12833005]], dtype=float32)

time = 167	action = 0	current_phase = 1	next_phase = 0	reward = -1.152971	array([[1.0461948, 0.5835725]], dtype=float32)

time = 172	action = 1	current_phase = 1	next_phase = 0	reward = -1.662859	array([[1.0559887, 0.5836344]], dtype=float32)

time = 180	action = 0	current_phase = 0	next_phase = 1	reward = -1.227042	array([[-0.9823637 , -0.14047104]], dtype=float32)

time = 185	action = 0	current_phase = 0	next_phase = 1	reward = -0.905214	array([[-0.9788744, -0.1598449]], dtype=float32)

time = 190	action = 1	current_phase = 0	next_phase = 1	reward = -1.911154	array([[-0.9863542 , -0.15460753]], dtype=float32)

time = 198	action = 0	current_phase = 1	next_phase = 0	reward = -1.272634	array([[1.0379404, 0.5861744]], dtype=float32)

time = 203	action = 1	current_phase = 1	next_phase = 0	reward = -0.856013	array([[1.0606958, 0.5831741]], dtype=float32)

time = 211	action = 0	current_phase = 0	next_phase = 1	reward = -0.408620	array([[-0.9962716 , -0.16630743]], dtype=float32)

time = 216	action = 0	current_phase = 0	next_phase = 1	reward = -0.640328	array([[-0.9981575 , -0.14283851]], dtype=float32)

time = 221	action = 1	current_phase = 0	next_phase = 1	reward = -2.136866	array([[-0.9842273 , -0.17016655]], dtype=float32)

time = 229	action = 0	current_phase = 1	next_phase = 0	reward = -1.321333	array([[1.0376315 , 0.58756083]], dtype=float32)

time = 234	action = 1	current_phase = 1	next_phase = 0	reward = -0.092584	array([[1.0588655 , 0.58376575]], dtype=float32)

time = 242	action = 0	current_phase = 0	next_phase = 1	reward = 0.153807	array([[-0.9779978 , -0.17908597]], dtype=float32)

time = 247	action = 0	current_phase = 0	next_phase = 1	reward = -0.069486	array([[-0.98855585, -0.1411351 ]], dtype=float32)

time = 252	action = 1	current_phase = 0	next_phase = 1	reward = -2.315088	array([[-0.9766104 , -0.17002043]], dtype=float32)

time = 260	action = 0	current_phase = 1	next_phase = 0	reward = -0.945113	array([[1.0376768, 0.5890284]], dtype=float32)

time = 265	action = 1	current_phase = 1	next_phase = 0	reward = -0.587706	array([[1.0693662, 0.584796 ]], dtype=float32)

time = 273	action = 0	current_phase = 0	next_phase = 1	reward = -0.145384	array([[-0.97555983, -0.16990401]], dtype=float32)

time = 278	action = 0	current_phase = 0	next_phase = 1	reward = -0.066538	array([[-0.98797596, -0.14016497]], dtype=float32)

time = 283	action = 1	current_phase = 0	next_phase = 1	reward = -2.323175	array([[-0.97609746, -0.17024276]], dtype=float32)

time = 291	action = 0	current_phase = 1	next_phase = 0	reward = -1.040204	array([[1.0539829 , 0.58127975]], dtype=float32)

time = 296	action = 1	current_phase = 1	next_phase = 0	reward = -1.364247	array([[1.055852 , 0.5972287]], dtype=float32)

time = 304	action = 0	current_phase = 0	next_phase = 1	reward = 0.490045	array([[-0.975332  , -0.17030814]], dtype=float32)

time = 309	action = 0	current_phase = 0	next_phase = 1	reward = -0.043077	array([[-0.9885398 , -0.14001405]], dtype=float32)

time = 314	action = 1	current_phase = 0	next_phase = 1	reward = -1.356002	array([[-0.9768127 , -0.17065111]], dtype=float32)

time = 322	action = 0	current_phase = 1	next_phase = 0	reward = -0.488386	array([[1.0704409, 0.5702759]], dtype=float32)

time = 327	action = 1	current_phase = 1	next_phase = 0	reward = -1.243896	array([[1.0346801, 0.6013218]], dtype=float32)

time = 335	action = 0	current_phase = 0	next_phase = 1	reward = 0.477547	array([[-0.97127146, -0.16083407]], dtype=float32)

time = 340	action = 0	current_phase = 0	next_phase = 1	reward = -0.016943	array([[-0.991095 , -0.1489963]], dtype=float32)

time = 345	action = 1	current_phase = 0	next_phase = 1	reward = -1.668586	array([[-0.98314905, -0.16279337]], dtype=float32)

time = 353	action = 0	current_phase = 1	next_phase = 0	reward = -0.698089	array([[1.0389271, 0.5874519]], dtype=float32)

time = 358	action = 1	current_phase = 1	next_phase = 0	reward = -0.828084	array([[1.0410298, 0.6033373]], dtype=float32)

time = 366	action = 0	current_phase = 0	next_phase = 1	reward = 0.215298	array([[-0.98469484, -0.1320379 ]], dtype=float32)

time = 371	action = 0	current_phase = 0	next_phase = 1	reward = -0.010564	array([[-0.97558165, -0.1691815 ]], dtype=float32)

time = 376	action = 1	current_phase = 0	next_phase = 1	reward = -2.129344	array([[-0.98943514, -0.13991027]], dtype=float32)

time = 384	action = 0	current_phase = 1	next_phase = 0	reward = -0.807047	array([[1.0383782 , 0.58745116]], dtype=float32)

time = 389	action = 1	current_phase = 1	next_phase = 0	reward = -1.223994	array([[1.0406537 , 0.60356563]], dtype=float32)

time = 397	action = 0	current_phase = 0	next_phase = 1	reward = 0.540809	array([[-0.99595004, -0.12501979]], dtype=float32)

time = 402	action = 0	current_phase = 0	next_phase = 1	reward = 0.017910	array([[-0.97566754, -0.17005557]], dtype=float32)

time = 407	action = 1	current_phase = 0	next_phase = 1	reward = -1.601471	array([[-0.98885965, -0.14105451]], dtype=float32)

time = 415	action = 0	current_phase = 1	next_phase = 0	reward = -1.394908	array([[1.0457942, 0.5892086]], dtype=float32)

time = 420	action = 1	current_phase = 1	next_phase = 0	reward = -0.932358	array([[1.0396274, 0.6036205]], dtype=float32)

time = 428	action = 0	current_phase = 0	next_phase = 1	reward = -0.063145	array([[-0.9870697 , -0.13968658]], dtype=float32)

time = 433	action = 0	current_phase = 0	next_phase = 1	reward = 0.013753	array([[-0.97556645, -0.16955978]], dtype=float32)

time = 438	action = 1	current_phase = 0	next_phase = 1	reward = -1.724826	array([[-0.98937255, -0.13880396]], dtype=float32)

time = 446	action = 0	current_phase = 1	next_phase = 0	reward = -1.640853	array([[1.0325868, 0.6012343]], dtype=float32)

time = 451	action = 1	current_phase = 1	next_phase = 0	reward = -0.962999	array([[1.046295  , 0.59702736]], dtype=float32)

time = 459	action = 0	current_phase = 0	next_phase = 1	reward = -0.052770	array([[-0.9874902 , -0.14125717]], dtype=float32)

time = 464	action = 0	current_phase = 0	next_phase = 1	reward = 0.029140	array([[-0.9763003 , -0.16939673]], dtype=float32)

time = 469	action = 1	current_phase = 0	next_phase = 1	reward = -0.660731	array([[-0.9701353 , -0.15070303]], dtype=float32)

time = 477	action = 0	current_phase = 1	next_phase = 0	reward = -1.188717	array([[1.0566792, 0.5874341]], dtype=float32)

time = 482	action = 1	current_phase = 1	next_phase = 0	reward = -1.051562	array([[1.0542122, 0.5845947]], dtype=float32)

time = 490	action = 0	current_phase = 0	next_phase = 1	reward = 0.257501	array([[-0.98731065, -0.14042774]], dtype=float32)

time = 495	action = 0	current_phase = 0	next_phase = 1	reward = 0.342667	array([[-0.9734826 , -0.16042396]], dtype=float32)

time = 500	action = 1	current_phase = 0	next_phase = 1	reward = -1.333263	array([[-0.98001295, -0.15420446]], dtype=float32)

time = 508	action = 0	current_phase = 1	next_phase = 0	reward = -1.460046	array([[1.0484871, 0.5906626]], dtype=float32)

time = 513	action = 1	current_phase = 1	next_phase = 0	reward = -1.156732	array([[1.060788  , 0.58363795]], dtype=float32)

time = 521	action = 0	current_phase = 0	next_phase = 1	reward = 0.556398	array([[-0.9838151 , -0.15592459]], dtype=float32)

time = 526	action = 0	current_phase = 0	next_phase = 1	reward = 0.064850	array([[-0.987247  , -0.13932261]], dtype=float32)

time = 531	action = 1	current_phase = 0	next_phase = 1	reward = -1.348762	array([[-0.9766704 , -0.17027633]], dtype=float32)

time = 539	action = 0	current_phase = 1	next_phase = 0	reward = -1.527005	array([[1.0487553, 0.5907764]], dtype=float32)

time = 544	action = 1	current_phase = 1	next_phase = 0	reward = -0.698082	array([[1.0607723, 0.5843133]], dtype=float32)

time = 552	action = 0	current_phase = 0	next_phase = 1	reward = 0.024822	array([[-0.975907  , -0.16926655]], dtype=float32)

time = 557	action = 0	current_phase = 0	next_phase = 1	reward = 0.091052	array([[-0.9883196 , -0.14101076]], dtype=float32)

time = 562	action = 1	current_phase = 0	next_phase = 1	reward = -1.617459	array([[-0.97624654, -0.17118245]], dtype=float32)

time = 570	action = 0	current_phase = 1	next_phase = 0	reward = -1.784510	array([[1.0472404 , 0.58834624]], dtype=float32)

time = 575	action = 1	current_phase = 1	next_phase = 0	reward = -1.067811	array([[1.0610963, 0.5832978]], dtype=float32)

time = 583	action = 0	current_phase = 0	next_phase = 1	reward = 0.312839	array([[-0.9678766 , -0.17525303]], dtype=float32)

time = 588	action = 0	current_phase = 0	next_phase = 1	reward = 0.062121	array([[-0.9883657 , -0.14044823]], dtype=float32)

time = 593	action = 1	current_phase = 0	next_phase = 1	reward = -1.792062	array([[-0.97633153, -0.16938886]], dtype=float32)

time = 601	action = 0	current_phase = 1	next_phase = 0	reward = -1.830249	array([[1.0535327 , 0.58139795]], dtype=float32)

time = 606	action = 1	current_phase = 1	next_phase = 0	reward = -0.820355	array([[1.0459799, 0.596485 ]], dtype=float32)

time = 614	action = 0	current_phase = 0	next_phase = 1	reward = 0.043514	array([[-0.9758003 , -0.17073113]], dtype=float32)

time = 619	action = 0	current_phase = 0	next_phase = 1	reward = 0.061808	array([[-0.9887017 , -0.13955104]], dtype=float32)

time = 624	action = 1	current_phase = 0	next_phase = 1	reward = -1.140052	array([[-0.9754903 , -0.17044747]], dtype=float32)

time = 632	action = 0	current_phase = 1	next_phase = 0	reward = -1.380108	array([[1.0719414, 0.5684348]], dtype=float32)

time = 637	action = 1	current_phase = 1	next_phase = 0	reward = -0.992192	array([[1.0253683, 0.6005729]], dtype=float32)

time = 645	action = 0	current_phase = 0	next_phase = 1	reward = 0.056507	array([[-0.97574526, -0.1705738 ]], dtype=float32)

time = 650	action = 0	current_phase = 0	next_phase = 1	reward = 0.323072	array([[-0.98841673, -0.13925597]], dtype=float32)

time = 655	action = 1	current_phase = 0	next_phase = 1	reward = -1.672038	array([[-0.9736769 , -0.16060513]], dtype=float32)

time = 663	action = 0	current_phase = 1	next_phase = 0	reward = -1.593036	array([[1.0394727, 0.5877046]], dtype=float32)

Terminal occured. Episode end.

Simulation ended at time: 669.00

Reason: TraCI requested termination.

Performance: 

 Duration: 18444ms

 Real time factor: 36.272

 UPS: 1014.747343

Vehicles: 

 Inserted: 314

 Running: 30

 Waiting: 0



DijkstraRouter answered 314 queries and explored 2.29 edges on average.

DijkstraRouter spent 0ms answering queries (0.00ms on average).

Could not connect to TraCI server at localhost:44841 [Errno 111] Connection refused

 Retrying in 1 seconds

Loading configuration... done.

***Starting server on port 44841 ***

Loading net-file from '/home/soup/IntelliLight/data/one_run/cross.net.xml'... done (2ms).

Warning: Missing yellow phase in tlLogic 'node0', program '0' for tl-index 5 when switching to phase 1

Loading additional-files from '/home/soup/IntelliLight/data/one_run/cross.tls.switch.xml'... done (1ms).

Loading additional-files from '/home/soup/IntelliLight/data/one_run/cross.car.type.xml'... done (0ms).

Loading done.

Simulation started with time: 0.00

time = 20	action = 0	current_phase = 0	next_phase = 1	reward = -0.096708	array([[-0.97998637, -0.15543285]], dtype=float32)

time = 25	action = 0	current_phase = 0	next_phase = 1	reward = -0.777623	array([[-0.98078287, -0.15541553]], dtype=float32)

time = 30	action = 0	current_phase = 0	next_phase = 1	reward = -1.290711	array([[-0.9753964, -0.1559512]], dtype=float32)

time = 35	action = 1	current_phase = 0	next_phase = 1	reward = -2.437328	array([[-0.9839037 , -0.14417462]], dtype=float32)

time = 43	action = 0	current_phase = 1	next_phase = 0	reward = -1.342156	array([[1.0499525 , 0.57240367]], dtype=float32)

time = 48	action = 1	current_phase = 1	next_phase = 0	reward = -1.249396	array([[1.0321229 , 0.60216177]], dtype=float32)

time = 56	action = 0	current_phase = 0	next_phase = 1	reward = 0.104215	array([[-0.9954834 , -0.14777407]], dtype=float32)

time = 61	action = 0	current_phase = 0	next_phase = 1	reward = -1.211859	array([[-0.9789388, -0.1616971]], dtype=float32)

time = 66	action = 1	current_phase = 0	next_phase = 1	reward = -2.368308	array([[-0.99856937, -0.14876443]], dtype=float32)

time = 74	action = 0	current_phase = 1	next_phase = 0	reward = -1.431685	array([[1.0383581 , 0.58537704]], dtype=float32)

time = 79	action = 1	current_phase = 1	next_phase = 0	reward = -0.844323	array([[1.0327301, 0.6004283]], dtype=float32)

time = 87	action = 0	current_phase = 0	next_phase = 1	reward = -0.699967	array([[-1.0000488 , -0.13415515]], dtype=float32)

time = 92	action = 0	current_phase = 0	next_phase = 1	reward = -1.229784	array([[-0.979118  , -0.16167895]], dtype=float32)

time = 97	action = 1	current_phase = 0	next_phase = 1	reward = -2.592219	array([[-0.99854463, -0.14709136]], dtype=float32)

time = 105	action = 0	current_phase = 1	next_phase = 0	reward = -1.459577	array([[1.0389872 , 0.58440936]], dtype=float32)

time = 110	action = 1	current_phase = 1	next_phase = 0	reward = -0.877185	array([[1.0321963, 0.6003723]], dtype=float32)

time = 118	action = 0	current_phase = 0	next_phase = 1	reward = -1.091484	array([[-0.9743582 , -0.13386919]], dtype=float32)

time = 123	action = 0	current_phase = 0	next_phase = 1	reward = -1.208483	array([[-0.9794737 , -0.15883482]], dtype=float32)

time = 128	action = 1	current_phase = 0	next_phase = 1	reward = -1.950244	array([[-0.9972359 , -0.14703068]], dtype=float32)

time = 136	action = 0	current_phase = 1	next_phase = 0	reward = -1.217297	array([[1.0432721 , 0.58921945]], dtype=float32)

time = 141	action = 1	current_phase = 1	next_phase = 0	reward = -1.646101	array([[1.0539237 , 0.58770525]], dtype=float32)

time = 149	action = 0	current_phase = 0	next_phase = 1	reward = -0.872803	array([[-0.9849275, -0.1495746]], dtype=float32)

time = 154	action = 0	current_phase = 0	next_phase = 1	reward = -1.199412	array([[-0.9797229 , -0.16077071]], dtype=float32)

time = 159	action = 1	current_phase = 0	next_phase = 1	reward = -1.535896	array([[-0.9916085 , -0.12833005]], dtype=float32)

time = 167	action = 0	current_phase = 1	next_phase = 0	reward = -1.152971	array([[1.0461948, 0.5835725]], dtype=float32)

time = 172	action = 1	current_phase = 1	next_phase = 0	reward = -1.662859	array([[1.0559887, 0.5836344]], dtype=float32)

time = 180	action = 0	current_phase = 0	next_phase = 1	reward = -1.227042	array([[-0.9823637 , -0.14047104]], dtype=float32)

time = 185	action = 0	current_phase = 0	next_phase = 1	reward = -0.905214	array([[-0.9788744, -0.1598449]], dtype=float32)

time = 190	action = 1	current_phase = 0	next_phase = 1	reward = -1.911154	array([[-0.9863542 , -0.15460753]], dtype=float32)

time = 198	action = 0	current_phase = 1	next_phase = 0	reward = -1.272634	array([[1.0379404, 0.5861744]], dtype=float32)

time = 203	action = 1	current_phase = 1	next_phase = 0	reward = -0.856013	array([[1.0606958, 0.5831741]], dtype=float32)

time = 211	action = 0	current_phase = 0	next_phase = 1	reward = -0.408620	array([[-0.9962716 , -0.16630743]], dtype=float32)

time = 216	action = 0	current_phase = 0	next_phase = 1	reward = -0.640328	array([[-0.9981575 , -0.14283851]], dtype=float32)

time = 221	action = 1	current_phase = 0	next_phase = 1	reward = -2.136866	array([[-0.9842273 , -0.17016655]], dtype=float32)

time = 229	action = 0	current_phase = 1	next_phase = 0	reward = -1.321333	array([[1.0376315 , 0.58756083]], dtype=float32)

time = 234	action = 1	current_phase = 1	next_phase = 0	reward = -0.092584	array([[1.0588655 , 0.58376575]], dtype=float32)

time = 242	action = 0	current_phase = 0	next_phase = 1	reward = 0.153807	array([[-0.9779978 , -0.17908597]], dtype=float32)

time = 247	action = 0	current_phase = 0	next_phase = 1	reward = -0.069486	array([[-0.98855585, -0.1411351 ]], dtype=float32)

time = 252	action = 1	current_phase = 0	next_phase = 1	reward = -2.315088	array([[-0.9766104 , -0.17002043]], dtype=float32)

time = 260	action = 0	current_phase = 1	next_phase = 0	reward = -0.945113	array([[1.0376768, 0.5890284]], dtype=float32)

time = 265	action = 1	current_phase = 1	next_phase = 0	reward = -0.587706	array([[1.0693662, 0.584796 ]], dtype=float32)

time = 273	action = 0	current_phase = 0	next_phase = 1	reward = -0.145384	array([[-0.97555983, -0.16990401]], dtype=float32)

time = 278	action = 0	current_phase = 0	next_phase = 1	reward = -0.066538	array([[-0.98797596, -0.14016497]], dtype=float32)

time = 283	action = 1	current_phase = 0	next_phase = 1	reward = -2.323175	array([[-0.97609746, -0.17024276]], dtype=float32)

time = 291	action = 0	current_phase = 1	next_phase = 0	reward = -1.040204	array([[1.0539829 , 0.58127975]], dtype=float32)

time = 296	action = 1	current_phase = 1	next_phase = 0	reward = -1.364247	array([[1.055852 , 0.5972287]], dtype=float32)

time = 304	action = 0	current_phase = 0	next_phase = 1	reward = 0.490045	array([[-0.975332  , -0.17030814]], dtype=float32)

time = 309	action = 0	current_phase = 0	next_phase = 1	reward = -0.043077	array([[-0.9885398 , -0.14001405]], dtype=float32)

time = 314	action = 1	current_phase = 0	next_phase = 1	reward = -1.356002	array([[-0.9768127 , -0.17065111]], dtype=float32)

time = 322	action = 0	current_phase = 1	next_phase = 0	reward = -0.488386	array([[1.0704409, 0.5702759]], dtype=float32)

time = 327	action = 1	current_phase = 1	next_phase = 0	reward = -1.243896	array([[1.0346801, 0.6013218]], dtype=float32)

time = 335	action = 0	current_phase = 0	next_phase = 1	reward = 0.477547	array([[-0.97127146, -0.16083407]], dtype=float32)

time = 340	action = 0	current_phase = 0	next_phase = 1	reward = -0.016943	array([[-0.991095 , -0.1489963]], dtype=float32)

time = 345	action = 1	current_phase = 0	next_phase = 1	reward = -1.668586	array([[-0.98314905, -0.16279337]], dtype=float32)

time = 353	action = 0	current_phase = 1	next_phase = 0	reward = -0.698089	array([[1.0389271, 0.5874519]], dtype=float32)

time = 358	action = 1	current_phase = 1	next_phase = 0	reward = -0.828084	array([[1.0410298, 0.6033373]], dtype=float32)

time = 366	action = 0	current_phase = 0	next_phase = 1	reward = 0.215298	array([[-0.98469484, -0.1320379 ]], dtype=float32)

time = 371	action = 0	current_phase = 0	next_phase = 1	reward = -0.010564	array([[-0.97558165, -0.1691815 ]], dtype=float32)

time = 376	action = 1	current_phase = 0	next_phase = 1	reward = -2.129344	array([[-0.98943514, -0.13991027]], dtype=float32)

time = 384	action = 0	current_phase = 1	next_phase = 0	reward = -0.807047	array([[1.0383782 , 0.58745116]], dtype=float32)

time = 389	action = 1	current_phase = 1	next_phase = 0	reward = -1.223994	array([[1.0406537 , 0.60356563]], dtype=float32)

time = 397	action = 0	current_phase = 0	next_phase = 1	reward = 0.540809	array([[-0.99595004, -0.12501979]], dtype=float32)

time = 402	action = 0	current_phase = 0	next_phase = 1	reward = 0.017910	array([[-0.97566754, -0.17005557]], dtype=float32)

time = 407	action = 1	current_phase = 0	next_phase = 1	reward = -1.601471	array([[-0.98885965, -0.14105451]], dtype=float32)

time = 415	action = 0	current_phase = 1	next_phase = 0	reward = -1.394908	array([[1.0457942, 0.5892086]], dtype=float32)

time = 420	action = 1	current_phase = 1	next_phase = 0	reward = -0.932358	array([[1.0396274, 0.6036205]], dtype=float32)

time = 428	action = 0	current_phase = 0	next_phase = 1	reward = -0.063145	array([[-0.9870697 , -0.13968658]], dtype=float32)

time = 433	action = 0	current_phase = 0	next_phase = 1	reward = 0.013753	array([[-0.97556645, -0.16955978]], dtype=float32)

time = 438	action = 1	current_phase = 0	next_phase = 1	reward = -1.724826	array([[-0.98937255, -0.13880396]], dtype=float32)

time = 446	action = 0	current_phase = 1	next_phase = 0	reward = -1.640853	array([[1.0325868, 0.6012343]], dtype=float32)

time = 451	action = 1	current_phase = 1	next_phase = 0	reward = -0.962999	array([[1.046295  , 0.59702736]], dtype=float32)

time = 459	action = 0	current_phase = 0	next_phase = 1	reward = -0.052770	array([[-0.9874902 , -0.14125717]], dtype=float32)

time = 464	action = 0	current_phase = 0	next_phase = 1	reward = 0.029140	array([[-0.9763003 , -0.16939673]], dtype=float32)

time = 469	action = 1	current_phase = 0	next_phase = 1	reward = -0.660731	array([[-0.9701353 , -0.15070303]], dtype=float32)

time = 477	action = 0	current_phase = 1	next_phase = 0	reward = -1.188717	array([[1.0566792, 0.5874341]], dtype=float32)

time = 482	action = 1	current_phase = 1	next_phase = 0	reward = -1.051562	array([[1.0542122, 0.5845947]], dtype=float32)

time = 490	action = 0	current_phase = 0	next_phase = 1	reward = 0.257501	array([[-0.98731065, -0.14042774]], dtype=float32)

time = 495	action = 0	current_phase = 0	next_phase = 1	reward = 0.342667	array([[-0.9734826 , -0.16042396]], dtype=float32)

time = 500	action = 1	current_phase = 0	next_phase = 1	reward = -1.333263	array([[-0.98001295, -0.15420446]], dtype=float32)

time = 508	action = 0	current_phase = 1	next_phase = 0	reward = -1.460046	array([[1.0484871, 0.5906626]], dtype=float32)

time = 513	action = 1	current_phase = 1	next_phase = 0	reward = -1.156732	array([[1.060788  , 0.58363795]], dtype=float32)

time = 521	action = 0	current_phase = 0	next_phase = 1	reward = 0.556398	array([[-0.9838151 , -0.15592459]], dtype=float32)

time = 526	action = 0	current_phase = 0	next_phase = 1	reward = 0.064850	array([[-0.987247  , -0.13932261]], dtype=float32)

time = 531	action = 1	current_phase = 0	next_phase = 1	reward = -1.348762	array([[-0.9766704 , -0.17027633]], dtype=float32)

time = 539	action = 0	current_phase = 1	next_phase = 0	reward = -1.527005	array([[1.0487553, 0.5907764]], dtype=float32)

time = 544	action = 1	current_phase = 1	next_phase = 0	reward = -0.698082	array([[1.0607723, 0.5843133]], dtype=float32)

time = 552	action = 0	current_phase = 0	next_phase = 1	reward = 0.024822	array([[-0.975907  , -0.16926655]], dtype=float32)

time = 557	action = 0	current_phase = 0	next_phase = 1	reward = 0.091052	array([[-0.9883196 , -0.14101076]], dtype=float32)

time = 562	action = 1	current_phase = 0	next_phase = 1	reward = -1.617459	array([[-0.97624654, -0.17118245]], dtype=float32)

time = 570	action = 0	current_phase = 1	next_phase = 0	reward = -1.784510	array([[1.0472404 , 0.58834624]], dtype=float32)

time = 575	action = 1	current_phase = 1	next_phase = 0	reward = -1.067811	array([[1.0610963, 0.5832978]], dtype=float32)

time = 583	action = 0	current_phase = 0	next_phase = 1	reward = 0.312839	array([[-0.9678766 , -0.17525303]], dtype=float32)

time = 588	action = 0	current_phase = 0	next_phase = 1	reward = 0.062121	array([[-0.9883657 , -0.14044823]], dtype=float32)

time = 593	action = 1	current_phase = 0	next_phase = 1	reward = -1.792062	array([[-0.97633153, -0.16938886]], dtype=float32)

time = 601	action = 0	current_phase = 1	next_phase = 0	reward = -1.830249	array([[1.0535327 , 0.58139795]], dtype=float32)

time = 606	action = 1	current_phase = 1	next_phase = 0	reward = -0.820355	array([[1.0459799, 0.596485 ]], dtype=float32)

time = 614	action = 0	current_phase = 0	next_phase = 1	reward = 0.043514	array([[-0.9758003 , -0.17073113]], dtype=float32)

time = 619	action = 0	current_phase = 0	next_phase = 1	reward = 0.061808	array([[-0.9887017 , -0.13955104]], dtype=float32)

time = 624	action = 1	current_phase = 0	next_phase = 1	reward = -1.140052	array([[-0.9754903 , -0.17044747]], dtype=float32)

time = 632	action = 0	current_phase = 1	next_phase = 0	reward = -1.380108	array([[1.0719414, 0.5684348]], dtype=float32)

time = 637	action = 1	current_phase = 1	next_phase = 0	reward = -0.992192	array([[1.0253683, 0.6005729]], dtype=float32)

time = 645	action = 0	current_phase = 0	next_phase = 1	reward = 0.056507	array([[-0.97574526, -0.1705738 ]], dtype=float32)

time = 650	action = 0	current_phase = 0	next_phase = 1	reward = 0.323072	array([[-0.98841673, -0.13925597]], dtype=float32)

time = 655	action = 1	current_phase = 0	next_phase = 1	reward = -1.672038	array([[-0.9736769 , -0.16060513]], dtype=float32)

time = 663	action = 0	current_phase = 1	next_phase = 0	reward = -1.593036	array([[1.0394727, 0.5877046]], dtype=float32)

Terminal occured. Episode end.

Simulation ended at time: 669.00

Reason: TraCI requested termination.

Performance: 

 Duration: 16672ms

 Real time factor: 40.1272

 UPS: 1122.600768

Vehicles: 

 Inserted: 314

 Running: 30

 Waiting: 0



DijkstraRouter answered 314 queries and explored 2.29 edges on average.

DijkstraRouter spent 0ms answering queries (0.00ms on average).

Could not connect to TraCI server at localhost:52151 [Errno 111] Connection refused

 Retrying in 1 seconds

Loading configuration... done.

***Starting server on port 52151 ***

Loading net-file from '/home/soup/IntelliLight/data/one_run/cross.net.xml'... done (1ms).

Warning: Missing yellow phase in tlLogic 'node0', program '0' for tl-index 5 when switching to phase 1

Loading additional-files from '/home/soup/IntelliLight/data/one_run/cross.tls.switch.xml'... done (0ms).

Loading additional-files from '/home/soup/IntelliLight/data/one_run/cross.car.type.xml'... done (1ms).

Loading done.

Simulation started with time: 0.00

time = 20	action = 0	current_phase = 0	next_phase = 1	reward = -0.096708	array([[-0.97998637, -0.15543285]], dtype=float32)

time = 25	action = 0	current_phase = 0	next_phase = 1	reward = -0.777623	array([[-0.98078287, -0.15541553]], dtype=float32)

time = 30	action = 0	current_phase = 0	next_phase = 1	reward = -1.290711	array([[-0.9753964, -0.1559512]], dtype=float32)

time = 35	action = 1	current_phase = 0	next_phase = 1	reward = -2.437328	array([[-0.9839037 , -0.14417462]], dtype=float32)

time = 43	action = 0	current_phase = 1	next_phase = 0	reward = -1.342156	array([[1.0499525 , 0.57240367]], dtype=float32)

time = 48	action = 1	current_phase = 1	next_phase = 0	reward = -1.249396	array([[1.0321229 , 0.60216177]], dtype=float32)

time = 56	action = 0	current_phase = 0	next_phase = 1	reward = 0.104215	array([[-0.9954834 , -0.14777407]], dtype=float32)

time = 61	action = 0	current_phase = 0	next_phase = 1	reward = -1.211859	array([[-0.9789388, -0.1616971]], dtype=float32)

time = 66	action = 1	current_phase = 0	next_phase = 1	reward = -2.368308	array([[-0.99856937, -0.14876443]], dtype=float32)

time = 74	action = 0	current_phase = 1	next_phase = 0	reward = -1.431685	array([[1.0383581 , 0.58537704]], dtype=float32)

time = 79	action = 1	current_phase = 1	next_phase = 0	reward = -0.844323	array([[1.0327301, 0.6004283]], dtype=float32)

time = 87	action = 0	current_phase = 0	next_phase = 1	reward = -0.699967	array([[-1.0000488 , -0.13415515]], dtype=float32)

time = 92	action = 0	current_phase = 0	next_phase = 1	reward = -1.229784	array([[-0.979118  , -0.16167895]], dtype=float32)

time = 97	action = 1	current_phase = 0	next_phase = 1	reward = -2.592219	array([[-0.99854463, -0.14709136]], dtype=float32)

time = 105	action = 0	current_phase = 1	next_phase = 0	reward = -1.459577	array([[1.0389872 , 0.58440936]], dtype=float32)

time = 110	action = 1	current_phase = 1	next_phase = 0	reward = -0.877185	array([[1.0321963, 0.6003723]], dtype=float32)

time = 118	action = 0	current_phase = 0	next_phase = 1	reward = -1.091484	array([[-0.9743582 , -0.13386919]], dtype=float32)

time = 123	action = 0	current_phase = 0	next_phase = 1	reward = -1.208483	array([[-0.9794737 , -0.15883482]], dtype=float32)

time = 128	action = 1	current_phase = 0	next_phase = 1	reward = -1.950244	array([[-0.9972359 , -0.14703068]], dtype=float32)

time = 136	action = 0	current_phase = 1	next_phase = 0	reward = -1.217297	array([[1.0432721 , 0.58921945]], dtype=float32)

time = 141	action = 1	current_phase = 1	next_phase = 0	reward = -1.646101	array([[1.0539237 , 0.58770525]], dtype=float32)

time = 149	action = 0	current_phase = 0	next_phase = 1	reward = -0.872803	array([[-0.9849275, -0.1495746]], dtype=float32)

time = 154	action = 0	current_phase = 0	next_phase = 1	reward = -1.199412	array([[-0.9797229 , -0.16077071]], dtype=float32)

time = 159	action = 1	current_phase = 0	next_phase = 1	reward = -1.535896	array([[-0.9916085 , -0.12833005]], dtype=float32)

time = 167	action = 0	current_phase = 1	next_phase = 0	reward = -1.152971	array([[1.0461948, 0.5835725]], dtype=float32)

time = 172	action = 1	current_phase = 1	next_phase = 0	reward = -1.662859	array([[1.0559887, 0.5836344]], dtype=float32)

time = 180	action = 0	current_phase = 0	next_phase = 1	reward = -1.227042	array([[-0.9823637 , -0.14047104]], dtype=float32)

time = 185	action = 0	current_phase = 0	next_phase = 1	reward = -0.905214	array([[-0.9788744, -0.1598449]], dtype=float32)

time = 190	action = 1	current_phase = 0	next_phase = 1	reward = -1.911154	array([[-0.9863542 , -0.15460753]], dtype=float32)

time = 198	action = 0	current_phase = 1	next_phase = 0	reward = -1.272634	array([[1.0379404, 0.5861744]], dtype=float32)

time = 203	action = 1	current_phase = 1	next_phase = 0	reward = -0.856013	array([[1.0606958, 0.5831741]], dtype=float32)

time = 211	action = 0	current_phase = 0	next_phase = 1	reward = -0.408620	array([[-0.9962716 , -0.16630743]], dtype=float32)

time = 216	action = 0	current_phase = 0	next_phase = 1	reward = -0.640328	array([[-0.9981575 , -0.14283851]], dtype=float32)

time = 221	action = 1	current_phase = 0	next_phase = 1	reward = -2.136866	array([[-0.9842273 , -0.17016655]], dtype=float32)

time = 229	action = 0	current_phase = 1	next_phase = 0	reward = -1.321333	array([[1.0376315 , 0.58756083]], dtype=float32)

time = 234	action = 1	current_phase = 1	next_phase = 0	reward = -0.092584	array([[1.0588655 , 0.58376575]], dtype=float32)

time = 242	action = 0	current_phase = 0	next_phase = 1	reward = 0.153807	array([[-0.9779978 , -0.17908597]], dtype=float32)

time = 247	action = 0	current_phase = 0	next_phase = 1	reward = -0.069486	array([[-0.98855585, -0.1411351 ]], dtype=float32)

time = 252	action = 1	current_phase = 0	next_phase = 1	reward = -2.315088	array([[-0.9766104 , -0.17002043]], dtype=float32)

time = 260	action = 0	current_phase = 1	next_phase = 0	reward = -0.945113	array([[1.0376768, 0.5890284]], dtype=float32)

time = 265	action = 1	current_phase = 1	next_phase = 0	reward = -0.587706	array([[1.0693662, 0.584796 ]], dtype=float32)

time = 273	action = 0	current_phase = 0	next_phase = 1	reward = -0.145384	array([[-0.97555983, -0.16990401]], dtype=float32)

time = 278	action = 0	current_phase = 0	next_phase = 1	reward = -0.066538	array([[-0.98797596, -0.14016497]], dtype=float32)

time = 283	action = 1	current_phase = 0	next_phase = 1	reward = -2.323175	array([[-0.97609746, -0.17024276]], dtype=float32)

time = 291	action = 0	current_phase = 1	next_phase = 0	reward = -1.040204	array([[1.0539829 , 0.58127975]], dtype=float32)

time = 296	action = 1	current_phase = 1	next_phase = 0	reward = -1.364247	array([[1.055852 , 0.5972287]], dtype=float32)

time = 304	action = 0	current_phase = 0	next_phase = 1	reward = 0.490045	array([[-0.975332  , -0.17030814]], dtype=float32)

time = 309	action = 0	current_phase = 0	next_phase = 1	reward = -0.043077	array([[-0.9885398 , -0.14001405]], dtype=float32)

time = 314	action = 1	current_phase = 0	next_phase = 1	reward = -1.356002	array([[-0.9768127 , -0.17065111]], dtype=float32)

time = 322	action = 0	current_phase = 1	next_phase = 0	reward = -0.488386	array([[1.0704409, 0.5702759]], dtype=float32)

time = 327	action = 1	current_phase = 1	next_phase = 0	reward = -1.243896	array([[1.0346801, 0.6013218]], dtype=float32)

time = 335	action = 0	current_phase = 0	next_phase = 1	reward = 0.477547	array([[-0.97127146, -0.16083407]], dtype=float32)

time = 340	action = 0	current_phase = 0	next_phase = 1	reward = -0.016943	array([[-0.991095 , -0.1489963]], dtype=float32)

time = 345	action = 1	current_phase = 0	next_phase = 1	reward = -1.668586	array([[-0.98314905, -0.16279337]], dtype=float32)

time = 353	action = 0	current_phase = 1	next_phase = 0	reward = -0.698089	array([[1.0389271, 0.5874519]], dtype=float32)

time = 358	action = 1	current_phase = 1	next_phase = 0	reward = -0.828084	array([[1.0410298, 0.6033373]], dtype=float32)

time = 366	action = 0	current_phase = 0	next_phase = 1	reward = 0.215298	array([[-0.98469484, -0.1320379 ]], dtype=float32)

time = 371	action = 0	current_phase = 0	next_phase = 1	reward = -0.010564	array([[-0.97558165, -0.1691815 ]], dtype=float32)

time = 376	action = 1	current_phase = 0	next_phase = 1	reward = -2.129344	array([[-0.98943514, -0.13991027]], dtype=float32)

time = 384	action = 0	current_phase = 1	next_phase = 0	reward = -0.807047	array([[1.0383782 , 0.58745116]], dtype=float32)

time = 389	action = 1	current_phase = 1	next_phase = 0	reward = -1.223994	array([[1.0406537 , 0.60356563]], dtype=float32)

time = 397	action = 0	current_phase = 0	next_phase = 1	reward = 0.540809	array([[-0.99595004, -0.12501979]], dtype=float32)

time = 402	action = 0	current_phase = 0	next_phase = 1	reward = 0.017910	array([[-0.97566754, -0.17005557]], dtype=float32)

time = 407	action = 1	current_phase = 0	next_phase = 1	reward = -1.601471	array([[-0.98885965, -0.14105451]], dtype=float32)

time = 415	action = 0	current_phase = 1	next_phase = 0	reward = -1.394908	array([[1.0457942, 0.5892086]], dtype=float32)

time = 420	action = 1	current_phase = 1	next_phase = 0	reward = -0.932358	array([[1.0396274, 0.6036205]], dtype=float32)

time = 428	action = 0	current_phase = 0	next_phase = 1	reward = -0.063145	array([[-0.9870697 , -0.13968658]], dtype=float32)

time = 433	action = 0	current_phase = 0	next_phase = 1	reward = 0.013753	array([[-0.97556645, -0.16955978]], dtype=float32)

time = 438	action = 1	current_phase = 0	next_phase = 1	reward = -1.724826	array([[-0.98937255, -0.13880396]], dtype=float32)

time = 446	action = 0	current_phase = 1	next_phase = 0	reward = -1.640853	array([[1.0325868, 0.6012343]], dtype=float32)

time = 451	action = 1	current_phase = 1	next_phase = 0	reward = -0.962999	array([[1.046295  , 0.59702736]], dtype=float32)

time = 459	action = 0	current_phase = 0	next_phase = 1	reward = -0.052770	array([[-0.9874902 , -0.14125717]], dtype=float32)

time = 464	action = 0	current_phase = 0	next_phase = 1	reward = 0.029140	array([[-0.9763003 , -0.16939673]], dtype=float32)

time = 469	action = 1	current_phase = 0	next_phase = 1	reward = -0.660731	array([[-0.9701353 , -0.15070303]], dtype=float32)

time = 477	action = 0	current_phase = 1	next_phase = 0	reward = -1.188717	array([[1.0566792, 0.5874341]], dtype=float32)

time = 482	action = 1	current_phase = 1	next_phase = 0	reward = -1.051562	array([[1.0542122, 0.5845947]], dtype=float32)

time = 490	action = 0	current_phase = 0	next_phase = 1	reward = 0.257501	array([[-0.98731065, -0.14042774]], dtype=float32)

time = 495	action = 0	current_phase = 0	next_phase = 1	reward = 0.342667	array([[-0.9734826 , -0.16042396]], dtype=float32)

time = 500	action = 1	current_phase = 0	next_phase = 1	reward = -1.333263	array([[-0.98001295, -0.15420446]], dtype=float32)

time = 508	action = 0	current_phase = 1	next_phase = 0	reward = -1.460046	array([[1.0484871, 0.5906626]], dtype=float32)

time = 513	action = 1	current_phase = 1	next_phase = 0	reward = -1.156732	array([[1.060788  , 0.58363795]], dtype=float32)

time = 521	action = 0	current_phase = 0	next_phase = 1	reward = 0.556398	array([[-0.9838151 , -0.15592459]], dtype=float32)

time = 526	action = 0	current_phase = 0	next_phase = 1	reward = 0.064850	array([[-0.987247  , -0.13932261]], dtype=float32)

time = 531	action = 1	current_phase = 0	next_phase = 1	reward = -1.348762	array([[-0.9766704 , -0.17027633]], dtype=float32)

time = 539	action = 0	current_phase = 1	next_phase = 0	reward = -1.527005	array([[1.0487553, 0.5907764]], dtype=float32)

time = 544	action = 1	current_phase = 1	next_phase = 0	reward = -0.698082	array([[1.0607723, 0.5843133]], dtype=float32)

time = 552	action = 0	current_phase = 0	next_phase = 1	reward = 0.024822	array([[-0.975907  , -0.16926655]], dtype=float32)

time = 557	action = 0	current_phase = 0	next_phase = 1	reward = 0.091052	array([[-0.9883196 , -0.14101076]], dtype=float32)

time = 562	action = 1	current_phase = 0	next_phase = 1	reward = -1.617459	array([[-0.97624654, -0.17118245]], dtype=float32)

time = 570	action = 0	current_phase = 1	next_phase = 0	reward = -1.784510	array([[1.0472404 , 0.58834624]], dtype=float32)

time = 575	action = 1	current_phase = 1	next_phase = 0	reward = -1.067811	array([[1.0610963, 0.5832978]], dtype=float32)

time = 583	action = 0	current_phase = 0	next_phase = 1	reward = 0.312839	array([[-0.9678766 , -0.17525303]], dtype=float32)

time = 588	action = 0	current_phase = 0	next_phase = 1	reward = 0.062121	array([[-0.9883657 , -0.14044823]], dtype=float32)

time = 593	action = 1	current_phase = 0	next_phase = 1	reward = -1.792062	array([[-0.97633153, -0.16938886]], dtype=float32)

time = 601	action = 0	current_phase = 1	next_phase = 0	reward = -1.830249	array([[1.0535327 , 0.58139795]], dtype=float32)

time = 606	action = 1	current_phase = 1	next_phase = 0	reward = -0.820355	array([[1.0459799, 0.596485 ]], dtype=float32)

time = 614	action = 0	current_phase = 0	next_phase = 1	reward = 0.043514	array([[-0.9758003 , -0.17073113]], dtype=float32)

time = 619	action = 0	current_phase = 0	next_phase = 1	reward = 0.061808	array([[-0.9887017 , -0.13955104]], dtype=float32)

time = 624	action = 1	current_phase = 0	next_phase = 1	reward = -1.140052	array([[-0.9754903 , -0.17044747]], dtype=float32)

time = 632	action = 0	current_phase = 1	next_phase = 0	reward = -1.380108	array([[1.0719414, 0.5684348]], dtype=float32)

time = 637	action = 1	current_phase = 1	next_phase = 0	reward = -0.992192	array([[1.0253683, 0.6005729]], dtype=float32)

time = 645	action = 0	current_phase = 0	next_phase = 1	reward = 0.056507	array([[-0.97574526, -0.1705738 ]], dtype=float32)

time = 650	action = 0	current_phase = 0	next_phase = 1	reward = 0.323072	array([[-0.98841673, -0.13925597]], dtype=float32)

time = 655	action = 1	current_phase = 0	next_phase = 1	reward = -1.672038	array([[-0.9736769 , -0.16060513]], dtype=float32)

time = 663	action = 0	current_phase = 1	next_phase = 0	reward = -1.593036	array([[1.0394727, 0.5877046]], dtype=float32)

Terminal occured. Episode end.

Simulation ended at time: 669.00

Reason: TraCI requested termination.

Performance: 

 Duration: 20420ms

 Real time factor: 32.762

 UPS: 916.552400

Vehicles: 

 Inserted: 314

 Running: 30

 Waiting: 0



DijkstraRouter answered 314 queries and explored 2.29 edges on average.

DijkstraRouter spent 1ms answering queries (0.00ms on average).

Could not connect to TraCI server at localhost:52695 [Errno 111] Connection refused

 Retrying in 1 seconds

Loading configuration... done.

***Starting server on port 52695 ***

Loading net-file from '/home/soup/IntelliLight/data/one_run/cross.net.xml'... done (1ms).

Warning: Missing yellow phase in tlLogic 'node0', program '0' for tl-index 5 when switching to phase 1

Loading additional-files from '/home/soup/IntelliLight/data/one_run/cross.tls.switch.xml'... done (0ms).

Loading additional-files from '/home/soup/IntelliLight/data/one_run/cross.car.type.xml'... done (0ms).

Loading done.

Simulation started with time: 0.00

time = 20	action = 0	current_phase = 0	next_phase = 1	reward = -0.096708	array([[-0.97998637, -0.15543285]], dtype=float32)

time = 25	action = 0	current_phase = 0	next_phase = 1	reward = -0.777623	array([[-0.98078287, -0.15541553]], dtype=float32)

time = 30	action = 0	current_phase = 0	next_phase = 1	reward = -1.290711	array([[-0.9753964, -0.1559512]], dtype=float32)

time = 35	action = 0	current_phase = 0	next_phase = 1	reward = -1.244540	array([[-0.9839037 , -0.14417462]], dtype=float32)

time = 40	action = 1	current_phase = 0	next_phase = 1	reward = -2.244437	array([[-0.9827778 , -0.14573789]], dtype=float32)

time = 48	action = 0	current_phase = 1	next_phase = 0	reward = -1.315274	array([[1.0408981, 0.5865162]], dtype=float32)

time = 53	action = 1	current_phase = 1	next_phase = 0	reward = -1.025381	array([[1.061719  , 0.58156013]], dtype=float32)

time = 61	action = 0	current_phase = 0	next_phase = 1	reward = -0.108240	array([[-0.9744863 , -0.17010769]], dtype=float32)

time = 66	action = 0	current_phase = 0	next_phase = 1	reward = -0.642931	array([[-0.9977076, -0.1427037]], dtype=float32)

time = 71	action = 0	current_phase = 0	next_phase = 1	reward = -0.572729	array([[-0.98330724, -0.17107452]], dtype=float32)

time = 76	action = 1	current_phase = 0	next_phase = 1	reward = -2.136958	array([[-0.9959114, -0.1407949]], dtype=float32)

time = 84	action = 0	current_phase = 1	next_phase = 0	reward = -0.598781	array([[1.0377662, 0.5875968]], dtype=float32)

time = 89	action = 1	current_phase = 1	next_phase = 0	reward = -0.667854	array([[1.0360236 , 0.60321593]], dtype=float32)

time = 97	action = 0	current_phase = 0	next_phase = 1	reward = -0.078679	array([[-0.9881034 , -0.14078023]], dtype=float32)

time = 102	action = 0	current_phase = 0	next_phase = 1	reward = 0.007119	array([[-0.9760588 , -0.17036009]], dtype=float32)

time = 107	action = 0	current_phase = 0	next_phase = 1	reward = 0.067084	array([[-0.96920675, -0.15133408]], dtype=float32)

time = 112	action = 1	current_phase = 0	next_phase = 1	reward = -1.530525	array([[-0.9759238 , -0.16874722]], dtype=float32)

time = 120	action = 0	current_phase = 1	next_phase = 0	reward = -1.781255	array([[1.0456734, 0.5905963]], dtype=float32)

time = 125	action = 1	current_phase = 1	next_phase = 0	reward = -1.420762	array([[1.0597464, 0.5849292]], dtype=float32)

time = 133	action = 0	current_phase = 0	next_phase = 1	reward = 0.633501	array([[-0.9719478 , -0.18526313]], dtype=float32)

time = 138	action = 0	current_phase = 0	next_phase = 1	reward = 0.070078	array([[-0.9882674 , -0.14008312]], dtype=float32)

time = 143	action = 0	current_phase = 0	next_phase = 1	reward = -0.318120	array([[-0.97637826, -0.16995893]], dtype=float32)

time = 148	action = 1	current_phase = 0	next_phase = 1	reward = -2.694325	array([[-0.9644633 , -0.15137017]], dtype=float32)

time = 156	action = 0	current_phase = 1	next_phase = 0	reward = -1.759393	array([[1.0258888 , 0.59625614]], dtype=float32)

time = 161	action = 1	current_phase = 1	next_phase = 0	reward = -0.862732	array([[1.046468 , 0.5971457]], dtype=float32)

time = 169	action = 0	current_phase = 0	next_phase = 1	reward = 0.358929	array([[-0.9810861 , -0.14489041]], dtype=float32)

time = 174	action = 0	current_phase = 0	next_phase = 1	reward = -0.197692	array([[-0.97636044, -0.17031708]], dtype=float32)

time = 179	action = 0	current_phase = 0	next_phase = 1	reward = -1.432417	array([[-0.98760456, -0.14807239]], dtype=float32)

time = 184	action = 1	current_phase = 0	next_phase = 1	reward = -2.227672	array([[-0.97939813, -0.16081625]], dtype=float32)

time = 192	action = 0	current_phase = 1	next_phase = 0	reward = -1.597014	array([[1.0691158, 0.5728811]], dtype=float32)

time = 197	action = 1	current_phase = 1	next_phase = 0	reward = -0.776358	array([[1.0378252, 0.5933882]], dtype=float32)

time = 205	action = 0	current_phase = 0	next_phase = 1	reward = -0.391317	array([[-0.975865  , -0.17086485]], dtype=float32)

time = 210	action = 0	current_phase = 0	next_phase = 1	reward = -0.950183	array([[-0.98688966, -0.1482369 ]], dtype=float32)

time = 215	action = 0	current_phase = 0	next_phase = 1	reward = -1.208591	array([[-0.9829897 , -0.14515197]], dtype=float32)

time = 220	action = 1	current_phase = 0	next_phase = 1	reward = -2.212414	array([[-0.9830907 , -0.14455068]], dtype=float32)

time = 228	action = 0	current_phase = 1	next_phase = 0	reward = -1.208776	array([[1.0398295, 0.5837778]], dtype=float32)

time = 233	action = 1	current_phase = 1	next_phase = 0	reward = -1.911598	array([[1.0587672, 0.5822721]], dtype=float32)

time = 241	action = 0	current_phase = 0	next_phase = 1	reward = -0.632244	array([[-0.9795759 , -0.16309658]], dtype=float32)

time = 246	action = 0	current_phase = 0	next_phase = 1	reward = -1.169498	array([[-0.9918343 , -0.13115972]], dtype=float32)

time = 251	action = 0	current_phase = 0	next_phase = 1	reward = -1.134752	array([[-0.9793186, -0.1595166]], dtype=float32)

time = 256	action = 1	current_phase = 0	next_phase = 1	reward = -2.179940	array([[-0.99124545, -0.12838149]], dtype=float32)

time = 264	action = 0	current_phase = 1	next_phase = 0	reward = -0.754267	array([[1.0379112, 0.586053 ]], dtype=float32)

time = 269	action = 1	current_phase = 1	next_phase = 0	reward = -1.384484	array([[1.0410421, 0.6023786]], dtype=float32)

time = 277	action = 0	current_phase = 0	next_phase = 1	reward = 0.517666	array([[-0.9919687 , -0.13890651]], dtype=float32)

time = 282	action = 0	current_phase = 0	next_phase = 1	reward = -0.000844	array([[-0.97674924, -0.17043649]], dtype=float32)

time = 287	action = 0	current_phase = 0	next_phase = 1	reward = 0.063875	array([[-0.9700999 , -0.15055303]], dtype=float32)

time = 292	action = 1	current_phase = 0	next_phase = 1	reward = -1.593725	array([[-0.9769983 , -0.16921231]], dtype=float32)

time = 300	action = 0	current_phase = 1	next_phase = 0	reward = -1.773940	array([[1.0464685, 0.5899094]], dtype=float32)

time = 305	action = 1	current_phase = 1	next_phase = 0	reward = -0.990078	array([[1.0613351, 0.585492 ]], dtype=float32)

time = 313	action = 0	current_phase = 0	next_phase = 1	reward = 0.338809	array([[-0.968448  , -0.17487305]], dtype=float32)

time = 318	action = 0	current_phase = 0	next_phase = 1	reward = 0.077701	array([[-0.98877156, -0.14098392]], dtype=float32)

time = 323	action = 0	current_phase = 0	next_phase = 1	reward = -0.326943	array([[-0.97562665, -0.16975334]], dtype=float32)

time = 328	action = 1	current_phase = 0	next_phase = 1	reward = -2.767802	array([[-0.9895717 , -0.15797053]], dtype=float32)

time = 336	action = 0	current_phase = 1	next_phase = 0	reward = -1.887775	array([[1.0253748 , 0.59716976]], dtype=float32)

time = 341	action = 1	current_phase = 1	next_phase = 0	reward = -1.347262	array([[1.0468776, 0.5964048]], dtype=float32)

time = 349	action = 0	current_phase = 0	next_phase = 1	reward = 0.668195	array([[-0.9879545 , -0.14041743]], dtype=float32)

time = 354	action = 0	current_phase = 0	next_phase = 1	reward = -0.411193	array([[-0.97570604, -0.17025575]], dtype=float32)

time = 359	action = 0	current_phase = 0	next_phase = 1	reward = -1.145557	array([[-0.9712187 , -0.16882648]], dtype=float32)

time = 364	action = 1	current_phase = 0	next_phase = 1	reward = -1.625690	array([[-0.9789514, -0.1599583]], dtype=float32)

time = 372	action = 0	current_phase = 1	next_phase = 0	reward = -1.326990	array([[1.0748968, 0.5661963]], dtype=float32)

time = 377	action = 1	current_phase = 1	next_phase = 0	reward = -0.831535	array([[1.0322142, 0.601963 ]], dtype=float32)

time = 385	action = 0	current_phase = 0	next_phase = 1	reward = -0.108186	array([[-0.97543114, -0.17172405]], dtype=float32)

time = 390	action = 0	current_phase = 0	next_phase = 1	reward = -1.222371	array([[-0.9755238 , -0.15492958]], dtype=float32)

time = 395	action = 0	current_phase = 0	next_phase = 1	reward = -1.206891	array([[-0.9837997 , -0.14569482]], dtype=float32)

time = 400	action = 1	current_phase = 0	next_phase = 1	reward = -2.269402	array([[-0.9829456, -0.1450987]], dtype=float32)

time = 408	action = 0	current_phase = 1	next_phase = 0	reward = -1.207272	array([[1.0378819, 0.5854318]], dtype=float32)

time = 413	action = 1	current_phase = 1	next_phase = 0	reward = -0.852109	array([[1.0594289, 0.5799993]], dtype=float32)

time = 421	action = 0	current_phase = 0	next_phase = 1	reward = -0.400962	array([[-0.9811564 , -0.16536623]], dtype=float32)

time = 426	action = 0	current_phase = 0	next_phase = 1	reward = -0.639989	array([[-0.9973074 , -0.14214309]], dtype=float32)

time = 431	action = 0	current_phase = 0	next_phase = 1	reward = -0.584539	array([[-0.984339  , -0.17124593]], dtype=float32)

time = 436	action = 1	current_phase = 0	next_phase = 1	reward = -2.170656	array([[-0.99619025, -0.14184058]], dtype=float32)

time = 444	action = 0	current_phase = 1	next_phase = 0	reward = -0.803127	array([[1.0381149 , 0.58626604]], dtype=float32)

time = 449	action = 1	current_phase = 1	next_phase = 0	reward = -0.663840	array([[1.0404241 , 0.60312176]], dtype=float32)

time = 457	action = 0	current_phase = 0	next_phase = 1	reward = -0.074142	array([[-0.988406  , -0.14033718]], dtype=float32)

time = 462	action = 0	current_phase = 0	next_phase = 1	reward = -0.001347	array([[-0.9764534 , -0.17064276]], dtype=float32)

time = 467	action = 0	current_phase = 0	next_phase = 1	reward = 0.058660	array([[-0.9950966 , -0.15948926]], dtype=float32)

time = 472	action = 1	current_phase = 0	next_phase = 1	reward = -1.658226	array([[-0.97686565, -0.17055145]], dtype=float32)

time = 480	action = 0	current_phase = 1	next_phase = 0	reward = -1.663866	array([[1.0470024, 0.5897607]], dtype=float32)

time = 485	action = 1	current_phase = 1	next_phase = 0	reward = -0.986090	array([[1.0599225, 0.5854994]], dtype=float32)

time = 493	action = 0	current_phase = 0	next_phase = 1	reward = 0.348535	array([[-0.9795555 , -0.17966765]], dtype=float32)

time = 498	action = 0	current_phase = 0	next_phase = 1	reward = 0.083116	array([[-0.9881303 , -0.13872251]], dtype=float32)

time = 503	action = 0	current_phase = 0	next_phase = 1	reward = -0.403457	array([[-0.9768602 , -0.17032532]], dtype=float32)

time = 508	action = 1	current_phase = 0	next_phase = 1	reward = -2.162239	array([[-0.96475774, -0.1522412 ]], dtype=float32)

time = 516	action = 0	current_phase = 1	next_phase = 0	reward = -1.500655	array([[1.0296504, 0.5907755]], dtype=float32)

time = 521	action = 1	current_phase = 1	next_phase = 0	reward = -1.264075	array([[1.0467911 , 0.58566755]], dtype=float32)

time = 529	action = 0	current_phase = 0	next_phase = 1	reward = 0.665849	array([[-1.0029875 , -0.14264703]], dtype=float32)

time = 534	action = 0	current_phase = 0	next_phase = 1	reward = -0.609351	array([[-0.97647303, -0.16991118]], dtype=float32)

time = 539	action = 0	current_phase = 0	next_phase = 1	reward = -1.173739	array([[-0.99068964, -0.15741996]], dtype=float32)

time = 544	action = 1	current_phase = 0	next_phase = 1	reward = -1.667797	array([[-0.9801769, -0.1603769]], dtype=float32)

time = 552	action = 0	current_phase = 1	next_phase = 0	reward = -1.162483	array([[1.069774  , 0.56826884]], dtype=float32)

time = 557	action = 1	current_phase = 1	next_phase = 0	reward = -1.102905	array([[1.025151 , 0.5992949]], dtype=float32)

time = 565	action = 0	current_phase = 0	next_phase = 1	reward = -0.080965	array([[-0.9711289 , -0.16073227]], dtype=float32)

time = 570	action = 0	current_phase = 0	next_phase = 1	reward = -0.947774	array([[-0.9718104 , -0.14661929]], dtype=float32)

time = 575	action = 0	current_phase = 0	next_phase = 1	reward = -1.482017	array([[-0.98406774, -0.14546442]], dtype=float32)

time = 580	action = 1	current_phase = 0	next_phase = 1	reward = -1.989399	array([[-0.97610945, -0.1491507 ]], dtype=float32)

time = 588	action = 0	current_phase = 1	next_phase = 0	reward = -1.272566	array([[1.0390533, 0.5835891]], dtype=float32)

time = 593	action = 1	current_phase = 1	next_phase = 0	reward = -1.475508	array([[1.0598595 , 0.58524734]], dtype=float32)

time = 601	action = 0	current_phase = 0	next_phase = 1	reward = 0.169675	array([[-0.97094357, -0.14799748]], dtype=float32)

time = 606	action = 0	current_phase = 0	next_phase = 1	reward = -0.649748	array([[-0.98238754, -0.1278629 ]], dtype=float32)

time = 611	action = 0	current_phase = 0	next_phase = 1	reward = -0.577721	array([[-0.9711421 , -0.15807526]], dtype=float32)

time = 616	action = 1	current_phase = 0	next_phase = 1	reward = -2.142698	array([[-0.98339653, -0.1264002 ]], dtype=float32)

time = 624	action = 0	current_phase = 1	next_phase = 0	reward = -0.708162	array([[1.0396974, 0.5858009]], dtype=float32)

time = 629	action = 1	current_phase = 1	next_phase = 0	reward = -1.050645	array([[1.0412942, 0.6026763]], dtype=float32)

time = 637	action = 0	current_phase = 0	next_phase = 1	reward = 0.214171	array([[-0.9814823 , -0.14628331]], dtype=float32)

time = 642	action = 0	current_phase = 0	next_phase = 1	reward = 0.006005	array([[-0.97596204, -0.16878173]], dtype=float32)

time = 647	action = 0	current_phase = 0	next_phase = 1	reward = 0.054990	array([[-0.9881018 , -0.13967776]], dtype=float32)

time = 652	action = 1	current_phase = 0	next_phase = 1	reward = -1.591074	array([[-0.9766075 , -0.17090699]], dtype=float32)

time = 660	action = 0	current_phase = 1	next_phase = 0	reward = -1.821477	array([[1.0470053 , 0.58909386]], dtype=float32)

time = 665	action = 1	current_phase = 1	next_phase = 0	reward = -1.406827	array([[1.060792  , 0.58405817]], dtype=float32)

Terminal occured. Episode end.

Simulation ended at time: 674.00

Reason: TraCI requested termination.

Performance: 

 Duration: 17913ms

 Real time factor: 37.6263

 UPS: 1048.233127

Vehicles: 

 Inserted: 316

 Running: 24

 Waiting: 0



DijkstraRouter answered 316 queries and explored 2.29 edges on average.

DijkstraRouter spent 0ms answering queries (0.00ms on average).

Could not connect to TraCI server at localhost:44631 [Errno 111] Connection refused

 Retrying in 1 seconds

Loading configuration... done.

***Starting server on port 44631 ***

Loading net-file from '/home/soup/IntelliLight/data/one_run/cross.net.xml'... done (1ms).

Warning: Missing yellow phase in tlLogic 'node0', program '0' for tl-index 5 when switching to phase 1

Loading additional-files from '/home/soup/IntelliLight/data/one_run/cross.tls.switch.xml'... done (3ms).

Loading additional-files from '/home/soup/IntelliLight/data/one_run/cross.car.type.xml'... done (0ms).

Loading done.

Simulation started with time: 0.00

time = 20	action = 0	current_phase = 0	next_phase = 1	reward = -0.096708	array([[-0.97998637, -0.15543285]], dtype=float32)

time = 25	action = 0	current_phase = 0	next_phase = 1	reward = -0.777623	array([[-0.98078287, -0.15541553]], dtype=float32)

time = 30	action = 0	current_phase = 0	next_phase = 1	reward = -1.290711	array([[-0.9753964, -0.1559512]], dtype=float32)

time = 35	action = 0	current_phase = 0	next_phase = 1	reward = -1.244540	array([[-0.9839037 , -0.14417462]], dtype=float32)

time = 40	action = 0	current_phase = 0	next_phase = 1	reward = -1.449329	array([[-0.9827778 , -0.14573789]], dtype=float32)

time = 45	action = 1	current_phase = 0	next_phase = 1	reward = -1.891956	array([[-0.98574513, -0.15464464]], dtype=float32)

time = 53	action = 0	current_phase = 1	next_phase = 0	reward = -0.576256	array([[1.0676326 , 0.56643414]], dtype=float32)

time = 58	action = 1	current_phase = 1	next_phase = 0	reward = -1.142915	array([[1.0374119, 0.6019951]], dtype=float32)

time = 66	action = 0	current_phase = 0	next_phase = 1	reward = 0.504011	array([[-0.9775479 , -0.13711603]], dtype=float32)

time = 71	action = 0	current_phase = 0	next_phase = 1	reward = 0.009877	array([[-0.97553754, -0.1706224 ]], dtype=float32)

time = 76	action = 0	current_phase = 0	next_phase = 1	reward = 0.078004	array([[-0.9886072 , -0.13955313]], dtype=float32)

time = 81	action = 0	current_phase = 0	next_phase = 1	reward = -0.056262	array([[-0.9762448 , -0.17020333]], dtype=float32)

time = 86	action = 1	current_phase = 0	next_phase = 1	reward = -2.254959	array([[-0.9897022 , -0.13998026]], dtype=float32)

time = 94	action = 0	current_phase = 1	next_phase = 0	reward = -1.656807	array([[1.0403512, 0.5880077]], dtype=float32)

time = 99	action = 1	current_phase = 1	next_phase = 0	reward = -1.110402	array([[1.0319741 , 0.60251594]], dtype=float32)

time = 107	action = 0	current_phase = 0	next_phase = 1	reward = 0.662480	array([[-0.97804487, -0.13730857]], dtype=float32)

time = 112	action = 0	current_phase = 0	next_phase = 1	reward = -0.162512	array([[-0.97668797, -0.17129335]], dtype=float32)

time = 117	action = 0	current_phase = 0	next_phase = 1	reward = -0.916330	array([[-0.9767926, -0.1325897]], dtype=float32)

time = 122	action = 0	current_phase = 0	next_phase = 1	reward = -1.203088	array([[-0.98042893, -0.15964946]], dtype=float32)

time = 127	action = 1	current_phase = 0	next_phase = 1	reward = -2.467881	array([[-0.9925267 , -0.12889063]], dtype=float32)

time = 135	action = 0	current_phase = 1	next_phase = 0	reward = -1.489563	array([[1.0377119 , 0.58567846]], dtype=float32)

time = 140	action = 1	current_phase = 1	next_phase = 0	reward = -0.876778	array([[1.032732  , 0.60097235]], dtype=float32)

time = 148	action = 0	current_phase = 0	next_phase = 1	reward = -1.091306	array([[-0.98368263, -0.13882934]], dtype=float32)

time = 153	action = 0	current_phase = 0	next_phase = 1	reward = -1.215966	array([[-0.9795842 , -0.16122015]], dtype=float32)

time = 158	action = 0	current_phase = 0	next_phase = 1	reward = -1.180215	array([[-0.97295934, -0.14164826]], dtype=float32)

time = 163	action = 0	current_phase = 0	next_phase = 1	reward = -1.128620	array([[-0.9787502 , -0.15856957]], dtype=float32)

time = 168	action = 1	current_phase = 0	next_phase = 1	reward = -1.845604	array([[-0.9980457 , -0.14561939]], dtype=float32)

time = 176	action = 0	current_phase = 1	next_phase = 0	reward = -1.642579	array([[1.0343723 , 0.60176134]], dtype=float32)

time = 181	action = 1	current_phase = 1	next_phase = 0	reward = -1.276794	array([[1.0469337 , 0.59597516]], dtype=float32)

time = 189	action = 0	current_phase = 0	next_phase = 1	reward = 0.276573	array([[-0.9847054, -0.1321353]], dtype=float32)

time = 194	action = 0	current_phase = 0	next_phase = 1	reward = 0.053647	array([[-0.97504085, -0.16875514]], dtype=float32)

time = 199	action = 0	current_phase = 0	next_phase = 1	reward = 0.067252	array([[-0.99488753, -0.15775457]], dtype=float32)

time = 204	action = 0	current_phase = 0	next_phase = 1	reward = -0.546315	array([[-0.97644573, -0.16989464]], dtype=float32)

time = 209	action = 1	current_phase = 0	next_phase = 1	reward = -1.671322	array([[-0.97167796, -0.16815758]], dtype=float32)

time = 217	action = 0	current_phase = 1	next_phase = 0	reward = -1.354809	array([[1.0476321, 0.5850096]], dtype=float32)

time = 222	action = 1	current_phase = 1	next_phase = 0	reward = -1.195425	array([[1.0573789, 0.5817144]], dtype=float32)

time = 230	action = 0	current_phase = 0	next_phase = 1	reward = 0.620167	array([[-0.9902365, -0.1493496]], dtype=float32)

time = 235	action = 0	current_phase = 0	next_phase = 1	reward = -0.555122	array([[-0.9837751 , -0.16478974]], dtype=float32)

time = 240	action = 0	current_phase = 0	next_phase = 1	reward = -0.939855	array([[-0.9681271 , -0.16045888]], dtype=float32)

time = 245	action = 0	current_phase = 0	next_phase = 1	reward = -1.202268	array([[-0.9829975 , -0.14570296]], dtype=float32)

time = 250	action = 1	current_phase = 0	next_phase = 1	reward = -2.210087	array([[-0.9827514 , -0.14470834]], dtype=float32)

time = 258	action = 0	current_phase = 1	next_phase = 0	reward = -1.227705	array([[1.0397207, 0.5838095]], dtype=float32)

time = 263	action = 1	current_phase = 1	next_phase = 0	reward = -2.177147	array([[1.0565921 , 0.58331954]], dtype=float32)

time = 271	action = 0	current_phase = 0	next_phase = 1	reward = -0.343700	array([[-0.9801259 , -0.15048629]], dtype=float32)

time = 276	action = 0	current_phase = 0	next_phase = 1	reward = -1.176033	array([[-0.9922073 , -0.13145095]], dtype=float32)

time = 281	action = 0	current_phase = 0	next_phase = 1	reward = -1.136253	array([[-0.9795853 , -0.15977316]], dtype=float32)

time = 286	action = 0	current_phase = 0	next_phase = 1	reward = -1.112476	array([[-0.9915421 , -0.12872866]], dtype=float32)

time = 291	action = 1	current_phase = 0	next_phase = 1	reward = -1.602790	array([[-0.9792526 , -0.15738706]], dtype=float32)

time = 299	action = 0	current_phase = 1	next_phase = 0	reward = -1.646785	array([[1.0469385, 0.5890807]], dtype=float32)

time = 304	action = 1	current_phase = 1	next_phase = 0	reward = -0.986758	array([[1.0600805 , 0.58536327]], dtype=float32)

time = 312	action = 0	current_phase = 0	next_phase = 1	reward = 0.305577	array([[-0.978495 , -0.1793609]], dtype=float32)

time = 317	action = 0	current_phase = 0	next_phase = 1	reward = 0.070492	array([[-0.98843014, -0.14095148]], dtype=float32)

time = 322	action = 0	current_phase = 0	next_phase = 1	reward = -0.139650	array([[-0.9757988 , -0.17052248]], dtype=float32)

time = 327	action = 0	current_phase = 0	next_phase = 1	reward = -1.004792	array([[-0.9895338 , -0.13858116]], dtype=float32)

time = 332	action = 1	current_phase = 0	next_phase = 1	reward = -2.677587	array([[-0.9796312 , -0.16062632]], dtype=float32)

time = 340	action = 0	current_phase = 1	next_phase = 0	reward = -1.685253	array([[1.0394993, 0.5842954]], dtype=float32)

time = 345	action = 1	current_phase = 1	next_phase = 0	reward = -0.737325	array([[1.0602269 , 0.58371127]], dtype=float32)

time = 353	action = 0	current_phase = 0	next_phase = 1	reward = 0.003977	array([[-0.9794966, -0.1797874]], dtype=float32)

time = 358	action = 0	current_phase = 0	next_phase = 1	reward = -1.074318	array([[-0.9967128 , -0.14659426]], dtype=float32)

time = 363	action = 0	current_phase = 0	next_phase = 1	reward = -1.199872	array([[-0.9800701 , -0.16041717]], dtype=float32)

time = 368	action = 0	current_phase = 0	next_phase = 1	reward = -1.173485	array([[-0.99063   , -0.12919536]], dtype=float32)

time = 373	action = 1	current_phase = 0	next_phase = 1	reward = -2.577720	array([[-0.97906077, -0.16001269]], dtype=float32)

time = 381	action = 0	current_phase = 1	next_phase = 0	reward = -1.104254	array([[1.0539578, 0.5788706]], dtype=float32)

time = 386	action = 1	current_phase = 1	next_phase = 0	reward = -1.375068	array([[1.0547892, 0.5977913]], dtype=float32)

time = 394	action = 0	current_phase = 0	next_phase = 1	reward = 0.510680	array([[-0.9713572 , -0.18390794]], dtype=float32)

time = 399	action = 0	current_phase = 0	next_phase = 1	reward = -0.037516	array([[-0.98913825, -0.1382998 ]], dtype=float32)

time = 404	action = 0	current_phase = 0	next_phase = 1	reward = 0.026321	array([[-0.97550476, -0.16913629]], dtype=float32)

time = 409	action = 0	current_phase = 0	next_phase = 1	reward = 0.042046	array([[-0.97042346, -0.15163729]], dtype=float32)

time = 414	action = 1	current_phase = 0	next_phase = 1	reward = -1.182899	array([[-0.9759844 , -0.16901413]], dtype=float32)

time = 422	action = 0	current_phase = 1	next_phase = 0	reward = -1.373529	array([[1.0744983, 0.5685986]], dtype=float32)

time = 427	action = 1	current_phase = 1	next_phase = 0	reward = -0.884862	array([[1.0258722, 0.6010035]], dtype=float32)

time = 435	action = 0	current_phase = 0	next_phase = 1	reward = 0.057754	array([[-0.9760328 , -0.17009169]], dtype=float32)

time = 440	action = 0	current_phase = 0	next_phase = 1	reward = 0.578956	array([[-0.98794526, -0.14078799]], dtype=float32)

time = 445	action = 0	current_phase = 0	next_phase = 1	reward = -0.586218	array([[-0.98106426, -0.15642254]], dtype=float32)

time = 450	action = 0	current_phase = 0	next_phase = 1	reward = -1.492068	array([[-0.97421765, -0.1541985 ]], dtype=float32)

time = 455	action = 1	current_phase = 0	next_phase = 1	reward = -2.000003	array([[-0.98624504, -0.15560135]], dtype=float32)

time = 463	action = 0	current_phase = 1	next_phase = 0	reward = -1.374363	array([[1.0380154, 0.5847533]], dtype=float32)

time = 468	action = 1	current_phase = 1	next_phase = 0	reward = -0.767131	array([[1.0323046 , 0.60143733]], dtype=float32)

time = 476	action = 0	current_phase = 0	next_phase = 1	reward = -0.538432	array([[-0.99121726, -0.15024748]], dtype=float32)

time = 481	action = 0	current_phase = 0	next_phase = 1	reward = -1.206956	array([[-0.9788479, -0.1622438]], dtype=float32)

time = 486	action = 0	current_phase = 0	next_phase = 1	reward = -1.180663	array([[-0.99185467, -0.1298469 ]], dtype=float32)

time = 491	action = 0	current_phase = 0	next_phase = 1	reward = -1.139437	array([[-0.97865695, -0.15960349]], dtype=float32)

time = 496	action = 1	current_phase = 0	next_phase = 1	reward = -2.036911	array([[-0.99169266, -0.12879679]], dtype=float32)

time = 504	action = 0	current_phase = 1	next_phase = 0	reward = -0.801934	array([[1.0415967, 0.5850912]], dtype=float32)

time = 509	action = 1	current_phase = 1	next_phase = 0	reward = -0.714063	array([[1.0412498, 0.6026057]], dtype=float32)

time = 517	action = 0	current_phase = 0	next_phase = 1	reward = -0.073198	array([[-0.9879499 , -0.14045238]], dtype=float32)

time = 522	action = 0	current_phase = 0	next_phase = 1	reward = -0.000839	array([[-0.9770888, -0.1711599]], dtype=float32)

time = 527	action = 0	current_phase = 0	next_phase = 1	reward = 0.066634	array([[-0.98811567, -0.1401254 ]], dtype=float32)

time = 532	action = 0	current_phase = 0	next_phase = 1	reward = -0.096265	array([[-0.97619665, -0.16892967]], dtype=float32)

time = 537	action = 1	current_phase = 0	next_phase = 1	reward = -2.506699	array([[-0.9899394 , -0.14098147]], dtype=float32)

time = 545	action = 0	current_phase = 1	next_phase = 0	reward = -1.702943	array([[1.0380996, 0.5863545]], dtype=float32)

time = 550	action = 1	current_phase = 1	next_phase = 0	reward = -0.657280	array([[1.0325449, 0.6015214]], dtype=float32)

time = 558	action = 0	current_phase = 0	next_phase = 1	reward = 0.066951	array([[-0.98860633, -0.14002562]], dtype=float32)

time = 563	action = 0	current_phase = 0	next_phase = 1	reward = -0.343759	array([[-0.9765284 , -0.17230238]], dtype=float32)

time = 568	action = 0	current_phase = 0	next_phase = 1	reward = -1.094633	array([[-0.9707484 , -0.16895209]], dtype=float32)

time = 573	action = 0	current_phase = 0	next_phase = 1	reward = -1.199709	array([[-0.979957 , -0.1610157]], dtype=float32)

time = 578	action = 1	current_phase = 0	next_phase = 1	reward = -2.518457	array([[-0.99787056, -0.14777628]], dtype=float32)

time = 586	action = 0	current_phase = 1	next_phase = 0	reward = -1.703299	array([[1.0267875 , 0.59628946]], dtype=float32)

time = 591	action = 1	current_phase = 1	next_phase = 0	reward = -1.572696	array([[1.0463443, 0.5961886]], dtype=float32)

time = 599	action = 0	current_phase = 0	next_phase = 1	reward = -0.866446	array([[-0.9800303, -0.1310707]], dtype=float32)

time = 604	action = 0	current_phase = 0	next_phase = 1	reward = -1.216578	array([[-0.9795445 , -0.16018796]], dtype=float32)

time = 609	action = 0	current_phase = 0	next_phase = 1	reward = -1.164431	array([[-0.97242266, -0.1396347 ]], dtype=float32)

time = 614	action = 0	current_phase = 0	next_phase = 1	reward = -1.131547	array([[-0.97886324, -0.15758716]], dtype=float32)

time = 619	action = 1	current_phase = 0	next_phase = 1	reward = -1.322600	array([[-0.978258  , -0.15726906]], dtype=float32)

time = 627	action = 0	current_phase = 1	next_phase = 0	reward = -1.462901	array([[1.0518873, 0.5947238]], dtype=float32)

time = 632	action = 1	current_phase = 1	next_phase = 0	reward = -1.037816	array([[1.0569425, 0.5905074]], dtype=float32)

time = 640	action = 0	current_phase = 0	next_phase = 1	reward = -0.019322	array([[-0.9878707 , -0.13899532]], dtype=float32)

time = 645	action = 0	current_phase = 0	next_phase = 1	reward = 0.614393	array([[-0.9764387 , -0.16911605]], dtype=float32)

time = 650	action = 0	current_phase = 0	next_phase = 1	reward = 0.012373	array([[-0.97997445, -0.15440491]], dtype=float32)

time = 655	action = 0	current_phase = 0	next_phase = 1	reward = -0.917762	array([[-0.9617045, -0.1659347]], dtype=float32)

time = 660	action = 1	current_phase = 0	next_phase = 1	reward = -2.063947	array([[-0.96858513, -0.1591179 ]], dtype=float32)

Terminal occured. Episode end.

Simulation ended at time: 669.00

Reason: TraCI requested termination.

Performance: 

 Duration: 17420ms

 Real time factor: 38.4041

 UPS: 1065.212400

Vehicles: 

 Inserted: 314

 Running: 33

 Waiting: 0



DijkstraRouter answered 314 queries and explored 2.29 edges on average.

DijkstraRouter spent 0ms answering queries (0.00ms on average).

Could not connect to TraCI server at localhost:41871 [Errno 111] Connection refused

 Retrying in 1 seconds

Loading configuration... done.

***Starting server on port 41871 ***

Loading net-file from '/home/soup/IntelliLight/data/one_run/cross.net.xml'... done (1ms).

Warning: Missing yellow phase in tlLogic 'node0', program '0' for tl-index 5 when switching to phase 1

Loading additional-files from '/home/soup/IntelliLight/data/one_run/cross.tls.switch.xml'... done (1ms).

Loading additional-files from '/home/soup/IntelliLight/data/one_run/cross.car.type.xml'... done (0ms).

Loading done.

Simulation started with time: 0.00

time = 20	action = 0	current_phase = 0	next_phase = 1	reward = -0.096708	array([[-0.97998637, -0.15543285]], dtype=float32)

time = 25	action = 0	current_phase = 0	next_phase = 1	reward = -0.777623	array([[-0.98078287, -0.15541553]], dtype=float32)

time = 30	action = 1	current_phase = 0	next_phase = 1	reward = -2.600898	array([[-0.9753964, -0.1559512]], dtype=float32)

time = 38	action = 0	current_phase = 1	next_phase = 0	reward = -1.414874	array([[1.0384277 , 0.58529764]], dtype=float32)

time = 43	action = 0	current_phase = 1	next_phase = 0	reward = -1.970617	array([[1.059602  , 0.58407116]], dtype=float32)

time = 48	action = 1	current_phase = 1	next_phase = 0	reward = -0.918527	array([[1.0393659, 0.5964626]], dtype=float32)

time = 56	action = 0	current_phase = 0	next_phase = 1	reward = -0.223930	array([[-0.98491657, -0.15545832]], dtype=float32)

time = 61	action = 1	current_phase = 0	next_phase = 1	reward = -2.515687	array([[-0.9801806 , -0.16042405]], dtype=float32)

time = 69	action = 0	current_phase = 1	next_phase = 0	reward = -1.515878	array([[1.0386387 , 0.58529645]], dtype=float32)

time = 74	action = 0	current_phase = 1	next_phase = 0	reward = -2.084352	array([[1.0579385, 0.5850177]], dtype=float32)

time = 79	action = 1	current_phase = 1	next_phase = 0	reward = -1.422369	array([[1.0390482 , 0.59569347]], dtype=float32)

time = 87	action = 0	current_phase = 0	next_phase = 1	reward = -0.137895	array([[-0.96711195, -0.1395403 ]], dtype=float32)

time = 92	action = 1	current_phase = 0	next_phase = 1	reward = -2.619377	array([[-0.97964656, -0.15912418]], dtype=float32)

time = 100	action = 0	current_phase = 1	next_phase = 0	reward = -1.679776	array([[1.0396085 , 0.58460665]], dtype=float32)

time = 105	action = 0	current_phase = 1	next_phase = 0	reward = -2.254294	array([[1.060136  , 0.58378965]], dtype=float32)

time = 110	action = 1	current_phase = 1	next_phase = 0	reward = -1.976023	array([[1.0404307, 0.5948055]], dtype=float32)

time = 118	action = 0	current_phase = 0	next_phase = 1	reward = 0.106145	array([[-0.9927279 , -0.15705569]], dtype=float32)

time = 123	action = 1	current_phase = 0	next_phase = 1	reward = -2.756031	array([[-0.9799321 , -0.15957856]], dtype=float32)

time = 131	action = 0	current_phase = 1	next_phase = 0	reward = -1.803679	array([[1.0540769 , 0.57829344]], dtype=float32)

time = 136	action = 0	current_phase = 1	next_phase = 0	reward = -2.382397	array([[1.0448611, 0.5937927]], dtype=float32)

time = 141	action = 1	current_phase = 1	next_phase = 0	reward = -2.070182	array([[1.0535623 , 0.59017557]], dtype=float32)

time = 149	action = 0	current_phase = 0	next_phase = 1	reward = -0.262854	array([[-0.9810853 , -0.16395038]], dtype=float32)

time = 154	action = 1	current_phase = 0	next_phase = 1	reward = -2.941279	array([[-0.97902584, -0.1590128 ]], dtype=float32)

time = 162	action = 0	current_phase = 1	next_phase = 0	reward = -1.843216	array([[1.0566915 , 0.57962227]], dtype=float32)

time = 167	action = 0	current_phase = 1	next_phase = 0	reward = -2.448476	array([[1.0462447, 0.5955827]], dtype=float32)

time = 172	action = 1	current_phase = 1	next_phase = 0	reward = -2.086334	array([[1.0582114 , 0.58894706]], dtype=float32)

time = 180	action = 0	current_phase = 0	next_phase = 1	reward = -0.381763	array([[-0.9784612 , -0.15397498]], dtype=float32)

time = 185	action = 1	current_phase = 0	next_phase = 1	reward = -2.045218	array([[-0.97680265, -0.14980787]], dtype=float32)

time = 193	action = 0	current_phase = 1	next_phase = 0	reward = -1.267744	array([[1.054556  , 0.57910275]], dtype=float32)

time = 198	action = 0	current_phase = 1	next_phase = 0	reward = -2.004059	array([[1.0303565, 0.5993509]], dtype=float32)

time = 203	action = 1	current_phase = 1	next_phase = 0	reward = -0.956189	array([[1.0542306 , 0.59712875]], dtype=float32)

time = 211	action = 0	current_phase = 0	next_phase = 1	reward = -0.428172	array([[-0.97161835, -0.12651397]], dtype=float32)

time = 216	action = 1	current_phase = 0	next_phase = 1	reward = -2.068316	array([[-0.977926  , -0.11881757]], dtype=float32)

time = 224	action = 0	current_phase = 1	next_phase = 0	reward = -1.442308	array([[1.0406857 , 0.58461064]], dtype=float32)

time = 229	action = 0	current_phase = 1	next_phase = 0	reward = -2.033019	array([[1.0316263 , 0.60163176]], dtype=float32)

time = 234	action = 1	current_phase = 1	next_phase = 0	reward = -1.304524	array([[1.0540086 , 0.59762335]], dtype=float32)

time = 242	action = 0	current_phase = 0	next_phase = 1	reward = 1.347837	array([[-0.9843015, -0.1424256]], dtype=float32)

time = 247	action = 1	current_phase = 0	next_phase = 1	reward = -2.390992	array([[-0.9885641, -0.1382949]], dtype=float32)

time = 255	action = 0	current_phase = 1	next_phase = 0	reward = -1.554593	array([[1.037813  , 0.58736783]], dtype=float32)

time = 260	action = 0	current_phase = 1	next_phase = 0	reward = -1.933803	array([[1.0312154, 0.6020055]], dtype=float32)

time = 265	action = 1	current_phase = 1	next_phase = 0	reward = -1.639341	array([[1.0551546 , 0.59943837]], dtype=float32)

time = 273	action = 0	current_phase = 0	next_phase = 1	reward = 1.371643	array([[-0.97670627, -0.15966515]], dtype=float32)

time = 278	action = 1	current_phase = 0	next_phase = 1	reward = -2.526145	array([[-0.9879626, -0.1398766]], dtype=float32)

time = 286	action = 0	current_phase = 1	next_phase = 0	reward = -1.724018	array([[1.0387263, 0.6006255]], dtype=float32)

time = 291	action = 0	current_phase = 1	next_phase = 0	reward = -1.769149	array([[1.0454041 , 0.59522986]], dtype=float32)

time = 296	action = 1	current_phase = 1	next_phase = 0	reward = -1.836346	array([[1.0476675, 0.6091793]], dtype=float32)

time = 304	action = 0	current_phase = 0	next_phase = 1	reward = 1.095136	array([[-0.973314  , -0.14945334]], dtype=float32)

time = 309	action = 1	current_phase = 0	next_phase = 1	reward = -1.395267	array([[-0.98834497, -0.13856766]], dtype=float32)

time = 317	action = 0	current_phase = 1	next_phase = 0	reward = -1.154625	array([[1.0474002, 0.5868148]], dtype=float32)

time = 322	action = 0	current_phase = 1	next_phase = 0	reward = -1.213580	array([[1.0533195 , 0.58193076]], dtype=float32)

time = 327	action = 1	current_phase = 1	next_phase = 0	reward = -1.174499	array([[1.0490261 , 0.59883875]], dtype=float32)

time = 335	action = 0	current_phase = 0	next_phase = 1	reward = 0.188102	array([[-0.987257  , -0.16429031]], dtype=float32)

time = 340	action = 1	current_phase = 0	next_phase = 1	reward = -1.469070	array([[-0.98845255, -0.14033228]], dtype=float32)

time = 348	action = 0	current_phase = 1	next_phase = 0	reward = -1.256040	array([[1.0443894 , 0.58545697]], dtype=float32)

time = 353	action = 0	current_phase = 1	next_phase = 0	reward = -1.276626	array([[1.0593989, 0.5836487]], dtype=float32)

time = 358	action = 1	current_phase = 1	next_phase = 0	reward = -1.285345	array([[1.0474927, 0.5978457]], dtype=float32)

time = 366	action = 0	current_phase = 0	next_phase = 1	reward = 0.778255	array([[-0.9866899, -0.1645011]], dtype=float32)

time = 371	action = 1	current_phase = 0	next_phase = 1	reward = -2.187370	array([[-0.9755108 , -0.16983567]], dtype=float32)

time = 379	action = 0	current_phase = 1	next_phase = 0	reward = -1.444377	array([[1.0381857, 0.5872982]], dtype=float32)

time = 384	action = 0	current_phase = 1	next_phase = 0	reward = -1.394199	array([[1.0597308, 0.584537 ]], dtype=float32)

time = 389	action = 1	current_phase = 1	next_phase = 0	reward = -1.403631	array([[1.0478579 , 0.59750915]], dtype=float32)

time = 397	action = 0	current_phase = 0	next_phase = 1	reward = 0.796924	array([[-0.99476117, -0.14767945]], dtype=float32)

time = 402	action = 1	current_phase = 0	next_phase = 1	reward = -2.177719	array([[-0.9761967 , -0.17144975]], dtype=float32)

time = 410	action = 0	current_phase = 1	next_phase = 0	reward = -0.927866	array([[1.0376886 , 0.58792007]], dtype=float32)

time = 415	action = 0	current_phase = 1	next_phase = 0	reward = -2.082347	array([[1.0683768 , 0.58517843]], dtype=float32)

time = 420	action = 1	current_phase = 1	next_phase = 0	reward = -2.308370	array([[1.0463167 , 0.59753805]], dtype=float32)

time = 428	action = 0	current_phase = 0	next_phase = 1	reward = 1.455590	array([[-0.99426925, -0.14934278]], dtype=float32)

time = 433	action = 1	current_phase = 0	next_phase = 1	reward = -2.428492	array([[-0.9759849 , -0.16886944]], dtype=float32)

time = 441	action = 0	current_phase = 1	next_phase = 0	reward = -1.056975	array([[1.0531098 , 0.58219445]], dtype=float32)

time = 446	action = 0	current_phase = 1	next_phase = 0	reward = -2.304111	array([[1.0543716 , 0.59682924]], dtype=float32)

time = 451	action = 1	current_phase = 1	next_phase = 0	reward = -1.774803	array([[1.0543302, 0.5901322]], dtype=float32)

time = 459	action = 0	current_phase = 0	next_phase = 1	reward = 0.863205	array([[-0.97602904, -0.15946168]], dtype=float32)

time = 464	action = 1	current_phase = 0	next_phase = 1	reward = -1.844085	array([[-0.9759921 , -0.17053628]], dtype=float32)

time = 472	action = 0	current_phase = 1	next_phase = 0	reward = -0.895215	array([[1.06646   , 0.57723624]], dtype=float32)

time = 477	action = 0	current_phase = 1	next_phase = 0	reward = -2.161007	array([[1.0453757, 0.6026894]], dtype=float32)

time = 482	action = 1	current_phase = 1	next_phase = 0	reward = -1.502184	array([[1.0464152, 0.5950866]], dtype=float32)

time = 490	action = 0	current_phase = 0	next_phase = 1	reward = 0.561652	array([[-0.9833629 , -0.15550089]], dtype=float32)

time = 495	action = 1	current_phase = 0	next_phase = 1	reward = -2.054140	array([[-0.9943472 , -0.15829058]], dtype=float32)

time = 503	action = 0	current_phase = 1	next_phase = 0	reward = -1.014462	array([[1.053309 , 0.5862777]], dtype=float32)

time = 508	action = 0	current_phase = 1	next_phase = 0	reward = -2.427522	array([[1.0387414 , 0.60514355]], dtype=float32)

time = 513	action = 1	current_phase = 1	next_phase = 0	reward = -1.602265	array([[1.0548396, 0.5980714]], dtype=float32)

time = 521	action = 0	current_phase = 0	next_phase = 1	reward = 0.886255	array([[-0.9879643 , -0.15230405]], dtype=float32)

time = 526	action = 1	current_phase = 0	next_phase = 1	reward = -1.776830	array([[-0.9812272 , -0.14492935]], dtype=float32)

time = 534	action = 0	current_phase = 1	next_phase = 0	reward = -0.814145	array([[1.0383614, 0.5874498]], dtype=float32)

time = 539	action = 0	current_phase = 1	next_phase = 0	reward = -2.288800	array([[1.0398173, 0.6034713]], dtype=float32)

time = 544	action = 1	current_phase = 1	next_phase = 0	reward = -1.372362	array([[1.0526928, 0.5993707]], dtype=float32)

time = 552	action = 0	current_phase = 0	next_phase = 1	reward = 0.867619	array([[-0.9871993 , -0.16277084]], dtype=float32)

time = 557	action = 1	current_phase = 0	next_phase = 1	reward = -1.659149	array([[-0.98864627, -0.13896146]], dtype=float32)

time = 565	action = 0	current_phase = 1	next_phase = 0	reward = -1.441870	array([[1.0457295, 0.5902684]], dtype=float32)

time = 570	action = 0	current_phase = 1	next_phase = 0	reward = -2.537886	array([[1.0393641 , 0.60238075]], dtype=float32)

time = 575	action = 1	current_phase = 1	next_phase = 0	reward = -1.547030	array([[1.0538445, 0.5966168]], dtype=float32)

time = 583	action = 0	current_phase = 0	next_phase = 1	reward = 0.886868	array([[-0.97689146, -0.16040711]], dtype=float32)

time = 588	action = 1	current_phase = 0	next_phase = 1	reward = -1.113143	array([[-0.9883414 , -0.13940068]], dtype=float32)

time = 596	action = 0	current_phase = 1	next_phase = 0	reward = -1.264922	array([[1.0501403, 0.5940705]], dtype=float32)

time = 601	action = 0	current_phase = 1	next_phase = 0	reward = -2.322495	array([[1.0547286 , 0.58740544]], dtype=float32)

time = 606	action = 1	current_phase = 1	next_phase = 0	reward = -1.435771	array([[1.0468801, 0.6028279]], dtype=float32)

time = 614	action = 0	current_phase = 0	next_phase = 1	reward = 0.623522	array([[-0.96583164, -0.16599701]], dtype=float32)

time = 619	action = 1	current_phase = 0	next_phase = 1	reward = -0.664633	array([[-0.9875332, -0.1381912]], dtype=float32)

time = 627	action = 0	current_phase = 1	next_phase = 0	reward = -1.190834	array([[1.0554267 , 0.58811134]], dtype=float32)

time = 632	action = 0	current_phase = 1	next_phase = 0	reward = -2.105441	array([[1.0521797, 0.5815779]], dtype=float32)

time = 637	action = 1	current_phase = 1	next_phase = 0	reward = -0.862138	array([[1.038985 , 0.5972221]], dtype=float32)

time = 645	action = 0	current_phase = 0	next_phase = 1	reward = 0.321135	array([[-0.9758525 , -0.16930702]], dtype=float32)

time = 650	action = 1	current_phase = 0	next_phase = 1	reward = -1.074322	array([[-0.9920503 , -0.14849293]], dtype=float32)

time = 658	action = 0	current_phase = 1	next_phase = 0	reward = -1.328998	array([[1.0468532 , 0.58950436]], dtype=float32)

time = 663	action = 0	current_phase = 1	next_phase = 0	reward = -2.155257	array([[1.0578247 , 0.58051586]], dtype=float32)

Terminal occured. Episode end.

Simulation ended at time: 669.00

Reason: TraCI requested termination.

Performance: 

 Duration: 14531ms

 Real time factor: 46.0395

 UPS: 1368.660106

Vehicles: 

 Inserted: 314

 Running: 32

 Waiting: 0



DijkstraRouter answered 314 queries and explored 2.29 edges on average.

DijkstraRouter spent 0ms answering queries (0.00ms on average).

Could not connect to TraCI server at localhost:56783 [Errno 111] Connection refused

 Retrying in 1 seconds

Loading configuration... done.

***Starting server on port 56783 ***

Loading net-file from '/home/soup/IntelliLight/data/one_run/cross.net.xml'... done (1ms).

Warning: Missing yellow phase in tlLogic 'node0', program '0' for tl-index 5 when switching to phase 1

Loading additional-files from '/home/soup/IntelliLight/data/one_run/cross.tls.switch.xml'... done (0ms).

Loading additional-files from '/home/soup/IntelliLight/data/one_run/cross.car.type.xml'... done (0ms).

Loading done.

Simulation started with time: 0.00

time = 20	action = 0	current_phase = 0	next_phase = 1	reward = -0.096708	array([[-0.97998637, -0.15543285]], dtype=float32)

time = 25	action = 0	current_phase = 0	next_phase = 1	reward = -0.777623	array([[-0.98078287, -0.15541553]], dtype=float32)

time = 30	action = 1	current_phase = 0	next_phase = 1	reward = -2.600898	array([[-0.9753964, -0.1559512]], dtype=float32)

time = 38	action = 0	current_phase = 1	next_phase = 0	reward = -1.414874	array([[1.0384277 , 0.58529764]], dtype=float32)

time = 43	action = 0	current_phase = 1	next_phase = 0	reward = -1.970617	array([[1.059602  , 0.58407116]], dtype=float32)

time = 48	action = 1	current_phase = 1	next_phase = 0	reward = -0.918527	array([[1.0393659, 0.5964626]], dtype=float32)

time = 56	action = 0	current_phase = 0	next_phase = 1	reward = -0.223930	array([[-0.98491657, -0.15545832]], dtype=float32)

time = 61	action = 1	current_phase = 0	next_phase = 1	reward = -2.515687	array([[-0.9801806 , -0.16042405]], dtype=float32)

time = 69	action = 0	current_phase = 1	next_phase = 0	reward = -1.515878	array([[1.0386387 , 0.58529645]], dtype=float32)

time = 74	action = 0	current_phase = 1	next_phase = 0	reward = -2.084352	array([[1.0579385, 0.5850177]], dtype=float32)

time = 79	action = 1	current_phase = 1	next_phase = 0	reward = -1.422369	array([[1.0390482 , 0.59569347]], dtype=float32)

time = 87	action = 0	current_phase = 0	next_phase = 1	reward = -0.137895	array([[-0.96711195, -0.1395403 ]], dtype=float32)

time = 92	action = 1	current_phase = 0	next_phase = 1	reward = -2.619377	array([[-0.97964656, -0.15912418]], dtype=float32)

time = 100	action = 0	current_phase = 1	next_phase = 0	reward = -1.679776	array([[1.0396085 , 0.58460665]], dtype=float32)

time = 105	action = 0	current_phase = 1	next_phase = 0	reward = -2.254294	array([[1.060136  , 0.58378965]], dtype=float32)

time = 110	action = 1	current_phase = 1	next_phase = 0	reward = -1.976023	array([[1.0404307, 0.5948055]], dtype=float32)

time = 118	action = 0	current_phase = 0	next_phase = 1	reward = 0.106145	array([[-0.9927279 , -0.15705569]], dtype=float32)

time = 123	action = 1	current_phase = 0	next_phase = 1	reward = -2.756031	array([[-0.9799321 , -0.15957856]], dtype=float32)

time = 131	action = 0	current_phase = 1	next_phase = 0	reward = -1.803679	array([[1.0540769 , 0.57829344]], dtype=float32)

time = 136	action = 0	current_phase = 1	next_phase = 0	reward = -2.382397	array([[1.0448611, 0.5937927]], dtype=float32)

time = 141	action = 1	current_phase = 1	next_phase = 0	reward = -2.070182	array([[1.0535623 , 0.59017557]], dtype=float32)

time = 149	action = 0	current_phase = 0	next_phase = 1	reward = -0.262854	array([[-0.9810853 , -0.16395038]], dtype=float32)

time = 154	action = 1	current_phase = 0	next_phase = 1	reward = -2.941279	array([[-0.97902584, -0.1590128 ]], dtype=float32)

time = 162	action = 0	current_phase = 1	next_phase = 0	reward = -1.843216	array([[1.0566915 , 0.57962227]], dtype=float32)

time = 167	action = 0	current_phase = 1	next_phase = 0	reward = -2.448476	array([[1.0462447, 0.5955827]], dtype=float32)

time = 172	action = 1	current_phase = 1	next_phase = 0	reward = -2.086334	array([[1.0582114 , 0.58894706]], dtype=float32)

time = 180	action = 0	current_phase = 0	next_phase = 1	reward = -0.381763	array([[-0.9784612 , -0.15397498]], dtype=float32)

time = 185	action = 1	current_phase = 0	next_phase = 1	reward = -2.045218	array([[-0.97680265, -0.14980787]], dtype=float32)

time = 193	action = 0	current_phase = 1	next_phase = 0	reward = -1.267744	array([[1.054556  , 0.57910275]], dtype=float32)

time = 198	action = 0	current_phase = 1	next_phase = 0	reward = -2.004059	array([[1.0303565, 0.5993509]], dtype=float32)

time = 203	action = 1	current_phase = 1	next_phase = 0	reward = -0.956189	array([[1.0542306 , 0.59712875]], dtype=float32)

time = 211	action = 0	current_phase = 0	next_phase = 1	reward = -0.428172	array([[-0.97161835, -0.12651397]], dtype=float32)

time = 216	action = 1	current_phase = 0	next_phase = 1	reward = -2.068316	array([[-0.977926  , -0.11881757]], dtype=float32)

time = 224	action = 0	current_phase = 1	next_phase = 0	reward = -1.442308	array([[1.0406857 , 0.58461064]], dtype=float32)

time = 229	action = 0	current_phase = 1	next_phase = 0	reward = -2.033019	array([[1.0316263 , 0.60163176]], dtype=float32)

time = 234	action = 1	current_phase = 1	next_phase = 0	reward = -1.304524	array([[1.0540086 , 0.59762335]], dtype=float32)

time = 242	action = 0	current_phase = 0	next_phase = 1	reward = 1.347837	array([[-0.9843015, -0.1424256]], dtype=float32)

time = 247	action = 1	current_phase = 0	next_phase = 1	reward = -2.390992	array([[-0.9885641, -0.1382949]], dtype=float32)

time = 255	action = 0	current_phase = 1	next_phase = 0	reward = -1.554593	array([[1.037813  , 0.58736783]], dtype=float32)

time = 260	action = 0	current_phase = 1	next_phase = 0	reward = -1.933803	array([[1.0312154, 0.6020055]], dtype=float32)

time = 265	action = 1	current_phase = 1	next_phase = 0	reward = -1.639341	array([[1.0551546 , 0.59943837]], dtype=float32)

time = 273	action = 0	current_phase = 0	next_phase = 1	reward = 1.371643	array([[-0.97670627, -0.15966515]], dtype=float32)

time = 278	action = 1	current_phase = 0	next_phase = 1	reward = -2.526145	array([[-0.9879626, -0.1398766]], dtype=float32)

time = 286	action = 0	current_phase = 1	next_phase = 0	reward = -1.724018	array([[1.0387263, 0.6006255]], dtype=float32)

time = 291	action = 0	current_phase = 1	next_phase = 0	reward = -1.769149	array([[1.0454041 , 0.59522986]], dtype=float32)

time = 296	action = 1	current_phase = 1	next_phase = 0	reward = -1.836346	array([[1.0476675, 0.6091793]], dtype=float32)

time = 304	action = 0	current_phase = 0	next_phase = 1	reward = 1.095136	array([[-0.973314  , -0.14945334]], dtype=float32)

time = 309	action = 1	current_phase = 0	next_phase = 1	reward = -1.395267	array([[-0.98834497, -0.13856766]], dtype=float32)

time = 317	action = 0	current_phase = 1	next_phase = 0	reward = -1.154625	array([[1.0474002, 0.5868148]], dtype=float32)

time = 322	action = 0	current_phase = 1	next_phase = 0	reward = -1.213580	array([[1.0533195 , 0.58193076]], dtype=float32)

time = 327	action = 1	current_phase = 1	next_phase = 0	reward = -1.174499	array([[1.0490261 , 0.59883875]], dtype=float32)

time = 335	action = 0	current_phase = 0	next_phase = 1	reward = 0.188102	array([[-0.987257  , -0.16429031]], dtype=float32)

time = 340	action = 1	current_phase = 0	next_phase = 1	reward = -1.469070	array([[-0.98845255, -0.14033228]], dtype=float32)

time = 348	action = 0	current_phase = 1	next_phase = 0	reward = -1.256040	array([[1.0443894 , 0.58545697]], dtype=float32)

time = 353	action = 0	current_phase = 1	next_phase = 0	reward = -1.276626	array([[1.0593989, 0.5836487]], dtype=float32)

time = 358	action = 1	current_phase = 1	next_phase = 0	reward = -1.285345	array([[1.0474927, 0.5978457]], dtype=float32)

time = 366	action = 0	current_phase = 0	next_phase = 1	reward = 0.778255	array([[-0.9866899, -0.1645011]], dtype=float32)

time = 371	action = 1	current_phase = 0	next_phase = 1	reward = -2.187370	array([[-0.9755108 , -0.16983567]], dtype=float32)

time = 379	action = 0	current_phase = 1	next_phase = 0	reward = -1.444377	array([[1.0381857, 0.5872982]], dtype=float32)

time = 384	action = 0	current_phase = 1	next_phase = 0	reward = -1.394199	array([[1.0597308, 0.584537 ]], dtype=float32)

time = 389	action = 1	current_phase = 1	next_phase = 0	reward = -1.403631	array([[1.0478579 , 0.59750915]], dtype=float32)

time = 397	action = 0	current_phase = 0	next_phase = 1	reward = 0.796924	array([[-0.99476117, -0.14767945]], dtype=float32)

time = 402	action = 1	current_phase = 0	next_phase = 1	reward = -2.177719	array([[-0.9761967 , -0.17144975]], dtype=float32)

time = 410	action = 0	current_phase = 1	next_phase = 0	reward = -0.927866	array([[1.0376886 , 0.58792007]], dtype=float32)

time = 415	action = 0	current_phase = 1	next_phase = 0	reward = -2.082347	array([[1.0683768 , 0.58517843]], dtype=float32)

time = 420	action = 1	current_phase = 1	next_phase = 0	reward = -2.308370	array([[1.0463167 , 0.59753805]], dtype=float32)

time = 428	action = 0	current_phase = 0	next_phase = 1	reward = 1.455590	array([[-0.99426925, -0.14934278]], dtype=float32)

time = 433	action = 1	current_phase = 0	next_phase = 1	reward = -2.428492	array([[-0.9759849 , -0.16886944]], dtype=float32)

time = 441	action = 0	current_phase = 1	next_phase = 0	reward = -1.056975	array([[1.0531098 , 0.58219445]], dtype=float32)

time = 446	action = 0	current_phase = 1	next_phase = 0	reward = -2.304111	array([[1.0543716 , 0.59682924]], dtype=float32)

time = 451	action = 1	current_phase = 1	next_phase = 0	reward = -1.774803	array([[1.0543302, 0.5901322]], dtype=float32)

time = 459	action = 0	current_phase = 0	next_phase = 1	reward = 0.863205	array([[-0.97602904, -0.15946168]], dtype=float32)

time = 464	action = 1	current_phase = 0	next_phase = 1	reward = -1.844085	array([[-0.9759921 , -0.17053628]], dtype=float32)

time = 472	action = 0	current_phase = 1	next_phase = 0	reward = -0.895215	array([[1.06646   , 0.57723624]], dtype=float32)

time = 477	action = 0	current_phase = 1	next_phase = 0	reward = -2.161007	array([[1.0453757, 0.6026894]], dtype=float32)

time = 482	action = 1	current_phase = 1	next_phase = 0	reward = -1.502184	array([[1.0464152, 0.5950866]], dtype=float32)

time = 490	action = 0	current_phase = 0	next_phase = 1	reward = 0.561652	array([[-0.9833629 , -0.15550089]], dtype=float32)

time = 495	action = 1	current_phase = 0	next_phase = 1	reward = -2.054140	array([[-0.9943472 , -0.15829058]], dtype=float32)

time = 503	action = 0	current_phase = 1	next_phase = 0	reward = -1.014462	array([[1.053309 , 0.5862777]], dtype=float32)

time = 508	action = 0	current_phase = 1	next_phase = 0	reward = -2.427522	array([[1.0387414 , 0.60514355]], dtype=float32)

time = 513	action = 1	current_phase = 1	next_phase = 0	reward = -1.602265	array([[1.0548396, 0.5980714]], dtype=float32)

time = 521	action = 0	current_phase = 0	next_phase = 1	reward = 0.886255	array([[-0.9879643 , -0.15230405]], dtype=float32)

time = 526	action = 1	current_phase = 0	next_phase = 1	reward = -1.776830	array([[-0.9812272 , -0.14492935]], dtype=float32)

time = 534	action = 0	current_phase = 1	next_phase = 0	reward = -0.814145	array([[1.0383614, 0.5874498]], dtype=float32)

time = 539	action = 0	current_phase = 1	next_phase = 0	reward = -2.288800	array([[1.0398173, 0.6034713]], dtype=float32)

time = 544	action = 1	current_phase = 1	next_phase = 0	reward = -1.372362	array([[1.0526928, 0.5993707]], dtype=float32)

time = 552	action = 0	current_phase = 0	next_phase = 1	reward = 0.867619	array([[-0.9871993 , -0.16277084]], dtype=float32)

time = 557	action = 1	current_phase = 0	next_phase = 1	reward = -1.659149	array([[-0.98864627, -0.13896146]], dtype=float32)

time = 565	action = 0	current_phase = 1	next_phase = 0	reward = -1.441870	array([[1.0457295, 0.5902684]], dtype=float32)

time = 570	action = 0	current_phase = 1	next_phase = 0	reward = -2.537886	array([[1.0393641 , 0.60238075]], dtype=float32)

time = 575	action = 1	current_phase = 1	next_phase = 0	reward = -1.547030	array([[1.0538445, 0.5966168]], dtype=float32)

time = 583	action = 0	current_phase = 0	next_phase = 1	reward = 0.886868	array([[-0.97689146, -0.16040711]], dtype=float32)

time = 588	action = 1	current_phase = 0	next_phase = 1	reward = -1.113143	array([[-0.9883414 , -0.13940068]], dtype=float32)

time = 596	action = 0	current_phase = 1	next_phase = 0	reward = -1.264922	array([[1.0501403, 0.5940705]], dtype=float32)

time = 601	action = 0	current_phase = 1	next_phase = 0	reward = -2.322495	array([[1.0547286 , 0.58740544]], dtype=float32)

time = 606	action = 1	current_phase = 1	next_phase = 0	reward = -1.435771	array([[1.0468801, 0.6028279]], dtype=float32)

time = 614	action = 0	current_phase = 0	next_phase = 1	reward = 0.623522	array([[-0.96583164, -0.16599701]], dtype=float32)

time = 619	action = 1	current_phase = 0	next_phase = 1	reward = -0.664633	array([[-0.9875332, -0.1381912]], dtype=float32)

time = 627	action = 0	current_phase = 1	next_phase = 0	reward = -1.190834	array([[1.0554267 , 0.58811134]], dtype=float32)

time = 632	action = 0	current_phase = 1	next_phase = 0	reward = -2.105441	array([[1.0521797, 0.5815779]], dtype=float32)

time = 637	action = 1	current_phase = 1	next_phase = 0	reward = -0.862138	array([[1.038985 , 0.5972221]], dtype=float32)

time = 645	action = 0	current_phase = 0	next_phase = 1	reward = 0.321135	array([[-0.9758525 , -0.16930702]], dtype=float32)

time = 650	action = 1	current_phase = 0	next_phase = 1	reward = -1.074322	array([[-0.9920503 , -0.14849293]], dtype=float32)

time = 658	action = 0	current_phase = 1	next_phase = 0	reward = -1.328998	array([[1.0468532 , 0.58950436]], dtype=float32)

time = 663	action = 0	current_phase = 1	next_phase = 0	reward = -2.155257	array([[1.0578247 , 0.58051586]], dtype=float32)

Terminal occured. Episode end.

Simulation ended at time: 669.00

Reason: TraCI requested termination.

Performance: 

 Duration: 15433ms

 Real time factor: 43.3487

 UPS: 1288.667142

Vehicles: 

 Inserted: 314

 Running: 32

 Waiting: 0



DijkstraRouter answered 314 queries and explored 2.29 edges on average.

DijkstraRouter spent 0ms answering queries (0.00ms on average).

Could not connect to TraCI server at localhost:52531 [Errno 111] Connection refused

 Retrying in 1 seconds

Loading configuration... done.

***Starting server on port 52531 ***

Loading net-file from '/home/soup/IntelliLight/data/one_run/cross.net.xml'... done (3ms).

Warning: Missing yellow phase in tlLogic 'node0', program '0' for tl-index 5 when switching to phase 1

Loading additional-files from '/home/soup/IntelliLight/data/one_run/cross.tls.switch.xml'... done (1ms).

Loading additional-files from '/home/soup/IntelliLight/data/one_run/cross.car.type.xml'... done (0ms).

Loading done.

Simulation started with time: 0.00

time = 20	action = 0	current_phase = 0	next_phase = 1	reward = -0.096708	array([[-0.97998637, -0.15543285]], dtype=float32)

time = 25	action = 0	current_phase = 0	next_phase = 1	reward = -0.777623	array([[-0.98078287, -0.15541553]], dtype=float32)

time = 30	action = 1	current_phase = 0	next_phase = 1	reward = -2.600898	array([[-0.9753964, -0.1559512]], dtype=float32)

time = 38	action = 0	current_phase = 1	next_phase = 0	reward = -1.414874	array([[1.0384277 , 0.58529764]], dtype=float32)

time = 43	action = 0	current_phase = 1	next_phase = 0	reward = -1.970617	array([[1.059602  , 0.58407116]], dtype=float32)

time = 48	action = 1	current_phase = 1	next_phase = 0	reward = -0.918527	array([[1.0393659, 0.5964626]], dtype=float32)

time = 56	action = 0	current_phase = 0	next_phase = 1	reward = -0.223930	array([[-0.98491657, -0.15545832]], dtype=float32)

time = 61	action = 1	current_phase = 0	next_phase = 1	reward = -2.515687	array([[-0.9801806 , -0.16042405]], dtype=float32)

time = 69	action = 0	current_phase = 1	next_phase = 0	reward = -1.515878	array([[1.0386387 , 0.58529645]], dtype=float32)

time = 74	action = 0	current_phase = 1	next_phase = 0	reward = -2.084352	array([[1.0579385, 0.5850177]], dtype=float32)

time = 79	action = 1	current_phase = 1	next_phase = 0	reward = -1.422369	array([[1.0390482 , 0.59569347]], dtype=float32)

time = 87	action = 0	current_phase = 0	next_phase = 1	reward = -0.137895	array([[-0.96711195, -0.1395403 ]], dtype=float32)

time = 92	action = 1	current_phase = 0	next_phase = 1	reward = -2.619377	array([[-0.97964656, -0.15912418]], dtype=float32)

time = 100	action = 0	current_phase = 1	next_phase = 0	reward = -1.679776	array([[1.0396085 , 0.58460665]], dtype=float32)

time = 105	action = 0	current_phase = 1	next_phase = 0	reward = -2.254294	array([[1.060136  , 0.58378965]], dtype=float32)

time = 110	action = 1	current_phase = 1	next_phase = 0	reward = -1.976023	array([[1.0404307, 0.5948055]], dtype=float32)

time = 118	action = 0	current_phase = 0	next_phase = 1	reward = 0.106145	array([[-0.9927279 , -0.15705569]], dtype=float32)

time = 123	action = 1	current_phase = 0	next_phase = 1	reward = -2.756031	array([[-0.9799321 , -0.15957856]], dtype=float32)

time = 131	action = 0	current_phase = 1	next_phase = 0	reward = -1.803679	array([[1.0540769 , 0.57829344]], dtype=float32)

time = 136	action = 0	current_phase = 1	next_phase = 0	reward = -2.382397	array([[1.0448611, 0.5937927]], dtype=float32)

time = 141	action = 1	current_phase = 1	next_phase = 0	reward = -2.070182	array([[1.0535623 , 0.59017557]], dtype=float32)

time = 149	action = 0	current_phase = 0	next_phase = 1	reward = -0.262854	array([[-0.9810853 , -0.16395038]], dtype=float32)

time = 154	action = 1	current_phase = 0	next_phase = 1	reward = -2.941279	array([[-0.97902584, -0.1590128 ]], dtype=float32)

time = 162	action = 0	current_phase = 1	next_phase = 0	reward = -1.843216	array([[1.0566915 , 0.57962227]], dtype=float32)

time = 167	action = 0	current_phase = 1	next_phase = 0	reward = -2.448476	array([[1.0462447, 0.5955827]], dtype=float32)

time = 172	action = 1	current_phase = 1	next_phase = 0	reward = -2.086334	array([[1.0582114 , 0.58894706]], dtype=float32)

time = 180	action = 0	current_phase = 0	next_phase = 1	reward = -0.381763	array([[-0.9784612 , -0.15397498]], dtype=float32)

time = 185	action = 1	current_phase = 0	next_phase = 1	reward = -2.045218	array([[-0.97680265, -0.14980787]], dtype=float32)

time = 193	action = 0	current_phase = 1	next_phase = 0	reward = -1.267744	array([[1.054556  , 0.57910275]], dtype=float32)

time = 198	action = 0	current_phase = 1	next_phase = 0	reward = -2.004059	array([[1.0303565, 0.5993509]], dtype=float32)

time = 203	action = 1	current_phase = 1	next_phase = 0	reward = -0.956189	array([[1.0542306 , 0.59712875]], dtype=float32)

time = 211	action = 0	current_phase = 0	next_phase = 1	reward = -0.428172	array([[-0.97161835, -0.12651397]], dtype=float32)

time = 216	action = 1	current_phase = 0	next_phase = 1	reward = -2.068316	array([[-0.977926  , -0.11881757]], dtype=float32)

time = 224	action = 0	current_phase = 1	next_phase = 0	reward = -1.442308	array([[1.0406857 , 0.58461064]], dtype=float32)

time = 229	action = 0	current_phase = 1	next_phase = 0	reward = -2.033019	array([[1.0316263 , 0.60163176]], dtype=float32)

time = 234	action = 1	current_phase = 1	next_phase = 0	reward = -1.304524	array([[1.0540086 , 0.59762335]], dtype=float32)

time = 242	action = 0	current_phase = 0	next_phase = 1	reward = 1.347837	array([[-0.9843015, -0.1424256]], dtype=float32)

time = 247	action = 1	current_phase = 0	next_phase = 1	reward = -2.390992	array([[-0.9885641, -0.1382949]], dtype=float32)

time = 255	action = 0	current_phase = 1	next_phase = 0	reward = -1.554593	array([[1.037813  , 0.58736783]], dtype=float32)

time = 260	action = 0	current_phase = 1	next_phase = 0	reward = -1.933803	array([[1.0312154, 0.6020055]], dtype=float32)

time = 265	action = 1	current_phase = 1	next_phase = 0	reward = -1.639341	array([[1.0551546 , 0.59943837]], dtype=float32)

time = 273	action = 0	current_phase = 0	next_phase = 1	reward = 1.371643	array([[-0.97670627, -0.15966515]], dtype=float32)

time = 278	action = 1	current_phase = 0	next_phase = 1	reward = -2.526145	array([[-0.9879626, -0.1398766]], dtype=float32)

time = 286	action = 0	current_phase = 1	next_phase = 0	reward = -1.724018	array([[1.0387263, 0.6006255]], dtype=float32)

time = 291	action = 0	current_phase = 1	next_phase = 0	reward = -1.769149	array([[1.0454041 , 0.59522986]], dtype=float32)

time = 296	action = 1	current_phase = 1	next_phase = 0	reward = -1.836346	array([[1.0476675, 0.6091793]], dtype=float32)

time = 304	action = 0	current_phase = 0	next_phase = 1	reward = 1.095136	array([[-0.973314  , -0.14945334]], dtype=float32)

time = 309	action = 1	current_phase = 0	next_phase = 1	reward = -1.395267	array([[-0.98834497, -0.13856766]], dtype=float32)

time = 317	action = 0	current_phase = 1	next_phase = 0	reward = -1.154625	array([[1.0474002, 0.5868148]], dtype=float32)

time = 322	action = 0	current_phase = 1	next_phase = 0	reward = -1.213580	array([[1.0533195 , 0.58193076]], dtype=float32)

time = 327	action = 1	current_phase = 1	next_phase = 0	reward = -1.174499	array([[1.0490261 , 0.59883875]], dtype=float32)

time = 335	action = 0	current_phase = 0	next_phase = 1	reward = 0.188102	array([[-0.987257  , -0.16429031]], dtype=float32)

time = 340	action = 1	current_phase = 0	next_phase = 1	reward = -1.469070	array([[-0.98845255, -0.14033228]], dtype=float32)

time = 348	action = 0	current_phase = 1	next_phase = 0	reward = -1.256040	array([[1.0443894 , 0.58545697]], dtype=float32)

time = 353	action = 0	current_phase = 1	next_phase = 0	reward = -1.276626	array([[1.0593989, 0.5836487]], dtype=float32)

time = 358	action = 1	current_phase = 1	next_phase = 0	reward = -1.285345	array([[1.0474927, 0.5978457]], dtype=float32)

time = 366	action = 0	current_phase = 0	next_phase = 1	reward = 0.778255	array([[-0.9866899, -0.1645011]], dtype=float32)

time = 371	action = 1	current_phase = 0	next_phase = 1	reward = -2.187370	array([[-0.9755108 , -0.16983567]], dtype=float32)

time = 379	action = 0	current_phase = 1	next_phase = 0	reward = -1.444377	array([[1.0381857, 0.5872982]], dtype=float32)

time = 384	action = 0	current_phase = 1	next_phase = 0	reward = -1.394199	array([[1.0597308, 0.584537 ]], dtype=float32)

time = 389	action = 1	current_phase = 1	next_phase = 0	reward = -1.403631	array([[1.0478579 , 0.59750915]], dtype=float32)

time = 397	action = 0	current_phase = 0	next_phase = 1	reward = 0.796924	array([[-0.99476117, -0.14767945]], dtype=float32)

time = 402	action = 1	current_phase = 0	next_phase = 1	reward = -2.177719	array([[-0.9761967 , -0.17144975]], dtype=float32)

time = 410	action = 0	current_phase = 1	next_phase = 0	reward = -0.927866	array([[1.0376886 , 0.58792007]], dtype=float32)

time = 415	action = 0	current_phase = 1	next_phase = 0	reward = -2.082347	array([[1.0683768 , 0.58517843]], dtype=float32)

time = 420	action = 1	current_phase = 1	next_phase = 0	reward = -2.308370	array([[1.0463167 , 0.59753805]], dtype=float32)

time = 428	action = 0	current_phase = 0	next_phase = 1	reward = 1.455590	array([[-0.99426925, -0.14934278]], dtype=float32)

time = 433	action = 1	current_phase = 0	next_phase = 1	reward = -2.428492	array([[-0.9759849 , -0.16886944]], dtype=float32)

time = 441	action = 0	current_phase = 1	next_phase = 0	reward = -1.056975	array([[1.0531098 , 0.58219445]], dtype=float32)

time = 446	action = 0	current_phase = 1	next_phase = 0	reward = -2.304111	array([[1.0543716 , 0.59682924]], dtype=float32)

time = 451	action = 1	current_phase = 1	next_phase = 0	reward = -1.774803	array([[1.0543302, 0.5901322]], dtype=float32)

time = 459	action = 0	current_phase = 0	next_phase = 1	reward = 0.863205	array([[-0.97602904, -0.15946168]], dtype=float32)

time = 464	action = 1	current_phase = 0	next_phase = 1	reward = -1.844085	array([[-0.9759921 , -0.17053628]], dtype=float32)

time = 472	action = 0	current_phase = 1	next_phase = 0	reward = -0.895215	array([[1.06646   , 0.57723624]], dtype=float32)

time = 477	action = 0	current_phase = 1	next_phase = 0	reward = -2.161007	array([[1.0453757, 0.6026894]], dtype=float32)

time = 482	action = 1	current_phase = 1	next_phase = 0	reward = -1.502184	array([[1.0464152, 0.5950866]], dtype=float32)

time = 490	action = 0	current_phase = 0	next_phase = 1	reward = 0.561652	array([[-0.9833629 , -0.15550089]], dtype=float32)

time = 495	action = 1	current_phase = 0	next_phase = 1	reward = -2.054140	array([[-0.9943472 , -0.15829058]], dtype=float32)

time = 503	action = 0	current_phase = 1	next_phase = 0	reward = -1.014462	array([[1.053309 , 0.5862777]], dtype=float32)

time = 508	action = 0	current_phase = 1	next_phase = 0	reward = -2.427522	array([[1.0387414 , 0.60514355]], dtype=float32)

time = 513	action = 1	current_phase = 1	next_phase = 0	reward = -1.602265	array([[1.0548396, 0.5980714]], dtype=float32)

time = 521	action = 0	current_phase = 0	next_phase = 1	reward = 0.886255	array([[-0.9879643 , -0.15230405]], dtype=float32)

time = 526	action = 1	current_phase = 0	next_phase = 1	reward = -1.776830	array([[-0.9812272 , -0.14492935]], dtype=float32)

time = 534	action = 0	current_phase = 1	next_phase = 0	reward = -0.814145	array([[1.0383614, 0.5874498]], dtype=float32)

time = 539	action = 0	current_phase = 1	next_phase = 0	reward = -2.288800	array([[1.0398173, 0.6034713]], dtype=float32)

time = 544	action = 1	current_phase = 1	next_phase = 0	reward = -1.372362	array([[1.0526928, 0.5993707]], dtype=float32)

time = 552	action = 0	current_phase = 0	next_phase = 1	reward = 0.867619	array([[-0.9871993 , -0.16277084]], dtype=float32)

time = 557	action = 1	current_phase = 0	next_phase = 1	reward = -1.659149	array([[-0.98864627, -0.13896146]], dtype=float32)

time = 565	action = 0	current_phase = 1	next_phase = 0	reward = -1.441870	array([[1.0457295, 0.5902684]], dtype=float32)

time = 570	action = 0	current_phase = 1	next_phase = 0	reward = -2.537886	array([[1.0393641 , 0.60238075]], dtype=float32)

time = 575	action = 1	current_phase = 1	next_phase = 0	reward = -1.547030	array([[1.0538445, 0.5966168]], dtype=float32)

time = 583	action = 0	current_phase = 0	next_phase = 1	reward = 0.886868	array([[-0.97689146, -0.16040711]], dtype=float32)

time = 588	action = 1	current_phase = 0	next_phase = 1	reward = -1.113143	array([[-0.9883414 , -0.13940068]], dtype=float32)

time = 596	action = 0	current_phase = 1	next_phase = 0	reward = -1.264922	array([[1.0501403, 0.5940705]], dtype=float32)

time = 601	action = 0	current_phase = 1	next_phase = 0	reward = -2.322495	array([[1.0547286 , 0.58740544]], dtype=float32)

time = 606	action = 1	current_phase = 1	next_phase = 0	reward = -1.435771	array([[1.0468801, 0.6028279]], dtype=float32)

time = 614	action = 0	current_phase = 0	next_phase = 1	reward = 0.623522	array([[-0.96583164, -0.16599701]], dtype=float32)

time = 619	action = 1	current_phase = 0	next_phase = 1	reward = -0.664633	array([[-0.9875332, -0.1381912]], dtype=float32)

time = 627	action = 0	current_phase = 1	next_phase = 0	reward = -1.190834	array([[1.0554267 , 0.58811134]], dtype=float32)

time = 632	action = 0	current_phase = 1	next_phase = 0	reward = -2.105441	array([[1.0521797, 0.5815779]], dtype=float32)

time = 637	action = 1	current_phase = 1	next_phase = 0	reward = -0.862138	array([[1.038985 , 0.5972221]], dtype=float32)

time = 645	action = 0	current_phase = 0	next_phase = 1	reward = 0.321135	array([[-0.9758525 , -0.16930702]], dtype=float32)

time = 650	action = 1	current_phase = 0	next_phase = 1	reward = -1.074322	array([[-0.9920503 , -0.14849293]], dtype=float32)

time = 658	action = 0	current_phase = 1	next_phase = 0	reward = -1.328998	array([[1.0468532 , 0.58950436]], dtype=float32)

time = 663	action = 0	current_phase = 1	next_phase = 0	reward = -2.155257	array([[1.0578247 , 0.58051586]], dtype=float32)

Terminal occured. Episode end.

Simulation ended at time: 669.00

Reason: TraCI requested termination.

Performance: 

 Duration: 18468ms

 Real time factor: 36.2248

 UPS: 1076.889755

Vehicles: 

 Inserted: 314

 Running: 32

 Waiting: 0



DijkstraRouter answered 314 queries and explored 2.29 edges on average.

DijkstraRouter spent 0ms answering queries (0.00ms on average).

Could not connect to TraCI server at localhost:33563 [Errno 111] Connection refused

 Retrying in 1 seconds

Loading configuration... done.

***Starting server on port 33563 ***

Loading net-file from '/home/soup/IntelliLight/data/one_run/cross.net.xml'... done (3ms).

Warning: Missing yellow phase in tlLogic 'node0', program '0' for tl-index 5 when switching to phase 1

Loading additional-files from '/home/soup/IntelliLight/data/one_run/cross.tls.switch.xml'... done (1ms).

Loading additional-files from '/home/soup/IntelliLight/data/one_run/cross.car.type.xml'... done (0ms).

Loading done.

Simulation started with time: 0.00

time = 20	action = 0	current_phase = 0	next_phase = 1	reward = -0.096708	array([[-0.97998637, -0.15543285]], dtype=float32)

time = 25	action = 0	current_phase = 0	next_phase = 1	reward = -0.777623	array([[-0.98078287, -0.15541553]], dtype=float32)

time = 30	action = 1	current_phase = 0	next_phase = 1	reward = -2.600898	array([[-0.9753964, -0.1559512]], dtype=float32)

time = 38	action = 0	current_phase = 1	next_phase = 0	reward = -1.414874	array([[1.0384277 , 0.58529764]], dtype=float32)

time = 43	action = 0	current_phase = 1	next_phase = 0	reward = -1.970617	array([[1.059602  , 0.58407116]], dtype=float32)

time = 48	action = 1	current_phase = 1	next_phase = 0	reward = -0.918527	array([[1.0393659, 0.5964626]], dtype=float32)

time = 56	action = 0	current_phase = 0	next_phase = 1	reward = -0.223930	array([[-0.98491657, -0.15545832]], dtype=float32)

time = 61	action = 1	current_phase = 0	next_phase = 1	reward = -2.515687	array([[-0.9801806 , -0.16042405]], dtype=float32)

time = 69	action = 0	current_phase = 1	next_phase = 0	reward = -1.515878	array([[1.0386387 , 0.58529645]], dtype=float32)

time = 74	action = 0	current_phase = 1	next_phase = 0	reward = -2.084352	array([[1.0579385, 0.5850177]], dtype=float32)

time = 79	action = 1	current_phase = 1	next_phase = 0	reward = -1.422369	array([[1.0390482 , 0.59569347]], dtype=float32)

time = 87	action = 0	current_phase = 0	next_phase = 1	reward = -0.137895	array([[-0.96711195, -0.1395403 ]], dtype=float32)

time = 92	action = 1	current_phase = 0	next_phase = 1	reward = -2.619377	array([[-0.97964656, -0.15912418]], dtype=float32)

time = 100	action = 0	current_phase = 1	next_phase = 0	reward = -1.679776	array([[1.0396085 , 0.58460665]], dtype=float32)

time = 105	action = 0	current_phase = 1	next_phase = 0	reward = -2.254294	array([[1.060136  , 0.58378965]], dtype=float32)

time = 110	action = 1	current_phase = 1	next_phase = 0	reward = -1.976023	array([[1.0404307, 0.5948055]], dtype=float32)

time = 118	action = 0	current_phase = 0	next_phase = 1	reward = 0.106145	array([[-0.9927279 , -0.15705569]], dtype=float32)

time = 123	action = 1	current_phase = 0	next_phase = 1	reward = -2.756031	array([[-0.9799321 , -0.15957856]], dtype=float32)

time = 131	action = 0	current_phase = 1	next_phase = 0	reward = -1.803679	array([[1.0540769 , 0.57829344]], dtype=float32)

time = 136	action = 0	current_phase = 1	next_phase = 0	reward = -2.382397	array([[1.0448611, 0.5937927]], dtype=float32)

time = 141	action = 1	current_phase = 1	next_phase = 0	reward = -2.070182	array([[1.0535623 , 0.59017557]], dtype=float32)

time = 149	action = 0	current_phase = 0	next_phase = 1	reward = -0.262854	array([[-0.9810853 , -0.16395038]], dtype=float32)

time = 154	action = 1	current_phase = 0	next_phase = 1	reward = -2.941279	array([[-0.97902584, -0.1590128 ]], dtype=float32)

time = 162	action = 0	current_phase = 1	next_phase = 0	reward = -1.843216	array([[1.0566915 , 0.57962227]], dtype=float32)

time = 167	action = 0	current_phase = 1	next_phase = 0	reward = -2.448476	array([[1.0462447, 0.5955827]], dtype=float32)

time = 172	action = 1	current_phase = 1	next_phase = 0	reward = -2.086334	array([[1.0582114 , 0.58894706]], dtype=float32)

time = 180	action = 0	current_phase = 0	next_phase = 1	reward = -0.381763	array([[-0.9784612 , -0.15397498]], dtype=float32)

time = 185	action = 1	current_phase = 0	next_phase = 1	reward = -2.045218	array([[-0.97680265, -0.14980787]], dtype=float32)

time = 193	action = 0	current_phase = 1	next_phase = 0	reward = -1.267744	array([[1.054556  , 0.57910275]], dtype=float32)

time = 198	action = 0	current_phase = 1	next_phase = 0	reward = -2.004059	array([[1.0303565, 0.5993509]], dtype=float32)

time = 203	action = 1	current_phase = 1	next_phase = 0	reward = -0.956189	array([[1.0542306 , 0.59712875]], dtype=float32)

time = 211	action = 0	current_phase = 0	next_phase = 1	reward = -0.428172	array([[-0.97161835, -0.12651397]], dtype=float32)

time = 216	action = 1	current_phase = 0	next_phase = 1	reward = -2.068316	array([[-0.977926  , -0.11881757]], dtype=float32)

time = 224	action = 0	current_phase = 1	next_phase = 0	reward = -1.442308	array([[1.0406857 , 0.58461064]], dtype=float32)

time = 229	action = 0	current_phase = 1	next_phase = 0	reward = -2.033019	array([[1.0316263 , 0.60163176]], dtype=float32)

time = 234	action = 1	current_phase = 1	next_phase = 0	reward = -1.304524	array([[1.0540086 , 0.59762335]], dtype=float32)

time = 242	action = 0	current_phase = 0	next_phase = 1	reward = 1.347837	array([[-0.9843015, -0.1424256]], dtype=float32)

time = 247	action = 1	current_phase = 0	next_phase = 1	reward = -2.390992	array([[-0.9885641, -0.1382949]], dtype=float32)

time = 255	action = 0	current_phase = 1	next_phase = 0	reward = -1.554593	array([[1.037813  , 0.58736783]], dtype=float32)

time = 260	action = 0	current_phase = 1	next_phase = 0	reward = -1.933803	array([[1.0312154, 0.6020055]], dtype=float32)

time = 265	action = 1	current_phase = 1	next_phase = 0	reward = -1.639341	array([[1.0551546 , 0.59943837]], dtype=float32)

time = 273	action = 0	current_phase = 0	next_phase = 1	reward = 1.371643	array([[-0.97670627, -0.15966515]], dtype=float32)

time = 278	action = 1	current_phase = 0	next_phase = 1	reward = -2.526145	array([[-0.9879626, -0.1398766]], dtype=float32)

time = 286	action = 0	current_phase = 1	next_phase = 0	reward = -1.724018	array([[1.0387263, 0.6006255]], dtype=float32)

time = 291	action = 0	current_phase = 1	next_phase = 0	reward = -1.769149	array([[1.0454041 , 0.59522986]], dtype=float32)

time = 296	action = 1	current_phase = 1	next_phase = 0	reward = -1.836346	array([[1.0476675, 0.6091793]], dtype=float32)

time = 304	action = 0	current_phase = 0	next_phase = 1	reward = 1.095136	array([[-0.973314  , -0.14945334]], dtype=float32)

time = 309	action = 1	current_phase = 0	next_phase = 1	reward = -1.395267	array([[-0.98834497, -0.13856766]], dtype=float32)

time = 317	action = 0	current_phase = 1	next_phase = 0	reward = -1.154625	array([[1.0474002, 0.5868148]], dtype=float32)

time = 322	action = 0	current_phase = 1	next_phase = 0	reward = -1.213580	array([[1.0533195 , 0.58193076]], dtype=float32)

time = 327	action = 1	current_phase = 1	next_phase = 0	reward = -1.174499	array([[1.0490261 , 0.59883875]], dtype=float32)

time = 335	action = 0	current_phase = 0	next_phase = 1	reward = 0.188102	array([[-0.987257  , -0.16429031]], dtype=float32)

time = 340	action = 1	current_phase = 0	next_phase = 1	reward = -1.469070	array([[-0.98845255, -0.14033228]], dtype=float32)

time = 348	action = 0	current_phase = 1	next_phase = 0	reward = -1.256040	array([[1.0443894 , 0.58545697]], dtype=float32)

time = 353	action = 0	current_phase = 1	next_phase = 0	reward = -1.276626	array([[1.0593989, 0.5836487]], dtype=float32)

time = 358	action = 1	current_phase = 1	next_phase = 0	reward = -1.285345	array([[1.0474927, 0.5978457]], dtype=float32)

time = 366	action = 0	current_phase = 0	next_phase = 1	reward = 0.778255	array([[-0.9866899, -0.1645011]], dtype=float32)

time = 371	action = 1	current_phase = 0	next_phase = 1	reward = -2.187370	array([[-0.9755108 , -0.16983567]], dtype=float32)

time = 379	action = 0	current_phase = 1	next_phase = 0	reward = -1.444377	array([[1.0381857, 0.5872982]], dtype=float32)

time = 384	action = 0	current_phase = 1	next_phase = 0	reward = -1.394199	array([[1.0597308, 0.584537 ]], dtype=float32)

time = 389	action = 1	current_phase = 1	next_phase = 0	reward = -1.403631	array([[1.0478579 , 0.59750915]], dtype=float32)

time = 397	action = 0	current_phase = 0	next_phase = 1	reward = 0.796924	array([[-0.99476117, -0.14767945]], dtype=float32)

time = 402	action = 1	current_phase = 0	next_phase = 1	reward = -2.177719	array([[-0.9761967 , -0.17144975]], dtype=float32)

time = 410	action = 0	current_phase = 1	next_phase = 0	reward = -0.927866	array([[1.0376886 , 0.58792007]], dtype=float32)

time = 415	action = 0	current_phase = 1	next_phase = 0	reward = -2.082347	array([[1.0683768 , 0.58517843]], dtype=float32)

time = 420	action = 1	current_phase = 1	next_phase = 0	reward = -2.308370	array([[1.0463167 , 0.59753805]], dtype=float32)

time = 428	action = 0	current_phase = 0	next_phase = 1	reward = 1.455590	array([[-0.99426925, -0.14934278]], dtype=float32)

time = 433	action = 1	current_phase = 0	next_phase = 1	reward = -2.428492	array([[-0.9759849 , -0.16886944]], dtype=float32)

time = 441	action = 0	current_phase = 1	next_phase = 0	reward = -1.056975	array([[1.0531098 , 0.58219445]], dtype=float32)

time = 446	action = 0	current_phase = 1	next_phase = 0	reward = -2.304111	array([[1.0543716 , 0.59682924]], dtype=float32)

time = 451	action = 1	current_phase = 1	next_phase = 0	reward = -1.774803	array([[1.0543302, 0.5901322]], dtype=float32)

time = 459	action = 0	current_phase = 0	next_phase = 1	reward = 0.863205	array([[-0.97602904, -0.15946168]], dtype=float32)

time = 464	action = 1	current_phase = 0	next_phase = 1	reward = -1.844085	array([[-0.9759921 , -0.17053628]], dtype=float32)

time = 472	action = 0	current_phase = 1	next_phase = 0	reward = -0.895215	array([[1.06646   , 0.57723624]], dtype=float32)

time = 477	action = 0	current_phase = 1	next_phase = 0	reward = -2.161007	array([[1.0453757, 0.6026894]], dtype=float32)

time = 482	action = 1	current_phase = 1	next_phase = 0	reward = -1.502184	array([[1.0464152, 0.5950866]], dtype=float32)

time = 490	action = 0	current_phase = 0	next_phase = 1	reward = 0.561652	array([[-0.9833629 , -0.15550089]], dtype=float32)

time = 495	action = 1	current_phase = 0	next_phase = 1	reward = -2.054140	array([[-0.9943472 , -0.15829058]], dtype=float32)

time = 503	action = 0	current_phase = 1	next_phase = 0	reward = -1.014462	array([[1.053309 , 0.5862777]], dtype=float32)

time = 508	action = 0	current_phase = 1	next_phase = 0	reward = -2.427522	array([[1.0387414 , 0.60514355]], dtype=float32)

time = 513	action = 1	current_phase = 1	next_phase = 0	reward = -1.602265	array([[1.0548396, 0.5980714]], dtype=float32)

time = 521	action = 0	current_phase = 0	next_phase = 1	reward = 0.886255	array([[-0.9879643 , -0.15230405]], dtype=float32)

time = 526	action = 1	current_phase = 0	next_phase = 1	reward = -1.776830	array([[-0.9812272 , -0.14492935]], dtype=float32)

time = 534	action = 0	current_phase = 1	next_phase = 0	reward = -0.814145	array([[1.0383614, 0.5874498]], dtype=float32)

time = 539	action = 0	current_phase = 1	next_phase = 0	reward = -2.288800	array([[1.0398173, 0.6034713]], dtype=float32)

time = 544	action = 1	current_phase = 1	next_phase = 0	reward = -1.372362	array([[1.0526928, 0.5993707]], dtype=float32)

time = 552	action = 0	current_phase = 0	next_phase = 1	reward = 0.867619	array([[-0.9871993 , -0.16277084]], dtype=float32)

time = 557	action = 1	current_phase = 0	next_phase = 1	reward = -1.659149	array([[-0.98864627, -0.13896146]], dtype=float32)

time = 565	action = 0	current_phase = 1	next_phase = 0	reward = -1.441870	array([[1.0457295, 0.5902684]], dtype=float32)

time = 570	action = 0	current_phase = 1	next_phase = 0	reward = -2.537886	array([[1.0393641 , 0.60238075]], dtype=float32)

time = 575	action = 1	current_phase = 1	next_phase = 0	reward = -1.547030	array([[1.0538445, 0.5966168]], dtype=float32)

time = 583	action = 0	current_phase = 0	next_phase = 1	reward = 0.886868	array([[-0.97689146, -0.16040711]], dtype=float32)

time = 588	action = 1	current_phase = 0	next_phase = 1	reward = -1.113143	array([[-0.9883414 , -0.13940068]], dtype=float32)

time = 596	action = 0	current_phase = 1	next_phase = 0	reward = -1.264922	array([[1.0501403, 0.5940705]], dtype=float32)

time = 601	action = 0	current_phase = 1	next_phase = 0	reward = -2.322495	array([[1.0547286 , 0.58740544]], dtype=float32)

time = 606	action = 1	current_phase = 1	next_phase = 0	reward = -1.435771	array([[1.0468801, 0.6028279]], dtype=float32)

time = 614	action = 0	current_phase = 0	next_phase = 1	reward = 0.623522	array([[-0.96583164, -0.16599701]], dtype=float32)

time = 619	action = 1	current_phase = 0	next_phase = 1	reward = -0.664633	array([[-0.9875332, -0.1381912]], dtype=float32)

time = 627	action = 0	current_phase = 1	next_phase = 0	reward = -1.190834	array([[1.0554267 , 0.58811134]], dtype=float32)

time = 632	action = 0	current_phase = 1	next_phase = 0	reward = -2.105441	array([[1.0521797, 0.5815779]], dtype=float32)

time = 637	action = 1	current_phase = 1	next_phase = 0	reward = -0.862138	array([[1.038985 , 0.5972221]], dtype=float32)

time = 645	action = 0	current_phase = 0	next_phase = 1	reward = 0.321135	array([[-0.9758525 , -0.16930702]], dtype=float32)

time = 650	action = 1	current_phase = 0	next_phase = 1	reward = -1.074322	array([[-0.9920503 , -0.14849293]], dtype=float32)

time = 658	action = 0	current_phase = 1	next_phase = 0	reward = -1.328998	array([[1.0468532 , 0.58950436]], dtype=float32)

time = 663	action = 0	current_phase = 1	next_phase = 0	reward = -2.155257	array([[1.0578247 , 0.58051586]], dtype=float32)

Terminal occured. Episode end.

Simulation ended at time: 669.00

Reason: TraCI requested termination.

Performance: 

 Duration: 15210ms

 Real time factor: 43.9842

 UPS: 1307.560815

Vehicles: 

 Inserted: 314

 Running: 32

 Waiting: 0



DijkstraRouter answered 314 queries and explored 2.29 edges on average.

DijkstraRouter spent 1ms answering queries (0.00ms on average).

Could not connect to TraCI server at localhost:55797 [Errno 111] Connection refused

 Retrying in 1 seconds

Loading configuration... done.

***Starting server on port 55797 ***

Loading net-file from '/home/soup/IntelliLight/data/one_run/cross.net.xml'... done (1ms).

Warning: Missing yellow phase in tlLogic 'node0', program '0' for tl-index 5 when switching to phase 1

Loading additional-files from '/home/soup/IntelliLight/data/one_run/cross.tls.switch.xml'... done (0ms).

Loading additional-files from '/home/soup/IntelliLight/data/one_run/cross.car.type.xml'... done (0ms).

Loading done.

Simulation started with time: 0.00

time = 20	action = 0	current_phase = 0	next_phase = 1	reward = -0.096708	array([[-0.97998637, -0.15543285]], dtype=float32)

time = 25	action = 0	current_phase = 0	next_phase = 1	reward = -0.777623	array([[-0.98078287, -0.15541553]], dtype=float32)

time = 30	action = 1	current_phase = 0	next_phase = 1	reward = -2.600898	array([[-0.9753964, -0.1559512]], dtype=float32)

time = 38	action = 0	current_phase = 1	next_phase = 0	reward = -1.414874	array([[1.0384277 , 0.58529764]], dtype=float32)

time = 43	action = 0	current_phase = 1	next_phase = 0	reward = -1.970617	array([[1.059602  , 0.58407116]], dtype=float32)

time = 48	action = 1	current_phase = 1	next_phase = 0	reward = -0.918527	array([[1.0393659, 0.5964626]], dtype=float32)

time = 56	action = 0	current_phase = 0	next_phase = 1	reward = -0.223930	array([[-0.98491657, -0.15545832]], dtype=float32)

time = 61	action = 1	current_phase = 0	next_phase = 1	reward = -2.515687	array([[-0.9801806 , -0.16042405]], dtype=float32)

time = 69	action = 0	current_phase = 1	next_phase = 0	reward = -1.515878	array([[1.0386387 , 0.58529645]], dtype=float32)

time = 74	action = 0	current_phase = 1	next_phase = 0	reward = -2.084352	array([[1.0579385, 0.5850177]], dtype=float32)

time = 79	action = 1	current_phase = 1	next_phase = 0	reward = -1.422369	array([[1.0390482 , 0.59569347]], dtype=float32)

time = 87	action = 0	current_phase = 0	next_phase = 1	reward = -0.137895	array([[-0.96711195, -0.1395403 ]], dtype=float32)

time = 92	action = 1	current_phase = 0	next_phase = 1	reward = -2.619377	array([[-0.97964656, -0.15912418]], dtype=float32)

time = 100	action = 0	current_phase = 1	next_phase = 0	reward = -1.679776	array([[1.0396085 , 0.58460665]], dtype=float32)

time = 105	action = 0	current_phase = 1	next_phase = 0	reward = -2.254294	array([[1.060136  , 0.58378965]], dtype=float32)

time = 110	action = 1	current_phase = 1	next_phase = 0	reward = -1.976023	array([[1.0404307, 0.5948055]], dtype=float32)

time = 118	action = 0	current_phase = 0	next_phase = 1	reward = 0.106145	array([[-0.9927279 , -0.15705569]], dtype=float32)

time = 123	action = 1	current_phase = 0	next_phase = 1	reward = -2.756031	array([[-0.9799321 , -0.15957856]], dtype=float32)

time = 131	action = 0	current_phase = 1	next_phase = 0	reward = -1.803679	array([[1.0540769 , 0.57829344]], dtype=float32)

time = 136	action = 0	current_phase = 1	next_phase = 0	reward = -2.382397	array([[1.0448611, 0.5937927]], dtype=float32)

time = 141	action = 1	current_phase = 1	next_phase = 0	reward = -2.070182	array([[1.0535623 , 0.59017557]], dtype=float32)

time = 149	action = 0	current_phase = 0	next_phase = 1	reward = -0.262854	array([[-0.9810853 , -0.16395038]], dtype=float32)

time = 154	action = 1	current_phase = 0	next_phase = 1	reward = -2.941279	array([[-0.97902584, -0.1590128 ]], dtype=float32)

time = 162	action = 0	current_phase = 1	next_phase = 0	reward = -1.843216	array([[1.0566915 , 0.57962227]], dtype=float32)

time = 167	action = 0	current_phase = 1	next_phase = 0	reward = -2.448476	array([[1.0462447, 0.5955827]], dtype=float32)

time = 172	action = 1	current_phase = 1	next_phase = 0	reward = -2.086334	array([[1.0582114 , 0.58894706]], dtype=float32)

time = 180	action = 0	current_phase = 0	next_phase = 1	reward = -0.381763	array([[-0.9784612 , -0.15397498]], dtype=float32)

time = 185	action = 1	current_phase = 0	next_phase = 1	reward = -2.045218	array([[-0.97680265, -0.14980787]], dtype=float32)

time = 193	action = 0	current_phase = 1	next_phase = 0	reward = -1.267744	array([[1.054556  , 0.57910275]], dtype=float32)

time = 198	action = 0	current_phase = 1	next_phase = 0	reward = -2.004059	array([[1.0303565, 0.5993509]], dtype=float32)

time = 203	action = 1	current_phase = 1	next_phase = 0	reward = -0.956189	array([[1.0542306 , 0.59712875]], dtype=float32)

time = 211	action = 0	current_phase = 0	next_phase = 1	reward = -0.428172	array([[-0.97161835, -0.12651397]], dtype=float32)

time = 216	action = 1	current_phase = 0	next_phase = 1	reward = -2.068316	array([[-0.977926  , -0.11881757]], dtype=float32)

time = 224	action = 0	current_phase = 1	next_phase = 0	reward = -1.442308	array([[1.0406857 , 0.58461064]], dtype=float32)

time = 229	action = 0	current_phase = 1	next_phase = 0	reward = -2.033019	array([[1.0316263 , 0.60163176]], dtype=float32)

time = 234	action = 1	current_phase = 1	next_phase = 0	reward = -1.304524	array([[1.0540086 , 0.59762335]], dtype=float32)

time = 242	action = 0	current_phase = 0	next_phase = 1	reward = 1.347837	array([[-0.9843015, -0.1424256]], dtype=float32)

time = 247	action = 1	current_phase = 0	next_phase = 1	reward = -2.390992	array([[-0.9885641, -0.1382949]], dtype=float32)

time = 255	action = 0	current_phase = 1	next_phase = 0	reward = -1.554593	array([[1.037813  , 0.58736783]], dtype=float32)

time = 260	action = 0	current_phase = 1	next_phase = 0	reward = -1.933803	array([[1.0312154, 0.6020055]], dtype=float32)

time = 265	action = 1	current_phase = 1	next_phase = 0	reward = -1.639341	array([[1.0551546 , 0.59943837]], dtype=float32)

time = 273	action = 0	current_phase = 0	next_phase = 1	reward = 1.371643	array([[-0.97670627, -0.15966515]], dtype=float32)

time = 278	action = 1	current_phase = 0	next_phase = 1	reward = -2.526145	array([[-0.9879626, -0.1398766]], dtype=float32)

time = 286	action = 0	current_phase = 1	next_phase = 0	reward = -1.724018	array([[1.0387263, 0.6006255]], dtype=float32)

time = 291	action = 0	current_phase = 1	next_phase = 0	reward = -1.769149	array([[1.0454041 , 0.59522986]], dtype=float32)

time = 296	action = 1	current_phase = 1	next_phase = 0	reward = -1.836346	array([[1.0476675, 0.6091793]], dtype=float32)

time = 304	action = 0	current_phase = 0	next_phase = 1	reward = 1.095136	array([[-0.973314  , -0.14945334]], dtype=float32)

time = 309	action = 1	current_phase = 0	next_phase = 1	reward = -1.395267	array([[-0.98834497, -0.13856766]], dtype=float32)

time = 317	action = 0	current_phase = 1	next_phase = 0	reward = -1.154625	array([[1.0474002, 0.5868148]], dtype=float32)

time = 322	action = 0	current_phase = 1	next_phase = 0	reward = -1.213580	array([[1.0533195 , 0.58193076]], dtype=float32)

time = 327	action = 1	current_phase = 1	next_phase = 0	reward = -1.174499	array([[1.0490261 , 0.59883875]], dtype=float32)

time = 335	action = 0	current_phase = 0	next_phase = 1	reward = 0.188102	array([[-0.987257  , -0.16429031]], dtype=float32)

time = 340	action = 1	current_phase = 0	next_phase = 1	reward = -1.469070	array([[-0.98845255, -0.14033228]], dtype=float32)

time = 348	action = 0	current_phase = 1	next_phase = 0	reward = -1.256040	array([[1.0443894 , 0.58545697]], dtype=float32)

time = 353	action = 0	current_phase = 1	next_phase = 0	reward = -1.276626	array([[1.0593989, 0.5836487]], dtype=float32)

time = 358	action = 1	current_phase = 1	next_phase = 0	reward = -1.285345	array([[1.0474927, 0.5978457]], dtype=float32)

time = 366	action = 0	current_phase = 0	next_phase = 1	reward = 0.778255	array([[-0.9866899, -0.1645011]], dtype=float32)

time = 371	action = 1	current_phase = 0	next_phase = 1	reward = -2.187370	array([[-0.9755108 , -0.16983567]], dtype=float32)

time = 379	action = 0	current_phase = 1	next_phase = 0	reward = -1.444377	array([[1.0381857, 0.5872982]], dtype=float32)

time = 384	action = 0	current_phase = 1	next_phase = 0	reward = -1.394199	array([[1.0597308, 0.584537 ]], dtype=float32)

time = 389	action = 1	current_phase = 1	next_phase = 0	reward = -1.403631	array([[1.0478579 , 0.59750915]], dtype=float32)

time = 397	action = 0	current_phase = 0	next_phase = 1	reward = 0.796924	array([[-0.99476117, -0.14767945]], dtype=float32)

time = 402	action = 1	current_phase = 0	next_phase = 1	reward = -2.177719	array([[-0.9761967 , -0.17144975]], dtype=float32)

time = 410	action = 0	current_phase = 1	next_phase = 0	reward = -0.927866	array([[1.0376886 , 0.58792007]], dtype=float32)

time = 415	action = 0	current_phase = 1	next_phase = 0	reward = -2.082347	array([[1.0683768 , 0.58517843]], dtype=float32)

time = 420	action = 1	current_phase = 1	next_phase = 0	reward = -2.308370	array([[1.0463167 , 0.59753805]], dtype=float32)

time = 428	action = 0	current_phase = 0	next_phase = 1	reward = 1.455590	array([[-0.99426925, -0.14934278]], dtype=float32)

time = 433	action = 1	current_phase = 0	next_phase = 1	reward = -2.428492	array([[-0.9759849 , -0.16886944]], dtype=float32)

time = 441	action = 0	current_phase = 1	next_phase = 0	reward = -1.056975	array([[1.0531098 , 0.58219445]], dtype=float32)

time = 446	action = 0	current_phase = 1	next_phase = 0	reward = -2.304111	array([[1.0543716 , 0.59682924]], dtype=float32)

time = 451	action = 1	current_phase = 1	next_phase = 0	reward = -1.774803	array([[1.0543302, 0.5901322]], dtype=float32)

time = 459	action = 0	current_phase = 0	next_phase = 1	reward = 0.863205	array([[-0.97602904, -0.15946168]], dtype=float32)

time = 464	action = 1	current_phase = 0	next_phase = 1	reward = -1.844085	array([[-0.9759921 , -0.17053628]], dtype=float32)

time = 472	action = 0	current_phase = 1	next_phase = 0	reward = -0.895215	array([[1.06646   , 0.57723624]], dtype=float32)

time = 477	action = 0	current_phase = 1	next_phase = 0	reward = -2.161007	array([[1.0453757, 0.6026894]], dtype=float32)

time = 482	action = 1	current_phase = 1	next_phase = 0	reward = -1.502184	array([[1.0464152, 0.5950866]], dtype=float32)

time = 490	action = 0	current_phase = 0	next_phase = 1	reward = 0.561652	array([[-0.9833629 , -0.15550089]], dtype=float32)

time = 495	action = 1	current_phase = 0	next_phase = 1	reward = -2.054140	array([[-0.9943472 , -0.15829058]], dtype=float32)

time = 503	action = 0	current_phase = 1	next_phase = 0	reward = -1.014462	array([[1.053309 , 0.5862777]], dtype=float32)

time = 508	action = 0	current_phase = 1	next_phase = 0	reward = -2.427522	array([[1.0387414 , 0.60514355]], dtype=float32)

time = 513	action = 1	current_phase = 1	next_phase = 0	reward = -1.602265	array([[1.0548396, 0.5980714]], dtype=float32)

time = 521	action = 0	current_phase = 0	next_phase = 1	reward = 0.886255	array([[-0.9879643 , -0.15230405]], dtype=float32)

time = 526	action = 1	current_phase = 0	next_phase = 1	reward = -1.776830	array([[-0.9812272 , -0.14492935]], dtype=float32)

time = 534	action = 0	current_phase = 1	next_phase = 0	reward = -0.814145	array([[1.0383614, 0.5874498]], dtype=float32)

time = 539	action = 0	current_phase = 1	next_phase = 0	reward = -2.288800	array([[1.0398173, 0.6034713]], dtype=float32)

time = 544	action = 1	current_phase = 1	next_phase = 0	reward = -1.372362	array([[1.0526928, 0.5993707]], dtype=float32)

time = 552	action = 0	current_phase = 0	next_phase = 1	reward = 0.867619	array([[-0.9871993 , -0.16277084]], dtype=float32)

time = 557	action = 1	current_phase = 0	next_phase = 1	reward = -1.659149	array([[-0.98864627, -0.13896146]], dtype=float32)

time = 565	action = 0	current_phase = 1	next_phase = 0	reward = -1.441870	array([[1.0457295, 0.5902684]], dtype=float32)

time = 570	action = 0	current_phase = 1	next_phase = 0	reward = -2.537886	array([[1.0393641 , 0.60238075]], dtype=float32)

time = 575	action = 1	current_phase = 1	next_phase = 0	reward = -1.547030	array([[1.0538445, 0.5966168]], dtype=float32)

time = 583	action = 0	current_phase = 0	next_phase = 1	reward = 0.886868	array([[-0.97689146, -0.16040711]], dtype=float32)

time = 588	action = 1	current_phase = 0	next_phase = 1	reward = -1.113143	array([[-0.9883414 , -0.13940068]], dtype=float32)

time = 596	action = 0	current_phase = 1	next_phase = 0	reward = -1.264922	array([[1.0501403, 0.5940705]], dtype=float32)

time = 601	action = 0	current_phase = 1	next_phase = 0	reward = -2.322495	array([[1.0547286 , 0.58740544]], dtype=float32)

time = 606	action = 1	current_phase = 1	next_phase = 0	reward = -1.435771	array([[1.0468801, 0.6028279]], dtype=float32)

time = 614	action = 0	current_phase = 0	next_phase = 1	reward = 0.623522	array([[-0.96583164, -0.16599701]], dtype=float32)

time = 619	action = 1	current_phase = 0	next_phase = 1	reward = -0.664633	array([[-0.9875332, -0.1381912]], dtype=float32)

time = 627	action = 0	current_phase = 1	next_phase = 0	reward = -1.190834	array([[1.0554267 , 0.58811134]], dtype=float32)

time = 632	action = 0	current_phase = 1	next_phase = 0	reward = -2.105441	array([[1.0521797, 0.5815779]], dtype=float32)

time = 637	action = 1	current_phase = 1	next_phase = 0	reward = -0.862138	array([[1.038985 , 0.5972221]], dtype=float32)

time = 645	action = 0	current_phase = 0	next_phase = 1	reward = 0.321135	array([[-0.9758525 , -0.16930702]], dtype=float32)

time = 650	action = 1	current_phase = 0	next_phase = 1	reward = -1.074322	array([[-0.9920503 , -0.14849293]], dtype=float32)

time = 658	action = 0	current_phase = 1	next_phase = 0	reward = -1.328998	array([[1.0468532 , 0.58950436]], dtype=float32)

time = 663	action = 0	current_phase = 1	next_phase = 0	reward = -2.155257	array([[1.0578247 , 0.58051586]], dtype=float32)

Terminal occured. Episode end.

Simulation ended at time: 669.00

Reason: TraCI requested termination.

Performance: 

 Duration: 15026ms

 Real time factor: 44.5228

 UPS: 1323.572474

Vehicles: 

 Inserted: 314

 Running: 32

 Waiting: 0



DijkstraRouter answered 314 queries and explored 2.29 edges on average.

DijkstraRouter spent 0ms answering queries (0.00ms on average).

Could not connect to TraCI server at localhost:34703 [Errno 111] Connection refused

 Retrying in 1 seconds

Loading configuration... done.

***Starting server on port 34703 ***

Loading net-file from '/home/soup/IntelliLight/data/one_run/cross.net.xml'... done (3ms).

Warning: Missing yellow phase in tlLogic 'node0', program '0' for tl-index 5 when switching to phase 1

Loading additional-files from '/home/soup/IntelliLight/data/one_run/cross.tls.switch.xml'... done (0ms).

Loading additional-files from '/home/soup/IntelliLight/data/one_run/cross.car.type.xml'... done (0ms).

Loading done.

Simulation started with time: 0.00

time = 20	action = 0	current_phase = 0	next_phase = 1	reward = -0.096708	array([[-0.97998637, -0.15543285]], dtype=float32)

time = 25	action = 0	current_phase = 0	next_phase = 1	reward = -0.777623	array([[-0.98078287, -0.15541553]], dtype=float32)

time = 30	action = 1	current_phase = 0	next_phase = 1	reward = -2.600898	array([[-0.9753964, -0.1559512]], dtype=float32)

time = 38	action = 0	current_phase = 1	next_phase = 0	reward = -1.414874	array([[1.0384277 , 0.58529764]], dtype=float32)

time = 43	action = 0	current_phase = 1	next_phase = 0	reward = -1.970617	array([[1.059602  , 0.58407116]], dtype=float32)

time = 48	action = 0	current_phase = 1	next_phase = 0	reward = -2.622497	array([[1.0393659, 0.5964626]], dtype=float32)

time = 53	action = 1	current_phase = 1	next_phase = 0	reward = -2.728074	array([[1.0600495, 0.5932465]], dtype=float32)

time = 61	action = 0	current_phase = 0	next_phase = 1	reward = -0.038896	array([[-0.9875027 , -0.16830954]], dtype=float32)

time = 66	action = 1	current_phase = 0	next_phase = 1	reward = -1.924317	array([[-0.98464465, -0.13388735]], dtype=float32)

time = 74	action = 0	current_phase = 1	next_phase = 0	reward = -1.426884	array([[1.039011 , 0.5839221]], dtype=float32)

time = 79	action = 0	current_phase = 1	next_phase = 0	reward = -2.128974	array([[1.0303701 , 0.60220164]], dtype=float32)

time = 84	action = 0	current_phase = 1	next_phase = 0	reward = -2.168431	array([[1.0534948, 0.5974767]], dtype=float32)

time = 89	action = 1	current_phase = 1	next_phase = 0	reward = -2.454932	array([[1.0415084 , 0.61014175]], dtype=float32)

time = 97	action = 0	current_phase = 0	next_phase = 1	reward = 1.472565	array([[-1.0002903, -0.1236654]], dtype=float32)

time = 102	action = 1	current_phase = 0	next_phase = 1	reward = -2.757104	array([[-0.98754007, -0.16223939]], dtype=float32)

time = 110	action = 0	current_phase = 1	next_phase = 0	reward = -1.315274	array([[1.0292519 , 0.59501755]], dtype=float32)

time = 115	action = 0	current_phase = 1	next_phase = 0	reward = -2.429796	array([[1.0601289 , 0.59159005]], dtype=float32)

time = 120	action = 0	current_phase = 1	next_phase = 0	reward = -3.511082	array([[1.0400535, 0.6025884]], dtype=float32)

time = 125	action = 1	current_phase = 1	next_phase = 0	reward = -2.371568	array([[1.0531052 , 0.59908056]], dtype=float32)

time = 133	action = 0	current_phase = 0	next_phase = 1	reward = 1.833351	array([[-0.9879171 , -0.15300354]], dtype=float32)

time = 138	action = 1	current_phase = 0	next_phase = 1	reward = -1.745069	array([[-0.9890535 , -0.14126289]], dtype=float32)

time = 146	action = 0	current_phase = 1	next_phase = 0	reward = -1.701412	array([[1.0482429, 0.600699 ]], dtype=float32)

time = 151	action = 0	current_phase = 1	next_phase = 0	reward = -2.594116	array([[1.0463054, 0.5944538]], dtype=float32)

time = 156	action = 0	current_phase = 1	next_phase = 0	reward = -3.152955	array([[1.0397975, 0.6083291]], dtype=float32)

time = 161	action = 1	current_phase = 1	next_phase = 0	reward = -1.724474	array([[1.0459342, 0.6030884]], dtype=float32)

time = 169	action = 0	current_phase = 0	next_phase = 1	reward = 1.231204	array([[-0.99241686, -0.13725998]], dtype=float32)

time = 174	action = 1	current_phase = 0	next_phase = 1	reward = -1.071569	array([[-0.9764536, -0.1679376]], dtype=float32)

time = 182	action = 0	current_phase = 1	next_phase = 0	reward = -1.425380	array([[1.0659003 , 0.57091653]], dtype=float32)

time = 187	action = 0	current_phase = 1	next_phase = 0	reward = -2.031698	array([[1.0270534, 0.6019707]], dtype=float32)

time = 192	action = 0	current_phase = 1	next_phase = 0	reward = -2.563113	array([[1.054416 , 0.5976984]], dtype=float32)

time = 197	action = 1	current_phase = 1	next_phase = 0	reward = -1.942971	array([[1.0407606, 0.6069492]], dtype=float32)

time = 205	action = 0	current_phase = 0	next_phase = 1	reward = 0.560862	array([[-0.9802799 , -0.16921785]], dtype=float32)

time = 210	action = 1	current_phase = 0	next_phase = 1	reward = -1.843150	array([[-0.9837886 , -0.13685295]], dtype=float32)

time = 218	action = 0	current_phase = 1	next_phase = 0	reward = -1.440567	array([[1.0383022 , 0.58482707]], dtype=float32)

time = 223	action = 0	current_phase = 1	next_phase = 0	reward = -1.966046	array([[1.0607078, 0.5833831]], dtype=float32)

time = 228	action = 0	current_phase = 1	next_phase = 0	reward = -2.625508	array([[1.0379009, 0.5957586]], dtype=float32)

time = 233	action = 1	current_phase = 1	next_phase = 0	reward = -2.203335	array([[1.0608778 , 0.59262186]], dtype=float32)

time = 241	action = 0	current_phase = 0	next_phase = 1	reward = 0.534621	array([[-0.99290144, -0.1680354 ]], dtype=float32)

time = 246	action = 1	current_phase = 0	next_phase = 1	reward = -2.579711	array([[-0.99324274, -0.15691873]], dtype=float32)

time = 254	action = 0	current_phase = 1	next_phase = 0	reward = -1.796040	array([[1.0504024 , 0.59030247]], dtype=float32)

time = 259	action = 0	current_phase = 1	next_phase = 0	reward = -2.278685	array([[1.0300574 , 0.60191905]], dtype=float32)

time = 264	action = 0	current_phase = 1	next_phase = 0	reward = -2.528769	array([[1.0518378, 0.5987793]], dtype=float32)

time = 269	action = 1	current_phase = 1	next_phase = 0	reward = -1.991773	array([[1.0433908, 0.6089771]], dtype=float32)

time = 277	action = 0	current_phase = 0	next_phase = 1	reward = 0.804012	array([[-0.98548365, -0.14392215]], dtype=float32)

time = 282	action = 1	current_phase = 0	next_phase = 1	reward = -3.172055	array([[-0.9802688 , -0.16833729]], dtype=float32)

time = 290	action = 0	current_phase = 1	next_phase = 0	reward = -1.530628	array([[1.0281141, 0.596143 ]], dtype=float32)

time = 295	action = 0	current_phase = 1	next_phase = 0	reward = -2.751198	array([[1.059878 , 0.5934161]], dtype=float32)

time = 300	action = 0	current_phase = 1	next_phase = 0	reward = -3.849051	array([[1.040607  , 0.60363126]], dtype=float32)

time = 305	action = 1	current_phase = 1	next_phase = 0	reward = -2.445942	array([[1.056315  , 0.59911925]], dtype=float32)

time = 313	action = 0	current_phase = 0	next_phase = 1	reward = 1.545402	array([[-0.9836756 , -0.16561161]], dtype=float32)

time = 318	action = 1	current_phase = 0	next_phase = 1	reward = -2.012203	array([[-0.9817166 , -0.14480464]], dtype=float32)

time = 326	action = 0	current_phase = 1	next_phase = 0	reward = -1.996265	array([[1.0467184 , 0.60334593]], dtype=float32)

time = 331	action = 0	current_phase = 1	next_phase = 0	reward = -2.816972	array([[1.0469321, 0.5952907]], dtype=float32)

time = 336	action = 0	current_phase = 1	next_phase = 0	reward = -3.538899	array([[1.0397741 , 0.60921276]], dtype=float32)

time = 341	action = 1	current_phase = 1	next_phase = 0	reward = -2.758385	array([[1.0479589 , 0.60391146]], dtype=float32)

time = 349	action = 0	current_phase = 0	next_phase = 1	reward = 1.654486	array([[-0.98869103, -0.14067505]], dtype=float32)

time = 354	action = 1	current_phase = 0	next_phase = 1	reward = -2.675650	array([[-0.98064375, -0.16716039]], dtype=float32)

time = 362	action = 0	current_phase = 1	next_phase = 0	reward = -2.380697	array([[1.0507548 , 0.58616686]], dtype=float32)

time = 367	action = 0	current_phase = 1	next_phase = 0	reward = -2.982966	array([[1.0203855, 0.6104056]], dtype=float32)

time = 372	action = 0	current_phase = 1	next_phase = 0	reward = -3.618671	array([[1.0438511, 0.6048076]], dtype=float32)

time = 377	action = 1	current_phase = 1	next_phase = 0	reward = -2.646964	array([[1.0303779, 0.6149381]], dtype=float32)

time = 385	action = 0	current_phase = 0	next_phase = 1	reward = 0.989475	array([[-0.9838337 , -0.16517998]], dtype=float32)

time = 390	action = 1	current_phase = 0	next_phase = 1	reward = -2.800673	array([[-0.976611  , -0.14225003]], dtype=float32)

time = 398	action = 0	current_phase = 1	next_phase = 0	reward = -2.126808	array([[1.0270139 , 0.59336907]], dtype=float32)

time = 403	action = 0	current_phase = 1	next_phase = 0	reward = -2.668989	array([[1.0507157 , 0.59113675]], dtype=float32)

time = 408	action = 0	current_phase = 1	next_phase = 0	reward = -3.321323	array([[1.0307558, 0.603943 ]], dtype=float32)

time = 413	action = 1	current_phase = 1	next_phase = 0	reward = -2.850110	array([[1.0537755 , 0.60004395]], dtype=float32)

time = 421	action = 0	current_phase = 0	next_phase = 1	reward = 1.164563	array([[-0.9770973 , -0.15603673]], dtype=float32)

time = 426	action = 1	current_phase = 0	next_phase = 1	reward = -2.396371	array([[-0.9966135 , -0.12883069]], dtype=float32)

time = 434	action = 0	current_phase = 1	next_phase = 0	reward = -1.789437	array([[1.045426 , 0.5869358]], dtype=float32)

time = 439	action = 0	current_phase = 1	next_phase = 0	reward = -2.414221	array([[1.0314839, 0.6024958]], dtype=float32)

time = 444	action = 0	current_phase = 1	next_phase = 0	reward = -2.522406	array([[1.053913 , 0.5983736]], dtype=float32)

time = 449	action = 1	current_phase = 1	next_phase = 0	reward = -2.366231	array([[1.0435148 , 0.60950613]], dtype=float32)

time = 457	action = 0	current_phase = 0	next_phase = 1	reward = 1.434756	array([[-0.9773402 , -0.14906804]], dtype=float32)

time = 462	action = 1	current_phase = 0	next_phase = 1	reward = -2.766398	array([[-0.9626659 , -0.15713853]], dtype=float32)

time = 470	action = 0	current_phase = 1	next_phase = 0	reward = -1.376945	array([[1.0360935, 0.5958942]], dtype=float32)

time = 475	action = 0	current_phase = 1	next_phase = 0	reward = -2.494914	array([[1.0601033, 0.5924801]], dtype=float32)

time = 480	action = 0	current_phase = 1	next_phase = 0	reward = -3.464378	array([[1.0399276 , 0.60292596]], dtype=float32)

time = 485	action = 1	current_phase = 1	next_phase = 0	reward = -2.236513	array([[1.0549103, 0.5982945]], dtype=float32)

time = 493	action = 0	current_phase = 0	next_phase = 1	reward = 1.554476	array([[-1.0029981 , -0.15483579]], dtype=float32)

time = 498	action = 1	current_phase = 0	next_phase = 1	reward = -2.155435	array([[-0.99964714, -0.1326774 ]], dtype=float32)

time = 506	action = 0	current_phase = 1	next_phase = 0	reward = -2.069025	array([[1.0386636 , 0.60689735]], dtype=float32)

time = 511	action = 0	current_phase = 1	next_phase = 0	reward = -2.946742	array([[1.0450447, 0.5966199]], dtype=float32)

time = 516	action = 0	current_phase = 1	next_phase = 0	reward = -3.620989	array([[1.0395142, 0.6086175]], dtype=float32)

time = 521	action = 1	current_phase = 1	next_phase = 0	reward = -2.784269	array([[1.034547  , 0.60682774]], dtype=float32)

time = 529	action = 0	current_phase = 0	next_phase = 1	reward = 1.972900	array([[-0.99878997, -0.14621162]], dtype=float32)

time = 534	action = 1	current_phase = 0	next_phase = 1	reward = -2.045211	array([[-0.9881552 , -0.16355087]], dtype=float32)

time = 542	action = 0	current_phase = 1	next_phase = 0	reward = -1.945474	array([[1.0538942, 0.5816488]], dtype=float32)

time = 547	action = 0	current_phase = 1	next_phase = 0	reward = -2.577502	array([[1.0412422 , 0.59809595]], dtype=float32)

time = 552	action = 0	current_phase = 1	next_phase = 0	reward = -3.241296	array([[1.0545217 , 0.59016365]], dtype=float32)

time = 557	action = 1	current_phase = 1	next_phase = 0	reward = -2.181165	array([[1.0480537 , 0.60189444]], dtype=float32)

time = 565	action = 0	current_phase = 0	next_phase = 1	reward = 0.907438	array([[-0.97210336, -0.17287055]], dtype=float32)

time = 570	action = 1	current_phase = 0	next_phase = 1	reward = -3.226251	array([[-0.9841623, -0.1365135]], dtype=float32)

time = 578	action = 0	current_phase = 1	next_phase = 0	reward = -2.146187	array([[1.0322865 , 0.59771585]], dtype=float32)

time = 583	action = 0	current_phase = 1	next_phase = 0	reward = -2.735326	array([[1.0529217, 0.596372 ]], dtype=float32)

time = 588	action = 0	current_phase = 1	next_phase = 0	reward = -3.379616	array([[1.031986  , 0.60739005]], dtype=float32)

time = 593	action = 1	current_phase = 1	next_phase = 0	reward = -1.659279	array([[1.0532157 , 0.60420585]], dtype=float32)

time = 601	action = 0	current_phase = 0	next_phase = 1	reward = 1.352259	array([[-0.9898222, -0.1620539]], dtype=float32)

time = 606	action = 1	current_phase = 0	next_phase = 1	reward = -2.560112	array([[-0.98356545, -0.15474197]], dtype=float32)

time = 614	action = 0	current_phase = 1	next_phase = 0	reward = -1.825896	array([[1.0515583 , 0.58146524]], dtype=float32)

time = 619	action = 0	current_phase = 1	next_phase = 0	reward = -2.486255	array([[1.0386033 , 0.59615064]], dtype=float32)

time = 624	action = 0	current_phase = 1	next_phase = 0	reward = -2.493419	array([[1.0611445 , 0.59153724]], dtype=float32)

time = 629	action = 1	current_phase = 1	next_phase = 0	reward = -2.687179	array([[1.0504487, 0.6032755]], dtype=float32)

time = 637	action = 0	current_phase = 0	next_phase = 1	reward = 1.767500	array([[-0.9995266 , -0.14637971]], dtype=float32)

time = 642	action = 1	current_phase = 0	next_phase = 1	reward = -2.834264	array([[-0.9875941 , -0.16292945]], dtype=float32)

time = 650	action = 0	current_phase = 1	next_phase = 0	reward = -1.617035	array([[1.0286129, 0.5958551]], dtype=float32)

time = 655	action = 0	current_phase = 1	next_phase = 0	reward = -2.143011	array([[1.0535864, 0.5921616]], dtype=float32)

time = 660	action = 0	current_phase = 1	next_phase = 0	reward = -3.471717	array([[1.0404382, 0.6025704]], dtype=float32)

time = 665	action = 1	current_phase = 1	next_phase = 0	reward = -2.344219	array([[1.0536143 , 0.59977853]], dtype=float32)

Terminal occured. Episode end.

Simulation ended at time: 674.00

Reason: TraCI requested termination.

Performance: 

 Duration: 15079ms

 Real time factor: 44.6979

 UPS: 1419.258572

Vehicles: 

 Inserted: 316

 Running: 29

 Waiting: 0



DijkstraRouter answered 316 queries and explored 2.29 edges on average.

DijkstraRouter spent 0ms answering queries (0.00ms on average).

Could not connect to TraCI server at localhost:43081 [Errno 111] Connection refused

 Retrying in 1 seconds

Loading configuration... done.

***Starting server on port 43081 ***

Loading net-file from '/home/soup/IntelliLight/data/one_run/cross.net.xml'... done (3ms).

Warning: Missing yellow phase in tlLogic 'node0', program '0' for tl-index 5 when switching to phase 1

Loading additional-files from '/home/soup/IntelliLight/data/one_run/cross.tls.switch.xml'... done (1ms).

Loading additional-files from '/home/soup/IntelliLight/data/one_run/cross.car.type.xml'... done (1ms).

Loading done.

Simulation started with time: 0.00

time = 20	action = 0	current_phase = 0	next_phase = 1	reward = -0.096708	array([[-0.97998637, -0.15543285]], dtype=float32)

time = 25	action = 0	current_phase = 0	next_phase = 1	reward = -0.777623	array([[-0.98078287, -0.15541553]], dtype=float32)

time = 30	action = 1	current_phase = 0	next_phase = 1	reward = -2.600898	array([[-0.9753964, -0.1559512]], dtype=float32)

time = 38	action = 0	current_phase = 1	next_phase = 0	reward = -1.414874	array([[1.0384277 , 0.58529764]], dtype=float32)

time = 43	action = 0	current_phase = 1	next_phase = 0	reward = -1.970617	array([[1.059602  , 0.58407116]], dtype=float32)

time = 48	action = 0	current_phase = 1	next_phase = 0	reward = -2.622497	array([[1.0393659, 0.5964626]], dtype=float32)

time = 53	action = 0	current_phase = 1	next_phase = 0	reward = -2.638868	array([[1.0600495, 0.5932465]], dtype=float32)

time = 58	action = 1	current_phase = 1	next_phase = 0	reward = -2.555212	array([[1.0486662, 0.6050037]], dtype=float32)

time = 66	action = 0	current_phase = 0	next_phase = 1	reward = 1.131922	array([[-0.99939054, -0.14644623]], dtype=float32)

time = 71	action = 1	current_phase = 0	next_phase = 1	reward = -2.789502	array([[-0.9770047 , -0.15889543]], dtype=float32)

time = 79	action = 0	current_phase = 1	next_phase = 0	reward = -2.079350	array([[1.0279442 , 0.59650433]], dtype=float32)

time = 84	action = 0	current_phase = 1	next_phase = 0	reward = -2.167360	array([[1.0520563, 0.5920192]], dtype=float32)

time = 89	action = 0	current_phase = 1	next_phase = 0	reward = -3.617849	array([[1.0405599, 0.6042582]], dtype=float32)

time = 94	action = 0	current_phase = 1	next_phase = 0	reward = -4.513503	array([[1.0550905, 0.5996885]], dtype=float32)

time = 99	action = 1	current_phase = 1	next_phase = 0	reward = -3.180263	array([[1.0376003, 0.6094425]], dtype=float32)

time = 107	action = 0	current_phase = 0	next_phase = 1	reward = 1.983051	array([[-0.9945699, -0.1602458]], dtype=float32)

time = 112	action = 1	current_phase = 0	next_phase = 1	reward = -2.623950	array([[-0.9829828 , -0.17553975]], dtype=float32)

time = 120	action = 0	current_phase = 1	next_phase = 0	reward = -2.814550	array([[1.0386819 , 0.60445035]], dtype=float32)

time = 125	action = 0	current_phase = 1	next_phase = 0	reward = -3.435227	array([[1.0506366 , 0.60015374]], dtype=float32)

time = 130	action = 0	current_phase = 1	next_phase = 0	reward = -4.044242	array([[1.0336685 , 0.60906833]], dtype=float32)

time = 135	action = 0	current_phase = 1	next_phase = 0	reward = -4.744959	array([[1.0543252, 0.6047385]], dtype=float32)

time = 140	action = 1	current_phase = 1	next_phase = 0	reward = -4.916153	array([[1.0378748 , 0.61408067]], dtype=float32)

time = 148	action = 0	current_phase = 0	next_phase = 1	reward = 0.651348	array([[-0.9885847 , -0.15508011]], dtype=float32)

time = 153	action = 1	current_phase = 0	next_phase = 1	reward = -5.337900	array([[-0.9843021 , -0.14919758]], dtype=float32)

time = 161	action = 0	current_phase = 1	next_phase = 0	reward = -3.935295	array([[1.042178 , 0.6022774]], dtype=float32)

time = 166	action = 0	current_phase = 1	next_phase = 0	reward = -4.471673	array([[1.03578   , 0.61644596]], dtype=float32)

time = 171	action = 0	current_phase = 1	next_phase = 0	reward = -4.938997	array([[1.0454346, 0.611104 ]], dtype=float32)

time = 176	action = 0	current_phase = 1	next_phase = 0	reward = -6.336077	array([[1.0419724, 0.624807 ]], dtype=float32)

time = 181	action = 1	current_phase = 1	next_phase = 0	reward = -6.085065	array([[1.0494485, 0.6168275]], dtype=float32)

time = 189	action = 0	current_phase = 0	next_phase = 1	reward = 2.155755	array([[-0.9817557 , -0.14690599]], dtype=float32)

time = 194	action = 1	current_phase = 0	next_phase = 1	reward = -4.584526	array([[-0.9740469 , -0.16190058]], dtype=float32)

time = 202	action = 0	current_phase = 1	next_phase = 0	reward = -3.653076	array([[1.0344831 , 0.61200124]], dtype=float32)

time = 207	action = 0	current_phase = 1	next_phase = 0	reward = -5.079132	array([[1.0326774, 0.6236105]], dtype=float32)

time = 212	action = 0	current_phase = 1	next_phase = 0	reward = -6.137091	array([[1.0426209 , 0.61757326]], dtype=float32)

time = 217	action = 0	current_phase = 1	next_phase = 0	reward = -6.987220	array([[1.0284278, 0.6278447]], dtype=float32)

time = 222	action = 1	current_phase = 1	next_phase = 0	reward = -6.419865	array([[1.046737  , 0.62510204]], dtype=float32)

time = 230	action = 0	current_phase = 0	next_phase = 1	reward = 1.492086	array([[-1.0144881 , -0.13602221]], dtype=float32)

time = 235	action = 1	current_phase = 0	next_phase = 1	reward = -5.745942	array([[-1.0017562 , -0.14501616]], dtype=float32)

time = 243	action = 0	current_phase = 1	next_phase = 0	reward = -5.333716	array([[1.0359432, 0.621233 ]], dtype=float32)

time = 248	action = 0	current_phase = 1	next_phase = 0	reward = -6.270187	array([[1.0208269, 0.630972 ]], dtype=float32)

time = 253	action = 0	current_phase = 1	next_phase = 0	reward = -7.156897	array([[1.0407476 , 0.62512124]], dtype=float32)

time = 258	action = 0	current_phase = 1	next_phase = 0	reward = -7.999960	array([[1.0266227, 0.6330377]], dtype=float32)

time = 263	action = 1	current_phase = 1	next_phase = 0	reward = -8.445870	array([[1.0442587 , 0.63117325]], dtype=float32)

time = 271	action = 0	current_phase = 0	next_phase = 1	reward = 0.910183	array([[-1.0180309 , -0.17260191]], dtype=float32)

time = 276	action = 1	current_phase = 0	next_phase = 1	reward = -5.502530	array([[-1.0177505 , -0.13298172]], dtype=float32)

time = 284	action = 0	current_phase = 1	next_phase = 0	reward = -5.889083	array([[1.0464315 , 0.61723334]], dtype=float32)

time = 289	action = 0	current_phase = 1	next_phase = 0	reward = -6.908864	array([[1.0253661, 0.6313361]], dtype=float32)

time = 294	action = 0	current_phase = 1	next_phase = 0	reward = -7.237043	array([[1.0429338 , 0.62907183]], dtype=float32)

time = 299	action = 0	current_phase = 1	next_phase = 0	reward = -9.024424	array([[1.0456748, 0.635292 ]], dtype=float32)

time = 304	action = 1	current_phase = 1	next_phase = 0	reward = -9.400537	array([[1.0537148 , 0.63195455]], dtype=float32)

time = 312	action = 0	current_phase = 0	next_phase = 1	reward = 1.457260	array([[-0.9807266 , -0.17560464]], dtype=float32)

time = 317	action = 1	current_phase = 0	next_phase = 1	reward = -4.177463	array([[-0.9843129, -0.1485162]], dtype=float32)

time = 325	action = 0	current_phase = 1	next_phase = 0	reward = -5.773202	array([[1.0411165 , 0.62054884]], dtype=float32)

time = 330	action = 0	current_phase = 1	next_phase = 0	reward = -7.404605	array([[1.0341535 , 0.63356864]], dtype=float32)

time = 335	action = 0	current_phase = 1	next_phase = 0	reward = -8.368529	array([[1.04416  , 0.6317353]], dtype=float32)

time = 340	action = 0	current_phase = 1	next_phase = 0	reward = -9.377896	array([[1.031791 , 0.6383452]], dtype=float32)

time = 345	action = 1	current_phase = 1	next_phase = 0	reward = -10.231629	array([[1.0484508 , 0.63618386]], dtype=float32)

time = 353	action = 0	current_phase = 0	next_phase = 1	reward = 1.217161	array([[-0.9831965 , -0.17247978]], dtype=float32)

time = 358	action = 1	current_phase = 0	next_phase = 1	reward = -6.047496	array([[-1.001165 , -0.1339984]], dtype=float32)

time = 366	action = 0	current_phase = 1	next_phase = 0	reward = -6.897777	array([[1.0173604 , 0.63258123]], dtype=float32)

time = 371	action = 0	current_phase = 1	next_phase = 0	reward = -8.017819	array([[1.0311744 , 0.63731223]], dtype=float32)

time = 376	action = 0	current_phase = 1	next_phase = 0	reward = -9.034791	array([[1.0253315, 0.6433655]], dtype=float32)

time = 381	action = 0	current_phase = 1	next_phase = 0	reward = -9.447261	array([[1.0344435 , 0.64219904]], dtype=float32)

time = 386	action = 1	current_phase = 1	next_phase = 0	reward = -11.656250	array([[1.0416188 , 0.64726216]], dtype=float32)

time = 394	action = 0	current_phase = 0	next_phase = 1	reward = 1.368845	array([[-0.98342854, -0.18662047]], dtype=float32)

time = 399	action = 1	current_phase = 0	next_phase = 1	reward = -5.290180	array([[-1.015368 , -0.1291433]], dtype=float32)

time = 407	action = 0	current_phase = 1	next_phase = 0	reward = -6.581566	array([[1.0209361 , 0.63549185]], dtype=float32)

time = 412	action = 0	current_phase = 1	next_phase = 0	reward = -7.682847	array([[1.0188838 , 0.64274645]], dtype=float32)

time = 417	action = 0	current_phase = 1	next_phase = 0	reward = -9.432113	array([[1.0180817 , 0.64828444]], dtype=float32)

time = 422	action = 0	current_phase = 1	next_phase = 0	reward = -10.654046	array([[1.0279009 , 0.64619195]], dtype=float32)

time = 427	action = 1	current_phase = 1	next_phase = 0	reward = -12.052818	array([[1.0191611 , 0.65233994]], dtype=float32)

time = 435	action = 0	current_phase = 0	next_phase = 1	reward = 0.463426	array([[-1.0090399, -0.1879732]], dtype=float32)

time = 440	action = 1	current_phase = 0	next_phase = 1	reward = -5.119533	array([[-1.0145102, -0.1491706]], dtype=float32)

time = 448	action = 0	current_phase = 1	next_phase = 0	reward = -7.500854	array([[1.0232673, 0.6348833]], dtype=float32)

time = 453	action = 0	current_phase = 1	next_phase = 0	reward = -9.302221	array([[1.0226182, 0.6408738]], dtype=float32)

time = 458	action = 0	current_phase = 1	next_phase = 0	reward = -10.343560	array([[1.0134493, 0.6493274]], dtype=float32)

time = 463	action = 0	current_phase = 1	next_phase = 0	reward = -11.422901	array([[1.0324864 , 0.64663965]], dtype=float32)

time = 468	action = 1	current_phase = 1	next_phase = 0	reward = -12.899925	array([[1.0234902 , 0.65386415]], dtype=float32)

time = 476	action = 0	current_phase = 0	next_phase = 1	reward = -0.316580	array([[-0.9972805 , -0.17755848]], dtype=float32)

time = 481	action = 1	current_phase = 0	next_phase = 1	reward = -4.811133	array([[-1.014799  , -0.15873475]], dtype=float32)

time = 489	action = 0	current_phase = 1	next_phase = 0	reward = -6.160942	array([[1.0271116 , 0.62516636]], dtype=float32)

time = 494	action = 0	current_phase = 1	next_phase = 0	reward = -8.876012	array([[1.025164  , 0.63826644]], dtype=float32)

time = 499	action = 0	current_phase = 1	next_phase = 0	reward = -10.056534	array([[1.0110824, 0.6489394]], dtype=float32)

time = 504	action = 0	current_phase = 1	next_phase = 0	reward = -10.636970	array([[1.0286387 , 0.64683235]], dtype=float32)

time = 509	action = 1	current_phase = 1	next_phase = 0	reward = -14.654914	array([[1.0288278 , 0.65272194]], dtype=float32)

time = 517	action = 0	current_phase = 0	next_phase = 1	reward = 0.285628	array([[-0.99864507, -0.1714807 ]], dtype=float32)

time = 522	action = 1	current_phase = 0	next_phase = 1	reward = -6.325387	array([[-1.011855  , -0.16929859]], dtype=float32)

time = 530	action = 0	current_phase = 1	next_phase = 0	reward = -7.650068	array([[1.0181925, 0.6413654]], dtype=float32)

time = 535	action = 0	current_phase = 1	next_phase = 0	reward = -10.027158	array([[1.018618  , 0.64959276]], dtype=float32)

time = 540	action = 0	current_phase = 1	next_phase = 0	reward = -11.913771	array([[1.0173146 , 0.65589815]], dtype=float32)

time = 545	action = 0	current_phase = 1	next_phase = 0	reward = -13.096274	array([[1.0244747 , 0.65630937]], dtype=float32)

time = 550	action = 1	current_phase = 1	next_phase = 0	reward = -16.611578	array([[1.018534  , 0.66117907]], dtype=float32)

time = 558	action = 0	current_phase = 0	next_phase = 1	reward = -1.232877	array([[-0.99170196, -0.16533828]], dtype=float32)

time = 563	action = 1	current_phase = 0	next_phase = 1	reward = -5.888675	array([[-1.0121118 , -0.17187943]], dtype=float32)

time = 571	action = 0	current_phase = 1	next_phase = 0	reward = -7.802000	array([[1.0363269, 0.636803 ]], dtype=float32)

time = 576	action = 0	current_phase = 1	next_phase = 0	reward = -10.974213	array([[1.0123855 , 0.65469545]], dtype=float32)

time = 581	action = 0	current_phase = 1	next_phase = 0	reward = -12.620706	array([[1.0110456, 0.6623347]], dtype=float32)

time = 586	action = 0	current_phase = 1	next_phase = 0	reward = -13.984684	array([[1.0061635 , 0.66767406]], dtype=float32)

time = 591	action = 1	current_phase = 1	next_phase = 0	reward = -18.514423	array([[1.0197992, 0.6686218]], dtype=float32)

time = 599	action = 0	current_phase = 0	next_phase = 1	reward = -4.310268	array([[-0.9801482 , -0.17997292]], dtype=float32)

time = 604	action = 1	current_phase = 0	next_phase = 1	reward = -8.147591	array([[-1.0110928 , -0.16212772]], dtype=float32)

time = 612	action = 0	current_phase = 1	next_phase = 0	reward = -8.123117	array([[1.0372784, 0.6342922]], dtype=float32)

time = 617	action = 0	current_phase = 1	next_phase = 0	reward = -11.871343	array([[1.0088781, 0.6600232]], dtype=float32)

time = 622	action = 0	current_phase = 1	next_phase = 0	reward = -13.270331	array([[1.0105673 , 0.66651523]], dtype=float32)

time = 627	action = 0	current_phase = 1	next_phase = 0	reward = -15.468335	array([[1.0138218, 0.6710396]], dtype=float32)

time = 632	action = 1	current_phase = 1	next_phase = 0	reward = -19.174684	array([[1.0193291 , 0.67309624]], dtype=float32)

time = 640	action = 0	current_phase = 0	next_phase = 1	reward = -3.064878	array([[-0.98723996, -0.17587139]], dtype=float32)

time = 645	action = 1	current_phase = 0	next_phase = 1	reward = -7.040916	array([[-1.000144  , -0.17793384]], dtype=float32)

time = 653	action = 0	current_phase = 1	next_phase = 0	reward = -6.762347	array([[1.0415382, 0.6380496]], dtype=float32)

time = 658	action = 0	current_phase = 1	next_phase = 0	reward = -11.299107	array([[1.0217757 , 0.65716314]], dtype=float32)

time = 663	action = 0	current_phase = 1	next_phase = 0	reward = -13.928674	array([[1.0078564, 0.6665573]], dtype=float32)

Terminal occured. Episode end.

Simulation ended at time: 669.00

Reason: TraCI requested termination.

Performance: 

 Duration: 19751ms

 Real time factor: 33.8717

 UPS: 1567.464938

Vehicles: 

 Inserted: 314

 Running: 74

 Waiting: 0



DijkstraRouter answered 314 queries and explored 2.29 edges on average.

DijkstraRouter spent 0ms answering queries (0.00ms on average).

Train on 1113 samples, validate on 478 samples

Epoch 1/500

 - 4s - loss: 3.9846 - val_loss: 3.6047

Epoch 2/500

 - 4s - loss: 2.2103 - val_loss: 2.6035

Epoch 3/500

 - 4s - loss: 1.6050 - val_loss: 2.2269

Epoch 4/500

 - 4s - loss: 1.3802 - val_loss: 2.0758

Epoch 5/500

 - 4s - loss: 1.2662 - val_loss: 1.9427

Epoch 6/500

 - 4s - loss: 1.1741 - val_loss: 1.8469

Epoch 7/500

 - 4s - loss: 1.0962 - val_loss: 1.7533

Epoch 8/500

 - 4s - loss: 1.0325 - val_loss: 1.6896

Epoch 9/500

 - 4s - loss: 0.9611 - val_loss: 1.6145

Epoch 10/500

 - 4s - loss: 0.9057 - val_loss: 1.5269

Epoch 11/500

 - 4s - loss: 0.8518 - val_loss: 1.4735

Epoch 12/500

 - 4s - loss: 0.8068 - val_loss: 1.4254

Epoch 13/500

 - 4s - loss: 0.7569 - val_loss: 1.3781

Epoch 14/500

 - 4s - loss: 0.7206 - val_loss: 1.3360

Epoch 15/500

 - 4s - loss: 0.6843 - val_loss: 1.3008

Epoch 16/500

 - 4s - loss: 0.6516 - val_loss: 1.2807

Epoch 17/500

 - 4s - loss: 0.6243 - val_loss: 1.2425

Epoch 18/500

 - 4s - loss: 0.6070 - val_loss: 1.2476

Epoch 19/500

 - 4s - loss: 0.5745 - val_loss: 1.2109

Epoch 20/500

 - 4s - loss: 0.5582 - val_loss: 1.2012

Epoch 21/500

 - 4s - loss: 0.5497 - val_loss: 1.1884

Epoch 22/500

 - 4s - loss: 0.5277 - val_loss: 1.1724

Epoch 23/500

 - 4s - loss: 0.5112 - val_loss: 1.1594

Epoch 24/500

 - 4s - loss: 0.4892 - val_loss: 1.1480

Epoch 25/500

 - 4s - loss: 0.4795 - val_loss: 1.1441

Epoch 26/500

 - 4s - loss: 0.4506 - val_loss: 1.1820

Epoch 27/500

 - 4s - loss: 0.4490 - val_loss: 1.1535

Epoch 28/500

 - 4s - loss: 0.4311 - val_loss: 1.1263

Epoch 29/500

 - 4s - loss: 0.4111 - val_loss: 1.1341

Epoch 30/500

 - 4s - loss: 0.4121 - val_loss: 1.1371

Epoch 31/500

 - 4s - loss: 0.4016 - val_loss: 1.0966

Epoch 32/500

 - 4s - loss: 0.3778 - val_loss: 1.0950

Epoch 33/500

 - 4s - loss: 0.3710 - val_loss: 1.1170

Epoch 34/500

 - 4s - loss: 0.3639 - val_loss: 1.1423

Epoch 35/500

 - 4s - loss: 0.3512 - val_loss: 1.0864

Epoch 36/500

 - 4s - loss: 0.3442 - val_loss: 1.0612

Epoch 37/500

 - 4s - loss: 0.3267 - val_loss: 1.0675

Epoch 38/500

 - 4s - loss: 0.3342 - val_loss: 1.1452

Epoch 39/500

 - 4s - loss: 0.3227 - val_loss: 0.9890

Epoch 40/500

 - 4s - loss: 0.3129 - val_loss: 1.0522

Epoch 41/500

 - 4s - loss: 0.3004 - val_loss: 1.0488

Epoch 42/500

 - 4s - loss: 0.2940 - val_loss: 1.0795

Epoch 43/500

 - 4s - loss: 0.3045 - val_loss: 1.0366

Epoch 44/500

 - 4s - loss: 0.2762 - val_loss: 0.9783

Epoch 45/500

 - 4s - loss: 0.2864 - val_loss: 0.9516

Epoch 46/500

 - 4s - loss: 0.2684 - val_loss: 1.0201

Epoch 47/500

 - 4s - loss: 0.2659 - val_loss: 1.0966

Epoch 48/500

 - 4s - loss: 0.2617 - val_loss: 0.9300

Epoch 49/500

 - 4s - loss: 0.2564 - val_loss: 1.0134

Epoch 50/500

 - 5s - loss: 0.2408 - val_loss: 1.0099

Epoch 51/500

 - 4s - loss: 0.2393 - val_loss: 0.9686

Epoch 52/500

 - 4s - loss: 0.2452 - val_loss: 0.9079

Epoch 53/500

 - 4s - loss: 0.2301 - val_loss: 0.9034

Epoch 54/500

 - 4s - loss: 0.2323 - val_loss: 0.8870

Epoch 55/500

 - 4s - loss: 0.2145 - val_loss: 0.9050

Epoch 56/500

 - 4s - loss: 0.2208 - val_loss: 0.9484

Epoch 57/500

 - 4s - loss: 0.2171 - val_loss: 0.9422

Epoch 58/500

 - 4s - loss: 0.2083 - val_loss: 0.8855

Epoch 59/500

 - 5s - loss: 0.1985 - val_loss: 0.8549

Epoch 60/500

 - 5s - loss: 0.2035 - val_loss: 0.8718

Epoch 61/500

 - 5s - loss: 0.1959 - val_loss: 0.8064

Epoch 62/500

 - 5s - loss: 0.1970 - val_loss: 0.8017

Epoch 63/500

 - 6s - loss: 0.1955 - val_loss: 0.8877

Epoch 64/500

 - 5s - loss: 0.1850 - val_loss: 0.8503

Epoch 65/500

 - 4s - loss: 0.1860 - val_loss: 0.7919

Epoch 66/500

 - 7s - loss: 0.1792 - val_loss: 0.8358

Epoch 67/500

 - 5s - loss: 0.1770 - val_loss: 0.8549

Epoch 68/500

 - 4s - loss: 0.1834 - val_loss: 0.7908

Epoch 69/500

 - 4s - loss: 0.1752 - val_loss: 0.8651

Epoch 70/500

 - 4s - loss: 0.1618 - val_loss: 0.7636

Epoch 71/500

 - 5s - loss: 0.1651 - val_loss: 0.8831

Epoch 72/500

 - 5s - loss: 0.1570 - val_loss: 0.8625

Epoch 73/500

 - 4s - loss: 0.1637 - val_loss: 0.8446

Epoch 74/500

 - 4s - loss: 0.1534 - val_loss: 0.8281

Epoch 75/500

 - 4s - loss: 0.1581 - val_loss: 0.7826

Epoch 76/500

 - 4s - loss: 0.1470 - val_loss: 0.8552

Epoch 77/500

 - 5s - loss: 0.1521 - val_loss: 0.8344

Epoch 78/500

 - 4s - loss: 0.1395 - val_loss: 0.7895

Epoch 79/500

 - 5s - loss: 0.1436 - val_loss: 0.7689

Epoch 80/500

 - 5s - loss: 0.1448 - val_loss: 0.7522

Epoch 81/500

 - 5s - loss: 0.1381 - val_loss: 0.7531

Epoch 82/500

 - 5s - loss: 0.1407 - val_loss: 0.8212

Epoch 83/500

 - 5s - loss: 0.1320 - val_loss: 0.7806

Epoch 84/500

 - 5s - loss: 0.1356 - val_loss: 0.7426

Epoch 85/500

 - 5s - loss: 0.1266 - val_loss: 0.8399

Epoch 86/500

 - 5s - loss: 0.1294 - val_loss: 0.7035

Epoch 87/500

 - 5s - loss: 0.1247 - val_loss: 0.7717

Epoch 88/500

 - 5s - loss: 0.1273 - val_loss: 0.7174

Epoch 89/500

 - 4s - loss: 0.1262 - val_loss: 0.6637

Epoch 90/500

 - 5s - loss: 0.1230 - val_loss: 0.7428

Epoch 91/500

 - 5s - loss: 0.1183 - val_loss: 0.8432

Epoch 92/500

 - 5s - loss: 0.1206 - val_loss: 0.7899

Epoch 93/500

 - 5s - loss: 0.1158 - val_loss: 0.7715

Epoch 94/500

 - 5s - loss: 0.1145 - val_loss: 0.7094

Epoch 95/500

 - 5s - loss: 0.1181 - val_loss: 0.7062

Epoch 96/500

 - 5s - loss: 0.1162 - val_loss: 0.7705

Epoch 97/500

 - 4s - loss: 0.1152 - val_loss: 0.8007

Epoch 98/500

 - 5s - loss: 0.1104 - val_loss: 0.6814

Epoch 99/500

 - 5s - loss: 0.1080 - val_loss: 0.7098

length of memory (state 0, action 0): 507, after forget

length of memory (state 0, action 1): 303, after forget

length of memory (state 1, action 0): 490, after forget

length of memory (state 1, action 1): 291, after forget

END

Could not connect to TraCI server at localhost:53023 [Errno 111] Connection refused

 Retrying in 1 seconds

Loading configuration... done.

***Starting server on port 53023 ***

Loading net-file from '/home/soup/IntelliLight/data/one_run/cross.net.xml'... done (5ms).

Warning: Missing yellow phase in tlLogic 'node0', program '0' for tl-index 5 when switching to phase 1

Loading additional-files from '/home/soup/IntelliLight/data/one_run/cross.tls.switch.xml'... done (1ms).

Loading additional-files from '/home/soup/IntelliLight/data/one_run/cross.car.type.xml'... done (0ms).

Loading done.

Simulation started with time: 0.00

time = 20	action = 0	current_phase = 0	next_phase = 1	reward = -0.096708	array([[-0.07326838, -2.1604145 ]], dtype=float32)

time = 25	action = 0	current_phase = 0	next_phase = 1	reward = -0.777623	array([[-0.8500839, -2.1455936]], dtype=float32)

time = 30	action = 0	current_phase = 0	next_phase = 1	reward = -1.290711	array([[-0.63708174, -2.4066277 ]], dtype=float32)

time = 35	action = 0	current_phase = 0	next_phase = 1	reward = -1.244540	array([[-0.6893098, -2.247154 ]], dtype=float32)

time = 40	action = 0	current_phase = 0	next_phase = 1	reward = -1.449329	array([[-1.2152644, -2.132504 ]], dtype=float32)

time = 45	action = 0	current_phase = 0	next_phase = 1	reward = -1.142214	array([[-0.11420475, -2.1720269 ]], dtype=float32)

time = 50	action = 0	current_phase = 0	next_phase = 1	reward = -1.465065	array([[-0.7739435, -2.140604 ]], dtype=float32)

time = 55	action = 0	current_phase = 0	next_phase = 1	reward = -1.365807	array([[-0.41431597, -2.1849678 ]], dtype=float32)

time = 60	action = 0	current_phase = 0	next_phase = 1	reward = -2.387407	array([[-0.70759785, -2.2427464 ]], dtype=float32)

time = 65	action = 0	current_phase = 0	next_phase = 1	reward = -1.891768	array([[-0.4257284, -2.3627882]], dtype=float32)

time = 70	action = 0	current_phase = 0	next_phase = 1	reward = -2.483355	array([[-0.30615515, -2.1858213 ]], dtype=float32)

time = 75	action = 0	current_phase = 0	next_phase = 1	reward = -1.978736	array([[-0.32993978, -2.1872323 ]], dtype=float32)

time = 80	action = 0	current_phase = 0	next_phase = 1	reward = -2.895452	array([[-0.25819704, -2.137528  ]], dtype=float32)

time = 85	action = 0	current_phase = 0	next_phase = 1	reward = -2.671594	array([[-0.62270147, -2.1791873 ]], dtype=float32)

time = 90	action = 0	current_phase = 0	next_phase = 1	reward = -2.885353	array([[-1.1042625, -2.2747006]], dtype=float32)

time = 95	action = 0	current_phase = 0	next_phase = 1	reward = -3.272877	array([[-1.3387059, -2.1562746]], dtype=float32)

time = 100	action = 0	current_phase = 0	next_phase = 1	reward = -3.614418	array([[-1.119011 , -2.2638173]], dtype=float32)

time = 105	action = 0	current_phase = 0	next_phase = 1	reward = -3.146849	array([[-0.8253594, -2.3465788]], dtype=float32)

time = 110	action = 0	current_phase = 0	next_phase = 1	reward = -3.585327	array([[-1.3219867, -2.1538708]], dtype=float32)

time = 115	action = 0	current_phase = 0	next_phase = 1	reward = -4.467092	array([[-1.3760761, -2.122198 ]], dtype=float32)

time = 120	action = 0	current_phase = 0	next_phase = 1	reward = -4.210024	array([[-1.3992018, -2.1183891]], dtype=float32)

time = 125	action = 0	current_phase = 0	next_phase = 1	reward = -4.650942	array([[-1.2640243, -2.3053353]], dtype=float32)

time = 130	action = 0	current_phase = 0	next_phase = 1	reward = -4.773026	array([[-1.2418797, -2.3552084]], dtype=float32)

time = 135	action = 0	current_phase = 0	next_phase = 1	reward = -4.924979	array([[-1.1815263, -2.2654817]], dtype=float32)

time = 140	action = 0	current_phase = 0	next_phase = 1	reward = -5.111539	array([[-1.3853421, -2.1079943]], dtype=float32)

time = 145	action = 0	current_phase = 0	next_phase = 1	reward = -5.756808	array([[-1.3729129, -2.1207304]], dtype=float32)

time = 150	action = 0	current_phase = 0	next_phase = 1	reward = -6.336121	array([[-1.3975832, -2.121268 ]], dtype=float32)

time = 155	action = 0	current_phase = 0	next_phase = 1	reward = -5.971153	array([[-1.3803567, -2.1540947]], dtype=float32)

time = 160	action = 0	current_phase = 0	next_phase = 1	reward = -6.697564	array([[-1.3546103, -2.1780753]], dtype=float32)

time = 165	action = 0	current_phase = 0	next_phase = 1	reward = -6.324280	array([[-1.38395 , -2.170573]], dtype=float32)

time = 170	action = 0	current_phase = 0	next_phase = 1	reward = -6.835039	array([[-1.3908625, -2.1163154]], dtype=float32)

time = 175	action = 0	current_phase = 0	next_phase = 1	reward = -7.843689	array([[-1.3961254, -2.121605 ]], dtype=float32)

time = 180	action = 0	current_phase = 0	next_phase = 1	reward = -7.594002	array([[-1.3925642, -2.1196377]], dtype=float32)

time = 185	action = 0	current_phase = 0	next_phase = 1	reward = -8.664117	array([[-1.397774 , -2.1271613]], dtype=float32)

time = 190	action = 0	current_phase = 0	next_phase = 1	reward = -7.756956	array([[-1.3684399, -2.19709  ]], dtype=float32)

time = 195	action = 0	current_phase = 0	next_phase = 1	reward = -8.560112	array([[-1.3595977, -2.2111754]], dtype=float32)

time = 200	action = 0	current_phase = 0	next_phase = 1	reward = -8.834458	array([[-1.3963177, -2.1240313]], dtype=float32)

time = 205	action = 0	current_phase = 0	next_phase = 1	reward = -9.702357	array([[-1.396937, -2.123305]], dtype=float32)

time = 210	action = 0	current_phase = 0	next_phase = 1	reward = -10.211801	array([[-1.3966904, -2.1163292]], dtype=float32)

time = 215	action = 0	current_phase = 0	next_phase = 1	reward = -9.933096	array([[-1.3597486, -2.2545078]], dtype=float32)

time = 220	action = 0	current_phase = 0	next_phase = 1	reward = -10.477923	array([[-1.2695917, -2.3175812]], dtype=float32)

time = 225	action = 0	current_phase = 0	next_phase = 1	reward = -10.778416	array([[-1.3794824, -2.167798 ]], dtype=float32)

time = 230	action = 0	current_phase = 0	next_phase = 1	reward = -11.349954	array([[-1.3915554, -2.141254 ]], dtype=float32)

time = 235	action = 0	current_phase = 0	next_phase = 1	reward = -11.629541	array([[-1.401327, -2.105061]], dtype=float32)

time = 240	action = 0	current_phase = 0	next_phase = 1	reward = -12.271274	array([[-1.3923528, -2.1273558]], dtype=float32)

time = 245	action = 0	current_phase = 0	next_phase = 1	reward = -13.156904	array([[-1.4009658, -2.1155293]], dtype=float32)

time = 250	action = 0	current_phase = 0	next_phase = 1	reward = -12.344113	array([[-1.3732299, -2.168225 ]], dtype=float32)

time = 255	action = 0	current_phase = 0	next_phase = 1	reward = -13.515969	array([[-1.3854266, -2.1737077]], dtype=float32)

time = 260	action = 0	current_phase = 0	next_phase = 1	reward = -13.562598	array([[-1.3958081, -2.1273983]], dtype=float32)

time = 265	action = 0	current_phase = 0	next_phase = 1	reward = -14.149735	array([[-1.3787246, -2.134284 ]], dtype=float32)

time = 270	action = 0	current_phase = 0	next_phase = 1	reward = -14.825220	array([[-1.3959395, -2.113303 ]], dtype=float32)

time = 275	action = 0	current_phase = 0	next_phase = 1	reward = -15.468090	array([[-1.3579986, -2.2154422]], dtype=float32)

time = 280	action = 0	current_phase = 0	next_phase = 1	reward = -15.533509	array([[-1.3561244, -2.265944 ]], dtype=float32)

time = 285	action = 0	current_phase = 0	next_phase = 1	reward = -15.625540	array([[-1.3519521, -2.2944045]], dtype=float32)

time = 290	action = 0	current_phase = 0	next_phase = 1	reward = -16.288517	array([[-1.4007779, -2.1021266]], dtype=float32)

time = 295	action = 0	current_phase = 0	next_phase = 1	reward = -17.039330	array([[-1.3990868, -2.1118398]], dtype=float32)

Train on 833 samples, validate on 358 samples

Epoch 1/50

 - 3s - loss: 1.7813 - val_loss: 1.3688

Epoch 2/50

 - 3s - loss: 1.5240 - val_loss: 1.3078

Epoch 3/50

 - 3s - loss: 1.3559 - val_loss: 1.1201

Epoch 4/50

 - 3s - loss: 1.1891 - val_loss: 1.0656

Epoch 5/50

 - 3s - loss: 1.1141 - val_loss: 1.0231

Epoch 6/50

 - 3s - loss: 1.0356 - val_loss: 0.9674

Epoch 7/50

 - 3s - loss: 0.9850 - val_loss: 0.9286

Epoch 8/50

 - 3s - loss: 0.9285 - val_loss: 0.8870

Epoch 9/50

 - 3s - loss: 0.8681 - val_loss: 0.8425

Epoch 10/50

 - 3s - loss: 0.8148 - val_loss: 0.8170

Epoch 11/50

 - 4s - loss: 0.7527 - val_loss: 0.8175

Epoch 12/50

 - 3s - loss: 0.7316 - val_loss: 0.7454

Epoch 13/50

 - 4s - loss: 0.6894 - val_loss: 0.7330

Epoch 14/50

 - 6s - loss: 0.6581 - val_loss: 0.7014

Epoch 15/50

 - 6s - loss: 0.6288 - val_loss: 0.6980

Epoch 16/50

 - 6s - loss: 0.6119 - val_loss: 0.6665

Epoch 17/50

 - 3s - loss: 0.5955 - val_loss: 0.6849

Epoch 18/50

 - 4s - loss: 0.5539 - val_loss: 0.6353

Epoch 19/50

 - 4s - loss: 0.5345 - val_loss: 0.6449

Epoch 20/50

 - 4s - loss: 0.5009 - val_loss: 0.6087

Epoch 21/50

 - 5s - loss: 0.4958 - val_loss: 0.5592

Epoch 22/50

 - 4s - loss: 0.4718 - val_loss: 0.5612

Epoch 23/50

 - 4s - loss: 0.4458 - val_loss: 0.5678

Epoch 24/50

 - 3s - loss: 0.4421 - val_loss: 0.5867

Epoch 25/50

 - 4s - loss: 0.4337 - val_loss: 0.5370

Epoch 26/50

 - 4s - loss: 0.4048 - val_loss: 0.5450

Epoch 27/50

 - 4s - loss: 0.3957 - val_loss: 0.5275

Epoch 28/50

 - 4s - loss: 0.3923 - val_loss: 0.4847

Epoch 29/50

 - 4s - loss: 0.3615 - val_loss: 0.4533

Epoch 30/50

 - 3s - loss: 0.3636 - val_loss: 0.4641

Epoch 31/50

 - 3s - loss: 0.3508 - val_loss: 0.4754

Epoch 32/50

 - 3s - loss: 0.3294 - val_loss: 0.4576

Epoch 33/50

 - 4s - loss: 0.3329 - val_loss: 0.4321

Epoch 34/50

 - 3s - loss: 0.3200 - val_loss: 0.4673

Epoch 35/50

 - 3s - loss: 0.3160 - val_loss: 0.4539

Epoch 36/50

 - 3s - loss: 0.2996 - val_loss: 0.4274

Epoch 37/50

 - 4s - loss: 0.2979 - val_loss: 0.4401

Epoch 38/50

 - 4s - loss: 0.2927 - val_loss: 0.4249

Epoch 39/50

 - 3s - loss: 0.2808 - val_loss: 0.4797

Epoch 40/50

 - 4s - loss: 0.2828 - val_loss: 0.4068

Epoch 41/50

 - 4s - loss: 0.2859 - val_loss: 0.4443

Epoch 42/50

 - 4s - loss: 0.2622 - val_loss: 0.4013

Epoch 43/50

 - 4s - loss: 0.2684 - val_loss: 0.3964

Epoch 44/50

 - 4s - loss: 0.2636 - val_loss: 0.4008

Epoch 45/50

 - 4s - loss: 0.2480 - val_loss: 0.3878

Epoch 46/50

 - 4s - loss: 0.2374 - val_loss: 0.3802

Epoch 47/50

 - 3s - loss: 0.2308 - val_loss: 0.3960

Epoch 48/50

 - 3s - loss: 0.2387 - val_loss: 0.3879

Epoch 49/50

 - 4s - loss: 0.2346 - val_loss: 0.3728

Epoch 50/50

 - 4s - loss: 0.2206 - val_loss: 0.4126

length of memory (state 0, action 0): 563, after forget

length of memory (state 0, action 1): 303, after forget

length of memory (state 1, action 0): 490, after forget

length of memory (state 1, action 1): 291, after forget

time = 300	action = 1	current_phase = 0	next_phase = 1	reward = -10.682959	array([[-12.429883 ,  -2.3646302]], dtype=float32)

time = 308	action = 0	current_phase = 1	next_phase = 0	reward = 2.702756	array([[-2.767872, -4.204914]], dtype=float32)

time = 313	action = 1	current_phase = 1	next_phase = 0	reward = -4.444161	array([[-2.3449826, -1.9801986]], dtype=float32)

time = 321	action = 1	current_phase = 0	next_phase = 1	reward = -2.833474	array([[-3.1558485, -1.8529602]], dtype=float32)

time = 329	action = 1	current_phase = 1	next_phase = 0	reward = -4.135912	array([[-2.9523852, -1.8413453]], dtype=float32)

time = 337	action = 0	current_phase = 0	next_phase = 1	reward = -0.694929	array([[-0.18011248, -2.8521461 ]], dtype=float32)

time = 342	action = 0	current_phase = 0	next_phase = 1	reward = -1.894734	array([[-0.2642495, -3.0470757]], dtype=float32)

time = 347	action = 0	current_phase = 0	next_phase = 1	reward = -1.949073	array([[-1.0582006, -2.3879528]], dtype=float32)

time = 352	action = 0	current_phase = 0	next_phase = 1	reward = -2.205414	array([[-1.2709668, -2.5644522]], dtype=float32)

time = 357	action = 1	current_phase = 0	next_phase = 1	reward = -2.912845	array([[-2.3127456, -2.2129557]], dtype=float32)

time = 365	action = 1	current_phase = 1	next_phase = 0	reward = -2.497102	array([[-2.0975492, -2.032854 ]], dtype=float32)

time = 373	action = 0	current_phase = 0	next_phase = 1	reward = -1.064020	array([[-0.683208, -2.708274]], dtype=float32)

time = 378	action = 0	current_phase = 0	next_phase = 1	reward = -1.050825	array([[-2.1778278, -2.4023929]], dtype=float32)

time = 383	action = 0	current_phase = 0	next_phase = 1	reward = -1.493287	array([[-1.7672932, -2.4818268]], dtype=float32)

time = 388	action = 0	current_phase = 0	next_phase = 1	reward = -2.057146	array([[-0.6112702, -3.2073116]], dtype=float32)

time = 393	action = 0	current_phase = 0	next_phase = 1	reward = -2.082015	array([[-0.45438153, -2.8775759 ]], dtype=float32)

time = 398	action = 0	current_phase = 0	next_phase = 1	reward = -2.115969	array([[-0.6622454, -3.1139832]], dtype=float32)

time = 403	action = 0	current_phase = 0	next_phase = 1	reward = -2.165021	array([[-0.52251166, -2.8714938 ]], dtype=float32)

time = 408	action = 1	current_phase = 0	next_phase = 1	reward = -1.933938	array([[-2.3194816, -2.2090063]], dtype=float32)

time = 416	action = 1	current_phase = 1	next_phase = 0	reward = -0.548364	array([[-2.1482058, -1.7193748]], dtype=float32)

time = 424	action = 0	current_phase = 0	next_phase = 1	reward = -0.122535	array([[ 0.4162364, -2.0812464]], dtype=float32)

time = 429	action = 0	current_phase = 0	next_phase = 1	reward = -0.051417	array([[ 0.23810709, -2.3060856 ]], dtype=float32)

time = 434	action = 0	current_phase = 0	next_phase = 1	reward = 0.031021	array([[-0.10476223, -2.4678216 ]], dtype=float32)

time = 439	action = 0	current_phase = 0	next_phase = 1	reward = 0.073713	array([[-0.28144884, -2.404871  ]], dtype=float32)

time = 444	action = 0	current_phase = 0	next_phase = 1	reward = -0.479578	array([[-0.02867696, -2.7296314 ]], dtype=float32)

time = 449	action = 0	current_phase = 0	next_phase = 1	reward = -1.169561	array([[-0.20618096, -2.7548118 ]], dtype=float32)

time = 454	action = 0	current_phase = 0	next_phase = 1	reward = -1.209964	array([[-0.71797866, -3.0290189 ]], dtype=float32)

time = 459	action = 0	current_phase = 0	next_phase = 1	reward = -1.169570	array([[-0.794521 , -2.8227735]], dtype=float32)

time = 464	action = 0	current_phase = 0	next_phase = 1	reward = -1.146925	array([[-0.7922312, -2.8213477]], dtype=float32)

time = 469	action = 0	current_phase = 0	next_phase = 1	reward = -1.137679	array([[-1.840275 , -2.3987255]], dtype=float32)

time = 474	action = 0	current_phase = 0	next_phase = 1	reward = -1.713966	array([[-1.1156278, -2.785046 ]], dtype=float32)

time = 479	action = 1	current_phase = 0	next_phase = 1	reward = -1.457612	array([[-2.7560234, -2.2854495]], dtype=float32)

time = 487	action = 1	current_phase = 1	next_phase = 0	reward = -0.913136	array([[-2.316772 , -1.0934738]], dtype=float32)

time = 495	action = 0	current_phase = 0	next_phase = 1	reward = 0.342075	array([[-0.20200536, -1.8350343 ]], dtype=float32)

time = 500	action = 0	current_phase = 0	next_phase = 1	reward = 0.014444	array([[-0.78852624, -2.4029236 ]], dtype=float32)

time = 505	action = 0	current_phase = 0	next_phase = 1	reward = -0.628829	array([[-0.60957086, -2.5864973 ]], dtype=float32)

time = 510	action = 0	current_phase = 0	next_phase = 1	reward = -1.224686	array([[-0.701385 , -2.7049646]], dtype=float32)

time = 515	action = 0	current_phase = 0	next_phase = 1	reward = -1.202323	array([[-0.7132399, -2.9341912]], dtype=float32)

time = 520	action = 0	current_phase = 0	next_phase = 1	reward = -1.152207	array([[-0.3493579, -2.8142953]], dtype=float32)

time = 525	action = 0	current_phase = 0	next_phase = 1	reward = -1.116375	array([[-0.8664979, -2.5312424]], dtype=float32)

time = 530	action = 0	current_phase = 0	next_phase = 1	reward = -1.196878	array([[-1.7185652, -2.2298994]], dtype=float32)

time = 535	action = 0	current_phase = 0	next_phase = 1	reward = -2.167042	array([[-1.998414 , -2.4714627]], dtype=float32)

time = 540	action = 1	current_phase = 0	next_phase = 1	reward = -1.881516	array([[-2.489964 , -2.3016052]], dtype=float32)

time = 548	action = 1	current_phase = 1	next_phase = 0	reward = -0.643271	array([[-2.3118727, -1.6965625]], dtype=float32)

time = 556	action = 0	current_phase = 0	next_phase = 1	reward = 0.059547	array([[ 0.076648 , -2.4192295]], dtype=float32)

time = 561	action = 0	current_phase = 0	next_phase = 1	reward = -0.038992	array([[-0.609437, -2.461989]], dtype=float32)

time = 566	action = 0	current_phase = 0	next_phase = 1	reward = -0.884352	array([[-0.4121964, -2.7353516]], dtype=float32)

time = 571	action = 0	current_phase = 0	next_phase = 1	reward = -1.225823	array([[-0.5992696, -2.8944554]], dtype=float32)

time = 576	action = 0	current_phase = 0	next_phase = 1	reward = -1.204935	array([[-0.55285656, -3.0984726 ]], dtype=float32)

time = 581	action = 0	current_phase = 0	next_phase = 1	reward = -1.175227	array([[-0.76519704, -2.5989866 ]], dtype=float32)

time = 586	action = 0	current_phase = 0	next_phase = 1	reward = -1.146162	array([[-0.6714549, -2.6665983]], dtype=float32)

time = 591	action = 0	current_phase = 0	next_phase = 1	reward = -1.277212	array([[-1.0819559, -2.541183 ]], dtype=float32)

time = 596	action = 0	current_phase = 0	next_phase = 1	reward = -2.016872	array([[-1.5154097, -2.526598 ]], dtype=float32)

Train on 837 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.3302 - val_loss: 0.3349

Epoch 2/50

 - 3s - loss: 0.2856 - val_loss: 0.3449

Epoch 3/50

 - 3s - loss: 0.2498 - val_loss: 0.3127

Epoch 4/50

 - 3s - loss: 0.2834 - val_loss: 0.3034

Epoch 5/50

 - 3s - loss: 0.2412 - val_loss: 0.3491

Epoch 6/50

 - 3s - loss: 0.2420 - val_loss: 0.3118

Epoch 7/50

 - 3s - loss: 0.2497 - val_loss: 0.3012

Epoch 8/50

 - 3s - loss: 0.2474 - val_loss: 0.3161

Epoch 9/50

 - 3s - loss: 0.2429 - val_loss: 0.3013

Epoch 10/50

 - 3s - loss: 0.2377 - val_loss: 0.3016

Epoch 11/50

 - 3s - loss: 0.2310 - val_loss: 0.2948

Epoch 12/50

 - 3s - loss: 0.2294 - val_loss: 0.2999

Epoch 13/50

 - 3s - loss: 0.2249 - val_loss: 0.3220

Epoch 14/50

 - 3s - loss: 0.2064 - val_loss: 0.3125

Epoch 15/50

 - 3s - loss: 0.2039 - val_loss: 0.3197

Epoch 16/50

 - 3s - loss: 0.1910 - val_loss: 0.3020

Epoch 17/50

 - 4s - loss: 0.1923 - val_loss: 0.3040

Epoch 18/50

 - 3s - loss: 0.1967 - val_loss: 0.3002

Epoch 19/50

 - 4s - loss: 0.2067 - val_loss: 0.2970

Epoch 20/50

 - 4s - loss: 0.1842 - val_loss: 0.3107

Epoch 21/50

 - 4s - loss: 0.1821 - val_loss: 0.3098

length of memory (state 0, action 0): 603, after forget

length of memory (state 0, action 1): 309, after forget

length of memory (state 1, action 0): 491, after forget

length of memory (state 1, action 1): 297, after forget

time = 601	action = 1	current_phase = 0	next_phase = 1	reward = -2.154212	array([[-3.1907818, -2.6386096]], dtype=float32)

time = 609	action = 1	current_phase = 1	next_phase = 0	reward = -0.665493	array([[-2.317366 , -1.2715476]], dtype=float32)

time = 617	action = 0	current_phase = 0	next_phase = 1	reward = 0.073004	array([[ 0.29919142, -2.1735575 ]], dtype=float32)

time = 622	action = 0	current_phase = 0	next_phase = 1	reward = -0.172760	array([[-0.73330486, -2.458122  ]], dtype=float32)

time = 627	action = 0	current_phase = 0	next_phase = 1	reward = -0.962992	array([[-0.40193507, -2.8845181 ]], dtype=float32)

time = 632	action = 0	current_phase = 0	next_phase = 1	reward = -1.200567	array([[-0.65407956, -3.1373196 ]], dtype=float32)

time = 637	action = 0	current_phase = 0	next_phase = 1	reward = -1.178790	array([[-0.9815382, -3.5052652]], dtype=float32)

time = 642	action = 0	current_phase = 0	next_phase = 1	reward = -1.143427	array([[-0.828236 , -2.9622605]], dtype=float32)

time = 647	action = 0	current_phase = 0	next_phase = 1	reward = -1.131965	array([[-1.5447353, -2.7860892]], dtype=float32)

time = 652	action = 0	current_phase = 0	next_phase = 1	reward = -1.355930	array([[-1.2106974, -2.7467034]], dtype=float32)

time = 657	action = 0	current_phase = 0	next_phase = 1	reward = -2.115489	array([[-1.8402667, -3.019997 ]], dtype=float32)

time = 662	action = 0	current_phase = 0	next_phase = 1	reward = -2.131043	array([[-2.1309748, -2.8030558]], dtype=float32)

time = 667	action = 0	current_phase = 0	next_phase = 1	reward = -2.189039	array([[-1.7249161, -3.3574588]], dtype=float32)

time = 672	action = 0	current_phase = 0	next_phase = 1	reward = -2.220819	array([[-2.4076283, -2.7686572]], dtype=float32)

time = 677	action = 1	current_phase = 0	next_phase = 1	reward = -1.993318	array([[-2.8586965, -2.4728892]], dtype=float32)

time = 685	action = 0	current_phase = 1	next_phase = 0	reward = -0.809351	array([[-2.0458136, -2.131447 ]], dtype=float32)

time = 690	action = 1	current_phase = 1	next_phase = 0	reward = -1.787940	array([[-2.4101229, -1.0958102]], dtype=float32)

time = 698	action = 0	current_phase = 0	next_phase = 1	reward = 0.845993	array([[ 0.39805484, -2.2727425 ]], dtype=float32)

time = 703	action = 0	current_phase = 0	next_phase = 1	reward = 0.001210	array([[-0.19521016, -2.7241874 ]], dtype=float32)

time = 708	action = 0	current_phase = 0	next_phase = 1	reward = 0.074602	array([[-0.20639265, -2.7975175 ]], dtype=float32)

time = 713	action = 0	current_phase = 0	next_phase = 1	reward = -0.308887	array([[-0.21053877, -2.819226  ]], dtype=float32)

time = 718	action = 0	current_phase = 0	next_phase = 1	reward = -1.081955	array([[-0.804791 , -3.7858803]], dtype=float32)

time = 723	action = 0	current_phase = 0	next_phase = 1	reward = -1.206479	array([[-1.703688 , -3.1628888]], dtype=float32)

time = 728	action = 0	current_phase = 0	next_phase = 1	reward = -1.162641	array([[-1.096643 , -2.7566094]], dtype=float32)

time = 733	action = 0	current_phase = 0	next_phase = 1	reward = -1.110081	array([[-2.0800002, -2.6836522]], dtype=float32)

time = 738	action = 0	current_phase = 0	next_phase = 1	reward = -1.119739	array([[-1.3785508, -2.9725456]], dtype=float32)

time = 743	action = 0	current_phase = 0	next_phase = 1	reward = -1.639296	array([[-2.470505, -2.674103]], dtype=float32)

time = 748	action = 0	current_phase = 0	next_phase = 1	reward = -2.108580	array([[-2.1413307, -3.38067  ]], dtype=float32)

time = 753	action = 1	current_phase = 0	next_phase = 1	reward = -2.549369	array([[-3.1473365, -2.8359432]], dtype=float32)

time = 761	action = 1	current_phase = 1	next_phase = 0	reward = -0.895899	array([[-2.3713644, -1.594574 ]], dtype=float32)

time = 769	action = 0	current_phase = 0	next_phase = 1	reward = 0.065917	array([[-0.2503813, -2.280346 ]], dtype=float32)

time = 774	action = 0	current_phase = 0	next_phase = 1	reward = -0.273933	array([[-0.6876919, -2.338348 ]], dtype=float32)

time = 779	action = 0	current_phase = 0	next_phase = 1	reward = -1.439777	array([[-1.0610884, -2.7068377]], dtype=float32)

time = 784	action = 0	current_phase = 0	next_phase = 1	reward = -1.198577	array([[-0.75115067, -3.0284886 ]], dtype=float32)

time = 789	action = 0	current_phase = 0	next_phase = 1	reward = -1.160731	array([[-0.60864085, -3.0737185 ]], dtype=float32)

time = 794	action = 0	current_phase = 0	next_phase = 1	reward = -1.131508	array([[-0.9856109, -2.7535217]], dtype=float32)

time = 799	action = 0	current_phase = 0	next_phase = 1	reward = -1.154315	array([[-1.8728006, -2.7699623]], dtype=float32)

time = 804	action = 0	current_phase = 0	next_phase = 1	reward = -1.856012	array([[-1.7800272, -2.7813756]], dtype=float32)

time = 809	action = 1	current_phase = 0	next_phase = 1	reward = -1.578960	array([[-3.3351555, -2.6306803]], dtype=float32)

time = 817	action = 1	current_phase = 1	next_phase = 0	reward = -0.633974	array([[-2.2737942, -1.8396548]], dtype=float32)

time = 825	action = 0	current_phase = 0	next_phase = 1	reward = 0.052168	array([[ 0.17453027, -2.0748992 ]], dtype=float32)

time = 830	action = 0	current_phase = 0	next_phase = 1	reward = 0.023734	array([[-0.61705446, -2.5437102 ]], dtype=float32)

time = 835	action = 0	current_phase = 0	next_phase = 1	reward = -0.624733	array([[-0.61542773, -2.6739793 ]], dtype=float32)

time = 840	action = 0	current_phase = 0	next_phase = 1	reward = -1.227053	array([[-0.5493605, -2.8394756]], dtype=float32)

time = 845	action = 0	current_phase = 0	next_phase = 1	reward = -1.478014	array([[-0.7151425, -3.147079 ]], dtype=float32)

time = 850	action = 0	current_phase = 0	next_phase = 1	reward = -0.869647	array([[-0.7276041, -2.88991  ]], dtype=float32)

time = 855	action = 0	current_phase = 0	next_phase = 1	reward = -1.134934	array([[-1.4241045, -2.5601747]], dtype=float32)

time = 860	action = 1	current_phase = 0	next_phase = 1	reward = -1.434604	array([[-2.7787938, -2.6044676]], dtype=float32)

time = 868	action = 1	current_phase = 1	next_phase = 0	reward = -0.713973	array([[-2.3229432, -1.7067775]], dtype=float32)

time = 876	action = 0	current_phase = 0	next_phase = 1	reward = -0.077360	array([[ 0.2549771, -2.3194559]], dtype=float32)

time = 881	action = 0	current_phase = 0	next_phase = 1	reward = -0.007895	array([[-0.0732038, -2.774292 ]], dtype=float32)

time = 886	action = 0	current_phase = 0	next_phase = 1	reward = 0.060085	array([[-0.229446 , -2.7329464]], dtype=float32)

time = 891	action = 0	current_phase = 0	next_phase = 1	reward = -0.029005	array([[-0.2376399, -2.90776  ]], dtype=float32)

time = 896	action = 0	current_phase = 0	next_phase = 1	reward = -0.890458	array([[-0.38204873, -2.9931982 ]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.3024 - val_loss: 0.1115

Epoch 2/50

 - 3s - loss: 0.3235 - val_loss: 0.1023

Epoch 3/50

 - 4s - loss: 0.2885 - val_loss: 0.1244

Epoch 4/50

 - 3s - loss: 0.2779 - val_loss: 0.1192

Epoch 5/50

 - 3s - loss: 0.2678 - val_loss: 0.1126

Epoch 6/50

 - 3s - loss: 0.2680 - val_loss: 0.1130

Epoch 7/50

 - 4s - loss: 0.2678 - val_loss: 0.1155

Epoch 8/50

 - 3s - loss: 0.2743 - val_loss: 0.1076

Epoch 9/50

 - 3s - loss: 0.2545 - val_loss: 0.1108

Epoch 10/50

 - 3s - loss: 0.2555 - val_loss: 0.1032

Epoch 11/50

 - 3s - loss: 0.2417 - val_loss: 0.1048

Epoch 12/50

 - 3s - loss: 0.2371 - val_loss: 0.1241

length of memory (state 0, action 0): 646, after forget

length of memory (state 0, action 1): 314, after forget

length of memory (state 1, action 0): 492, after forget

length of memory (state 1, action 1): 302, after forget

time = 901	action = 0	current_phase = 0	next_phase = 1	reward = -1.217135	array([[-1.9381945, -3.1336372]], dtype=float32)

time = 906	action = 0	current_phase = 0	next_phase = 1	reward = -1.192034	array([[-2.1130667, -3.0747712]], dtype=float32)

time = 911	action = 0	current_phase = 0	next_phase = 1	reward = -1.150619	array([[-1.3462524, -2.7409012]], dtype=float32)

time = 916	action = 1	current_phase = 0	next_phase = 1	reward = -2.153400	array([[-2.4756393, -2.4184775]], dtype=float32)

time = 924	action = 1	current_phase = 1	next_phase = 0	reward = -0.262273	array([[-2.2060182, -1.9527357]], dtype=float32)

time = 932	action = 0	current_phase = 0	next_phase = 1	reward = 0.149304	array([[ 0.11204022, -2.210347  ]], dtype=float32)

time = 937	action = 0	current_phase = 0	next_phase = 1	reward = -0.063526	array([[ 0.02053541, -2.5720298 ]], dtype=float32)

time = 942	action = 0	current_phase = 0	next_phase = 1	reward = 0.009361	array([[-0.12994447, -2.6523573 ]], dtype=float32)

time = 947	action = 0	current_phase = 0	next_phase = 1	reward = 0.066709	array([[-0.33062127, -2.8719966 ]], dtype=float32)

time = 952	action = 0	current_phase = 0	next_phase = 1	reward = -0.147360	array([[-0.5257552, -2.902824 ]], dtype=float32)

time = 957	action = 0	current_phase = 0	next_phase = 1	reward = -1.004019	array([[-0.6483105, -3.0084043]], dtype=float32)

time = 962	action = 0	current_phase = 0	next_phase = 1	reward = -1.208861	array([[-1.359726 , -2.9797835]], dtype=float32)

time = 967	action = 0	current_phase = 0	next_phase = 1	reward = -1.177596	array([[-1.8696754, -3.0642889]], dtype=float32)

time = 972	action = 0	current_phase = 0	next_phase = 1	reward = -1.143789	array([[-1.2408935, -3.074665 ]], dtype=float32)

time = 977	action = 0	current_phase = 0	next_phase = 1	reward = -1.125252	array([[-1.5852337, -2.9745972]], dtype=float32)

time = 982	action = 0	current_phase = 0	next_phase = 1	reward = -1.352846	array([[-2.0740952, -2.6793916]], dtype=float32)

time = 987	action = 1	current_phase = 0	next_phase = 1	reward = -2.403915	array([[-2.9045541, -2.8443696]], dtype=float32)

time = 995	action = 1	current_phase = 1	next_phase = 0	reward = -0.889694	array([[-2.3720243, -1.6641017]], dtype=float32)

time = 1003	action = 0	current_phase = 0	next_phase = 1	reward = 0.017669	array([[ 0.03876495, -2.4700553 ]], dtype=float32)

time = 1008	action = 0	current_phase = 0	next_phase = 1	reward = 0.076981	array([[-0.41736415, -2.8309479 ]], dtype=float32)

time = 1013	action = 0	current_phase = 0	next_phase = 1	reward = -0.308215	array([[-0.3670694, -2.8747585]], dtype=float32)

time = 1018	action = 0	current_phase = 0	next_phase = 1	reward = -1.089858	array([[-0.8477273, -3.208113 ]], dtype=float32)

time = 1023	action = 0	current_phase = 0	next_phase = 1	reward = -1.204282	array([[-1.288506, -3.556089]], dtype=float32)

time = 1028	action = 0	current_phase = 0	next_phase = 1	reward = -1.158832	array([[-1.1945056, -3.0388496]], dtype=float32)

time = 1033	action = 0	current_phase = 0	next_phase = 1	reward = -1.143971	array([[-1.9422238, -2.6613023]], dtype=float32)

time = 1038	action = 0	current_phase = 0	next_phase = 1	reward = -1.133022	array([[-1.986059 , -2.6331725]], dtype=float32)

time = 1043	action = 0	current_phase = 0	next_phase = 1	reward = -1.631410	array([[-2.7303543, -2.8320897]], dtype=float32)

time = 1048	action = 1	current_phase = 0	next_phase = 1	reward = -2.517083	array([[-2.9266396, -2.8705032]], dtype=float32)

time = 1056	action = 1	current_phase = 1	next_phase = 0	reward = -0.867395	array([[-2.471141 , -1.1942554]], dtype=float32)

time = 1064	action = 0	current_phase = 0	next_phase = 1	reward = 0.043808	array([[ 0.19350654, -2.1182518 ]], dtype=float32)

time = 1069	action = 0	current_phase = 0	next_phase = 1	reward = 0.074727	array([[-0.6157384, -2.5756953]], dtype=float32)

time = 1074	action = 0	current_phase = 0	next_phase = 1	reward = -0.488026	array([[-0.61185825, -2.5451965 ]], dtype=float32)

time = 1079	action = 0	current_phase = 0	next_phase = 1	reward = -1.159996	array([[-1.6247447, -3.0605228]], dtype=float32)

time = 1084	action = 0	current_phase = 0	next_phase = 1	reward = -1.190491	array([[-1.1671621, -3.4198935]], dtype=float32)

time = 1089	action = 0	current_phase = 0	next_phase = 1	reward = -1.168004	array([[-1.7683942, -2.7029963]], dtype=float32)

time = 1094	action = 0	current_phase = 0	next_phase = 1	reward = -1.129633	array([[-1.1102273, -2.7870815]], dtype=float32)

time = 1099	action = 0	current_phase = 0	next_phase = 1	reward = -1.149635	array([[-2.4416182, -2.8149135]], dtype=float32)

time = 1104	action = 0	current_phase = 0	next_phase = 1	reward = -1.842706	array([[-2.6998696, -2.9536161]], dtype=float32)

time = 1109	action = 1	current_phase = 0	next_phase = 1	reward = -1.466444	array([[-3.726741 , -2.6694517]], dtype=float32)

time = 1117	action = 1	current_phase = 1	next_phase = 0	reward = -0.574066	array([[-2.3495913, -1.0528797]], dtype=float32)

time = 1125	action = 0	current_phase = 0	next_phase = 1	reward = 0.054002	array([[ 0.11308396, -2.1939096 ]], dtype=float32)

time = 1130	action = 0	current_phase = 0	next_phase = 1	reward = 0.021260	array([[-0.6833923, -2.541439 ]], dtype=float32)

time = 1135	action = 0	current_phase = 0	next_phase = 1	reward = -0.635612	array([[-1.270689 , -2.3818104]], dtype=float32)

time = 1140	action = 0	current_phase = 0	next_phase = 1	reward = -1.224918	array([[-1.4483657, -2.795071 ]], dtype=float32)

time = 1145	action = 0	current_phase = 0	next_phase = 1	reward = -1.197401	array([[-1.362035, -2.759676]], dtype=float32)

time = 1150	action = 0	current_phase = 0	next_phase = 1	reward = -1.158095	array([[-1.2508973, -3.0226514]], dtype=float32)

time = 1155	action = 0	current_phase = 0	next_phase = 1	reward = -1.122994	array([[-1.8622024, -2.6670425]], dtype=float32)

time = 1160	action = 0	current_phase = 0	next_phase = 1	reward = -1.191550	array([[-2.494315 , -2.6521435]], dtype=float32)

time = 1165	action = 1	current_phase = 0	next_phase = 1	reward = -2.018540	array([[-2.760814, -2.599032]], dtype=float32)

time = 1173	action = 1	current_phase = 1	next_phase = 0	reward = -0.731079	array([[-2.5051718, -1.6075447]], dtype=float32)

time = 1181	action = 0	current_phase = 0	next_phase = 1	reward = -0.002832	array([[ 0.23785824, -2.247473  ]], dtype=float32)

time = 1186	action = 0	current_phase = 0	next_phase = 1	reward = 0.069425	array([[-0.20067197, -2.7320197 ]], dtype=float32)

time = 1191	action = 0	current_phase = 0	next_phase = 1	reward = -0.062398	array([[-0.48162383, -2.72309   ]], dtype=float32)

time = 1196	action = 0	current_phase = 0	next_phase = 1	reward = -0.888026	array([[-0.80008346, -2.973129  ]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.2144 - val_loss: 0.1671

Epoch 2/50

 - 3s - loss: 0.2106 - val_loss: 0.1741

Epoch 3/50

 - 3s - loss: 0.2181 - val_loss: 0.1634

Epoch 4/50

 - 3s - loss: 0.1919 - val_loss: 0.1645

Epoch 5/50

 - 3s - loss: 0.2271 - val_loss: 0.1634

Epoch 6/50

 - 3s - loss: 0.1842 - val_loss: 0.1563

Epoch 7/50

 - 3s - loss: 0.1925 - val_loss: 0.1633

Epoch 8/50

 - 3s - loss: 0.2008 - val_loss: 0.1660

Epoch 9/50

 - 3s - loss: 0.1901 - val_loss: 0.1591

Epoch 10/50

 - 3s - loss: 0.1732 - val_loss: 0.1688

Epoch 11/50

 - 4s - loss: 0.1788 - val_loss: 0.1616

Epoch 12/50

 - 4s - loss: 0.1834 - val_loss: 0.1573

Epoch 13/50

 - 4s - loss: 0.1822 - val_loss: 0.1783

Epoch 14/50

 - 4s - loss: 0.1677 - val_loss: 0.1640

Epoch 15/50

 - 4s - loss: 0.1652 - val_loss: 0.1627

Epoch 16/50

 - 3s - loss: 0.1570 - val_loss: 0.1669

length of memory (state 0, action 0): 690, after forget

length of memory (state 0, action 1): 319, after forget

length of memory (state 1, action 0): 492, after forget

length of memory (state 1, action 1): 307, after forget

time = 1201	action = 0	current_phase = 0	next_phase = 1	reward = -1.230657	array([[-1.3967255, -3.1205096]], dtype=float32)

time = 1206	action = 0	current_phase = 0	next_phase = 1	reward = -1.199458	array([[-1.9906899, -3.4140635]], dtype=float32)

time = 1211	action = 0	current_phase = 0	next_phase = 1	reward = -1.153831	array([[-1.6972293, -2.9986336]], dtype=float32)

time = 1216	action = 0	current_phase = 0	next_phase = 1	reward = -1.139985	array([[-1.9060225, -2.8200889]], dtype=float32)

time = 1221	action = 0	current_phase = 0	next_phase = 1	reward = -1.242241	array([[-1.99778  , -2.7942882]], dtype=float32)

time = 1226	action = 0	current_phase = 0	next_phase = 1	reward = -2.024791	array([[-2.684585, -3.084701]], dtype=float32)

time = 1231	action = 0	current_phase = 0	next_phase = 1	reward = -2.131221	array([[-2.6163888, -3.3600292]], dtype=float32)

time = 1236	action = 0	current_phase = 0	next_phase = 1	reward = -2.177061	array([[-2.8586936, -3.2193542]], dtype=float32)

time = 1241	action = 0	current_phase = 0	next_phase = 1	reward = -2.213366	array([[-2.2544937, -2.8627648]], dtype=float32)

time = 1246	action = 0	current_phase = 0	next_phase = 1	reward = -2.276605	array([[-2.5421567, -2.9166398]], dtype=float32)

time = 1251	action = 1	current_phase = 0	next_phase = 1	reward = -1.583215	array([[-3.3608088, -2.8230948]], dtype=float32)

time = 1259	action = 1	current_phase = 1	next_phase = 0	reward = -1.408558	array([[-2.4733458, -1.7058043]], dtype=float32)

time = 1267	action = 0	current_phase = 0	next_phase = 1	reward = 0.513851	array([[ 0.3585894, -2.396875 ]], dtype=float32)

time = 1272	action = 0	current_phase = 0	next_phase = 1	reward = 0.020701	array([[-0.0673418, -2.6004233]], dtype=float32)

time = 1277	action = 0	current_phase = 0	next_phase = 1	reward = 0.072634	array([[-0.2668312, -2.718332 ]], dtype=float32)

time = 1282	action = 0	current_phase = 0	next_phase = 1	reward = -0.215326	array([[-0.3297068, -2.9278793]], dtype=float32)

time = 1287	action = 0	current_phase = 0	next_phase = 1	reward = -1.020632	array([[-0.56374454, -3.0628328 ]], dtype=float32)

time = 1292	action = 0	current_phase = 0	next_phase = 1	reward = -1.225070	array([[-0.77209395, -3.2077017 ]], dtype=float32)

time = 1297	action = 0	current_phase = 0	next_phase = 1	reward = -1.199312	array([[-1.5173029, -3.3321455]], dtype=float32)

time = 1302	action = 0	current_phase = 0	next_phase = 1	reward = -1.140961	array([[-1.485601, -2.763238]], dtype=float32)

time = 1307	action = 0	current_phase = 0	next_phase = 1	reward = -1.118211	array([[-1.9360996, -3.217712 ]], dtype=float32)

time = 1312	action = 0	current_phase = 0	next_phase = 1	reward = -1.401858	array([[-2.0676057, -2.6644368]], dtype=float32)

time = 1317	action = 0	current_phase = 0	next_phase = 1	reward = -2.062903	array([[-2.6528902, -3.2839608]], dtype=float32)

time = 1322	action = 0	current_phase = 0	next_phase = 1	reward = -2.125840	array([[-2.6745477, -3.1113124]], dtype=float32)

time = 1327	action = 0	current_phase = 0	next_phase = 1	reward = -2.174239	array([[-2.4542756, -3.8849735]], dtype=float32)

time = 1332	action = 0	current_phase = 0	next_phase = 1	reward = -2.209654	array([[-2.4785912, -2.954858 ]], dtype=float32)

time = 1337	action = 0	current_phase = 0	next_phase = 1	reward = -2.259245	array([[-3.1087985, -3.413606 ]], dtype=float32)

time = 1342	action = 1	current_phase = 0	next_phase = 1	reward = -1.837547	array([[-3.4368095, -3.0201442]], dtype=float32)

time = 1350	action = 1	current_phase = 1	next_phase = 0	reward = -1.669508	array([[-2.313888 , -1.8778769]], dtype=float32)

time = 1358	action = 0	current_phase = 0	next_phase = 1	reward = 0.509260	array([[ 0.31434935, -2.428095  ]], dtype=float32)

time = 1363	action = 0	current_phase = 0	next_phase = 1	reward = 0.015720	array([[-0.09265041, -2.7574244 ]], dtype=float32)

time = 1368	action = 0	current_phase = 0	next_phase = 1	reward = 0.068204	array([[-0.37714088, -2.9051902 ]], dtype=float32)

time = 1373	action = 0	current_phase = 0	next_phase = 1	reward = -0.434257	array([[-0.3357608, -2.7977934]], dtype=float32)

time = 1378	action = 0	current_phase = 0	next_phase = 1	reward = -1.092585	array([[-0.8553953, -3.3637733]], dtype=float32)

time = 1383	action = 0	current_phase = 0	next_phase = 1	reward = -1.198683	array([[-1.1518223, -3.3776796]], dtype=float32)

time = 1388	action = 0	current_phase = 0	next_phase = 1	reward = -1.175605	array([[-1.9196804, -3.0843966]], dtype=float32)

time = 1393	action = 0	current_phase = 0	next_phase = 1	reward = -1.138619	array([[-1.7502208, -2.7730997]], dtype=float32)

time = 1398	action = 0	current_phase = 0	next_phase = 1	reward = -1.131494	array([[-1.9912137, -3.3929205]], dtype=float32)

time = 1403	action = 0	current_phase = 0	next_phase = 1	reward = -1.630462	array([[-2.3000765, -3.2851794]], dtype=float32)

time = 1408	action = 0	current_phase = 0	next_phase = 1	reward = -2.136475	array([[-2.898508, -3.465084]], dtype=float32)

time = 1413	action = 0	current_phase = 0	next_phase = 1	reward = -2.144115	array([[-3.0885453, -3.3087506]], dtype=float32)

time = 1418	action = 0	current_phase = 0	next_phase = 1	reward = -2.191728	array([[-2.6481256, -3.5293064]], dtype=float32)

time = 1423	action = 0	current_phase = 0	next_phase = 1	reward = -2.235100	array([[-2.6715598, -3.6606388]], dtype=float32)

time = 1428	action = 1	current_phase = 0	next_phase = 1	reward = -2.356502	array([[-3.2730656, -3.205676 ]], dtype=float32)

time = 1436	action = 1	current_phase = 1	next_phase = 0	reward = -0.573036	array([[-2.3991218 , -0.99918234]], dtype=float32)

time = 1444	action = 0	current_phase = 0	next_phase = 1	reward = 0.185252	array([[ 0.37018597, -2.2672696 ]], dtype=float32)

time = 1449	action = 0	current_phase = 0	next_phase = 1	reward = -0.028109	array([[ 0.21881723, -2.3198266 ]], dtype=float32)

time = 1454	action = 0	current_phase = 0	next_phase = 1	reward = 0.063297	array([[-0.11276436, -2.5799694 ]], dtype=float32)

time = 1459	action = 0	current_phase = 0	next_phase = 1	reward = 0.066644	array([[-0.35349128, -2.4402492 ]], dtype=float32)

time = 1464	action = 0	current_phase = 0	next_phase = 1	reward = -0.539468	array([[-0.2328577, -2.7452524]], dtype=float32)

time = 1469	action = 0	current_phase = 0	next_phase = 1	reward = -1.159466	array([[-0.7660209, -2.959544 ]], dtype=float32)

time = 1474	action = 0	current_phase = 0	next_phase = 1	reward = -1.203658	array([[-1.752293 , -3.1845026]], dtype=float32)

time = 1479	action = 0	current_phase = 0	next_phase = 1	reward = -1.155290	array([[-1.4706985, -2.9096594]], dtype=float32)

time = 1484	action = 0	current_phase = 0	next_phase = 1	reward = -0.860149	array([[-1.7989584, -3.0796683]], dtype=float32)

time = 1489	action = 0	current_phase = 0	next_phase = 1	reward = -1.429504	array([[-2.4351559, -2.9190004]], dtype=float32)

time = 1494	action = 0	current_phase = 0	next_phase = 1	reward = -1.731364	array([[-2.4750376, -2.943203 ]], dtype=float32)

time = 1499	action = 1	current_phase = 0	next_phase = 1	reward = -1.585350	array([[-3.514349 , -2.8719778]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.1502 - val_loss: 0.2106

Epoch 2/50

 - 3s - loss: 0.1362 - val_loss: 0.2024

Epoch 3/50

 - 3s - loss: 0.1622 - val_loss: 0.2027

Epoch 4/50

 - 4s - loss: 0.1393 - val_loss: 0.2014

Epoch 5/50

 - 4s - loss: 0.1377 - val_loss: 0.2060

Epoch 6/50

 - 3s - loss: 0.1371 - val_loss: 0.1976

Epoch 7/50

 - 3s - loss: 0.1299 - val_loss: 0.1960

Epoch 8/50

 - 3s - loss: 0.1265 - val_loss: 0.1976

Epoch 9/50

 - 3s - loss: 0.1259 - val_loss: 0.1946

Epoch 10/50

 - 3s - loss: 0.1271 - val_loss: 0.1997

Epoch 11/50

 - 4s - loss: 0.1334 - val_loss: 0.2030

Epoch 12/50

 - 3s - loss: 0.1240 - val_loss: 0.2031

Epoch 13/50

 - 3s - loss: 0.1279 - val_loss: 0.2099

Epoch 14/50

 - 3s - loss: 0.1213 - val_loss: 0.2023

Epoch 15/50

 - 4s - loss: 0.1170 - val_loss: 0.1985

Epoch 16/50

 - 4s - loss: 0.1314 - val_loss: 0.2049

Epoch 17/50

 - 4s - loss: 0.1284 - val_loss: 0.1958

Epoch 18/50

 - 4s - loss: 0.1181 - val_loss: 0.1981

Epoch 19/50

 - 3s - loss: 0.1061 - val_loss: 0.2043

length of memory (state 0, action 0): 740, after forget

length of memory (state 0, action 1): 323, after forget

length of memory (state 1, action 0): 492, after forget

length of memory (state 1, action 1): 310, after forget

time = 1507	action = 1	current_phase = 1	next_phase = 0	reward = -0.913613	array([[-2.4575057 , -0.86370337]], dtype=float32)

time = 1515	action = 0	current_phase = 0	next_phase = 1	reward = 0.342296	array([[ 0.361587 , -2.3007116]], dtype=float32)

time = 1520	action = 0	current_phase = 0	next_phase = 1	reward = -0.275007	array([[-0.60661185, -2.4781706 ]], dtype=float32)

time = 1525	action = 0	current_phase = 0	next_phase = 1	reward = -0.355754	array([[-0.37353432, -2.5106285 ]], dtype=float32)

time = 1530	action = 0	current_phase = 0	next_phase = 1	reward = -1.499537	array([[-1.0647413, -3.0307827]], dtype=float32)

time = 1535	action = 0	current_phase = 0	next_phase = 1	reward = -0.909959	array([[-1.4199156, -3.132785 ]], dtype=float32)

time = 1540	action = 0	current_phase = 0	next_phase = 1	reward = -1.150541	array([[-1.6310132, -2.9550934]], dtype=float32)

time = 1545	action = 0	current_phase = 0	next_phase = 1	reward = -1.119169	array([[-1.3908501, -2.8368967]], dtype=float32)

time = 1550	action = 0	current_phase = 0	next_phase = 1	reward = -1.216451	array([[-2.2251368, -2.7949984]], dtype=float32)

time = 1555	action = 0	current_phase = 0	next_phase = 1	reward = -2.227027	array([[-2.4468389, -2.9478266]], dtype=float32)

time = 1560	action = 1	current_phase = 0	next_phase = 1	reward = -1.879501	array([[-3.2255847, -3.1016495]], dtype=float32)

time = 1568	action = 1	current_phase = 1	next_phase = 0	reward = -0.613211	array([[-2.4851224, -0.8517739]], dtype=float32)

time = 1576	action = 0	current_phase = 0	next_phase = 1	reward = 0.083905	array([[ 0.15492296, -2.3896587 ]], dtype=float32)

time = 1581	action = 0	current_phase = 0	next_phase = 1	reward = -0.083702	array([[-0.32098928, -2.7023833 ]], dtype=float32)

time = 1586	action = 0	current_phase = 0	next_phase = 1	reward = -0.933143	array([[-0.4909683, -2.8647923]], dtype=float32)

time = 1591	action = 0	current_phase = 0	next_phase = 1	reward = -1.232684	array([[-1.3002442, -3.249903 ]], dtype=float32)

time = 1596	action = 0	current_phase = 0	next_phase = 1	reward = -1.196515	array([[-1.1562103, -3.3277037]], dtype=float32)

time = 1601	action = 0	current_phase = 0	next_phase = 1	reward = -1.145577	array([[-1.5330057, -2.9642842]], dtype=float32)

time = 1606	action = 0	current_phase = 0	next_phase = 1	reward = -1.124100	array([[-1.8555125, -2.854676 ]], dtype=float32)

time = 1611	action = 0	current_phase = 0	next_phase = 1	reward = -1.358409	array([[-1.9749724, -2.9557936]], dtype=float32)

time = 1616	action = 0	current_phase = 0	next_phase = 1	reward = -2.069937	array([[-2.6708364, -3.3476017]], dtype=float32)

time = 1621	action = 0	current_phase = 0	next_phase = 1	reward = -2.134901	array([[-2.845675, -3.345277]], dtype=float32)

time = 1626	action = 0	current_phase = 0	next_phase = 1	reward = -2.189747	array([[-2.5826378, -3.2056277]], dtype=float32)

time = 1631	action = 0	current_phase = 0	next_phase = 1	reward = -2.224523	array([[-2.3850741, -3.1695335]], dtype=float32)

time = 1636	action = 0	current_phase = 0	next_phase = 1	reward = -2.282794	array([[-2.8310776, -3.0490124]], dtype=float32)

time = 1641	action = 1	current_phase = 0	next_phase = 1	reward = -1.609513	array([[-2.9420772, -2.8397963]], dtype=float32)

time = 1649	action = 1	current_phase = 1	next_phase = 0	reward = -1.125574	array([[-2.2571456, -1.5218751]], dtype=float32)

time = 1657	action = 0	current_phase = 0	next_phase = 1	reward = 0.231135	array([[ 0.45321822, -2.2574232 ]], dtype=float32)

time = 1662	action = 0	current_phase = 0	next_phase = 1	reward = 0.011421	array([[ 0.15824568, -2.5493882 ]], dtype=float32)

time = 1667	action = 0	current_phase = 0	next_phase = 1	reward = 0.069912	array([[-0.23110676, -2.7428079 ]], dtype=float32)

time = 1672	action = 0	current_phase = 0	next_phase = 1	reward = -0.176925	array([[-0.2256338, -2.7209306]], dtype=float32)

time = 1677	action = 0	current_phase = 0	next_phase = 1	reward = -0.968326	array([[-0.6803638, -3.2004135]], dtype=float32)

time = 1682	action = 0	current_phase = 0	next_phase = 1	reward = -1.213315	array([[-0.86758006, -3.194625  ]], dtype=float32)

time = 1687	action = 0	current_phase = 0	next_phase = 1	reward = -1.179877	array([[-1.8216412, -3.1082995]], dtype=float32)

time = 1692	action = 0	current_phase = 0	next_phase = 1	reward = -1.132401	array([[-1.2765875, -2.915178 ]], dtype=float32)

time = 1697	action = 0	current_phase = 0	next_phase = 1	reward = -1.116029	array([[-1.9597667, -3.0845215]], dtype=float32)

time = 1702	action = 0	current_phase = 0	next_phase = 1	reward = -1.426738	array([[-1.935696, -2.817592]], dtype=float32)

time = 1707	action = 0	current_phase = 0	next_phase = 1	reward = -2.109798	array([[-2.6195748, -3.212799 ]], dtype=float32)

time = 1712	action = 0	current_phase = 0	next_phase = 1	reward = -2.119173	array([[-2.5482895, -3.0716794]], dtype=float32)

time = 1717	action = 0	current_phase = 0	next_phase = 1	reward = -2.176519	array([[-2.503169 , -3.4250576]], dtype=float32)

time = 1722	action = 0	current_phase = 0	next_phase = 1	reward = -2.222871	array([[-2.1591485, -3.0123293]], dtype=float32)

time = 1727	action = 1	current_phase = 0	next_phase = 1	reward = -2.121284	array([[-3.3036146, -3.2429874]], dtype=float32)

time = 1735	action = 1	current_phase = 1	next_phase = 0	reward = -1.394941	array([[-2.021477 , -1.2915151]], dtype=float32)

time = 1743	action = 0	current_phase = 0	next_phase = 1	reward = -0.376669	array([[-0.39069498, -2.625464  ]], dtype=float32)

time = 1748	action = 0	current_phase = 0	next_phase = 1	reward = -0.618417	array([[-0.33195388, -3.1822011 ]], dtype=float32)

time = 1753	action = 0	current_phase = 0	next_phase = 1	reward = -0.574858	array([[-0.54674286, -2.9453874 ]], dtype=float32)

time = 1758	action = 0	current_phase = 0	next_phase = 1	reward = -0.537921	array([[-0.8493954, -3.189995 ]], dtype=float32)

time = 1763	action = 0	current_phase = 0	next_phase = 1	reward = -0.944413	array([[-0.46744484, -3.0079668 ]], dtype=float32)

time = 1768	action = 0	current_phase = 0	next_phase = 1	reward = -1.597638	array([[-1.4589568, -3.3120494]], dtype=float32)

time = 1773	action = 0	current_phase = 0	next_phase = 1	reward = -1.655229	array([[-2.6707916, -3.2938447]], dtype=float32)

time = 1778	action = 0	current_phase = 0	next_phase = 1	reward = -1.678909	array([[-2.5598888, -3.3996472]], dtype=float32)

time = 1783	action = 0	current_phase = 0	next_phase = 1	reward = -1.680873	array([[-2.602825 , -3.1811225]], dtype=float32)

time = 1788	action = 0	current_phase = 0	next_phase = 1	reward = -1.714186	array([[-2.4172654, -3.4278586]], dtype=float32)

time = 1793	action = 1	current_phase = 0	next_phase = 1	reward = -1.987474	array([[-3.5438426, -3.1791189]], dtype=float32)

time = 1801	action = 1	current_phase = 1	next_phase = 0	reward = -1.634340	array([[-2.4696636 , -0.71922076]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 4s - loss: 0.2773 - val_loss: 0.3132

Epoch 2/50

 - 3s - loss: 0.2492 - val_loss: 0.3227

Epoch 3/50

 - 3s - loss: 0.2350 - val_loss: 0.3199

Epoch 4/50

 - 3s - loss: 0.2046 - val_loss: 0.2896

Epoch 5/50

 - 4s - loss: 0.2009 - val_loss: 0.3213

Epoch 6/50

 - 3s - loss: 0.1980 - val_loss: 0.2889

Epoch 7/50

 - 3s - loss: 0.2005 - val_loss: 0.3014

Epoch 8/50

 - 3s - loss: 0.2089 - val_loss: 0.2912

Epoch 9/50

 - 3s - loss: 0.1814 - val_loss: 0.2851

Epoch 10/50

 - 3s - loss: 0.1817 - val_loss: 0.2938

Epoch 11/50

 - 3s - loss: 0.1682 - val_loss: 0.3059

Epoch 12/50

 - 4s - loss: 0.1780 - val_loss: 0.3272

Epoch 13/50

 - 3s - loss: 0.1696 - val_loss: 0.2929

Epoch 14/50

 - 3s - loss: 0.1850 - val_loss: 0.2881

Epoch 15/50

 - 3s - loss: 0.1974 - val_loss: 0.2917

Epoch 16/50

 - 3s - loss: 0.1614 - val_loss: 0.3130

Epoch 17/50

 - 3s - loss: 0.1871 - val_loss: 0.3010

Epoch 18/50

 - 3s - loss: 0.1706 - val_loss: 0.2830

Epoch 19/50

 - 4s - loss: 0.1582 - val_loss: 0.2902

Epoch 20/50

 - 3s - loss: 0.1628 - val_loss: 0.3119

Epoch 21/50

 - 4s - loss: 0.2061 - val_loss: 0.2994

Epoch 22/50

 - 3s - loss: 0.1423 - val_loss: 0.3099

Epoch 23/50

 - 4s - loss: 0.1516 - val_loss: 0.3028

Epoch 24/50

 - 3s - loss: 0.1515 - val_loss: 0.3101

Epoch 25/50

 - 3s - loss: 0.1361 - val_loss: 0.3101

Epoch 26/50

 - 3s - loss: 0.1415 - val_loss: 0.2867

Epoch 27/50

 - 4s - loss: 0.1724 - val_loss: 0.2901

Epoch 28/50

 - 4s - loss: 0.1545 - val_loss: 0.2922

length of memory (state 0, action 0): 786, after forget

length of memory (state 0, action 1): 327, after forget

length of memory (state 1, action 0): 492, after forget

length of memory (state 1, action 1): 315, after forget

time = 1809	action = 0	current_phase = 0	next_phase = 1	reward = 0.560336	array([[ 0.4551741, -2.1969495]], dtype=float32)

time = 1814	action = 0	current_phase = 0	next_phase = 1	reward = 0.031812	array([[ 0.12777913, -2.4785197 ]], dtype=float32)

time = 1819	action = 0	current_phase = 0	next_phase = 1	reward = 0.047727	array([[-0.42727596, -2.766698  ]], dtype=float32)

time = 1824	action = 0	current_phase = 0	next_phase = 1	reward = -0.548551	array([[-0.41784418, -2.7237217 ]], dtype=float32)

time = 1829	action = 0	current_phase = 0	next_phase = 1	reward = -1.167362	array([[-1.872673 , -3.0316448]], dtype=float32)

time = 1834	action = 0	current_phase = 0	next_phase = 1	reward = -1.208516	array([[-2.277829 , -2.8936458]], dtype=float32)

time = 1839	action = 0	current_phase = 0	next_phase = 1	reward = -1.178546	array([[-2.4773655, -2.703873 ]], dtype=float32)

time = 1844	action = 1	current_phase = 0	next_phase = 1	reward = -1.418344	array([[-3.1140783, -2.7432673]], dtype=float32)

time = 1852	action = 1	current_phase = 1	next_phase = 0	reward = -1.243458	array([[-2.1748943, -1.6715627]], dtype=float32)

time = 1860	action = 0	current_phase = 0	next_phase = 1	reward = -1.513504	array([[-1.9130089, -2.900327 ]], dtype=float32)

time = 1865	action = 0	current_phase = 0	next_phase = 1	reward = -1.194638	array([[-2.0624745, -2.8590457]], dtype=float32)

time = 1870	action = 0	current_phase = 0	next_phase = 1	reward = -0.891948	array([[-1.6040106, -2.9483802]], dtype=float32)

time = 1875	action = 0	current_phase = 0	next_phase = 1	reward = -1.424997	array([[-2.3458848, -3.0300128]], dtype=float32)

time = 1880	action = 0	current_phase = 0	next_phase = 1	reward = -1.208648	array([[-2.3346038, -3.117853 ]], dtype=float32)

time = 1885	action = 1	current_phase = 0	next_phase = 1	reward = -1.678916	array([[-4.516053 , -2.5698216]], dtype=float32)

time = 1893	action = 1	current_phase = 1	next_phase = 0	reward = -0.718301	array([[-2.3194745 , -0.79699785]], dtype=float32)

time = 1901	action = 0	current_phase = 0	next_phase = 1	reward = -0.012199	array([[ 0.16200048, -2.3791628 ]], dtype=float32)

time = 1906	action = 0	current_phase = 0	next_phase = 1	reward = 0.051263	array([[-0.09048045, -2.4245398 ]], dtype=float32)

time = 1911	action = 0	current_phase = 0	next_phase = 1	reward = -0.040177	array([[-0.4406324, -2.7443326]], dtype=float32)

time = 1916	action = 0	current_phase = 0	next_phase = 1	reward = -0.833418	array([[-0.42189997, -3.020732  ]], dtype=float32)

time = 1921	action = 0	current_phase = 0	next_phase = 1	reward = -1.210738	array([[-1.3031249, -2.9765918]], dtype=float32)

time = 1926	action = 0	current_phase = 0	next_phase = 1	reward = -1.189510	array([[-2.844832, -3.381386]], dtype=float32)

time = 1931	action = 0	current_phase = 0	next_phase = 1	reward = -1.155338	array([[-2.1577196, -2.9173448]], dtype=float32)

time = 1936	action = 0	current_phase = 0	next_phase = 1	reward = -1.130220	array([[-2.4681811, -3.0398128]], dtype=float32)

time = 1941	action = 1	current_phase = 0	next_phase = 1	reward = -1.607081	array([[-2.7938206, -2.779928 ]], dtype=float32)

time = 1949	action = 1	current_phase = 1	next_phase = 0	reward = -0.786733	array([[-2.119835 , -1.3838952]], dtype=float32)

time = 1957	action = 0	current_phase = 0	next_phase = 1	reward = -0.071194	array([[ 0.26450598, -2.5081782 ]], dtype=float32)

time = 1962	action = 0	current_phase = 0	next_phase = 1	reward = 0.010945	array([[-0.20807749, -2.8254304 ]], dtype=float32)

time = 1967	action = 0	current_phase = 0	next_phase = 1	reward = 0.066973	array([[-0.368627 , -2.8641295]], dtype=float32)

time = 1972	action = 0	current_phase = 0	next_phase = 1	reward = -0.184019	array([[-0.5062221, -2.8742313]], dtype=float32)

time = 1977	action = 0	current_phase = 0	next_phase = 1	reward = -1.020778	array([[-1.286716 , -3.5584006]], dtype=float32)

time = 1982	action = 0	current_phase = 0	next_phase = 1	reward = -1.219452	array([[-2.518736 , -3.2912133]], dtype=float32)

time = 1987	action = 0	current_phase = 0	next_phase = 1	reward = -1.182443	array([[-2.418856 , -3.2543852]], dtype=float32)

time = 1992	action = 0	current_phase = 0	next_phase = 1	reward = -1.153936	array([[-2.6669981, -3.1889014]], dtype=float32)

time = 1997	action = 0	current_phase = 0	next_phase = 1	reward = -1.137244	array([[-2.3813453, -3.151182 ]], dtype=float32)

time = 2002	action = 1	current_phase = 0	next_phase = 1	reward = -1.585792	array([[-3.2084587, -2.9453025]], dtype=float32)

time = 2010	action = 1	current_phase = 1	next_phase = 0	reward = -0.939117	array([[-2.3263636, -1.2714839]], dtype=float32)

time = 2018	action = 0	current_phase = 0	next_phase = 1	reward = -0.055745	array([[ 0.48172987, -2.29166   ]], dtype=float32)

time = 2023	action = 0	current_phase = 0	next_phase = 1	reward = 0.029874	array([[-0.4553712, -2.6407707]], dtype=float32)

time = 2028	action = 0	current_phase = 0	next_phase = 1	reward = 0.078349	array([[-0.3808089, -2.8715777]], dtype=float32)

time = 2033	action = 0	current_phase = 0	next_phase = 1	reward = -0.340941	array([[-0.8398945, -2.661579 ]], dtype=float32)

time = 2038	action = 0	current_phase = 0	next_phase = 1	reward = -1.093316	array([[-1.1629827, -3.0717564]], dtype=float32)

time = 2043	action = 0	current_phase = 0	next_phase = 1	reward = -1.224711	array([[-2.3322396, -2.9667294]], dtype=float32)

time = 2048	action = 0	current_phase = 0	next_phase = 1	reward = -1.185594	array([[-2.150074, -3.0075  ]], dtype=float32)

time = 2053	action = 0	current_phase = 0	next_phase = 1	reward = -1.146499	array([[-2.4262824, -3.0112092]], dtype=float32)

time = 2058	action = 0	current_phase = 0	next_phase = 1	reward = -1.121194	array([[-2.4574976, -3.0981944]], dtype=float32)

time = 2063	action = 0	current_phase = 0	next_phase = 1	reward = -1.566447	array([[-3.0469155, -3.072729 ]], dtype=float32)

time = 2068	action = 1	current_phase = 0	next_phase = 1	reward = -2.018102	array([[-3.8385274, -3.1739242]], dtype=float32)

time = 2076	action = 1	current_phase = 1	next_phase = 0	reward = -1.014474	array([[-2.469091  , -0.85657346]], dtype=float32)

time = 2084	action = 0	current_phase = 0	next_phase = 1	reward = 0.056309	array([[ 0.20506626, -2.1431339 ]], dtype=float32)

time = 2089	action = 0	current_phase = 0	next_phase = 1	reward = 0.046352	array([[-0.25399  , -2.3936453]], dtype=float32)

time = 2094	action = 0	current_phase = 0	next_phase = 1	reward = -0.549263	array([[-0.48430088, -2.3674228 ]], dtype=float32)

time = 2099	action = 0	current_phase = 0	next_phase = 1	reward = -1.145869	array([[-1.3253809, -2.7977662]], dtype=float32)

time = 2104	action = 0	current_phase = 0	next_phase = 1	reward = -1.197838	array([[-1.5205029, -3.0234642]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 4s - loss: 0.2016 - val_loss: 0.1799

Epoch 2/50

 - 4s - loss: 0.1609 - val_loss: 0.1971

Epoch 3/50

 - 4s - loss: 0.1375 - val_loss: 0.1794

Epoch 4/50

 - 4s - loss: 0.1393 - val_loss: 0.1880

Epoch 5/50

 - 4s - loss: 0.1378 - val_loss: 0.2029

Epoch 6/50

 - 4s - loss: 0.1269 - val_loss: 0.1856

Epoch 7/50

 - 4s - loss: 0.1373 - val_loss: 0.1892

Epoch 8/50

 - 3s - loss: 0.1155 - val_loss: 0.1905

Epoch 9/50

 - 3s - loss: 0.1458 - val_loss: 0.1927

Epoch 10/50

 - 3s - loss: 0.1327 - val_loss: 0.1924

Epoch 11/50

 - 4s - loss: 0.1417 - val_loss: 0.2079

Epoch 12/50

 - 3s - loss: 0.1407 - val_loss: 0.2022

Epoch 13/50

 - 3s - loss: 0.1415 - val_loss: 0.1831

length of memory (state 0, action 0): 830, after forget

length of memory (state 0, action 1): 332, after forget

length of memory (state 1, action 0): 492, after forget

length of memory (state 1, action 1): 320, after forget

time = 2109	action = 1	current_phase = 0	next_phase = 1	reward = -1.552104	array([[-3.087768 , -2.8195019]], dtype=float32)

time = 2117	action = 1	current_phase = 1	next_phase = 0	reward = -0.807426	array([[-2.3891351, -1.2349644]], dtype=float32)

time = 2125	action = 0	current_phase = 0	next_phase = 1	reward = -0.898393	array([[-1.3232849, -2.1545367]], dtype=float32)

time = 2130	action = 0	current_phase = 0	next_phase = 1	reward = -0.655566	array([[-2.2409768, -2.85016  ]], dtype=float32)

time = 2135	action = 0	current_phase = 0	next_phase = 1	reward = -1.197724	array([[-2.0857196, -3.0646234]], dtype=float32)

time = 2140	action = 0	current_phase = 0	next_phase = 1	reward = -1.156312	array([[-1.9094591, -3.1439593]], dtype=float32)

time = 2145	action = 0	current_phase = 0	next_phase = 1	reward = -1.125357	array([[-2.4528887, -2.8001928]], dtype=float32)

time = 2150	action = 1	current_phase = 0	next_phase = 1	reward = -1.325521	array([[-3.4511173, -2.5894308]], dtype=float32)

time = 2158	action = 1	current_phase = 1	next_phase = 0	reward = -0.697243	array([[-2.443136  , -0.75258803]], dtype=float32)

time = 2166	action = 0	current_phase = 0	next_phase = 1	reward = -0.085699	array([[ 0.12473947, -2.405685  ]], dtype=float32)

time = 2171	action = 0	current_phase = 0	next_phase = 1	reward = 0.002415	array([[-0.12022746, -2.6510837 ]], dtype=float32)

time = 2176	action = 0	current_phase = 0	next_phase = 1	reward = 0.056118	array([[-0.44606838, -2.9146006 ]], dtype=float32)

time = 2181	action = 0	current_phase = 0	next_phase = 1	reward = -0.053800	array([[-0.77612567, -2.7634892 ]], dtype=float32)

time = 2186	action = 0	current_phase = 0	next_phase = 1	reward = -0.898631	array([[-1.5641196, -2.8531997]], dtype=float32)

time = 2191	action = 0	current_phase = 0	next_phase = 1	reward = -1.218896	array([[-2.6846015, -2.791166 ]], dtype=float32)

time = 2196	action = 0	current_phase = 0	next_phase = 1	reward = -1.183474	array([[-3.1963155, -3.4411843]], dtype=float32)

time = 2201	action = 0	current_phase = 0	next_phase = 1	reward = -1.139902	array([[-2.5387523, -2.9902833]], dtype=float32)

time = 2206	action = 0	current_phase = 0	next_phase = 1	reward = -1.115774	array([[-2.7849402, -3.1516616]], dtype=float32)

time = 2211	action = 1	current_phase = 0	next_phase = 1	reward = -1.513096	array([[-3.391038 , -2.8995504]], dtype=float32)

time = 2219	action = 1	current_phase = 1	next_phase = 0	reward = -0.770774	array([[-2.129846 , -0.7935219]], dtype=float32)

time = 2227	action = 0	current_phase = 0	next_phase = 1	reward = -0.075329	array([[ 0.32949322, -2.3261094 ]], dtype=float32)

time = 2232	action = 0	current_phase = 0	next_phase = 1	reward = 0.000853	array([[-0.18874091, -2.8073165 ]], dtype=float32)

time = 2237	action = 0	current_phase = 0	next_phase = 1	reward = 0.060355	array([[-0.73582125, -2.9827597 ]], dtype=float32)

time = 2242	action = 0	current_phase = 0	next_phase = 1	reward = -0.103252	array([[-0.5513072, -2.7577307]], dtype=float32)

time = 2247	action = 0	current_phase = 0	next_phase = 1	reward = -1.025603	array([[-1.2536466, -2.9940846]], dtype=float32)

time = 2252	action = 0	current_phase = 0	next_phase = 1	reward = -1.216675	array([[-2.4470906, -3.301421 ]], dtype=float32)

time = 2257	action = 0	current_phase = 0	next_phase = 1	reward = -1.181485	array([[-2.7647097, -3.3973935]], dtype=float32)

time = 2262	action = 0	current_phase = 0	next_phase = 1	reward = -1.126765	array([[-2.747997 , -3.3732753]], dtype=float32)

time = 2267	action = 1	current_phase = 0	next_phase = 1	reward = -1.663500	array([[-3.090444, -2.991227]], dtype=float32)

time = 2275	action = 1	current_phase = 1	next_phase = 0	reward = -0.659807	array([[-2.091149  , -0.93412626]], dtype=float32)

time = 2283	action = 0	current_phase = 0	next_phase = 1	reward = -0.144151	array([[ 0.09346086, -2.1484482 ]], dtype=float32)

time = 2288	action = 0	current_phase = 0	next_phase = 1	reward = -0.058613	array([[-0.40215772, -2.9041235 ]], dtype=float32)

time = 2293	action = 0	current_phase = 0	next_phase = 1	reward = 0.025882	array([[-0.16457409, -2.7305202 ]], dtype=float32)

time = 2298	action = 0	current_phase = 0	next_phase = 1	reward = 0.081425	array([[-0.45096415, -2.867583  ]], dtype=float32)

time = 2303	action = 0	current_phase = 0	next_phase = 1	reward = -0.320099	array([[-1.1145097, -2.7886446]], dtype=float32)

time = 2308	action = 0	current_phase = 0	next_phase = 1	reward = -1.072406	array([[-1.8379321, -2.7455876]], dtype=float32)

time = 2313	action = 0	current_phase = 0	next_phase = 1	reward = -1.206231	array([[-2.4148135, -2.9634635]], dtype=float32)

time = 2318	action = 0	current_phase = 0	next_phase = 1	reward = -1.189475	array([[-2.9484138, -3.143529 ]], dtype=float32)

time = 2323	action = 0	current_phase = 0	next_phase = 1	reward = -1.153904	array([[-2.8623574, -3.106338 ]], dtype=float32)

time = 2328	action = 0	current_phase = 0	next_phase = 1	reward = -1.126688	array([[-2.83533  , -3.0784972]], dtype=float32)

time = 2333	action = 1	current_phase = 0	next_phase = 1	reward = -1.963880	array([[-3.777351, -2.7519  ]], dtype=float32)

time = 2341	action = 1	current_phase = 1	next_phase = 0	reward = -1.138136	array([[-2.4113688 , -0.87254107]], dtype=float32)

time = 2349	action = 0	current_phase = 0	next_phase = 1	reward = -0.029739	array([[ 0.20827669, -2.2549746 ]], dtype=float32)

time = 2354	action = 0	current_phase = 0	next_phase = 1	reward = 0.027325	array([[ 0.2154541, -2.299856 ]], dtype=float32)

time = 2359	action = 0	current_phase = 0	next_phase = 1	reward = 0.066159	array([[-0.4799007, -2.8478982]], dtype=float32)

time = 2364	action = 0	current_phase = 0	next_phase = 1	reward = -0.375802	array([[-0.92924416, -2.5229304 ]], dtype=float32)

time = 2369	action = 0	current_phase = 0	next_phase = 1	reward = -1.156236	array([[-1.2477007, -2.724629 ]], dtype=float32)

time = 2374	action = 1	current_phase = 0	next_phase = 1	reward = -1.638013	array([[-3.1308384, -2.6590276]], dtype=float32)

time = 2382	action = 1	current_phase = 1	next_phase = 0	reward = -1.069442	array([[-2.265609 , -1.5309687]], dtype=float32)

time = 2390	action = 0	current_phase = 0	next_phase = 1	reward = 0.577744	array([[-1.0048372, -2.4885316]], dtype=float32)

time = 2395	action = 0	current_phase = 0	next_phase = 1	reward = -0.632450	array([[-1.9384816, -2.1784644]], dtype=float32)

time = 2400	action = 0	current_phase = 0	next_phase = 1	reward = -1.487281	array([[-1.8493104, -3.0102358]], dtype=float32)

time = 2405	action = 0	current_phase = 0	next_phase = 1	reward = -0.895238	array([[-2.1135967, -3.3233407]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.2278 - val_loss: 0.0743

Epoch 2/50

 - 3s - loss: 0.2318 - val_loss: 0.0853

Epoch 3/50

 - 3s - loss: 0.2184 - val_loss: 0.0724

Epoch 4/50

 - 4s - loss: 0.2122 - val_loss: 0.0812

Epoch 5/50

 - 3s - loss: 0.1994 - val_loss: 0.0812

Epoch 6/50

 - 3s - loss: 0.2093 - val_loss: 0.0857

Epoch 7/50

 - 4s - loss: 0.2078 - val_loss: 0.0950

Epoch 8/50

 - 3s - loss: 0.2041 - val_loss: 0.0819

Epoch 9/50

 - 3s - loss: 0.1974 - val_loss: 0.1046

Epoch 10/50

 - 3s - loss: 0.2146 - val_loss: 0.0950

Epoch 11/50

 - 4s - loss: 0.1911 - val_loss: 0.0883

Epoch 12/50

 - 4s - loss: 0.1750 - val_loss: 0.1002

Epoch 13/50

 - 4s - loss: 0.2009 - val_loss: 0.0877

length of memory (state 0, action 0): 871, after forget

length of memory (state 0, action 1): 338, after forget

length of memory (state 1, action 0): 492, after forget

length of memory (state 1, action 1): 326, after forget

time = 2410	action = 0	current_phase = 0	next_phase = 1	reward = -1.141011	array([[-2.5806046, -2.9751182]], dtype=float32)

time = 2415	action = 1	current_phase = 0	next_phase = 1	reward = -2.080313	array([[-3.2325613, -3.0547388]], dtype=float32)

time = 2423	action = 1	current_phase = 1	next_phase = 0	reward = 0.110788	array([[-2.289213 , -1.5086796]], dtype=float32)

time = 2431	action = 0	current_phase = 0	next_phase = 1	reward = -0.161608	array([[-0.00275779, -2.2894425 ]], dtype=float32)

time = 2436	action = 0	current_phase = 0	next_phase = 1	reward = -0.093592	array([[ 0.05692315, -2.373946  ]], dtype=float32)

time = 2441	action = 0	current_phase = 0	next_phase = 1	reward = -0.004127	array([[-0.26336586, -2.9748042 ]], dtype=float32)

time = 2446	action = 0	current_phase = 0	next_phase = 1	reward = 0.067098	array([[-0.4596644, -3.0437176]], dtype=float32)

time = 2451	action = 0	current_phase = 0	next_phase = 1	reward = -0.066758	array([[-0.97861385, -2.7947457 ]], dtype=float32)

time = 2456	action = 0	current_phase = 0	next_phase = 1	reward = -0.841647	array([[-1.757567, -2.961624]], dtype=float32)

time = 2461	action = 0	current_phase = 0	next_phase = 1	reward = -1.226837	array([[-2.5442383, -2.9213347]], dtype=float32)

time = 2466	action = 0	current_phase = 0	next_phase = 1	reward = -1.179914	array([[-3.3793812, -3.4236202]], dtype=float32)

time = 2471	action = 0	current_phase = 0	next_phase = 1	reward = -1.131948	array([[-2.5478473, -3.112985 ]], dtype=float32)

time = 2476	action = 0	current_phase = 0	next_phase = 1	reward = -1.114116	array([[-2.886631 , -3.2028275]], dtype=float32)

time = 2481	action = 1	current_phase = 0	next_phase = 1	reward = -1.561644	array([[-3.3680084, -2.6574967]], dtype=float32)

time = 2489	action = 1	current_phase = 1	next_phase = 0	reward = -1.130392	array([[-2.21131   , -0.68594265]], dtype=float32)

time = 2497	action = 0	current_phase = 0	next_phase = 1	reward = 0.204413	array([[ 0.2982186, -2.306531 ]], dtype=float32)

time = 2502	action = 0	current_phase = 0	next_phase = 1	reward = -0.009891	array([[-0.10615456, -2.71075   ]], dtype=float32)

time = 2507	action = 0	current_phase = 0	next_phase = 1	reward = 0.050195	array([[-0.3908946, -3.0051153]], dtype=float32)

time = 2512	action = 0	current_phase = 0	next_phase = 1	reward = -0.090334	array([[-0.6802095, -2.8745399]], dtype=float32)

time = 2517	action = 0	current_phase = 0	next_phase = 1	reward = -1.004361	array([[-1.2655076, -3.057763 ]], dtype=float32)

time = 2522	action = 0	current_phase = 0	next_phase = 1	reward = -1.220938	array([[-2.6444154, -3.2520287]], dtype=float32)

time = 2527	action = 0	current_phase = 0	next_phase = 1	reward = -1.190835	array([[-2.608025 , -3.8125806]], dtype=float32)

time = 2532	action = 0	current_phase = 0	next_phase = 1	reward = -1.136962	array([[-2.9273436, -3.2183788]], dtype=float32)

time = 2537	action = 0	current_phase = 0	next_phase = 1	reward = -1.119328	array([[-3.038017 , -3.4505036]], dtype=float32)

time = 2542	action = 1	current_phase = 0	next_phase = 1	reward = -1.605765	array([[-3.558042 , -2.9215548]], dtype=float32)

time = 2550	action = 1	current_phase = 1	next_phase = 0	reward = -0.959235	array([[-2.1496844, -0.987688 ]], dtype=float32)

time = 2558	action = 0	current_phase = 0	next_phase = 1	reward = -0.048532	array([[ 0.19032604, -2.4513102 ]], dtype=float32)

time = 2563	action = 0	current_phase = 0	next_phase = 1	reward = 0.019317	array([[ 0.04716802, -2.5851183 ]], dtype=float32)

time = 2568	action = 0	current_phase = 0	next_phase = 1	reward = 0.078749	array([[-0.3924919, -3.080969 ]], dtype=float32)

time = 2573	action = 0	current_phase = 0	next_phase = 1	reward = -0.295725	array([[-0.5527665, -2.9781287]], dtype=float32)

time = 2578	action = 0	current_phase = 0	next_phase = 1	reward = -1.072760	array([[-2.300115, -3.545172]], dtype=float32)

time = 2583	action = 1	current_phase = 0	next_phase = 1	reward = -2.191362	array([[-3.4774675, -2.9980776]], dtype=float32)

time = 2591	action = 1	current_phase = 1	next_phase = 0	reward = -0.947539	array([[-2.3369243, -1.1322794]], dtype=float32)

time = 2599	action = 0	current_phase = 0	next_phase = 1	reward = 0.066599	array([[-0.39369056, -2.2908423 ]], dtype=float32)

time = 2604	action = 0	current_phase = 0	next_phase = 1	reward = -0.474666	array([[-1.3243545, -2.4591517]], dtype=float32)

time = 2609	action = 0	current_phase = 0	next_phase = 1	reward = -1.161963	array([[-1.3355227, -3.1921213]], dtype=float32)

time = 2614	action = 0	current_phase = 0	next_phase = 1	reward = -1.224182	array([[-1.6746494, -3.3965304]], dtype=float32)

time = 2619	action = 0	current_phase = 0	next_phase = 1	reward = -1.168687	array([[-2.6737437, -3.089963 ]], dtype=float32)

time = 2624	action = 1	current_phase = 0	next_phase = 1	reward = -1.391959	array([[-3.1642437, -2.6178048]], dtype=float32)

time = 2632	action = 0	current_phase = 1	next_phase = 0	reward = -0.584115	array([[-1.8370848, -2.6081266]], dtype=float32)

time = 2637	action = 1	current_phase = 1	next_phase = 0	reward = -1.313023	array([[-2.5510304, -0.8518399]], dtype=float32)

time = 2645	action = 0	current_phase = 0	next_phase = 1	reward = 0.479980	array([[ 0.1584444, -2.292189 ]], dtype=float32)

time = 2650	action = 0	current_phase = 0	next_phase = 1	reward = 0.251303	array([[-0.04762924, -2.8589869 ]], dtype=float32)

time = 2655	action = 0	current_phase = 0	next_phase = 1	reward = -0.238226	array([[-0.27573523, -2.9401023 ]], dtype=float32)

time = 2660	action = 0	current_phase = 0	next_phase = 1	reward = 0.318481	array([[-0.92598164, -2.8759305 ]], dtype=float32)

time = 2665	action = 0	current_phase = 0	next_phase = 1	reward = -0.903445	array([[-2.0064077, -2.8980513]], dtype=float32)

time = 2670	action = 1	current_phase = 0	next_phase = 1	reward = -2.175363	array([[-3.395118 , -3.0120044]], dtype=float32)

time = 2678	action = 1	current_phase = 1	next_phase = 0	reward = -0.891184	array([[-2.2792573, -1.4096596]], dtype=float32)

time = 2686	action = 0	current_phase = 0	next_phase = 1	reward = 0.356876	array([[-0.06116456, -2.3987198 ]], dtype=float32)

time = 2691	action = 0	current_phase = 0	next_phase = 1	reward = -0.061616	array([[-1.0627104, -2.5462387]], dtype=float32)

time = 2696	action = 0	current_phase = 0	next_phase = 1	reward = -0.842826	array([[-1.2262278, -2.8724983]], dtype=float32)

time = 2701	action = 0	current_phase = 0	next_phase = 1	reward = -1.224884	array([[-1.7412008, -3.0139887]], dtype=float32)

time = 2706	action = 0	current_phase = 0	next_phase = 1	reward = -1.192277	array([[-2.929606 , -3.3160179]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.1886 - val_loss: 0.0982

Epoch 2/50

 - 3s - loss: 0.1946 - val_loss: 0.1063

Epoch 3/50

 - 4s - loss: 0.1782 - val_loss: 0.1007

Epoch 4/50

 - 3s - loss: 0.1937 - val_loss: 0.1050

Epoch 5/50

 - 3s - loss: 0.1773 - val_loss: 0.1026

Epoch 6/50

 - 4s - loss: 0.1663 - val_loss: 0.1133

Epoch 7/50

 - 4s - loss: 0.1571 - val_loss: 0.0935

Epoch 8/50

 - 3s - loss: 0.1594 - val_loss: 0.1050

Epoch 9/50

 - 4s - loss: 0.1653 - val_loss: 0.1004

Epoch 10/50

 - 5s - loss: 0.1579 - val_loss: 0.1134

Epoch 11/50

 - 4s - loss: 0.1478 - val_loss: 0.1239

Epoch 12/50

 - 4s - loss: 0.1595 - val_loss: 0.0992

Epoch 13/50

 - 4s - loss: 0.1659 - val_loss: 0.1166

Epoch 14/50

 - 4s - loss: 0.1496 - val_loss: 0.1169

Epoch 15/50

 - 4s - loss: 0.1580 - val_loss: 0.1082

Epoch 16/50

 - 4s - loss: 0.1465 - val_loss: 0.1028

Epoch 17/50

 - 4s - loss: 0.1390 - val_loss: 0.1193

length of memory (state 0, action 0): 911, after forget

length of memory (state 0, action 1): 344, after forget

length of memory (state 1, action 0): 493, after forget

length of memory (state 1, action 1): 332, after forget

time = 2711	action = 0	current_phase = 0	next_phase = 1	reward = -1.155680	array([[-2.609887 , -3.1825807]], dtype=float32)

time = 2716	action = 0	current_phase = 0	next_phase = 1	reward = -1.133103	array([[-3.2489376, -3.3971796]], dtype=float32)

time = 2721	action = 1	current_phase = 0	next_phase = 1	reward = -1.395077	array([[-3.0413437, -2.7388508]], dtype=float32)

time = 2729	action = 1	current_phase = 1	next_phase = 0	reward = -0.757916	array([[-2.2572713 , -0.68723917]], dtype=float32)

time = 2737	action = 0	current_phase = 0	next_phase = 1	reward = -0.063901	array([[ 0.33620787, -2.4092948 ]], dtype=float32)

time = 2742	action = 0	current_phase = 0	next_phase = 1	reward = 0.016911	array([[-0.09809172, -2.8097725 ]], dtype=float32)

time = 2747	action = 0	current_phase = 0	next_phase = 1	reward = 0.076722	array([[-0.34112674, -2.8243616 ]], dtype=float32)

time = 2752	action = 0	current_phase = 0	next_phase = 1	reward = -0.142564	array([[-0.76995397, -2.86718   ]], dtype=float32)

time = 2757	action = 0	current_phase = 0	next_phase = 1	reward = -1.015398	array([[-1.2087513, -3.1204057]], dtype=float32)

time = 2762	action = 0	current_phase = 0	next_phase = 1	reward = -1.221009	array([[-2.880275 , -3.1337013]], dtype=float32)

time = 2767	action = 0	current_phase = 0	next_phase = 1	reward = -1.187064	array([[-2.4721117, -3.677877 ]], dtype=float32)

time = 2772	action = 0	current_phase = 0	next_phase = 1	reward = -1.130627	array([[-3.091969 , -3.2681956]], dtype=float32)

time = 2777	action = 0	current_phase = 0	next_phase = 1	reward = -1.114074	array([[-2.9054067, -3.3642035]], dtype=float32)

time = 2782	action = 1	current_phase = 0	next_phase = 1	reward = -1.662056	array([[-3.53311  , -3.1210098]], dtype=float32)

time = 2790	action = 1	current_phase = 1	next_phase = 0	reward = -1.005803	array([[-2.3262758, -0.7838169]], dtype=float32)

time = 2798	action = 0	current_phase = 0	next_phase = 1	reward = -0.063505	array([[ 0.22431695, -2.4417486 ]], dtype=float32)

time = 2803	action = 0	current_phase = 0	next_phase = 1	reward = 0.007099	array([[ 0.21570992, -2.5247915 ]], dtype=float32)

time = 2808	action = 0	current_phase = 0	next_phase = 1	reward = 0.072885	array([[-0.26889378, -2.9921968 ]], dtype=float32)

time = 2813	action = 0	current_phase = 0	next_phase = 1	reward = -0.302482	array([[-0.5025982, -2.927536 ]], dtype=float32)

time = 2818	action = 0	current_phase = 0	next_phase = 1	reward = -1.043244	array([[-1.7766083, -3.0684295]], dtype=float32)

time = 2823	action = 0	current_phase = 0	next_phase = 1	reward = -1.214594	array([[-2.9772623, -3.0740042]], dtype=float32)

time = 2828	action = 0	current_phase = 0	next_phase = 1	reward = -1.170363	array([[-2.4632678, -2.963664 ]], dtype=float32)

time = 2833	action = 0	current_phase = 0	next_phase = 1	reward = -1.143214	array([[-2.6958025, -3.4309301]], dtype=float32)

time = 2838	action = 0	current_phase = 0	next_phase = 1	reward = -1.111234	array([[-3.068852, -3.240707]], dtype=float32)

time = 2843	action = 1	current_phase = 0	next_phase = 1	reward = -1.863141	array([[-3.6412745, -3.149538 ]], dtype=float32)

time = 2851	action = 1	current_phase = 1	next_phase = 0	reward = -1.344645	array([[-2.4105968, -1.0691975]], dtype=float32)

time = 2859	action = 0	current_phase = 0	next_phase = 1	reward = 0.248840	array([[ 0.40117788, -2.221055  ]], dtype=float32)

time = 2864	action = 0	current_phase = 0	next_phase = 1	reward = 0.018894	array([[ 0.14899361, -2.4206889 ]], dtype=float32)

time = 2869	action = 0	current_phase = 0	next_phase = 1	reward = 0.057625	array([[-0.14439237, -2.693933  ]], dtype=float32)

time = 2874	action = 0	current_phase = 0	next_phase = 1	reward = -0.523838	array([[-0.89455074, -2.726301  ]], dtype=float32)

time = 2879	action = 0	current_phase = 0	next_phase = 1	reward = -1.168487	array([[-2.0135288, -3.0878265]], dtype=float32)

time = 2884	action = 1	current_phase = 0	next_phase = 1	reward = -1.646459	array([[-3.0201812, -2.9177532]], dtype=float32)

time = 2892	action = 1	current_phase = 1	next_phase = 0	reward = -0.492648	array([[-2.250906 , -1.5117007]], dtype=float32)

time = 2900	action = 0	current_phase = 0	next_phase = 1	reward = -0.522985	array([[-0.80610216, -2.5639327 ]], dtype=float32)

time = 2905	action = 0	current_phase = 0	next_phase = 1	reward = -0.558468	array([[-0.67785347, -2.378015  ]], dtype=float32)

time = 2910	action = 0	current_phase = 0	next_phase = 1	reward = -0.941342	array([[-1.4592267, -3.277523 ]], dtype=float32)

time = 2915	action = 0	current_phase = 0	next_phase = 1	reward = -1.193144	array([[-2.117275 , -3.2763371]], dtype=float32)

time = 2920	action = 0	current_phase = 0	next_phase = 1	reward = -0.873269	array([[-2.3752666, -3.0918376]], dtype=float32)

time = 2925	action = 1	current_phase = 0	next_phase = 1	reward = -1.991260	array([[-3.1570964, -2.786283 ]], dtype=float32)

time = 2933	action = 1	current_phase = 1	next_phase = 0	reward = -0.641675	array([[-2.400827, -1.039345]], dtype=float32)

time = 2941	action = 0	current_phase = 0	next_phase = 1	reward = -0.699155	array([[-1.3999639, -2.883065 ]], dtype=float32)

time = 2946	action = 0	current_phase = 0	next_phase = 1	reward = -0.662287	array([[-1.0351503, -3.1134758]], dtype=float32)

time = 2951	action = 0	current_phase = 0	next_phase = 1	reward = -0.590503	array([[-0.8590665, -3.056655 ]], dtype=float32)

time = 2956	action = 0	current_phase = 0	next_phase = 1	reward = -0.546723	array([[-1.3958592, -3.35639  ]], dtype=float32)

time = 2961	action = 0	current_phase = 0	next_phase = 1	reward = -0.608338	array([[-2.2163157, -2.9003007]], dtype=float32)

time = 2966	action = 1	current_phase = 0	next_phase = 1	reward = -2.124096	array([[-3.3779664, -3.118344 ]], dtype=float32)

time = 2974	action = 1	current_phase = 1	next_phase = 0	reward = -0.743284	array([[-2.5354614, -0.625983 ]], dtype=float32)

time = 2982	action = 0	current_phase = 0	next_phase = 1	reward = 0.004030	array([[-0.12121618, -2.36925   ]], dtype=float32)

time = 2987	action = 0	current_phase = 0	next_phase = 1	reward = 0.077640	array([[-0.31388733, -2.6774929 ]], dtype=float32)

time = 2992	action = 0	current_phase = 0	next_phase = 1	reward = -0.180013	array([[-0.8209437, -2.5114574]], dtype=float32)

time = 2997	action = 0	current_phase = 0	next_phase = 1	reward = -1.017424	array([[-0.835829, -3.427049]], dtype=float32)

time = 3002	action = 0	current_phase = 0	next_phase = 1	reward = -1.217511	array([[-2.3883095, -3.3354988]], dtype=float32)

time = 3007	action = 0	current_phase = 0	next_phase = 1	reward = -1.175711	array([[-2.8674643, -3.6352608]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.1681 - val_loss: 0.0614

Epoch 2/50

 - 3s - loss: 0.1591 - val_loss: 0.0726

Epoch 3/50

 - 3s - loss: 0.1566 - val_loss: 0.0606

Epoch 4/50

 - 3s - loss: 0.1613 - val_loss: 0.0612

Epoch 5/50

 - 4s - loss: 0.1581 - val_loss: 0.0602

Epoch 6/50

 - 3s - loss: 0.1669 - val_loss: 0.0793

Epoch 7/50

 - 4s - loss: 0.1615 - val_loss: 0.0672

Epoch 8/50

 - 4s - loss: 0.1523 - val_loss: 0.0688

Epoch 9/50

 - 4s - loss: 0.1437 - val_loss: 0.0900

Epoch 10/50

 - 4s - loss: 0.1592 - val_loss: 0.0554

Epoch 11/50

 - 4s - loss: 0.1487 - val_loss: 0.0662

Epoch 12/50

 - 3s - loss: 0.1388 - val_loss: 0.0783

Epoch 13/50

 - 4s - loss: 0.1531 - val_loss: 0.0721

Epoch 14/50

 - 3s - loss: 0.1432 - val_loss: 0.0763

Epoch 15/50

 - 4s - loss: 0.1490 - val_loss: 0.0952

Epoch 16/50

 - 3s - loss: 0.1442 - val_loss: 0.0603

Epoch 17/50

 - 3s - loss: 0.1480 - val_loss: 0.0645

Epoch 18/50

 - 3s - loss: 0.1415 - val_loss: 0.0623

Epoch 19/50

 - 4s - loss: 0.1383 - val_loss: 0.0695

Epoch 20/50

 - 3s - loss: 0.1337 - val_loss: 0.0954

length of memory (state 0, action 0): 952, after forget

length of memory (state 0, action 1): 350, after forget

length of memory (state 1, action 0): 493, after forget

length of memory (state 1, action 1): 338, after forget

time = 3012	action = 0	current_phase = 0	next_phase = 1	reward = -1.145837	array([[-2.2151518, -3.122145 ]], dtype=float32)

time = 3017	action = 0	current_phase = 0	next_phase = 1	reward = -1.121815	array([[-2.956159 , -3.1273568]], dtype=float32)

time = 3022	action = 1	current_phase = 0	next_phase = 1	reward = -1.726550	array([[-3.2830162, -2.9878628]], dtype=float32)

time = 3030	action = 1	current_phase = 1	next_phase = 0	reward = -1.065277	array([[-2.3893685, -0.9730253]], dtype=float32)

time = 3038	action = 0	current_phase = 0	next_phase = 1	reward = -0.057190	array([[ 0.20086837, -2.4670203 ]], dtype=float32)

time = 3043	action = 0	current_phase = 0	next_phase = 1	reward = 0.009795	array([[-0.32876375, -2.7916136 ]], dtype=float32)

time = 3048	action = 0	current_phase = 0	next_phase = 1	reward = 0.080055	array([[-0.40885836, -2.8310196 ]], dtype=float32)

time = 3053	action = 0	current_phase = 0	next_phase = 1	reward = -0.365548	array([[-0.81846786, -2.8281596 ]], dtype=float32)

time = 3058	action = 0	current_phase = 0	next_phase = 1	reward = -1.077594	array([[-2.5107217, -3.3990142]], dtype=float32)

time = 3063	action = 0	current_phase = 0	next_phase = 1	reward = -1.213436	array([[-2.9472265, -3.1995604]], dtype=float32)

time = 3068	action = 0	current_phase = 0	next_phase = 1	reward = -1.190217	array([[-2.421955, -3.146298]], dtype=float32)

time = 3073	action = 0	current_phase = 0	next_phase = 1	reward = -1.161451	array([[-2.4303193, -3.1559825]], dtype=float32)

time = 3078	action = 1	current_phase = 0	next_phase = 1	reward = -1.292622	array([[-3.0959134, -2.9655657]], dtype=float32)

time = 3086	action = 1	current_phase = 1	next_phase = 0	reward = -0.978007	array([[-2.4004192, -0.7400327]], dtype=float32)

time = 3094	action = 0	current_phase = 0	next_phase = 1	reward = -0.106188	array([[ 0.09992713, -2.3968623 ]], dtype=float32)

time = 3099	action = 0	current_phase = 0	next_phase = 1	reward = -0.030563	array([[-0.2684652, -2.974618 ]], dtype=float32)

time = 3104	action = 0	current_phase = 0	next_phase = 1	reward = 0.038288	array([[-0.25143814, -2.8686776 ]], dtype=float32)

time = 3109	action = 0	current_phase = 0	next_phase = 1	reward = 0.060479	array([[-0.4014224, -2.6759274]], dtype=float32)

time = 3114	action = 0	current_phase = 0	next_phase = 1	reward = -0.542681	array([[-1.1567891, -2.4575877]], dtype=float32)

time = 3119	action = 1	current_phase = 0	next_phase = 1	reward = -1.742002	array([[-3.050983 , -2.7127903]], dtype=float32)

time = 3127	action = 1	current_phase = 1	next_phase = 0	reward = -1.201056	array([[-2.4571671, -0.7753378]], dtype=float32)

time = 3135	action = 0	current_phase = 0	next_phase = 1	reward = 0.618099	array([[ 0.07567716, -2.4036849 ]], dtype=float32)

time = 3140	action = 0	current_phase = 0	next_phase = 1	reward = -0.262635	array([[-1.4621999, -2.4856088]], dtype=float32)

time = 3145	action = 0	current_phase = 0	next_phase = 1	reward = -0.307349	array([[-1.3140216, -2.9779482]], dtype=float32)

time = 3150	action = 0	current_phase = 0	next_phase = 1	reward = -1.497453	array([[-2.4193215, -3.1347384]], dtype=float32)

time = 3155	action = 0	current_phase = 0	next_phase = 1	reward = -1.185409	array([[-2.654889 , -3.2708645]], dtype=float32)

time = 3160	action = 0	current_phase = 0	next_phase = 1	reward = -0.865971	array([[-2.5425673, -3.2310028]], dtype=float32)

time = 3165	action = 0	current_phase = 0	next_phase = 1	reward = -1.125657	array([[-2.658719, -3.046322]], dtype=float32)

time = 3170	action = 1	current_phase = 0	next_phase = 1	reward = -1.324279	array([[-3.1392012, -2.8077955]], dtype=float32)

time = 3178	action = 1	current_phase = 1	next_phase = 0	reward = -0.701134	array([[-2.554141, -0.616529]], dtype=float32)

time = 3186	action = 0	current_phase = 0	next_phase = 1	reward = -0.080460	array([[-0.14307821, -2.4987514 ]], dtype=float32)

time = 3191	action = 0	current_phase = 0	next_phase = 1	reward = -0.004655	array([[-0.13667184, -2.8549066 ]], dtype=float32)

time = 3196	action = 0	current_phase = 0	next_phase = 1	reward = 0.058603	array([[-0.5745882, -3.10532  ]], dtype=float32)

time = 3201	action = 0	current_phase = 0	next_phase = 1	reward = -0.074868	array([[-0.5231485, -2.9073446]], dtype=float32)

time = 3206	action = 0	current_phase = 0	next_phase = 1	reward = -0.891766	array([[-1.8089905, -3.0210018]], dtype=float32)

time = 3211	action = 0	current_phase = 0	next_phase = 1	reward = -1.199954	array([[-2.6026974, -3.027519 ]], dtype=float32)

time = 3216	action = 0	current_phase = 0	next_phase = 1	reward = -1.178896	array([[-2.853899, -3.539608]], dtype=float32)

time = 3221	action = 0	current_phase = 0	next_phase = 1	reward = -1.137261	array([[-2.3741035, -3.1070073]], dtype=float32)

time = 3226	action = 0	current_phase = 0	next_phase = 1	reward = -1.127524	array([[-2.671806 , -3.2673683]], dtype=float32)

time = 3231	action = 1	current_phase = 0	next_phase = 1	reward = -1.525424	array([[-3.2129154, -2.8443284]], dtype=float32)

time = 3239	action = 1	current_phase = 1	next_phase = 0	reward = -1.113185	array([[-2.3961165 , -0.62108433]], dtype=float32)

time = 3247	action = 0	current_phase = 0	next_phase = 1	reward = 0.227194	array([[ 0.18882513, -2.375774  ]], dtype=float32)

time = 3252	action = 0	current_phase = 0	next_phase = 1	reward = 0.006222	array([[-0.3560013, -2.9967232]], dtype=float32)

time = 3257	action = 0	current_phase = 0	next_phase = 1	reward = 0.065666	array([[-0.26341695, -2.90277   ]], dtype=float32)

time = 3262	action = 0	current_phase = 0	next_phase = 1	reward = -0.189109	array([[-0.74256176, -2.8991542 ]], dtype=float32)

time = 3267	action = 0	current_phase = 0	next_phase = 1	reward = -1.013689	array([[-2.1248345, -3.3684335]], dtype=float32)

time = 3272	action = 0	current_phase = 0	next_phase = 1	reward = -1.213568	array([[-2.410091 , -3.3165529]], dtype=float32)

time = 3277	action = 0	current_phase = 0	next_phase = 1	reward = -1.180740	array([[-1.9627317, -3.6880615]], dtype=float32)

time = 3282	action = 0	current_phase = 0	next_phase = 1	reward = -1.140731	array([[-2.2472482, -3.182797 ]], dtype=float32)

time = 3287	action = 0	current_phase = 0	next_phase = 1	reward = -1.119715	array([[-3.015854, -3.172048]], dtype=float32)

time = 3292	action = 1	current_phase = 0	next_phase = 1	reward = -1.665010	array([[-3.4681306, -2.9204426]], dtype=float32)

time = 3300	action = 1	current_phase = 1	next_phase = 0	reward = -0.904964	array([[-2.3250756 , -0.84105337]], dtype=float32)

time = 3308	action = 0	current_phase = 0	next_phase = 1	reward = -0.038915	array([[ 0.12636381, -2.3891397 ]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 4s - loss: 0.2262 - val_loss: 0.1090

Epoch 2/50

 - 4s - loss: 0.2029 - val_loss: 0.1060

Epoch 3/50

 - 4s - loss: 0.2084 - val_loss: 0.1068

Epoch 4/50

 - 5s - loss: 0.2155 - val_loss: 0.0981

Epoch 5/50

 - 4s - loss: 0.1749 - val_loss: 0.0995

Epoch 6/50

 - 4s - loss: 0.1876 - val_loss: 0.1049

Epoch 7/50

 - 4s - loss: 0.1825 - val_loss: 0.1048

Epoch 8/50

 - 4s - loss: 0.1712 - val_loss: 0.0995

Epoch 9/50

 - 4s - loss: 0.1803 - val_loss: 0.1000

Epoch 10/50

 - 3s - loss: 0.1919 - val_loss: 0.1107

Epoch 11/50

 - 4s - loss: 0.1851 - val_loss: 0.1037

Epoch 12/50

 - 3s - loss: 0.1678 - val_loss: 0.0981

Epoch 13/50

 - 4s - loss: 0.1902 - val_loss: 0.1020

Epoch 14/50

 - 4s - loss: 0.1663 - val_loss: 0.1025

Epoch 15/50

 - 3s - loss: 0.1702 - val_loss: 0.1041

Epoch 16/50

 - 3s - loss: 0.1590 - val_loss: 0.1004

Epoch 17/50

 - 3s - loss: 0.1722 - val_loss: 0.1106

Epoch 18/50

 - 4s - loss: 0.1601 - val_loss: 0.0976

Epoch 19/50

 - 4s - loss: 0.1692 - val_loss: 0.0985

Epoch 20/50

 - 3s - loss: 0.1571 - val_loss: 0.1071

Epoch 21/50

 - 4s - loss: 0.1714 - val_loss: 0.1017

Epoch 22/50

 - 4s - loss: 0.1591 - val_loss: 0.0999

Epoch 23/50

 - 3s - loss: 0.1467 - val_loss: 0.1062

Epoch 24/50

 - 4s - loss: 0.1599 - val_loss: 0.0917

Epoch 25/50

 - 4s - loss: 0.1653 - val_loss: 0.1083

Epoch 26/50

 - 4s - loss: 0.1703 - val_loss: 0.1026

Epoch 27/50

 - 4s - loss: 0.1648 - val_loss: 0.1023

Epoch 28/50

 - 4s - loss: 0.1630 - val_loss: 0.1008

Epoch 29/50

 - 4s - loss: 0.1617 - val_loss: 0.1071

Epoch 30/50

 - 4s - loss: 0.1523 - val_loss: 0.1195

Epoch 31/50

 - 4s - loss: 0.1429 - val_loss: 0.1115

Epoch 32/50

 - 4s - loss: 0.1571 - val_loss: 0.1127

Epoch 33/50

 - 4s - loss: 0.1504 - val_loss: 0.1080

Epoch 34/50

 - 4s - loss: 0.1416 - val_loss: 0.1118

length of memory (state 0, action 0): 993, after forget

length of memory (state 0, action 1): 356, after forget

length of memory (state 1, action 0): 493, after forget

length of memory (state 1, action 1): 344, after forget

time = 3313	action = 0	current_phase = 0	next_phase = 1	reward = 0.023801	array([[-0.05239683, -2.9049397 ]], dtype=float32)

time = 3318	action = 0	current_phase = 0	next_phase = 1	reward = 0.068799	array([[-0.6762246, -3.0183222]], dtype=float32)

time = 3323	action = 0	current_phase = 0	next_phase = 1	reward = -0.301901	array([[-0.78444016, -2.9870775 ]], dtype=float32)

time = 3328	action = 0	current_phase = 0	next_phase = 1	reward = -1.087333	array([[-2.584506 , -3.2062504]], dtype=float32)

time = 3333	action = 1	current_phase = 0	next_phase = 1	reward = -2.662920	array([[-3.135715 , -3.1259978]], dtype=float32)

time = 3341	action = 1	current_phase = 1	next_phase = 0	reward = -0.790800	array([[-2.5846128, -0.9877397]], dtype=float32)

time = 3349	action = 0	current_phase = 0	next_phase = 1	reward = 0.065160	array([[-0.3573318, -2.159417 ]], dtype=float32)

time = 3354	action = 0	current_phase = 0	next_phase = 1	reward = -0.540743	array([[-1.3924628, -2.5453923]], dtype=float32)

time = 3359	action = 0	current_phase = 0	next_phase = 1	reward = -1.171404	array([[-2.7773986, -3.029966 ]], dtype=float32)

time = 3364	action = 0	current_phase = 0	next_phase = 1	reward = -1.185473	array([[-2.428619 , -3.3003812]], dtype=float32)

time = 3369	action = 1	current_phase = 0	next_phase = 1	reward = -1.500691	array([[-2.9959428, -2.9262755]], dtype=float32)

time = 3377	action = 1	current_phase = 1	next_phase = 0	reward = -0.793652	array([[-2.4714115 , -0.95735455]], dtype=float32)

time = 3385	action = 0	current_phase = 0	next_phase = 1	reward = -0.673432	array([[-1.7671955, -2.3209813]], dtype=float32)

time = 3390	action = 0	current_phase = 0	next_phase = 1	reward = -0.946309	array([[-2.5930233, -3.09502  ]], dtype=float32)

time = 3395	action = 0	current_phase = 0	next_phase = 1	reward = -1.197615	array([[-2.6940353, -3.0510693]], dtype=float32)

time = 3400	action = 0	current_phase = 0	next_phase = 1	reward = -1.162410	array([[-2.5938764, -3.074972 ]], dtype=float32)

time = 3405	action = 1	current_phase = 0	next_phase = 1	reward = -2.120864	array([[-3.2764742, -3.0070064]], dtype=float32)

time = 3413	action = 1	current_phase = 1	next_phase = 0	reward = -0.630043	array([[-2.3120465, -1.4457495]], dtype=float32)

time = 3421	action = 0	current_phase = 0	next_phase = 1	reward = -0.687694	array([[-1.2623404, -2.9126956]], dtype=float32)

time = 3426	action = 0	current_phase = 0	next_phase = 1	reward = -0.635580	array([[-1.4072967, -3.0000355]], dtype=float32)

time = 3431	action = 0	current_phase = 0	next_phase = 1	reward = -0.579849	array([[-0.8398938, -3.101857 ]], dtype=float32)

time = 3436	action = 0	current_phase = 0	next_phase = 1	reward = -0.529647	array([[-1.0729247, -3.2114155]], dtype=float32)

time = 3441	action = 0	current_phase = 0	next_phase = 1	reward = -0.640748	array([[-2.1726995, -2.7403908]], dtype=float32)

time = 3446	action = 1	current_phase = 0	next_phase = 1	reward = -2.288758	array([[-3.605232 , -2.7168539]], dtype=float32)

time = 3454	action = 1	current_phase = 1	next_phase = 0	reward = -0.747071	array([[-2.510992 , -0.7681774]], dtype=float32)

time = 3462	action = 0	current_phase = 0	next_phase = 1	reward = 0.011374	array([[-0.5059092, -2.431243 ]], dtype=float32)

time = 3467	action = 0	current_phase = 0	next_phase = 1	reward = 0.083143	array([[-0.6037457, -2.6717246]], dtype=float32)

time = 3472	action = 0	current_phase = 0	next_phase = 1	reward = -0.177370	array([[-0.5980035, -2.490295 ]], dtype=float32)

time = 3477	action = 0	current_phase = 0	next_phase = 1	reward = -1.027576	array([[-2.0738058, -3.3231947]], dtype=float32)

time = 3482	action = 0	current_phase = 0	next_phase = 1	reward = -1.227516	array([[-2.2866485, -3.3798556]], dtype=float32)

time = 3487	action = 0	current_phase = 0	next_phase = 1	reward = -1.185508	array([[-3.0692816, -3.54241  ]], dtype=float32)

time = 3492	action = 1	current_phase = 0	next_phase = 1	reward = -2.222702	array([[-3.3618808, -3.2416134]], dtype=float32)

time = 3500	action = 1	current_phase = 1	next_phase = 0	reward = -1.067247	array([[-2.1002984, -1.5396276]], dtype=float32)

time = 3508	action = 0	current_phase = 0	next_phase = 1	reward = -1.090407	array([[-2.7401237, -3.5192845]], dtype=float32)

time = 3513	action = 0	current_phase = 0	next_phase = 1	reward = -1.205466	array([[-2.6734324, -3.4207633]], dtype=float32)

time = 3518	action = 0	current_phase = 0	next_phase = 1	reward = -1.176893	array([[-3.1314516, -3.435018 ]], dtype=float32)

time = 3523	action = 0	current_phase = 0	next_phase = 1	reward = -1.141831	array([[-2.6907172, -3.4299686]], dtype=float32)

time = 3528	action = 0	current_phase = 0	next_phase = 1	reward = -1.131733	array([[-2.968482, -3.191446]], dtype=float32)

time = 3533	action = 1	current_phase = 0	next_phase = 1	reward = -1.946285	array([[-3.7060604, -2.7067225]], dtype=float32)

time = 3541	action = 1	current_phase = 1	next_phase = 0	reward = -1.295197	array([[-2.448731  , -0.78912956]], dtype=float32)

time = 3549	action = 0	current_phase = 0	next_phase = 1	reward = 0.264945	array([[ 0.08214968, -2.3727486 ]], dtype=float32)

time = 3554	action = 0	current_phase = 0	next_phase = 1	reward = 0.024290	array([[-0.02894974, -2.5590792 ]], dtype=float32)

time = 3559	action = 0	current_phase = 0	next_phase = 1	reward = 0.054736	array([[-0.66486037, -2.7459106 ]], dtype=float32)

time = 3564	action = 0	current_phase = 0	next_phase = 1	reward = -0.365882	array([[-1.3741515, -2.5388932]], dtype=float32)

time = 3569	action = 0	current_phase = 0	next_phase = 1	reward = -1.147548	array([[-2.3416514, -2.9393282]], dtype=float32)

time = 3574	action = 0	current_phase = 0	next_phase = 1	reward = -1.188879	array([[-2.357113 , -2.9285438]], dtype=float32)

time = 3579	action = 0	current_phase = 0	next_phase = 1	reward = -1.146631	array([[-2.6461167, -2.928976 ]], dtype=float32)

time = 3584	action = 1	current_phase = 0	next_phase = 1	reward = -1.354346	array([[-3.0689545, -2.8359025]], dtype=float32)

time = 3592	action = 1	current_phase = 1	next_phase = 0	reward = -1.098318	array([[-1.8498118, -1.6586092]], dtype=float32)

time = 3600	action = 1	current_phase = 0	next_phase = 1	reward = -2.458537	array([[-3.13758  , -2.6912706]], dtype=float32)

time = 3608	action = 1	current_phase = 1	next_phase = 0	reward = -0.616824	array([[-2.524345, -1.0752  ]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.1713 - val_loss: 0.0685

Epoch 2/50

 - 4s - loss: 0.1749 - val_loss: 0.0715

Epoch 3/50

 - 4s - loss: 0.1581 - val_loss: 0.0692

Epoch 4/50

 - 3s - loss: 0.1495 - val_loss: 0.0765

Epoch 5/50

 - 3s - loss: 0.1574 - val_loss: 0.0767

Epoch 6/50

 - 4s - loss: 0.1503 - val_loss: 0.0734

Epoch 7/50

 - 3s - loss: 0.1572 - val_loss: 0.0710

Epoch 8/50

 - 4s - loss: 0.1414 - val_loss: 0.0890

Epoch 9/50

 - 4s - loss: 0.1590 - val_loss: 0.0778

Epoch 10/50

 - 4s - loss: 0.1491 - val_loss: 0.0726

Epoch 11/50

 - 4s - loss: 0.1409 - val_loss: 0.0742

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 364, after forget

length of memory (state 1, action 0): 493, after forget

length of memory (state 1, action 1): 352, after forget

time = 3616	action = 0	current_phase = 0	next_phase = 1	reward = 0.069803	array([[-0.31400993, -2.7498627 ]], dtype=float32)

time = 3621	action = 0	current_phase = 0	next_phase = 1	reward = -0.041120	array([[-0.71881413, -2.760726  ]], dtype=float32)

time = 3626	action = 1	current_phase = 0	next_phase = 1	reward = -2.302460	array([[-2.7580438, -2.6341   ]], dtype=float32)

time = 3634	action = 1	current_phase = 1	next_phase = 0	reward = -0.801400	array([[-2.487586 , -0.9036724]], dtype=float32)

time = 3642	action = 0	current_phase = 0	next_phase = 1	reward = 0.004714	array([[-0.1881032, -2.6379929]], dtype=float32)

time = 3647	action = 0	current_phase = 0	next_phase = 1	reward = 0.065032	array([[-0.8078023, -2.8236756]], dtype=float32)

time = 3652	action = 0	current_phase = 0	next_phase = 1	reward = -0.173717	array([[-0.9905851, -2.5950255]], dtype=float32)

time = 3657	action = 0	current_phase = 0	next_phase = 1	reward = -0.955549	array([[-2.6316786, -3.3758576]], dtype=float32)

time = 3662	action = 0	current_phase = 0	next_phase = 1	reward = -1.210231	array([[-2.8647773, -3.224052 ]], dtype=float32)

time = 3667	action = 0	current_phase = 0	next_phase = 1	reward = -1.182736	array([[-3.2296019, -3.5006127]], dtype=float32)

time = 3672	action = 0	current_phase = 0	next_phase = 1	reward = -1.143828	array([[-3.2063892, -3.3319411]], dtype=float32)

time = 3677	action = 1	current_phase = 0	next_phase = 1	reward = -1.558818	array([[-3.4281378, -2.747992 ]], dtype=float32)

time = 3685	action = 1	current_phase = 1	next_phase = 0	reward = -0.958286	array([[-2.361116  , -0.73847735]], dtype=float32)

time = 3693	action = 0	current_phase = 0	next_phase = 1	reward = 0.167485	array([[ 0.23582816, -2.515228  ]], dtype=float32)

time = 3698	action = 0	current_phase = 0	next_phase = 1	reward = -0.059192	array([[-0.13164586, -2.66637   ]], dtype=float32)

time = 3703	action = 0	current_phase = 0	next_phase = 1	reward = 0.008706	array([[-0.33208376, -3.0263596 ]], dtype=float32)

time = 3708	action = 0	current_phase = 0	next_phase = 1	reward = 0.068200	array([[-1.1068059, -2.9716015]], dtype=float32)

time = 3713	action = 0	current_phase = 0	next_phase = 1	reward = -0.248957	array([[-1.442071 , -2.8182733]], dtype=float32)

time = 3718	action = 1	current_phase = 0	next_phase = 1	reward = -2.696023	array([[-3.4545035, -3.0771804]], dtype=float32)

time = 3726	action = 1	current_phase = 1	next_phase = 0	reward = -1.572669	array([[-2.6412773, -0.7744079]], dtype=float32)

time = 3734	action = 0	current_phase = 0	next_phase = 1	reward = 0.638602	array([[ 0.06550324, -2.3338244 ]], dtype=float32)

time = 3739	action = 0	current_phase = 0	next_phase = 1	reward = 0.068993	array([[-0.80770206, -2.3393826 ]], dtype=float32)

time = 3744	action = 0	current_phase = 0	next_phase = 1	reward = -0.603096	array([[-1.5837222, -2.7067397]], dtype=float32)

time = 3749	action = 0	current_phase = 0	next_phase = 1	reward = -1.165277	array([[-2.274723, -3.040353]], dtype=float32)

time = 3754	action = 1	current_phase = 0	next_phase = 1	reward = -1.629725	array([[-3.3252215, -2.7861218]], dtype=float32)

time = 3762	action = 1	current_phase = 1	next_phase = 0	reward = -0.516330	array([[-2.5289788, -1.4006182]], dtype=float32)

time = 3770	action = 0	current_phase = 0	next_phase = 1	reward = 0.044287	array([[-1.5343573, -1.8935997]], dtype=float32)

time = 3775	action = 0	current_phase = 0	next_phase = 1	reward = -0.565910	array([[-1.7014687, -2.3851347]], dtype=float32)

time = 3780	action = 0	current_phase = 0	next_phase = 1	reward = -1.210998	array([[-3.276259 , -3.3404074]], dtype=float32)

time = 3785	action = 0	current_phase = 0	next_phase = 1	reward = -1.181027	array([[-3.1313295, -3.3076208]], dtype=float32)

time = 3790	action = 0	current_phase = 0	next_phase = 1	reward = -1.145415	array([[-3.0060098, -3.1903572]], dtype=float32)

time = 3795	action = 1	current_phase = 0	next_phase = 1	reward = -2.102746	array([[-3.2071748, -2.9574833]], dtype=float32)

time = 3803	action = 1	current_phase = 1	next_phase = 0	reward = -1.580391	array([[-2.5187206, -1.0570662]], dtype=float32)

time = 3811	action = 1	current_phase = 0	next_phase = 1	reward = -2.551134	array([[-3.0677946, -2.7669852]], dtype=float32)

time = 3819	action = 1	current_phase = 1	next_phase = 0	reward = -0.598513	array([[-2.5994763, -0.9819594]], dtype=float32)

time = 3827	action = 0	current_phase = 0	next_phase = 1	reward = 0.066242	array([[-0.6592648, -2.7067327]], dtype=float32)

time = 3832	action = 0	current_phase = 0	next_phase = 1	reward = -0.199266	array([[-1.104089 , -2.5177484]], dtype=float32)

time = 3837	action = 0	current_phase = 0	next_phase = 1	reward = -1.007373	array([[-2.4379883, -3.0717144]], dtype=float32)

time = 3842	action = 0	current_phase = 0	next_phase = 1	reward = -1.218932	array([[-2.8676908, -3.3410144]], dtype=float32)

time = 3847	action = 0	current_phase = 0	next_phase = 1	reward = -1.185654	array([[-3.5272825, -3.5562897]], dtype=float32)

time = 3852	action = 1	current_phase = 0	next_phase = 1	reward = -2.299507	array([[-3.6616175, -3.352046 ]], dtype=float32)

time = 3860	action = 1	current_phase = 1	next_phase = 0	reward = -1.175159	array([[-2.1979342, -1.6490722]], dtype=float32)

time = 3868	action = 0	current_phase = 0	next_phase = 1	reward = -1.088578	array([[-2.0834627, -3.6144657]], dtype=float32)

time = 3873	action = 0	current_phase = 0	next_phase = 1	reward = -1.204618	array([[-1.9350963, -3.3140202]], dtype=float32)

time = 3878	action = 0	current_phase = 0	next_phase = 1	reward = -1.164674	array([[-2.7846298, -3.4763448]], dtype=float32)

time = 3883	action = 0	current_phase = 0	next_phase = 1	reward = -1.128445	array([[-3.1726985, -3.314043 ]], dtype=float32)

time = 3888	action = 1	current_phase = 0	next_phase = 1	reward = -0.730079	array([[-3.4530191, -2.8538165]], dtype=float32)

time = 3896	action = 1	current_phase = 1	next_phase = 0	reward = -1.102988	array([[-2.0646877 , -0.69854486]], dtype=float32)

time = 3904	action = 0	current_phase = 0	next_phase = 1	reward = -0.112186	array([[ 0.03817499, -2.3600156 ]], dtype=float32)

time = 3909	action = 0	current_phase = 0	next_phase = 1	reward = -0.032253	array([[-0.2027868, -2.6474192]], dtype=float32)

time = 3914	action = 0	current_phase = 0	next_phase = 1	reward = 0.046390	array([[-0.29232714, -2.6120634 ]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.1328 - val_loss: 0.1147

Epoch 2/50

 - 3s - loss: 0.1412 - val_loss: 0.1211

Epoch 3/50

 - 4s - loss: 0.1393 - val_loss: 0.1516

Epoch 4/50

 - 3s - loss: 0.1727 - val_loss: 0.1253

Epoch 5/50

 - 3s - loss: 0.1288 - val_loss: 0.1340

Epoch 6/50

 - 3s - loss: 0.1175 - val_loss: 0.1495

Epoch 7/50

 - 4s - loss: 0.1344 - val_loss: 0.1241

Epoch 8/50

 - 3s - loss: 0.1089 - val_loss: 0.1256

Epoch 9/50

 - 3s - loss: 0.1061 - val_loss: 0.1430

Epoch 10/50

 - 3s - loss: 0.0982 - val_loss: 0.1348

Epoch 11/50

 - 4s - loss: 0.1342 - val_loss: 0.1336

length of memory (state 0, action 0): 1035, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 372, after forget

length of memory (state 1, action 0): 493, after forget

length of memory (state 1, action 1): 360, after forget

time = 3919	action = 0	current_phase = 0	next_phase = 1	reward = 0.050991	array([[-1.6314895, -2.5883117]], dtype=float32)

time = 3924	action = 0	current_phase = 0	next_phase = 1	reward = -0.503235	array([[-1.5686538, -2.2968452]], dtype=float32)

time = 3929	action = 1	current_phase = 0	next_phase = 1	reward = -1.788663	array([[-3.7536619, -2.874216 ]], dtype=float32)

time = 3937	action = 1	current_phase = 1	next_phase = 0	reward = -0.910803	array([[-2.605108 , -1.0060047]], dtype=float32)

time = 3945	action = 0	current_phase = 0	next_phase = 1	reward = 0.311207	array([[-0.44868648, -2.5652266 ]], dtype=float32)

time = 3950	action = 0	current_phase = 0	next_phase = 1	reward = -0.531607	array([[-1.0478345, -2.5358672]], dtype=float32)

time = 3955	action = 0	current_phase = 0	next_phase = 1	reward = -0.113494	array([[-1.5772479, -2.761873 ]], dtype=float32)

time = 3960	action = 1	current_phase = 0	next_phase = 1	reward = -2.363795	array([[-3.3540301, -3.3497214]], dtype=float32)

time = 3968	action = 1	current_phase = 1	next_phase = 0	reward = -0.605271	array([[-2.5023699, -1.3097719]], dtype=float32)

time = 3976	action = 0	current_phase = 0	next_phase = 1	reward = 0.062722	array([[-0.3304103, -2.430483 ]], dtype=float32)

time = 3981	action = 0	current_phase = 0	next_phase = 1	reward = -0.069874	array([[-1.9120345, -2.4111369]], dtype=float32)

time = 3986	action = 0	current_phase = 0	next_phase = 1	reward = -0.893595	array([[-1.9692857, -2.9746485]], dtype=float32)

time = 3991	action = 0	current_phase = 0	next_phase = 1	reward = -1.223531	array([[-2.9061608, -3.1661627]], dtype=float32)

time = 3996	action = 1	current_phase = 0	next_phase = 1	reward = -2.372853	array([[-3.5885298, -3.4998908]], dtype=float32)

time = 4004	action = 1	current_phase = 1	next_phase = 0	reward = -0.546206	array([[-2.57716  , -1.0363528]], dtype=float32)

time = 4012	action = 0	current_phase = 0	next_phase = 1	reward = -0.186107	array([[-1.3131771, -2.3352473]], dtype=float32)

time = 4017	action = 0	current_phase = 0	next_phase = 1	reward = -1.018400	array([[-2.4519672, -3.000091 ]], dtype=float32)

time = 4022	action = 0	current_phase = 0	next_phase = 1	reward = -1.209110	array([[-2.2029843, -3.1349242]], dtype=float32)

time = 4027	action = 0	current_phase = 0	next_phase = 1	reward = -1.161085	array([[-2.9955256, -3.6656756]], dtype=float32)

time = 4032	action = 0	current_phase = 0	next_phase = 1	reward = -1.138828	array([[-3.1532128, -3.5842795]], dtype=float32)

time = 4037	action = 1	current_phase = 0	next_phase = 1	reward = -1.629233	array([[-3.6260085, -2.7063546]], dtype=float32)

time = 4045	action = 1	current_phase = 1	next_phase = 0	reward = -0.658712	array([[-2.2314687, -0.9375417]], dtype=float32)

time = 4053	action = 0	current_phase = 0	next_phase = 1	reward = -0.127094	array([[-0.16963166, -2.3466852 ]], dtype=float32)

time = 4058	action = 0	current_phase = 0	next_phase = 1	reward = -0.073466	array([[-0.31996652, -2.7624016 ]], dtype=float32)

time = 4063	action = 0	current_phase = 0	next_phase = 1	reward = -0.012570	array([[-0.5496981, -3.2503505]], dtype=float32)

time = 4068	action = 0	current_phase = 0	next_phase = 1	reward = 0.066342	array([[-1.3971515, -2.9182117]], dtype=float32)

time = 4073	action = 0	current_phase = 0	next_phase = 1	reward = -0.276277	array([[-1.5394105, -2.6378424]], dtype=float32)

time = 4078	action = 1	current_phase = 0	next_phase = 1	reward = -2.681393	array([[-4.188577 , -3.1435716]], dtype=float32)

time = 4086	action = 1	current_phase = 1	next_phase = 0	reward = -0.862100	array([[-2.652554 , -0.8588958]], dtype=float32)

time = 4094	action = 0	current_phase = 0	next_phase = 1	reward = 0.046509	array([[-0.01007438, -2.2932289 ]], dtype=float32)

time = 4099	action = 0	current_phase = 0	next_phase = 1	reward = 0.063122	array([[-0.74370944, -2.4281933 ]], dtype=float32)

time = 4104	action = 0	current_phase = 0	next_phase = 1	reward = -0.538232	array([[-1.5519187, -2.6082532]], dtype=float32)

time = 4109	action = 0	current_phase = 0	next_phase = 1	reward = -0.885614	array([[-2.4855137, -3.2821555]], dtype=float32)

time = 4114	action = 1	current_phase = 0	next_phase = 1	reward = -1.907945	array([[-4.0677204, -3.054394 ]], dtype=float32)

time = 4122	action = 1	current_phase = 1	next_phase = 0	reward = -0.723920	array([[-2.527194 , -1.3090069]], dtype=float32)

time = 4130	action = 0	current_phase = 0	next_phase = 1	reward = 0.017444	array([[-1.4168568, -2.2437491]], dtype=float32)

time = 4135	action = 0	current_phase = 0	next_phase = 1	reward = -0.677901	array([[-1.5940706, -2.578615 ]], dtype=float32)

time = 4140	action = 0	current_phase = 0	next_phase = 1	reward = -0.942540	array([[-3.1787522, -3.2608638]], dtype=float32)

time = 4145	action = 1	current_phase = 0	next_phase = 1	reward = -2.392398	array([[-3.5812073, -3.506136 ]], dtype=float32)

time = 4153	action = 1	current_phase = 1	next_phase = 0	reward = -0.498485	array([[-2.4667559, -1.2566522]], dtype=float32)

time = 4161	action = 0	current_phase = 0	next_phase = 1	reward = -0.070930	array([[-1.4486198, -2.438841 ]], dtype=float32)

time = 4166	action = 0	current_phase = 0	next_phase = 1	reward = -0.941859	array([[-1.5733126, -2.7418509]], dtype=float32)

time = 4171	action = 0	current_phase = 0	next_phase = 1	reward = -1.237411	array([[-2.680295, -3.107425]], dtype=float32)

time = 4176	action = 0	current_phase = 0	next_phase = 1	reward = -1.198223	array([[-3.2995934, -3.3757734]], dtype=float32)

time = 4181	action = 0	current_phase = 0	next_phase = 1	reward = -1.154597	array([[-3.1721818, -3.3568683]], dtype=float32)

time = 4186	action = 1	current_phase = 0	next_phase = 1	reward = -2.207429	array([[-3.477069 , -3.1787274]], dtype=float32)

time = 4194	action = 1	current_phase = 1	next_phase = 0	reward = -0.248977	array([[-2.1393185, -1.0189161]], dtype=float32)

time = 4202	action = 0	current_phase = 0	next_phase = 1	reward = 0.132529	array([[-0.25634503, -2.3503351 ]], dtype=float32)

time = 4207	action = 0	current_phase = 0	next_phase = 1	reward = -0.074004	array([[-0.17075622, -2.7688437 ]], dtype=float32)

time = 4212	action = 0	current_phase = 0	next_phase = 1	reward = -0.003582	array([[-0.20118129, -3.0528803 ]], dtype=float32)

time = 4217	action = 0	current_phase = 0	next_phase = 1	reward = 0.060387	array([[-0.7733337, -3.054191 ]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 4s - loss: 0.1500 - val_loss: 0.0883

Epoch 2/50

 - 4s - loss: 0.1615 - val_loss: 0.0877

Epoch 3/50

 - 3s - loss: 0.1490 - val_loss: 0.0874

Epoch 4/50

 - 4s - loss: 0.1531 - val_loss: 0.1035

Epoch 5/50

 - 4s - loss: 0.1455 - val_loss: 0.0813

Epoch 6/50

 - 4s - loss: 0.1380 - val_loss: 0.0749

Epoch 7/50

 - 4s - loss: 0.1483 - val_loss: 0.0948

Epoch 8/50

 - 4s - loss: 0.1365 - val_loss: 0.0938

Epoch 9/50

 - 4s - loss: 0.1680 - val_loss: 0.1039

Epoch 10/50

 - 4s - loss: 0.1351 - val_loss: 0.0967

Epoch 11/50

 - 4s - loss: 0.1374 - val_loss: 0.0900

Epoch 12/50

 - 4s - loss: 0.1371 - val_loss: 0.0832

Epoch 13/50

 - 3s - loss: 0.1388 - val_loss: 0.0864

Epoch 14/50

 - 4s - loss: 0.1272 - val_loss: 0.0893

Epoch 15/50

 - 4s - loss: 0.1316 - val_loss: 0.1040

Epoch 16/50

 - 5s - loss: 0.1152 - val_loss: 0.1042

length of memory (state 0, action 0): 1035, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 380, after forget

length of memory (state 1, action 0): 493, after forget

length of memory (state 1, action 1): 368, after forget

time = 4222	action = 0	current_phase = 0	next_phase = 1	reward = -0.162699	array([[-1.4698254, -2.5292356]], dtype=float32)

time = 4227	action = 1	current_phase = 0	next_phase = 1	reward = -2.440922	array([[-3.290397 , -3.1532638]], dtype=float32)

time = 4235	action = 1	current_phase = 1	next_phase = 0	reward = -1.121041	array([[-2.6313043, -1.254009 ]], dtype=float32)

time = 4243	action = 0	current_phase = 0	next_phase = 1	reward = 0.330288	array([[-0.01579636, -2.3987455 ]], dtype=float32)

time = 4248	action = 0	current_phase = 0	next_phase = 1	reward = 0.072493	array([[-0.968917 , -2.7888753]], dtype=float32)

time = 4253	action = 0	current_phase = 0	next_phase = 1	reward = -0.445527	array([[-1.605716 , -2.9491289]], dtype=float32)

time = 4258	action = 0	current_phase = 0	next_phase = 1	reward = -1.095310	array([[-2.685335 , -3.4853356]], dtype=float32)

time = 4263	action = 1	current_phase = 0	next_phase = 1	reward = -2.766015	array([[-3.4035347, -3.142862 ]], dtype=float32)

time = 4271	action = 1	current_phase = 1	next_phase = 0	reward = -0.839708	array([[-2.8463829, -1.4639853]], dtype=float32)

time = 4279	action = 0	current_phase = 0	next_phase = 1	reward = 0.057460	array([[-0.7492995, -2.226168 ]], dtype=float32)

time = 4284	action = 0	current_phase = 0	next_phase = 1	reward = -0.551429	array([[-1.4139543, -2.442216 ]], dtype=float32)

time = 4289	action = 0	current_phase = 0	next_phase = 1	reward = -1.163856	array([[-3.1982582, -3.2200139]], dtype=float32)

time = 4294	action = 1	current_phase = 0	next_phase = 1	reward = -1.620285	array([[-3.346224 , -3.2530444]], dtype=float32)

time = 4302	action = 1	current_phase = 1	next_phase = 0	reward = -0.750430	array([[-2.4404817, -1.549459 ]], dtype=float32)

time = 4310	action = 0	current_phase = 0	next_phase = 1	reward = 0.316640	array([[-1.1874677, -2.1038752]], dtype=float32)

time = 4315	action = 1	current_phase = 0	next_phase = 1	reward = -1.903401	array([[-2.7169764, -2.5495608]], dtype=float32)

time = 4323	action = 1	current_phase = 1	next_phase = 0	reward = -0.731157	array([[-2.5202627, -1.0942628]], dtype=float32)

time = 4331	action = 0	current_phase = 0	next_phase = 1	reward = -0.011159	array([[-0.25174826, -2.6216772 ]], dtype=float32)

time = 4336	action = 0	current_phase = 0	next_phase = 1	reward = 0.055443	array([[-0.44343045, -2.717068  ]], dtype=float32)

time = 4341	action = 0	current_phase = 0	next_phase = 1	reward = -0.056413	array([[-1.8181643, -2.6217954]], dtype=float32)

time = 4346	action = 0	current_phase = 0	next_phase = 1	reward = -0.869718	array([[-2.7642002, -3.0621243]], dtype=float32)

time = 4351	action = 1	current_phase = 0	next_phase = 1	reward = -2.552338	array([[-3.689709 , -2.7917914]], dtype=float32)

time = 4359	action = 1	current_phase = 1	next_phase = 0	reward = -0.628819	array([[-2.6239681, -1.0543752]], dtype=float32)

time = 4367	action = 0	current_phase = 0	next_phase = 1	reward = 0.077440	array([[-0.4744905, -2.6438928]], dtype=float32)

time = 4372	action = 0	current_phase = 0	next_phase = 1	reward = -0.121099	array([[-1.8872895, -2.5229294]], dtype=float32)

time = 4377	action = 0	current_phase = 0	next_phase = 1	reward = -1.018060	array([[-1.7145119, -2.9940133]], dtype=float32)

time = 4382	action = 0	current_phase = 0	next_phase = 1	reward = -1.223551	array([[-3.0995271, -3.343776 ]], dtype=float32)

time = 4387	action = 0	current_phase = 0	next_phase = 1	reward = -1.182362	array([[-3.436107 , -3.4754717]], dtype=float32)

time = 4392	action = 1	current_phase = 0	next_phase = 1	reward = -2.330552	array([[-3.5908513, -3.3468196]], dtype=float32)

time = 4400	action = 0	current_phase = 1	next_phase = 0	reward = -0.981721	array([[-1.7032084, -2.1244464]], dtype=float32)

time = 4405	action = 1	current_phase = 1	next_phase = 0	reward = -1.142386	array([[-2.8247557, -0.8294738]], dtype=float32)

time = 4413	action = 0	current_phase = 0	next_phase = 1	reward = 0.481246	array([[ 0.23541409, -2.3683069 ]], dtype=float32)

time = 4418	action = 0	current_phase = 0	next_phase = 1	reward = -0.057903	array([[ 0.09966344, -3.5534968 ]], dtype=float32)

time = 4423	action = 0	current_phase = 0	next_phase = 1	reward = 0.018468	array([[-0.39695862, -3.1125479 ]], dtype=float32)

time = 4428	action = 0	current_phase = 0	next_phase = 1	reward = 0.071116	array([[-1.5518694, -2.539051 ]], dtype=float32)

time = 4433	action = 0	current_phase = 0	next_phase = 1	reward = -0.385683	array([[-1.9854922, -2.7728283]], dtype=float32)

time = 4438	action = 1	current_phase = 0	next_phase = 1	reward = -2.686527	array([[-3.9926913, -3.155609 ]], dtype=float32)

time = 4446	action = 1	current_phase = 1	next_phase = 0	reward = -0.798704	array([[-2.7825224, -0.9573204]], dtype=float32)

time = 4454	action = 0	current_phase = 0	next_phase = 1	reward = 0.046083	array([[ 0.02688217, -2.2102199 ]], dtype=float32)

time = 4459	action = 0	current_phase = 0	next_phase = 1	reward = 0.053190	array([[-1.2569591, -2.285246 ]], dtype=float32)

time = 4464	action = 0	current_phase = 0	next_phase = 1	reward = -0.587755	array([[-1.8052211, -2.6231718]], dtype=float32)

time = 4469	action = 0	current_phase = 0	next_phase = 1	reward = -1.159505	array([[-2.8753598, -3.0596368]], dtype=float32)

time = 4474	action = 1	current_phase = 0	next_phase = 1	reward = -1.678223	array([[-3.9691958, -2.7375388]], dtype=float32)

time = 4482	action = 1	current_phase = 1	next_phase = 0	reward = -0.453616	array([[-2.6699133, -1.5352108]], dtype=float32)

time = 4490	action = 0	current_phase = 0	next_phase = 1	reward = 0.001846	array([[-1.697284 , -2.1507454]], dtype=float32)

time = 4495	action = 0	current_phase = 0	next_phase = 1	reward = -0.920295	array([[-1.9464061, -2.375306 ]], dtype=float32)

time = 4500	action = 0	current_phase = 0	next_phase = 1	reward = -0.939517	array([[-2.9581244, -2.9945967]], dtype=float32)

time = 4505	action = 1	current_phase = 0	next_phase = 1	reward = -2.227687	array([[-3.475907, -3.379658]], dtype=float32)

time = 4513	action = 1	current_phase = 1	next_phase = 0	reward = -0.477240	array([[-2.5698214, -1.1381938]], dtype=float32)

time = 4521	action = 0	current_phase = 0	next_phase = 1	reward = -0.072292	array([[-1.6369936, -2.2355406]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 5s - loss: 0.1272 - val_loss: 0.1069

Epoch 2/50

 - 4s - loss: 0.1166 - val_loss: 0.1262

Epoch 3/50

 - 4s - loss: 0.1151 - val_loss: 0.1218

Epoch 4/50

 - 4s - loss: 0.1170 - val_loss: 0.1113

Epoch 5/50

 - 4s - loss: 0.1059 - val_loss: 0.1034

Epoch 6/50

 - 4s - loss: 0.1175 - val_loss: 0.1237

Epoch 7/50

 - 4s - loss: 0.1204 - val_loss: 0.1254

Epoch 8/50

 - 5s - loss: 0.0976 - val_loss: 0.1266

Epoch 9/50

 - 4s - loss: 0.1012 - val_loss: 0.1171

Epoch 10/50

 - 4s - loss: 0.1069 - val_loss: 0.1216

Epoch 11/50

 - 4s - loss: 0.0960 - val_loss: 0.1223

Epoch 12/50

 - 4s - loss: 0.1111 - val_loss: 0.1235

Epoch 13/50

 - 6s - loss: 0.1017 - val_loss: 0.1347

Epoch 14/50

 - 4s - loss: 0.1012 - val_loss: 0.1241

Epoch 15/50

 - 4s - loss: 0.0934 - val_loss: 0.1522

length of memory (state 0, action 0): 1031, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 389, after forget

length of memory (state 1, action 0): 494, after forget

length of memory (state 1, action 1): 377, after forget

time = 4526	action = 0	current_phase = 0	next_phase = 1	reward = -0.937954	array([[-1.8297896, -2.7780483]], dtype=float32)

time = 4531	action = 0	current_phase = 0	next_phase = 1	reward = -1.237300	array([[-2.9999962, -3.3569448]], dtype=float32)

time = 4536	action = 0	current_phase = 0	next_phase = 1	reward = -1.214918	array([[-3.2550948, -3.5755317]], dtype=float32)

time = 4541	action = 0	current_phase = 0	next_phase = 1	reward = -1.173229	array([[-3.2515523, -3.3063822]], dtype=float32)

time = 4546	action = 1	current_phase = 0	next_phase = 1	reward = -2.096270	array([[-3.4465597, -3.2338254]], dtype=float32)

time = 4554	action = 1	current_phase = 1	next_phase = 0	reward = 0.001222	array([[-2.044961 , -1.1710551]], dtype=float32)

time = 4562	action = 0	current_phase = 0	next_phase = 1	reward = -0.154748	array([[-0.19842124, -2.2874622 ]], dtype=float32)

time = 4567	action = 0	current_phase = 0	next_phase = 1	reward = -0.074823	array([[-0.06386608, -2.624892  ]], dtype=float32)

time = 4572	action = 0	current_phase = 0	next_phase = 1	reward = 0.007408	array([[-0.712782 , -3.1546242]], dtype=float32)

time = 4577	action = 0	current_phase = 0	next_phase = 1	reward = 0.072724	array([[-1.3718531, -3.1862364]], dtype=float32)

time = 4582	action = 0	current_phase = 0	next_phase = 1	reward = -0.183784	array([[-1.6148038, -2.6802628]], dtype=float32)

time = 4587	action = 1	current_phase = 0	next_phase = 1	reward = -2.516058	array([[-3.4739401, -3.131967 ]], dtype=float32)

time = 4595	action = 1	current_phase = 1	next_phase = 0	reward = -0.779277	array([[-2.7242146, -0.7933622]], dtype=float32)

time = 4603	action = 0	current_phase = 0	next_phase = 1	reward = 0.021163	array([[-0.40484002, -2.6102192 ]], dtype=float32)

time = 4608	action = 0	current_phase = 0	next_phase = 1	reward = 0.079519	array([[-0.8975934, -2.9765892]], dtype=float32)

time = 4613	action = 0	current_phase = 0	next_phase = 1	reward = -0.248185	array([[-1.4636862, -2.801529 ]], dtype=float32)

time = 4618	action = 0	current_phase = 0	next_phase = 1	reward = -1.082184	array([[-2.459307 , -3.0795891]], dtype=float32)

time = 4623	action = 1	current_phase = 0	next_phase = 1	reward = -2.171874	array([[-3.6035078, -2.720038 ]], dtype=float32)

time = 4631	action = 1	current_phase = 1	next_phase = 0	reward = -0.931770	array([[-2.803413 , -1.5694901]], dtype=float32)

time = 4639	action = 0	current_phase = 0	next_phase = 1	reward = 0.059165	array([[-0.8554255, -2.4013336]], dtype=float32)

time = 4644	action = 0	current_phase = 0	next_phase = 1	reward = -0.610668	array([[-1.5567677, -2.6114492]], dtype=float32)

time = 4649	action = 0	current_phase = 0	next_phase = 1	reward = -1.167903	array([[-3.1566772, -3.1874585]], dtype=float32)

time = 4654	action = 1	current_phase = 0	next_phase = 1	reward = -1.691165	array([[-3.238399 , -2.9903488]], dtype=float32)

time = 4662	action = 1	current_phase = 1	next_phase = 0	reward = -0.781368	array([[-2.5716848, -1.5282307]], dtype=float32)

time = 4670	action = 0	current_phase = 0	next_phase = 1	reward = 0.310068	array([[-1.2512845, -2.4266121]], dtype=float32)

time = 4675	action = 0	current_phase = 0	next_phase = 1	reward = -0.578760	array([[-1.5477301, -2.7886877]], dtype=float32)

time = 4680	action = 0	current_phase = 0	next_phase = 1	reward = -1.226191	array([[-2.7339168, -3.110955 ]], dtype=float32)

time = 4685	action = 1	current_phase = 0	next_phase = 1	reward = -2.286532	array([[-3.2577467, -3.230208 ]], dtype=float32)

time = 4693	action = 1	current_phase = 1	next_phase = 0	reward = -0.512581	array([[-2.6179023, -1.3169371]], dtype=float32)

time = 4701	action = 0	current_phase = 0	next_phase = 1	reward = -0.034838	array([[-1.5560958, -2.5264506]], dtype=float32)

time = 4706	action = 0	current_phase = 0	next_phase = 1	reward = -0.882012	array([[-1.6422724, -2.8868802]], dtype=float32)

time = 4711	action = 0	current_phase = 0	next_phase = 1	reward = -1.223091	array([[-3.2142081, -3.3686633]], dtype=float32)

time = 4716	action = 0	current_phase = 0	next_phase = 1	reward = -1.181303	array([[-3.287336 , -3.5354745]], dtype=float32)

time = 4721	action = 0	current_phase = 0	next_phase = 1	reward = -1.150882	array([[-3.2021537, -3.297726 ]], dtype=float32)

time = 4726	action = 1	current_phase = 0	next_phase = 1	reward = -1.846748	array([[-3.3121986, -2.975079 ]], dtype=float32)

time = 4734	action = 1	current_phase = 1	next_phase = 0	reward = -0.213135	array([[-2.2394776 , -0.87012774]], dtype=float32)

time = 4742	action = 0	current_phase = 0	next_phase = 1	reward = -0.171063	array([[-0.46471167, -2.3629467 ]], dtype=float32)

time = 4747	action = 0	current_phase = 0	next_phase = 1	reward = -0.074534	array([[ 0.12039673, -2.7566848 ]], dtype=float32)

time = 4752	action = 0	current_phase = 0	next_phase = 1	reward = 0.014794	array([[-0.29704413, -3.0554204 ]], dtype=float32)

time = 4757	action = 0	current_phase = 0	next_phase = 1	reward = 0.069603	array([[-1.3985336, -3.084486 ]], dtype=float32)

time = 4762	action = 0	current_phase = 0	next_phase = 1	reward = -0.178073	array([[-1.735845 , -2.6342535]], dtype=float32)

time = 4767	action = 1	current_phase = 0	next_phase = 1	reward = -2.438986	array([[-3.8468683, -3.3822355]], dtype=float32)

time = 4775	action = 1	current_phase = 1	next_phase = 0	reward = -0.830769	array([[-2.6999245, -1.0142982]], dtype=float32)

time = 4783	action = 0	current_phase = 0	next_phase = 1	reward = 0.001079	array([[-0.08338404, -2.5376008 ]], dtype=float32)

time = 4788	action = 0	current_phase = 0	next_phase = 1	reward = 0.082296	array([[-0.90695596, -2.9094205 ]], dtype=float32)

time = 4793	action = 0	current_phase = 0	next_phase = 1	reward = -0.366530	array([[-1.7012665, -3.0700662]], dtype=float32)

time = 4798	action = 0	current_phase = 0	next_phase = 1	reward = -1.087045	array([[-3.25641  , -3.6197765]], dtype=float32)

time = 4803	action = 1	current_phase = 0	next_phase = 1	reward = -2.730966	array([[-3.939557 , -3.1900868]], dtype=float32)

time = 4811	action = 1	current_phase = 1	next_phase = 0	reward = -0.847022	array([[-2.8240132, -1.7822003]], dtype=float32)

time = 4819	action = 0	current_phase = 0	next_phase = 1	reward = 0.063595	array([[-0.61827147, -2.3730085 ]], dtype=float32)

time = 4824	action = 0	current_phase = 0	next_phase = 1	reward = -0.388862	array([[-1.7311914, -2.534689 ]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 4s - loss: 0.1874 - val_loss: 0.1075

Epoch 2/50

 - 4s - loss: 0.1810 - val_loss: 0.0979

Epoch 3/50

 - 4s - loss: 0.2012 - val_loss: 0.0915

Epoch 4/50

 - 4s - loss: 0.1705 - val_loss: 0.0939

Epoch 5/50

 - 4s - loss: 0.1500 - val_loss: 0.1071

Epoch 6/50

 - 4s - loss: 0.1582 - val_loss: 0.0894

Epoch 7/50

 - 4s - loss: 0.1562 - val_loss: 0.0907

Epoch 8/50

 - 4s - loss: 0.1716 - val_loss: 0.1079

Epoch 9/50

 - 4s - loss: 0.1338 - val_loss: 0.1029

Epoch 10/50

 - 4s - loss: 0.1560 - val_loss: 0.1071

Epoch 11/50

 - 4s - loss: 0.1466 - val_loss: 0.0934

Epoch 12/50

 - 4s - loss: 0.1398 - val_loss: 0.0982

Epoch 13/50

 - 4s - loss: 0.1683 - val_loss: 0.1046

Epoch 14/50

 - 3s - loss: 0.1521 - val_loss: 0.1081

Epoch 15/50

 - 4s - loss: 0.1303 - val_loss: 0.0890

Epoch 16/50

 - 4s - loss: 0.1351 - val_loss: 0.0943

Epoch 17/50

 - 3s - loss: 0.1325 - val_loss: 0.0948

Epoch 18/50

 - 4s - loss: 0.1376 - val_loss: 0.0979

Epoch 19/50

 - 4s - loss: 0.1399 - val_loss: 0.0953

Epoch 20/50

 - 4s - loss: 0.1302 - val_loss: 0.0929

Epoch 21/50

 - 4s - loss: 0.1303 - val_loss: 0.0931

Epoch 22/50

 - 4s - loss: 0.1395 - val_loss: 0.1066

Epoch 23/50

 - 3s - loss: 0.1230 - val_loss: 0.0942

Epoch 24/50

 - 3s - loss: 0.1188 - val_loss: 0.0939

Epoch 25/50

 - 3s - loss: 0.1298 - val_loss: 0.1131

length of memory (state 0, action 0): 1035, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 397, after forget

length of memory (state 1, action 0): 494, after forget

length of memory (state 1, action 1): 385, after forget

time = 4829	action = 1	current_phase = 0	next_phase = 1	reward = -2.258458	array([[-3.60447  , -3.2239194]], dtype=float32)

time = 4837	action = 1	current_phase = 1	next_phase = 0	reward = -1.350631	array([[-2.8230324, -1.309487 ]], dtype=float32)

time = 4845	action = 0	current_phase = 0	next_phase = 1	reward = 0.634067	array([[-0.22077328, -2.4352307 ]], dtype=float32)

time = 4850	action = 0	current_phase = 0	next_phase = 1	reward = 0.290036	array([[-1.1577561, -2.7257018]], dtype=float32)

time = 4855	action = 0	current_phase = 0	next_phase = 1	reward = -1.149837	array([[-2.6257803, -2.7169685]], dtype=float32)

time = 4860	action = 1	current_phase = 0	next_phase = 1	reward = -1.862141	array([[-3.5268352, -3.2338357]], dtype=float32)

time = 4868	action = 1	current_phase = 1	next_phase = 0	reward = -0.626127	array([[-2.7564206, -1.4085224]], dtype=float32)

time = 4876	action = 0	current_phase = 0	next_phase = 1	reward = 0.069230	array([[-0.6815381, -2.798182 ]], dtype=float32)

time = 4881	action = 0	current_phase = 0	next_phase = 1	reward = -0.028213	array([[-1.8930535, -2.4374027]], dtype=float32)

time = 4886	action = 1	current_phase = 0	next_phase = 1	reward = -2.252118	array([[-3.1896148, -3.1556616]], dtype=float32)

time = 4894	action = 1	current_phase = 1	next_phase = 0	reward = -0.787201	array([[-2.7852817 , -0.96719885]], dtype=float32)

time = 4902	action = 0	current_phase = 0	next_phase = 1	reward = 0.006561	array([[-0.55673444, -2.524242  ]], dtype=float32)

time = 4907	action = 0	current_phase = 0	next_phase = 1	reward = 0.072404	array([[-0.838661 , -2.8473177]], dtype=float32)

time = 4912	action = 0	current_phase = 0	next_phase = 1	reward = -0.201830	array([[-2.0292625, -2.555007 ]], dtype=float32)

time = 4917	action = 1	current_phase = 0	next_phase = 1	reward = -2.556076	array([[-3.8399723, -3.2612681]], dtype=float32)

time = 4925	action = 1	current_phase = 1	next_phase = 0	reward = -0.924560	array([[-2.8459368, -1.1672093]], dtype=float32)

time = 4933	action = 0	current_phase = 0	next_phase = 1	reward = 0.013413	array([[-0.23051423, -2.5314972 ]], dtype=float32)

time = 4938	action = 0	current_phase = 0	next_phase = 1	reward = 0.074873	array([[-1.4632907, -2.6527443]], dtype=float32)

time = 4943	action = 0	current_phase = 0	next_phase = 1	reward = -0.390084	array([[-2.6792154, -2.9448395]], dtype=float32)

time = 4948	action = 1	current_phase = 0	next_phase = 1	reward = -2.634317	array([[-3.910442 , -3.5183659]], dtype=float32)

time = 4956	action = 1	current_phase = 1	next_phase = 0	reward = -0.939167	array([[-2.8227196, -1.0181432]], dtype=float32)

time = 4964	action = 0	current_phase = 0	next_phase = 1	reward = 0.056602	array([[-0.18839508, -2.430409  ]], dtype=float32)

time = 4969	action = 0	current_phase = 0	next_phase = 1	reward = 0.055862	array([[-1.8981178, -2.5109987]], dtype=float32)

time = 4974	action = 0	current_phase = 0	next_phase = 1	reward = -0.558730	array([[-2.7079353, -2.9219618]], dtype=float32)

time = 4979	action = 1	current_phase = 0	next_phase = 1	reward = -1.760277	array([[-3.3357298, -3.2758188]], dtype=float32)

time = 4987	action = 1	current_phase = 1	next_phase = 0	reward = -0.645540	array([[-2.6617358, -1.4945871]], dtype=float32)

time = 4995	action = 0	current_phase = 0	next_phase = 1	reward = -0.231046	array([[-0.6657838, -2.7708707]], dtype=float32)

time = 5000	action = 0	current_phase = 0	next_phase = 1	reward = 0.322083	array([[-1.8024061, -2.4564848]], dtype=float32)

time = 5005	action = 1	current_phase = 0	next_phase = 1	reward = -1.893106	array([[-3.0756264, -3.0346804]], dtype=float32)

time = 5013	action = 1	current_phase = 1	next_phase = 0	reward = -0.757228	array([[-2.7734747, -1.1958432]], dtype=float32)

time = 5021	action = 0	current_phase = 0	next_phase = 1	reward = -0.026693	array([[-0.7271085, -2.813962 ]], dtype=float32)

time = 5026	action = 0	current_phase = 0	next_phase = 1	reward = 0.043258	array([[-0.7280474, -2.8251705]], dtype=float32)

time = 5031	action = 0	current_phase = 0	next_phase = 1	reward = -0.023414	array([[-2.258386 , -2.8091736]], dtype=float32)

time = 5036	action = 0	current_phase = 0	next_phase = 1	reward = -0.767857	array([[-2.9092965, -3.2199035]], dtype=float32)

time = 5041	action = 1	current_phase = 0	next_phase = 1	reward = -2.540594	array([[-3.7811034, -3.2627444]], dtype=float32)

time = 5049	action = 1	current_phase = 1	next_phase = 0	reward = -0.902032	array([[-2.7958622, -1.262546 ]], dtype=float32)

time = 5057	action = 0	current_phase = 0	next_phase = 1	reward = 0.360251	array([[-0.28835845, -2.7608485 ]], dtype=float32)

time = 5062	action = 0	current_phase = 0	next_phase = 1	reward = -0.197977	array([[-1.5929849, -2.5815072]], dtype=float32)

time = 5067	action = 0	current_phase = 0	next_phase = 1	reward = -0.947169	array([[-2.9561102, -3.3036427]], dtype=float32)

time = 5072	action = 0	current_phase = 0	next_phase = 1	reward = -1.189406	array([[-3.5741327, -3.5971637]], dtype=float32)

time = 5077	action = 1	current_phase = 0	next_phase = 1	reward = -2.478596	array([[-3.8238564, -3.63877  ]], dtype=float32)

time = 5085	action = 1	current_phase = 1	next_phase = 0	reward = -0.675126	array([[-2.9162798, -1.3551521]], dtype=float32)

time = 5093	action = 0	current_phase = 0	next_phase = 1	reward = -0.336742	array([[-1.6472993, -2.7792501]], dtype=float32)

time = 5098	action = 0	current_phase = 0	next_phase = 1	reward = -1.084578	array([[-1.4302745, -3.8219924]], dtype=float32)

time = 5103	action = 0	current_phase = 0	next_phase = 1	reward = -1.194060	array([[-3.4742982, -3.8335414]], dtype=float32)

time = 5108	action = 1	current_phase = 0	next_phase = 1	reward = -2.042765	array([[-3.5055614, -3.4829464]], dtype=float32)

time = 5116	action = 1	current_phase = 1	next_phase = 0	reward = -1.189727	array([[-3.1640053, -1.4967289]], dtype=float32)

time = 5124	action = 0	current_phase = 0	next_phase = 1	reward = -0.313971	array([[-1.1973581, -2.5009732]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.1565 - val_loss: 0.1133

Epoch 2/50

 - 4s - loss: 0.1208 - val_loss: 0.1131

Epoch 3/50

 - 4s - loss: 0.1380 - val_loss: 0.1079

Epoch 4/50

 - 3s - loss: 0.1249 - val_loss: 0.1140

Epoch 5/50

 - 3s - loss: 0.1253 - val_loss: 0.1357

Epoch 6/50

 - 3s - loss: 0.1105 - val_loss: 0.1181

Epoch 7/50

 - 3s - loss: 0.1527 - val_loss: 0.1054

Epoch 8/50

 - 3s - loss: 0.1225 - val_loss: 0.1128

Epoch 9/50

 - 3s - loss: 0.1103 - val_loss: 0.1084

Epoch 10/50

 - 3s - loss: 0.1229 - val_loss: 0.1176

Epoch 11/50

 - 3s - loss: 0.1324 - val_loss: 0.1145

Epoch 12/50

 - 3s - loss: 0.1129 - val_loss: 0.1183

Epoch 13/50

 - 3s - loss: 0.1168 - val_loss: 0.1219

Epoch 14/50

 - 3s - loss: 0.1165 - val_loss: 0.1160

Epoch 15/50

 - 3s - loss: 0.1040 - val_loss: 0.1301

Epoch 16/50

 - 3s - loss: 0.1166 - val_loss: 0.1338

Epoch 17/50

 - 4s - loss: 0.1166 - val_loss: 0.1325

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 407, after forget

length of memory (state 1, action 0): 494, after forget

length of memory (state 1, action 1): 395, after forget

time = 5129	action = 0	current_phase = 0	next_phase = 1	reward = -1.161139	array([[-3.0296612, -3.1194417]], dtype=float32)

time = 5134	action = 0	current_phase = 0	next_phase = 1	reward = -1.211622	array([[-3.1061234, -3.1842308]], dtype=float32)

time = 5139	action = 1	current_phase = 0	next_phase = 1	reward = -1.503746	array([[-3.2302625, -3.0990615]], dtype=float32)

time = 5147	action = 1	current_phase = 1	next_phase = 0	reward = -0.792688	array([[-3.0265274, -1.7397348]], dtype=float32)

time = 5155	action = 1	current_phase = 0	next_phase = 1	reward = -1.720527	array([[-2.357883 , -2.1820881]], dtype=float32)

time = 5163	action = 1	current_phase = 1	next_phase = 0	reward = -0.729475	array([[-2.7247338, -1.1433122]], dtype=float32)

time = 5171	action = 0	current_phase = 0	next_phase = 1	reward = -0.011129	array([[-0.9698601, -3.1044152]], dtype=float32)

time = 5176	action = 0	current_phase = 0	next_phase = 1	reward = 0.073520	array([[-0.8781353, -2.6555676]], dtype=float32)

time = 5181	action = 0	current_phase = 0	next_phase = 1	reward = -0.076762	array([[-2.576312 , -2.7083325]], dtype=float32)

time = 5186	action = 1	current_phase = 0	next_phase = 1	reward = -2.262955	array([[-3.0278628, -2.984484 ]], dtype=float32)

time = 5194	action = 1	current_phase = 1	next_phase = 0	reward = -0.780272	array([[-2.5564585, -0.9682461]], dtype=float32)

time = 5202	action = 0	current_phase = 0	next_phase = 1	reward = 0.011551	array([[-0.6432576, -2.7136555]], dtype=float32)

time = 5207	action = 0	current_phase = 0	next_phase = 1	reward = 0.067304	array([[-1.4064329, -2.7006645]], dtype=float32)

time = 5212	action = 0	current_phase = 0	next_phase = 1	reward = -0.196036	array([[-2.2537916, -2.678211 ]], dtype=float32)

time = 5217	action = 1	current_phase = 0	next_phase = 1	reward = -2.340832	array([[-3.6121378, -3.171876 ]], dtype=float32)

time = 5225	action = 1	current_phase = 1	next_phase = 0	reward = -1.196849	array([[-2.58369  , -1.5152373]], dtype=float32)

time = 5233	action = 0	current_phase = 0	next_phase = 1	reward = 0.301881	array([[-0.26082784, -2.564348  ]], dtype=float32)

time = 5238	action = 0	current_phase = 0	next_phase = 1	reward = 0.069714	array([[-1.6372168, -2.8272   ]], dtype=float32)

time = 5243	action = 0	current_phase = 0	next_phase = 1	reward = -0.233152	array([[-1.8932803, -2.7005692]], dtype=float32)

time = 5248	action = 1	current_phase = 0	next_phase = 1	reward = -2.707180	array([[-3.76457  , -2.9691594]], dtype=float32)

time = 5256	action = 1	current_phase = 1	next_phase = 0	reward = -1.318701	array([[-2.705555 , -1.2010777]], dtype=float32)

time = 5264	action = 0	current_phase = 0	next_phase = 1	reward = 0.330654	array([[-0.03139216, -2.4160848 ]], dtype=float32)

time = 5269	action = 0	current_phase = 0	next_phase = 1	reward = 0.065597	array([[-1.0502006, -2.312404 ]], dtype=float32)

time = 5274	action = 1	current_phase = 0	next_phase = 1	reward = -1.066083	array([[-2.9232388, -2.4655902]], dtype=float32)

time = 5282	action = 0	current_phase = 1	next_phase = 0	reward = -1.419380	array([[-1.6335944, -1.7425045]], dtype=float32)

time = 5287	action = 1	current_phase = 1	next_phase = 0	reward = -1.229494	array([[-2.9209545 , -0.96449727]], dtype=float32)

time = 5295	action = 0	current_phase = 0	next_phase = 1	reward = 0.649201	array([[-0.18227547, -2.4503715 ]], dtype=float32)

time = 5300	action = 0	current_phase = 0	next_phase = 1	reward = 0.286846	array([[-1.5459132, -2.4998798]], dtype=float32)

time = 5305	action = 1	current_phase = 0	next_phase = 1	reward = -2.086675	array([[-2.8534627, -2.71169  ]], dtype=float32)

time = 5313	action = 1	current_phase = 1	next_phase = 0	reward = -0.722712	array([[-1.8384285, -1.5177348]], dtype=float32)

time = 5321	action = 0	current_phase = 0	next_phase = 1	reward = -0.003512	array([[-0.57043386, -2.743974  ]], dtype=float32)

time = 5326	action = 0	current_phase = 0	next_phase = 1	reward = 0.062927	array([[-0.59268945, -2.7532496 ]], dtype=float32)

time = 5331	action = 1	current_phase = 0	next_phase = 1	reward = -1.443030	array([[-2.7812643, -2.563435 ]], dtype=float32)

time = 5339	action = 1	current_phase = 1	next_phase = 0	reward = -0.840968	array([[-2.7047482, -1.0216017]], dtype=float32)

time = 5347	action = 0	current_phase = 0	next_phase = 1	reward = -0.082900	array([[-0.03865129, -2.9235852 ]], dtype=float32)

time = 5352	action = 0	current_phase = 0	next_phase = 1	reward = -0.014373	array([[-0.21668375, -3.0639238 ]], dtype=float32)

time = 5357	action = 0	current_phase = 0	next_phase = 1	reward = 0.048406	array([[-0.7173866, -2.8645458]], dtype=float32)

time = 5362	action = 0	current_phase = 0	next_phase = 1	reward = -0.098927	array([[-2.2277925, -2.2884526]], dtype=float32)

time = 5367	action = 1	current_phase = 0	next_phase = 1	reward = -2.373935	array([[-3.4667523, -2.5619464]], dtype=float32)

time = 5375	action = 1	current_phase = 1	next_phase = 0	reward = -0.856852	array([[-2.731573 , -1.2953632]], dtype=float32)

time = 5383	action = 0	current_phase = 0	next_phase = 1	reward = 0.017022	array([[-0.4799046, -2.563877 ]], dtype=float32)

time = 5388	action = 0	current_phase = 0	next_phase = 1	reward = 0.074003	array([[-2.2358124, -2.9132147]], dtype=float32)

time = 5393	action = 0	current_phase = 0	next_phase = 1	reward = -0.385412	array([[-2.0810943, -2.5710752]], dtype=float32)

time = 5398	action = 1	current_phase = 0	next_phase = 1	reward = -2.638301	array([[-3.8637888, -3.5063689]], dtype=float32)

time = 5406	action = 1	current_phase = 1	next_phase = 0	reward = -1.267369	array([[-2.812363 , -1.0463848]], dtype=float32)

time = 5414	action = 0	current_phase = 0	next_phase = 1	reward = 0.349605	array([[ 0.17701733, -2.2213933 ]], dtype=float32)

time = 5419	action = 0	current_phase = 0	next_phase = 1	reward = 0.054710	array([[-1.7114176, -2.5059059]], dtype=float32)

time = 5424	action = 1	current_phase = 0	next_phase = 1	reward = -1.146352	array([[-3.360734 , -2.6885118]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.1595 - val_loss: 0.0529

Epoch 2/50

 - 3s - loss: 0.1746 - val_loss: 0.0584

Epoch 3/50

 - 3s - loss: 0.1615 - val_loss: 0.0647

Epoch 4/50

 - 3s - loss: 0.1542 - val_loss: 0.0665

Epoch 5/50

 - 3s - loss: 0.1543 - val_loss: 0.0578

Epoch 6/50

 - 3s - loss: 0.1634 - val_loss: 0.0663

Epoch 7/50

 - 3s - loss: 0.1444 - val_loss: 0.0745

Epoch 8/50

 - 3s - loss: 0.1401 - val_loss: 0.0615

Epoch 9/50

 - 3s - loss: 0.1340 - val_loss: 0.0653

Epoch 10/50

 - 3s - loss: 0.1256 - val_loss: 0.0667

Epoch 11/50

 - 3s - loss: 0.1227 - val_loss: 0.0715

length of memory (state 0, action 0): 1026, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 418, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 405, after forget

time = 5432	action = 1	current_phase = 1	next_phase = 0	reward = -1.017239	array([[-2.0314279, -1.6719567]], dtype=float32)

time = 5440	action = 0	current_phase = 0	next_phase = 1	reward = 0.255211	array([[-0.2738272, -2.549976 ]], dtype=float32)

time = 5445	action = 0	current_phase = 0	next_phase = 1	reward = 0.045420	array([[-0.6304327, -2.7330806]], dtype=float32)

time = 5450	action = 0	current_phase = 0	next_phase = 1	reward = 0.005038	array([[-2.502776, -2.643157]], dtype=float32)

time = 5455	action = 1	current_phase = 0	next_phase = 1	reward = -1.967257	array([[-3.3706949, -2.587817 ]], dtype=float32)

time = 5463	action = 1	current_phase = 1	next_phase = 0	reward = -0.732420	array([[-1.9502025, -1.1112448]], dtype=float32)

time = 5471	action = 0	current_phase = 0	next_phase = 1	reward = 0.000919	array([[-0.49943092, -2.7504    ]], dtype=float32)

time = 5476	action = 0	current_phase = 0	next_phase = 1	reward = 0.061809	array([[-0.7381918, -2.7756565]], dtype=float32)

time = 5481	action = 0	current_phase = 0	next_phase = 1	reward = -0.038232	array([[-1.8680601, -2.537425 ]], dtype=float32)

time = 5486	action = 1	current_phase = 0	next_phase = 1	reward = -2.266976	array([[-3.1571646, -2.9325697]], dtype=float32)

time = 5494	action = 1	current_phase = 1	next_phase = 0	reward = -0.747756	array([[-2.6109874 , -0.95402694]], dtype=float32)

time = 5502	action = 0	current_phase = 0	next_phase = 1	reward = 0.000096	array([[-0.19833511, -2.454555  ]], dtype=float32)

time = 5507	action = 0	current_phase = 0	next_phase = 1	reward = 0.087519	array([[-0.6892407, -3.1158488]], dtype=float32)

time = 5512	action = 0	current_phase = 0	next_phase = 1	reward = -0.179196	array([[-1.9766622, -2.436775 ]], dtype=float32)

time = 5517	action = 0	current_phase = 0	next_phase = 1	reward = -1.011573	array([[-3.1127167, -3.4960928]], dtype=float32)

time = 5522	action = 1	current_phase = 0	next_phase = 1	reward = -2.701634	array([[-3.8812842, -3.3691418]], dtype=float32)

time = 5530	action = 1	current_phase = 1	next_phase = 0	reward = -0.760436	array([[-2.9548106, -1.3765324]], dtype=float32)

time = 5538	action = 0	current_phase = 0	next_phase = 1	reward = 0.079839	array([[-0.841601, -2.471933]], dtype=float32)

time = 5543	action = 0	current_phase = 0	next_phase = 1	reward = -0.377153	array([[-2.2684598, -2.539181 ]], dtype=float32)

time = 5548	action = 0	current_phase = 0	next_phase = 1	reward = -1.079201	array([[-2.4692786, -3.55912  ]], dtype=float32)

time = 5553	action = 1	current_phase = 0	next_phase = 1	reward = -2.721966	array([[-3.6829405, -3.4574525]], dtype=float32)

time = 5561	action = 1	current_phase = 1	next_phase = 0	reward = -0.814741	array([[-2.8952594, -1.4624074]], dtype=float32)

time = 5569	action = 0	current_phase = 0	next_phase = 1	reward = 0.045628	array([[-0.6215728, -2.221408 ]], dtype=float32)

time = 5574	action = 0	current_phase = 0	next_phase = 1	reward = -0.623189	array([[-2.2956815, -2.4246764]], dtype=float32)

time = 5579	action = 0	current_phase = 0	next_phase = 1	reward = -1.175364	array([[-3.4510956, -3.50783  ]], dtype=float32)

time = 5584	action = 1	current_phase = 0	next_phase = 1	reward = -1.602082	array([[-3.4888093, -3.1733139]], dtype=float32)

time = 5592	action = 1	current_phase = 1	next_phase = 0	reward = -0.674178	array([[-2.55078  , -1.6464403]], dtype=float32)

time = 5600	action = 0	current_phase = 0	next_phase = 1	reward = 0.000338	array([[-1.7187908, -2.1354747]], dtype=float32)

time = 5605	action = 1	current_phase = 0	next_phase = 1	reward = -1.584421	array([[-2.7467124, -2.6721032]], dtype=float32)

time = 5613	action = 1	current_phase = 1	next_phase = 0	reward = -0.733220	array([[-2.6029212, -1.4979163]], dtype=float32)

time = 5621	action = 0	current_phase = 0	next_phase = 1	reward = -0.013851	array([[-0.6977701, -3.0411537]], dtype=float32)

time = 5626	action = 0	current_phase = 0	next_phase = 1	reward = 0.059701	array([[-0.8641306, -2.973829 ]], dtype=float32)

time = 5631	action = 1	current_phase = 0	next_phase = 1	reward = -1.445092	array([[-2.7694356, -2.4177036]], dtype=float32)

time = 5639	action = 1	current_phase = 1	next_phase = 0	reward = -0.775873	array([[-2.5357878, -1.075933 ]], dtype=float32)

time = 5647	action = 0	current_phase = 0	next_phase = 1	reward = -0.085328	array([[-0.309093 , -2.7030575]], dtype=float32)

time = 5652	action = 0	current_phase = 0	next_phase = 1	reward = -0.014078	array([[-0.47547802, -2.8931642 ]], dtype=float32)

time = 5657	action = 0	current_phase = 0	next_phase = 1	reward = 0.057686	array([[-1.0319363, -3.0596104]], dtype=float32)

time = 5662	action = 0	current_phase = 0	next_phase = 1	reward = -0.087391	array([[-2.0544033, -2.5014958]], dtype=float32)

time = 5667	action = 1	current_phase = 0	next_phase = 1	reward = -2.493798	array([[-3.79641  , -3.0170221]], dtype=float32)

time = 5675	action = 1	current_phase = 1	next_phase = 0	reward = -0.785033	array([[-2.7421858, -1.3880191]], dtype=float32)

time = 5683	action = 0	current_phase = 0	next_phase = 1	reward = 0.019610	array([[-0.33226532, -2.84405   ]], dtype=float32)

time = 5688	action = 0	current_phase = 0	next_phase = 1	reward = 0.077241	array([[-0.8804039, -2.9511275]], dtype=float32)

time = 5693	action = 0	current_phase = 0	next_phase = 1	reward = -0.255136	array([[-2.2473304, -2.4955385]], dtype=float32)

time = 5698	action = 0	current_phase = 0	next_phase = 1	reward = -1.095810	array([[-3.3536751, -3.3901803]], dtype=float32)

time = 5703	action = 1	current_phase = 0	next_phase = 1	reward = -2.777243	array([[-3.9165697, -3.5783818]], dtype=float32)

time = 5711	action = 1	current_phase = 1	next_phase = 0	reward = -1.043353	array([[-3.0963678, -1.6954539]], dtype=float32)

time = 5719	action = 0	current_phase = 0	next_phase = 1	reward = 0.372230	array([[-0.8906399, -2.3822825]], dtype=float32)

time = 5724	action = 0	current_phase = 0	next_phase = 1	reward = -0.529797	array([[-1.6997547, -2.6309946]], dtype=float32)

time = 5729	action = 1	current_phase = 0	next_phase = 1	reward = -1.690746	array([[-3.7406607, -3.4244037]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.1518 - val_loss: 0.1004

Epoch 2/50

 - 3s - loss: 0.1560 - val_loss: 0.0819

Epoch 3/50

 - 4s - loss: 0.1325 - val_loss: 0.0739

Epoch 4/50

 - 4s - loss: 0.1383 - val_loss: 0.0940

Epoch 5/50

 - 4s - loss: 0.1558 - val_loss: 0.0694

Epoch 6/50

 - 4s - loss: 0.1275 - val_loss: 0.0764

Epoch 7/50

 - 3s - loss: 0.1513 - val_loss: 0.1308

Epoch 8/50

 - 3s - loss: 0.1314 - val_loss: 0.0859

Epoch 9/50

 - 4s - loss: 0.1224 - val_loss: 0.0744

Epoch 10/50

 - 4s - loss: 0.1341 - val_loss: 0.0828

Epoch 11/50

 - 4s - loss: 0.1269 - val_loss: 0.0788

Epoch 12/50

 - 4s - loss: 0.1277 - val_loss: 0.0900

Epoch 13/50

 - 4s - loss: 0.1166 - val_loss: 0.0776

Epoch 14/50

 - 4s - loss: 0.1322 - val_loss: 0.0880

Epoch 15/50

 - 4s - loss: 0.1178 - val_loss: 0.0847

length of memory (state 0, action 0): 1029, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 428, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 415, after forget

time = 5737	action = 1	current_phase = 1	next_phase = 0	reward = -1.195921	array([[-2.8159556, -1.6998062]], dtype=float32)

time = 5745	action = 0	current_phase = 0	next_phase = 1	reward = 0.344838	array([[-0.59868455, -2.8426487 ]], dtype=float32)

time = 5750	action = 0	current_phase = 0	next_phase = 1	reward = 0.298703	array([[-1.6629324, -2.3990946]], dtype=float32)

time = 5755	action = 1	current_phase = 0	next_phase = 1	reward = -1.976649	array([[-3.4459383, -2.7893813]], dtype=float32)

time = 5763	action = 1	current_phase = 1	next_phase = 0	reward = -0.724258	array([[-2.6572018, -1.2360561]], dtype=float32)

time = 5771	action = 0	current_phase = 0	next_phase = 1	reward = 0.006475	array([[-0.42156798, -2.7281122 ]], dtype=float32)

time = 5776	action = 0	current_phase = 0	next_phase = 1	reward = 0.069037	array([[-0.94251525, -2.897609  ]], dtype=float32)

time = 5781	action = 0	current_phase = 0	next_phase = 1	reward = -0.097837	array([[-2.147309 , -2.4846017]], dtype=float32)

time = 5786	action = 1	current_phase = 0	next_phase = 1	reward = -2.258730	array([[-3.2612543, -2.8423538]], dtype=float32)

time = 5794	action = 1	current_phase = 1	next_phase = 0	reward = -0.728323	array([[-2.5885415, -1.0084752]], dtype=float32)

time = 5802	action = 0	current_phase = 0	next_phase = 1	reward = -0.000925	array([[-0.5129285, -2.8894656]], dtype=float32)

time = 5807	action = 0	current_phase = 0	next_phase = 1	reward = 0.076499	array([[-0.9868149, -2.864588 ]], dtype=float32)

time = 5812	action = 0	current_phase = 0	next_phase = 1	reward = -0.123806	array([[-2.418971 , -2.5145938]], dtype=float32)

time = 5817	action = 0	current_phase = 0	next_phase = 1	reward = -0.953793	array([[-2.9457915, -3.1913128]], dtype=float32)

time = 5822	action = 1	current_phase = 0	next_phase = 1	reward = -2.609556	array([[-3.9302237, -3.3653235]], dtype=float32)

time = 5830	action = 1	current_phase = 1	next_phase = 0	reward = -1.046288	array([[-2.876041 , -1.4555155]], dtype=float32)

time = 5838	action = 0	current_phase = 0	next_phase = 1	reward = 0.379272	array([[-0.4693788, -2.591302 ]], dtype=float32)

time = 5843	action = 0	current_phase = 0	next_phase = 1	reward = -0.362948	array([[-1.4882553, -2.7117796]], dtype=float32)

time = 5848	action = 1	current_phase = 0	next_phase = 1	reward = -2.754252	array([[-3.378251 , -3.2015228]], dtype=float32)

time = 5856	action = 1	current_phase = 1	next_phase = 0	reward = -0.927621	array([[-3.1315467, -1.4968541]], dtype=float32)

time = 5864	action = 0	current_phase = 0	next_phase = 1	reward = 0.035219	array([[ 0.0236178, -2.4945188]], dtype=float32)

time = 5869	action = 0	current_phase = 0	next_phase = 1	reward = 0.061518	array([[-0.7407929, -2.2140167]], dtype=float32)

time = 5874	action = 0	current_phase = 0	next_phase = 1	reward = -0.466145	array([[-1.9083164, -2.3716276]], dtype=float32)

time = 5879	action = 1	current_phase = 0	next_phase = 1	reward = -1.672362	array([[-3.3081515, -3.1095774]], dtype=float32)

time = 5887	action = 1	current_phase = 1	next_phase = 0	reward = -0.926749	array([[-2.634724 , -1.5546436]], dtype=float32)

time = 5895	action = 0	current_phase = 0	next_phase = 1	reward = 0.334376	array([[-0.7009567, -2.8774226]], dtype=float32)

time = 5900	action = 0	current_phase = 0	next_phase = 1	reward = -0.282469	array([[-1.9806206, -2.3264537]], dtype=float32)

time = 5905	action = 1	current_phase = 0	next_phase = 1	reward = -1.732087	array([[-2.966357, -2.463419]], dtype=float32)

time = 5913	action = 1	current_phase = 1	next_phase = 0	reward = -0.721698	array([[-2.5595546, -1.2339988]], dtype=float32)

time = 5921	action = 0	current_phase = 0	next_phase = 1	reward = 0.000323	array([[-0.41998488, -2.7091353 ]], dtype=float32)

time = 5926	action = 0	current_phase = 0	next_phase = 1	reward = 0.058496	array([[-0.70136297, -2.82741   ]], dtype=float32)

time = 5931	action = 0	current_phase = 0	next_phase = 1	reward = -0.063068	array([[-2.196559 , -2.4336066]], dtype=float32)

time = 5936	action = 1	current_phase = 0	next_phase = 1	reward = -2.272529	array([[-3.1269379, -2.9236302]], dtype=float32)

time = 5944	action = 1	current_phase = 1	next_phase = 0	reward = -0.741103	array([[-2.4515486 , -0.97952795]], dtype=float32)

time = 5952	action = 0	current_phase = 0	next_phase = 1	reward = 0.016365	array([[-0.54397225, -2.6682925 ]], dtype=float32)

time = 5957	action = 0	current_phase = 0	next_phase = 1	reward = 0.075288	array([[-0.8204435, -2.9184773]], dtype=float32)

time = 5962	action = 0	current_phase = 0	next_phase = 1	reward = -0.205406	array([[-1.7316093, -2.7647076]], dtype=float32)

time = 5967	action = 1	current_phase = 0	next_phase = 1	reward = -2.495342	array([[-3.2353194, -2.9933856]], dtype=float32)

time = 5975	action = 1	current_phase = 1	next_phase = 0	reward = -1.245214	array([[-2.9449522, -1.2303503]], dtype=float32)

time = 5983	action = 0	current_phase = 0	next_phase = 1	reward = 0.304791	array([[-0.14186269, -2.6501527 ]], dtype=float32)

time = 5988	action = 0	current_phase = 0	next_phase = 1	reward = 0.069684	array([[-0.584509 , -2.6401608]], dtype=float32)

time = 5993	action = 0	current_phase = 0	next_phase = 1	reward = -0.296834	array([[-1.8964243, -2.8493323]], dtype=float32)

time = 5998	action = 1	current_phase = 0	next_phase = 1	reward = -2.738193	array([[-3.7374635, -3.4595094]], dtype=float32)

time = 6006	action = 1	current_phase = 1	next_phase = 0	reward = -1.206347	array([[-3.0003083, -1.4418993]], dtype=float32)

time = 6014	action = 0	current_phase = 0	next_phase = 1	reward = 0.345036	array([[-0.06671244, -2.5394058 ]], dtype=float32)

time = 6019	action = 0	current_phase = 0	next_phase = 1	reward = 0.068623	array([[-1.0669594, -2.543433 ]], dtype=float32)

time = 6024	action = 0	current_phase = 0	next_phase = 1	reward = -0.543570	array([[-1.9818099, -2.256953 ]], dtype=float32)

time = 6029	action = 1	current_phase = 0	next_phase = 1	reward = -1.786717	array([[-3.7741585, -2.8989468]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.1247 - val_loss: 0.0635

Epoch 2/50

 - 4s - loss: 0.1282 - val_loss: 0.0688

Epoch 3/50

 - 4s - loss: 0.1496 - val_loss: 0.0573

Epoch 4/50

 - 3s - loss: 0.1118 - val_loss: 0.0549

Epoch 5/50

 - 4s - loss: 0.1177 - val_loss: 0.0606

Epoch 6/50

 - 4s - loss: 0.1284 - val_loss: 0.0579

Epoch 7/50

 - 4s - loss: 0.1049 - val_loss: 0.0663

Epoch 8/50

 - 5s - loss: 0.1155 - val_loss: 0.0601

Epoch 9/50

 - 5s - loss: 0.1150 - val_loss: 0.0622

Epoch 10/50

 - 4s - loss: 0.1150 - val_loss: 0.0626

Epoch 11/50

 - 4s - loss: 0.1044 - val_loss: 0.0644

Epoch 12/50

 - 4s - loss: 0.1321 - val_loss: 0.0611

Epoch 13/50

 - 3s - loss: 0.0946 - val_loss: 0.0773

Epoch 14/50

 - 3s - loss: 0.1211 - val_loss: 0.0584

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 438, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 425, after forget

time = 6037	action = 1	current_phase = 1	next_phase = 0	reward = -0.925249	array([[-2.8397784, -1.3397903]], dtype=float32)

time = 6045	action = 0	current_phase = 0	next_phase = 1	reward = 0.327571	array([[-0.65892553, -2.788197  ]], dtype=float32)

time = 6050	action = 0	current_phase = 0	next_phase = 1	reward = -0.248345	array([[-2.0006206, -2.2730935]], dtype=float32)

time = 6055	action = 1	current_phase = 0	next_phase = 1	reward = -1.721178	array([[-2.7768254, -2.5146286]], dtype=float32)

time = 6063	action = 1	current_phase = 1	next_phase = 0	reward = -0.724587	array([[-2.5846016, -1.2818278]], dtype=float32)

time = 6071	action = 0	current_phase = 0	next_phase = 1	reward = -0.014075	array([[-0.58386743, -2.7997775 ]], dtype=float32)

time = 6076	action = 0	current_phase = 0	next_phase = 1	reward = 0.063217	array([[-0.7130309, -2.7858944]], dtype=float32)

time = 6081	action = 0	current_phase = 0	next_phase = 1	reward = -0.059918	array([[-2.267661 , -2.4494996]], dtype=float32)

time = 6086	action = 1	current_phase = 0	next_phase = 1	reward = -2.265219	array([[-3.2542102, -3.0619192]], dtype=float32)

time = 6094	action = 1	current_phase = 1	next_phase = 0	reward = -0.735327	array([[-2.5121126, -1.0730491]], dtype=float32)

time = 6102	action = 0	current_phase = 0	next_phase = 1	reward = 0.018835	array([[-0.4737686, -2.9438038]], dtype=float32)

time = 6107	action = 0	current_phase = 0	next_phase = 1	reward = 0.072802	array([[-1.2063142, -2.92946  ]], dtype=float32)

time = 6112	action = 0	current_phase = 0	next_phase = 1	reward = -0.191792	array([[-2.239422 , -2.6211429]], dtype=float32)

time = 6117	action = 1	current_phase = 0	next_phase = 1	reward = -2.555841	array([[-3.4633548, -2.9978132]], dtype=float32)

time = 6125	action = 1	current_phase = 1	next_phase = 0	reward = -0.834931	array([[-2.717545 , -1.3551323]], dtype=float32)

time = 6133	action = 0	current_phase = 0	next_phase = 1	reward = 0.002500	array([[-0.45692572, -2.7777982 ]], dtype=float32)

time = 6138	action = 0	current_phase = 0	next_phase = 1	reward = 0.069302	array([[-1.6276777, -2.7795446]], dtype=float32)

time = 6143	action = 0	current_phase = 0	next_phase = 1	reward = -0.348737	array([[-2.6856527, -2.7206175]], dtype=float32)

time = 6148	action = 1	current_phase = 0	next_phase = 1	reward = -2.629231	array([[-3.8170917, -3.5219617]], dtype=float32)

time = 6156	action = 1	current_phase = 1	next_phase = 0	reward = -1.205959	array([[-2.912465 , -1.3199178]], dtype=float32)

time = 6164	action = 0	current_phase = 0	next_phase = 1	reward = 0.337940	array([[-0.0085339, -2.4526503]], dtype=float32)

time = 6169	action = 0	current_phase = 0	next_phase = 1	reward = 0.044283	array([[-1.2830598, -2.3931837]], dtype=float32)

time = 6174	action = 0	current_phase = 0	next_phase = 1	reward = -0.608699	array([[-2.1384664, -2.4501894]], dtype=float32)

time = 6179	action = 1	current_phase = 0	next_phase = 1	reward = -1.684157	array([[-3.6075234, -3.2925105]], dtype=float32)

time = 6187	action = 1	current_phase = 1	next_phase = 0	reward = -0.594704	array([[-2.6759174, -1.5389781]], dtype=float32)

time = 6195	action = 0	current_phase = 0	next_phase = 1	reward = -0.236082	array([[-0.5391556, -2.677857 ]], dtype=float32)

time = 6200	action = 0	current_phase = 0	next_phase = 1	reward = 0.310684	array([[-1.3239429, -2.3376417]], dtype=float32)

time = 6205	action = 1	current_phase = 0	next_phase = 1	reward = -1.856974	array([[-3.188845 , -2.7340367]], dtype=float32)

time = 6213	action = 1	current_phase = 1	next_phase = 0	reward = -0.735725	array([[-2.6500247, -1.2256734]], dtype=float32)

time = 6221	action = 0	current_phase = 0	next_phase = 1	reward = -0.030612	array([[-0.5073366, -2.7160795]], dtype=float32)

time = 6226	action = 0	current_phase = 0	next_phase = 1	reward = 0.048928	array([[-0.89304394, -2.7725258 ]], dtype=float32)

time = 6231	action = 0	current_phase = 0	next_phase = 1	reward = -0.017079	array([[-1.9724336, -2.286644 ]], dtype=float32)

time = 6236	action = 1	current_phase = 0	next_phase = 1	reward = -2.154395	array([[-2.9126847, -2.8787272]], dtype=float32)

time = 6244	action = 1	current_phase = 1	next_phase = 0	reward = -0.783368	array([[-2.4703612, -0.8575014]], dtype=float32)

time = 6252	action = 0	current_phase = 0	next_phase = 1	reward = 0.002385	array([[-0.5833191, -2.6373227]], dtype=float32)

time = 6257	action = 0	current_phase = 0	next_phase = 1	reward = 0.073691	array([[-1.7529273, -2.8864968]], dtype=float32)

time = 6262	action = 0	current_phase = 0	next_phase = 1	reward = -0.183752	array([[-2.2800527, -2.4525251]], dtype=float32)

time = 6267	action = 1	current_phase = 0	next_phase = 1	reward = -2.494425	array([[-3.1970994, -3.1899483]], dtype=float32)

time = 6275	action = 1	current_phase = 1	next_phase = 0	reward = -0.871652	array([[-2.9515219, -1.4667249]], dtype=float32)

time = 6283	action = 0	current_phase = 0	next_phase = 1	reward = 0.041979	array([[-0.15364492, -2.5565717 ]], dtype=float32)

time = 6288	action = 0	current_phase = 0	next_phase = 1	reward = 0.084891	array([[-0.8289569, -2.6974704]], dtype=float32)

time = 6293	action = 1	current_phase = 0	next_phase = 1	reward = -2.003528	array([[-2.6596508, -2.6508124]], dtype=float32)

time = 6301	action = 1	current_phase = 1	next_phase = 0	reward = -1.140477	array([[-2.6170278, -1.5408686]], dtype=float32)

time = 6309	action = 0	current_phase = 0	next_phase = 1	reward = -0.036439	array([[-0.19396669, -2.5591598 ]], dtype=float32)

time = 6314	action = 0	current_phase = 0	next_phase = 1	reward = 0.044224	array([[-0.3995516, -2.6635249]], dtype=float32)

time = 6319	action = 0	current_phase = 0	next_phase = 1	reward = 0.063777	array([[-1.0431811, -2.4654014]], dtype=float32)

time = 6324	action = 0	current_phase = 0	next_phase = 1	reward = -0.606581	array([[-2.3850543, -2.5605717]], dtype=float32)

time = 6329	action = 1	current_phase = 0	next_phase = 1	reward = -1.735879	array([[-4.2501845, -2.9740813]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.1663 - val_loss: 0.1127

Epoch 2/50

 - 4s - loss: 0.1505 - val_loss: 0.1125

Epoch 3/50

 - 3s - loss: 0.1424 - val_loss: 0.1159

Epoch 4/50

 - 4s - loss: 0.1310 - val_loss: 0.1204

Epoch 5/50

 - 3s - loss: 0.1430 - val_loss: 0.1184

Epoch 6/50

 - 3s - loss: 0.1215 - val_loss: 0.1250

Epoch 7/50

 - 4s - loss: 0.1118 - val_loss: 0.1153

Epoch 8/50

 - 4s - loss: 0.1104 - val_loss: 0.1253

Epoch 9/50

 - 3s - loss: 0.1264 - val_loss: 0.1282

Epoch 10/50

 - 3s - loss: 0.1063 - val_loss: 0.1254

Epoch 11/50

 - 4s - loss: 0.1226 - val_loss: 0.1259

Epoch 12/50

 - 3s - loss: 0.1113 - val_loss: 0.1343

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 448, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 435, after forget

time = 6337	action = 1	current_phase = 1	next_phase = 0	reward = -1.162195	array([[-2.8462343, -1.107559 ]], dtype=float32)

time = 6345	action = 0	current_phase = 0	next_phase = 1	reward = 0.612365	array([[-0.82076174, -2.8299942 ]], dtype=float32)

time = 6350	action = 0	current_phase = 0	next_phase = 1	reward = 0.052470	array([[-2.087306 , -2.4452963]], dtype=float32)

time = 6355	action = 1	current_phase = 0	next_phase = 1	reward = -1.894252	array([[-3.2982903, -2.8272223]], dtype=float32)

time = 6363	action = 1	current_phase = 1	next_phase = 0	reward = -0.750675	array([[-2.901402 , -1.2426885]], dtype=float32)

time = 6371	action = 0	current_phase = 0	next_phase = 1	reward = 0.005133	array([[-0.9710995, -2.957728 ]], dtype=float32)

time = 6376	action = 0	current_phase = 0	next_phase = 1	reward = 0.071638	array([[-1.7564753, -2.8256693]], dtype=float32)

time = 6381	action = 1	current_phase = 0	next_phase = 1	reward = -1.507779	array([[-2.6350877, -2.521566 ]], dtype=float32)

time = 6389	action = 1	current_phase = 1	next_phase = 0	reward = -0.776578	array([[-2.832261  , -0.91238075]], dtype=float32)

time = 6397	action = 0	current_phase = 0	next_phase = 1	reward = -0.072480	array([[-0.33869898, -2.7618876 ]], dtype=float32)

time = 6402	action = 0	current_phase = 0	next_phase = 1	reward = 0.005192	array([[-0.5508824, -3.0593934]], dtype=float32)

time = 6407	action = 0	current_phase = 0	next_phase = 1	reward = 0.072437	array([[-1.1127497, -2.7275376]], dtype=float32)

time = 6412	action = 1	current_phase = 0	next_phase = 1	reward = -1.663378	array([[-2.9302325, -2.5938654]], dtype=float32)

time = 6420	action = 1	current_phase = 1	next_phase = 0	reward = -1.000729	array([[-2.5197105, -1.1535981]], dtype=float32)

time = 6428	action = 0	current_phase = 0	next_phase = 1	reward = -0.048920	array([[-0.36437926, -2.7488208 ]], dtype=float32)

time = 6433	action = 0	current_phase = 0	next_phase = 1	reward = 0.003164	array([[-0.59621716, -3.231616  ]], dtype=float32)

time = 6438	action = 0	current_phase = 0	next_phase = 1	reward = 0.074341	array([[-1.2603908, -2.9737868]], dtype=float32)

time = 6443	action = 0	current_phase = 0	next_phase = 1	reward = -0.311736	array([[-2.6611638, -2.73737  ]], dtype=float32)

time = 6448	action = 1	current_phase = 0	next_phase = 1	reward = -2.749679	array([[-3.983848 , -3.4685655]], dtype=float32)

time = 6456	action = 1	current_phase = 1	next_phase = 0	reward = -1.217290	array([[-2.8611493, -1.0306424]], dtype=float32)

time = 6464	action = 0	current_phase = 0	next_phase = 1	reward = 0.345335	array([[-0.27341896, -2.638443  ]], dtype=float32)

time = 6469	action = 0	current_phase = 0	next_phase = 1	reward = 0.078861	array([[-1.8822849, -2.3788295]], dtype=float32)

time = 6474	action = 1	current_phase = 0	next_phase = 1	reward = -1.027561	array([[-2.7508738, -2.7422657]], dtype=float32)

time = 6482	action = 1	current_phase = 1	next_phase = 0	reward = -0.762079	array([[-2.6107306, -1.5966688]], dtype=float32)

time = 6490	action = 0	current_phase = 0	next_phase = 1	reward = -0.035761	array([[-0.42916355, -2.6029522 ]], dtype=float32)

time = 6495	action = 0	current_phase = 0	next_phase = 1	reward = -0.243583	array([[-1.1388037, -2.7700925]], dtype=float32)

time = 6500	action = 0	current_phase = 0	next_phase = 1	reward = 0.315258	array([[-2.067171 , -2.4974246]], dtype=float32)

time = 6505	action = 1	current_phase = 0	next_phase = 1	reward = -1.854085	array([[-3.001408 , -2.9019585]], dtype=float32)

time = 6513	action = 1	current_phase = 1	next_phase = 0	reward = -0.723590	array([[-2.2321491, -1.3164306]], dtype=float32)

time = 6521	action = 0	current_phase = 0	next_phase = 1	reward = -0.015075	array([[-0.78046507, -2.833137  ]], dtype=float32)

time = 6526	action = 0	current_phase = 0	next_phase = 1	reward = 0.056910	array([[-1.4798615, -2.8198395]], dtype=float32)

time = 6531	action = 0	current_phase = 0	next_phase = 1	reward = -0.048064	array([[-2.207545 , -2.3710966]], dtype=float32)

time = 6536	action = 1	current_phase = 0	next_phase = 1	reward = -2.254821	array([[-3.46985  , -3.1069856]], dtype=float32)

time = 6544	action = 1	current_phase = 1	next_phase = 0	reward = -0.856014	array([[-2.6483245 , -0.94245243]], dtype=float32)

time = 6552	action = 0	current_phase = 0	next_phase = 1	reward = 0.012011	array([[-0.53304064, -2.7586799 ]], dtype=float32)

time = 6557	action = 0	current_phase = 0	next_phase = 1	reward = 0.079842	array([[-1.1948216, -2.7276454]], dtype=float32)

time = 6562	action = 1	current_phase = 0	next_phase = 1	reward = -1.589754	array([[-2.7855735, -2.5330906]], dtype=float32)

time = 6570	action = 1	current_phase = 1	next_phase = 0	reward = -1.064159	array([[-2.7253866 , -0.92868626]], dtype=float32)

time = 6578	action = 0	current_phase = 0	next_phase = 1	reward = -0.044132	array([[-0.23123032, -2.571806  ]], dtype=float32)

time = 6583	action = 0	current_phase = 0	next_phase = 1	reward = 0.033257	array([[ 0.08663857, -3.271442  ]], dtype=float32)

time = 6588	action = 0	current_phase = 0	next_phase = 1	reward = 0.073800	array([[-1.0096328, -2.8213315]], dtype=float32)

time = 6593	action = 1	current_phase = 0	next_phase = 1	reward = -1.411564	array([[-2.8198433, -2.7506452]], dtype=float32)

time = 6601	action = 1	current_phase = 1	next_phase = 0	reward = -1.399268	array([[-2.2888412, -1.2623732]], dtype=float32)

time = 6609	action = 0	current_phase = 0	next_phase = 1	reward = 0.244511	array([[-0.17527002, -2.54536   ]], dtype=float32)

time = 6614	action = 0	current_phase = 0	next_phase = 1	reward = 0.032729	array([[-0.5642797, -2.8062234]], dtype=float32)

time = 6619	action = 0	current_phase = 0	next_phase = 1	reward = 0.077128	array([[-1.6019351, -2.3533978]], dtype=float32)

time = 6624	action = 1	current_phase = 0	next_phase = 1	reward = -1.112315	array([[-2.9502072, -2.594174 ]], dtype=float32)

time = 6632	action = 1	current_phase = 1	next_phase = 0	reward = -0.765179	array([[-2.6118724, -1.3029363]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.1508 - val_loss: 0.0604

Epoch 2/50

 - 3s - loss: 0.1705 - val_loss: 0.0624

Epoch 3/50

 - 3s - loss: 0.1455 - val_loss: 0.0724

Epoch 4/50

 - 3s - loss: 0.1477 - val_loss: 0.0665

Epoch 5/50

 - 3s - loss: 0.1373 - val_loss: 0.0649

Epoch 6/50

 - 4s - loss: 0.1621 - val_loss: 0.0736

Epoch 7/50

 - 3s - loss: 0.1506 - val_loss: 0.0701

Epoch 8/50

 - 3s - loss: 0.1512 - val_loss: 0.0766

Epoch 9/50

 - 4s - loss: 0.1250 - val_loss: 0.0746

Epoch 10/50

 - 4s - loss: 0.1294 - val_loss: 0.0699

Epoch 11/50

 - 4s - loss: 0.1379 - val_loss: 0.0783

length of memory (state 0, action 0): 1027, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 458, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 446, after forget

time = 6640	action = 0	current_phase = 0	next_phase = 1	reward = -0.017093	array([[-0.74904513, -2.685092  ]], dtype=float32)

time = 6645	action = 0	current_phase = 0	next_phase = 1	reward = 0.051759	array([[-1.5229001, -2.8965487]], dtype=float32)

time = 6650	action = 0	current_phase = 0	next_phase = 1	reward = 0.027181	array([[-2.3348024, -2.658175 ]], dtype=float32)

time = 6655	action = 1	current_phase = 0	next_phase = 1	reward = -1.976313	array([[-3.1355963, -2.7102408]], dtype=float32)

time = 6663	action = 1	current_phase = 1	next_phase = 0	reward = -0.752084	array([[-2.5524273, -1.254734 ]], dtype=float32)

time = 6671	action = 0	current_phase = 0	next_phase = 1	reward = -0.029456	array([[-0.6851311, -2.7236624]], dtype=float32)

time = 6676	action = 0	current_phase = 0	next_phase = 1	reward = 0.042509	array([[-1.4009695, -2.8538194]], dtype=float32)

time = 6681	action = 1	current_phase = 0	next_phase = 1	reward = -1.489858	array([[-2.4515185, -2.4146278]], dtype=float32)

time = 6689	action = 1	current_phase = 1	next_phase = 0	reward = -0.817764	array([[-2.8765435 , -0.93212974]], dtype=float32)

time = 6697	action = 0	current_phase = 0	next_phase = 1	reward = -0.066579	array([[-0.26903683, -2.9052873 ]], dtype=float32)

time = 6702	action = 0	current_phase = 0	next_phase = 1	reward = 0.007480	array([[-0.4601865, -2.667004 ]], dtype=float32)

time = 6707	action = 0	current_phase = 0	next_phase = 1	reward = 0.061185	array([[-1.7139182, -2.8569808]], dtype=float32)

time = 6712	action = 1	current_phase = 0	next_phase = 1	reward = -1.713172	array([[-2.8004127, -2.2889476]], dtype=float32)

time = 6720	action = 1	current_phase = 1	next_phase = 0	reward = -1.237804	array([[-2.491588 , -1.1489064]], dtype=float32)

time = 6728	action = 0	current_phase = 0	next_phase = 1	reward = 0.249227	array([[-0.33488208, -2.5675077 ]], dtype=float32)

time = 6733	action = 0	current_phase = 0	next_phase = 1	reward = 0.032411	array([[-0.21370405, -3.2919369 ]], dtype=float32)

time = 6738	action = 0	current_phase = 0	next_phase = 1	reward = 0.080855	array([[-1.6523676, -2.7611518]], dtype=float32)

time = 6743	action = 0	current_phase = 0	next_phase = 1	reward = -0.430722	array([[-2.4089215, -2.4390812]], dtype=float32)

time = 6748	action = 1	current_phase = 0	next_phase = 1	reward = -1.623448	array([[-4.0205073, -3.1406922]], dtype=float32)

time = 6756	action = 1	current_phase = 1	next_phase = 0	reward = -1.119943	array([[-2.7946496, -1.2332865]], dtype=float32)

time = 6764	action = 0	current_phase = 0	next_phase = 1	reward = 0.060927	array([[-0.67553747, -2.7303414 ]], dtype=float32)

time = 6769	action = 0	current_phase = 0	next_phase = 1	reward = 0.055986	array([[-2.325163 , -2.3400545]], dtype=float32)

time = 6774	action = 1	current_phase = 0	next_phase = 1	reward = -1.135225	array([[-2.9755108, -2.3761725]], dtype=float32)

time = 6782	action = 1	current_phase = 1	next_phase = 0	reward = -1.024302	array([[-2.5256436, -1.6296372]], dtype=float32)

time = 6790	action = 0	current_phase = 0	next_phase = 1	reward = 0.273774	array([[-0.6756692, -2.5699592]], dtype=float32)

time = 6795	action = 0	current_phase = 0	next_phase = 1	reward = 0.061546	array([[-1.232594 , -2.8332396]], dtype=float32)

time = 6800	action = 1	current_phase = 0	next_phase = 1	reward = -1.322820	array([[-2.8500228, -2.6558127]], dtype=float32)

time = 6808	action = 1	current_phase = 1	next_phase = 0	reward = -0.706204	array([[-2.8980803, -0.9814223]], dtype=float32)

time = 6816	action = 0	current_phase = 0	next_phase = 1	reward = -0.083662	array([[-0.58542055, -2.7916236 ]], dtype=float32)

time = 6821	action = 0	current_phase = 0	next_phase = 1	reward = 0.001879	array([[-0.6628294, -2.6697073]], dtype=float32)

time = 6826	action = 0	current_phase = 0	next_phase = 1	reward = 0.066398	array([[-1.5019054, -2.8664346]], dtype=float32)

time = 6831	action = 1	current_phase = 0	next_phase = 1	reward = -1.373764	array([[-2.3007221, -2.2027578]], dtype=float32)

time = 6839	action = 1	current_phase = 1	next_phase = 0	reward = -0.828598	array([[-2.474746 , -1.0463316]], dtype=float32)

time = 6847	action = 0	current_phase = 0	next_phase = 1	reward = -0.067549	array([[-0.4510073, -2.760387 ]], dtype=float32)

time = 6852	action = 0	current_phase = 0	next_phase = 1	reward = 0.017857	array([[-0.03799886, -3.053959  ]], dtype=float32)

time = 6857	action = 0	current_phase = 0	next_phase = 1	reward = 0.069973	array([[-1.6172693, -2.8863196]], dtype=float32)

time = 6862	action = 1	current_phase = 0	next_phase = 1	reward = -1.616458	array([[-2.8879735, -2.3955832]], dtype=float32)

time = 6870	action = 1	current_phase = 1	next_phase = 0	reward = -1.020724	array([[-2.594851 , -1.2586293]], dtype=float32)

time = 6878	action = 0	current_phase = 0	next_phase = 1	reward = -0.064177	array([[-0.12918502, -2.673562  ]], dtype=float32)

time = 6883	action = 0	current_phase = 0	next_phase = 1	reward = 0.013013	array([[ 0.09233671, -3.337813  ]], dtype=float32)

time = 6888	action = 0	current_phase = 0	next_phase = 1	reward = 0.067536	array([[-0.948454 , -2.6365147]], dtype=float32)

time = 6893	action = 0	current_phase = 0	next_phase = 1	reward = -0.341678	array([[-2.369109, -2.438023]], dtype=float32)

time = 6898	action = 1	current_phase = 0	next_phase = 1	reward = -2.646717	array([[-4.349933 , -3.1405177]], dtype=float32)

time = 6906	action = 1	current_phase = 1	next_phase = 0	reward = -0.965515	array([[-2.9174144, -1.0799584]], dtype=float32)

time = 6914	action = 0	current_phase = 0	next_phase = 1	reward = 0.042249	array([[-0.59128386, -2.639318  ]], dtype=float32)

time = 6919	action = 0	current_phase = 0	next_phase = 1	reward = 0.062802	array([[-1.7830448, -2.4259028]], dtype=float32)

time = 6924	action = 1	current_phase = 0	next_phase = 1	reward = -1.084141	array([[-2.7977307, -2.3021965]], dtype=float32)

time = 6932	action = 1	current_phase = 1	next_phase = 0	reward = -1.015567	array([[-2.3620987, -1.6041694]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.1087 - val_loss: 0.1290

Epoch 2/50

 - 3s - loss: 0.1203 - val_loss: 0.1155

Epoch 3/50

 - 3s - loss: 0.1085 - val_loss: 0.1238

Epoch 4/50

 - 4s - loss: 0.1086 - val_loss: 0.1324

Epoch 5/50

 - 3s - loss: 0.1126 - val_loss: 0.1261

Epoch 6/50

 - 3s - loss: 0.1128 - val_loss: 0.1245

Epoch 7/50

 - 4s - loss: 0.1102 - val_loss: 0.1376

Epoch 8/50

 - 4s - loss: 0.1005 - val_loss: 0.1288

Epoch 9/50

 - 4s - loss: 0.0939 - val_loss: 0.1393

Epoch 10/50

 - 4s - loss: 0.1031 - val_loss: 0.1433

Epoch 11/50

 - 4s - loss: 0.0916 - val_loss: 0.1304

Epoch 12/50

 - 4s - loss: 0.0923 - val_loss: 0.1266

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 468, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 456, after forget

time = 6940	action = 0	current_phase = 0	next_phase = 1	reward = 0.259381	array([[-0.30797774, -2.472662  ]], dtype=float32)

time = 6945	action = 0	current_phase = 0	next_phase = 1	reward = 0.039421	array([[-1.4160905, -2.700224 ]], dtype=float32)

time = 6950	action = 1	current_phase = 0	next_phase = 1	reward = -1.206812	array([[-2.571626 , -2.4675834]], dtype=float32)

time = 6958	action = 1	current_phase = 1	next_phase = 0	reward = -0.700480	array([[-2.7535253, -0.9367227]], dtype=float32)

time = 6966	action = 0	current_phase = 0	next_phase = 1	reward = -0.089668	array([[-0.6090636, -2.6785533]], dtype=float32)

time = 6971	action = 0	current_phase = 0	next_phase = 1	reward = -0.006439	array([[-0.70756483, -2.6145725 ]], dtype=float32)

time = 6976	action = 0	current_phase = 0	next_phase = 1	reward = 0.059647	array([[-1.0794363, -2.7639487]], dtype=float32)

time = 6981	action = 0	current_phase = 0	next_phase = 1	reward = -0.024985	array([[-2.2749627, -2.362381 ]], dtype=float32)

time = 6986	action = 1	current_phase = 0	next_phase = 1	reward = -2.166100	array([[-3.6302452, -2.677769 ]], dtype=float32)

time = 6994	action = 1	current_phase = 1	next_phase = 0	reward = -0.771811	array([[-2.8398223, -1.3558623]], dtype=float32)

time = 7002	action = 0	current_phase = 0	next_phase = 1	reward = 0.001494	array([[-1.0151417, -2.7699199]], dtype=float32)

time = 7007	action = 0	current_phase = 0	next_phase = 1	reward = 0.083688	array([[-1.1611562, -2.8753483]], dtype=float32)

time = 7012	action = 1	current_phase = 0	next_phase = 1	reward = -1.582817	array([[-2.7156441, -2.371941 ]], dtype=float32)

time = 7020	action = 1	current_phase = 1	next_phase = 0	reward = -0.896974	array([[-2.864501 , -0.9806926]], dtype=float32)

time = 7028	action = 0	current_phase = 0	next_phase = 1	reward = -0.062738	array([[-0.22399765, -2.6004183 ]], dtype=float32)

time = 7033	action = 0	current_phase = 0	next_phase = 1	reward = 0.005996	array([[-0.43075752, -2.6060104 ]], dtype=float32)

time = 7038	action = 0	current_phase = 0	next_phase = 1	reward = 0.077448	array([[-0.79256475, -2.5068123 ]], dtype=float32)

time = 7043	action = 1	current_phase = 0	next_phase = 1	reward = -1.900566	array([[-2.8455474, -2.3955946]], dtype=float32)

time = 7051	action = 1	current_phase = 1	next_phase = 0	reward = -0.990887	array([[-2.9301598, -1.5618644]], dtype=float32)

time = 7059	action = 0	current_phase = 0	next_phase = 1	reward = -0.025608	array([[-0.2194575, -2.5341344]], dtype=float32)

time = 7064	action = 0	current_phase = 0	next_phase = 1	reward = 0.049031	array([[-0.8362161, -2.7852411]], dtype=float32)

time = 7069	action = 0	current_phase = 0	next_phase = 1	reward = 0.066269	array([[-1.2679822, -2.3200967]], dtype=float32)

time = 7074	action = 0	current_phase = 0	next_phase = 1	reward = -0.498710	array([[-1.959122 , -2.3535154]], dtype=float32)

time = 7079	action = 1	current_phase = 0	next_phase = 1	reward = -1.682950	array([[-4.016702 , -2.9341614]], dtype=float32)

time = 7087	action = 1	current_phase = 1	next_phase = 0	reward = -1.187068	array([[-2.754546 , -1.3340473]], dtype=float32)

time = 7095	action = 0	current_phase = 0	next_phase = 1	reward = 0.618574	array([[-0.6266736, -2.579175 ]], dtype=float32)

time = 7100	action = 1	current_phase = 0	next_phase = 1	reward = -1.306227	array([[-2.610676 , -2.2095253]], dtype=float32)

time = 7108	action = 1	current_phase = 1	next_phase = 0	reward = -0.715095	array([[-2.5393305, -1.1014746]], dtype=float32)

time = 7116	action = 0	current_phase = 0	next_phase = 1	reward = -0.079459	array([[-0.5276781, -2.7224967]], dtype=float32)

time = 7121	action = 0	current_phase = 0	next_phase = 1	reward = -0.011164	array([[-0.85539377, -2.797928  ]], dtype=float32)

time = 7126	action = 0	current_phase = 0	next_phase = 1	reward = 0.056161	array([[-1.5189986, -2.8130486]], dtype=float32)

time = 7131	action = 0	current_phase = 0	next_phase = 1	reward = -0.056482	array([[-2.0655706, -2.459425 ]], dtype=float32)

time = 7136	action = 1	current_phase = 0	next_phase = 1	reward = -2.263341	array([[-3.4959366, -2.6956804]], dtype=float32)

time = 7144	action = 1	current_phase = 1	next_phase = 0	reward = -0.788122	array([[-2.773022 , -1.2641053]], dtype=float32)

time = 7152	action = 0	current_phase = 0	next_phase = 1	reward = -0.006386	array([[-0.80552256, -2.6344466 ]], dtype=float32)

time = 7157	action = 0	current_phase = 0	next_phase = 1	reward = 0.074458	array([[-1.8582582, -2.7869225]], dtype=float32)

time = 7162	action = 1	current_phase = 0	next_phase = 1	reward = -1.610240	array([[-2.7269645, -2.3163896]], dtype=float32)

time = 7170	action = 1	current_phase = 1	next_phase = 0	reward = -0.996423	array([[-2.8508062 , -0.86056125]], dtype=float32)

time = 7178	action = 0	current_phase = 0	next_phase = 1	reward = -0.042565	array([[-0.3934719, -2.7466807]], dtype=float32)

time = 7183	action = 0	current_phase = 0	next_phase = 1	reward = 0.030855	array([[-0.76233554, -2.7326849 ]], dtype=float32)

time = 7188	action = 0	current_phase = 0	next_phase = 1	reward = 0.076602	array([[-1.5445477, -2.6631668]], dtype=float32)

time = 7193	action = 1	current_phase = 0	next_phase = 1	reward = -1.925576	array([[-2.9566064, -2.4707303]], dtype=float32)

time = 7201	action = 1	current_phase = 1	next_phase = 0	reward = -1.027139	array([[-2.8404305, -1.3389233]], dtype=float32)

time = 7209	action = 0	current_phase = 0	next_phase = 1	reward = -0.021856	array([[-0.16029435, -2.5045586 ]], dtype=float32)

time = 7214	action = 0	current_phase = 0	next_phase = 1	reward = 0.036982	array([[-0.5908015, -2.7286096]], dtype=float32)

time = 7219	action = 0	current_phase = 0	next_phase = 1	reward = 0.070743	array([[-1.6195889, -2.4670005]], dtype=float32)

time = 7224	action = 1	current_phase = 0	next_phase = 1	reward = -1.071505	array([[-2.8441308, -2.3655694]], dtype=float32)

time = 7232	action = 1	current_phase = 1	next_phase = 0	reward = -0.759678	array([[-2.4278   , -1.6833694]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 4s - loss: 0.1199 - val_loss: 0.0573

Epoch 2/50

 - 5s - loss: 0.1101 - val_loss: 0.0504

Epoch 3/50

 - 5s - loss: 0.1087 - val_loss: 0.0607

Epoch 4/50

 - 4s - loss: 0.1108 - val_loss: 0.0540

Epoch 5/50

 - 4s - loss: 0.0969 - val_loss: 0.0550

Epoch 6/50

 - 3s - loss: 0.1060 - val_loss: 0.0658

Epoch 7/50

 - 4s - loss: 0.1005 - val_loss: 0.0625

Epoch 8/50

 - 4s - loss: 0.1025 - val_loss: 0.0540

Epoch 9/50

 - 4s - loss: 0.1053 - val_loss: 0.0685

Epoch 10/50

 - 3s - loss: 0.0938 - val_loss: 0.0571

Epoch 11/50

 - 4s - loss: 0.0979 - val_loss: 0.0593

Epoch 12/50

 - 4s - loss: 0.0877 - val_loss: 0.0566

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 478, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 466, after forget

time = 7240	action = 0	current_phase = 0	next_phase = 1	reward = -0.031953	array([[-0.47159854, -2.5113494 ]], dtype=float32)

time = 7245	action = 0	current_phase = 0	next_phase = 1	reward = 0.041993	array([[-1.1412969, -2.8184414]], dtype=float32)

time = 7250	action = 1	current_phase = 0	next_phase = 1	reward = -1.310384	array([[-2.6826968, -2.4069066]], dtype=float32)

time = 7258	action = 1	current_phase = 1	next_phase = 0	reward = -0.707541	array([[-3.0140269, -0.9870132]], dtype=float32)

time = 7266	action = 0	current_phase = 0	next_phase = 1	reward = -0.075813	array([[-0.479188 , -2.6491637]], dtype=float32)

time = 7271	action = 0	current_phase = 0	next_phase = 1	reward = -0.005392	array([[-0.8736861, -2.804105 ]], dtype=float32)

time = 7276	action = 0	current_phase = 0	next_phase = 1	reward = 0.052733	array([[-1.1183565, -2.8664122]], dtype=float32)

time = 7281	action = 0	current_phase = 0	next_phase = 1	reward = -0.045068	array([[-1.8636317, -2.4791243]], dtype=float32)

time = 7286	action = 1	current_phase = 0	next_phase = 1	reward = -2.256505	array([[-3.5279458, -2.812405 ]], dtype=float32)

time = 7294	action = 1	current_phase = 1	next_phase = 0	reward = -0.790559	array([[-2.8204622, -1.2316728]], dtype=float32)

time = 7302	action = 0	current_phase = 0	next_phase = 1	reward = -0.004844	array([[-0.7328053, -2.6337438]], dtype=float32)

time = 7307	action = 0	current_phase = 0	next_phase = 1	reward = 0.064265	array([[-1.0511682, -2.9565668]], dtype=float32)

time = 7312	action = 0	current_phase = 0	next_phase = 1	reward = -0.128826	array([[-2.30035 , -2.614026]], dtype=float32)

time = 7317	action = 0	current_phase = 0	next_phase = 1	reward = -0.962646	array([[-2.9214838, -2.9973779]], dtype=float32)

time = 7322	action = 1	current_phase = 0	next_phase = 1	reward = -2.484220	array([[-4.0727577, -3.5865798]], dtype=float32)

time = 7330	action = 1	current_phase = 1	next_phase = 0	reward = -0.968330	array([[-2.8949728, -1.4484093]], dtype=float32)

time = 7338	action = 0	current_phase = 0	next_phase = 1	reward = 0.367298	array([[-1.0151738, -2.4628782]], dtype=float32)

time = 7343	action = 0	current_phase = 0	next_phase = 1	reward = -0.355262	array([[-2.2815938, -2.5144942]], dtype=float32)

time = 7348	action = 1	current_phase = 0	next_phase = 1	reward = -2.160026	array([[-3.6812997, -3.3972964]], dtype=float32)

time = 7356	action = 1	current_phase = 1	next_phase = 0	reward = -1.070362	array([[-2.8712082, -1.5321044]], dtype=float32)

time = 7364	action = 0	current_phase = 0	next_phase = 1	reward = 0.040905	array([[-0.81669  , -2.7555296]], dtype=float32)

time = 7369	action = 0	current_phase = 0	next_phase = 1	reward = 0.065292	array([[-1.412529 , -2.0700605]], dtype=float32)

time = 7374	action = 1	current_phase = 0	next_phase = 1	reward = -1.127022	array([[-2.5149448, -2.3123102]], dtype=float32)

time = 7382	action = 1	current_phase = 1	next_phase = 0	reward = -0.752892	array([[-2.3666925, -1.5820816]], dtype=float32)

time = 7390	action = 0	current_phase = 0	next_phase = 1	reward = -0.020654	array([[-0.8534794, -2.8177104]], dtype=float32)

time = 7395	action = 0	current_phase = 0	next_phase = 1	reward = 0.053920	array([[-1.1493337, -2.8020356]], dtype=float32)

time = 7400	action = 0	current_phase = 0	next_phase = 1	reward = 0.009852	array([[-2.3373117, -2.4214375]], dtype=float32)

time = 7405	action = 1	current_phase = 0	next_phase = 1	reward = -2.017691	array([[-2.9226096, -2.749703 ]], dtype=float32)

time = 7413	action = 1	current_phase = 1	next_phase = 0	reward = -0.734758	array([[-2.835174 , -1.0306923]], dtype=float32)

time = 7421	action = 0	current_phase = 0	next_phase = 1	reward = 0.012273	array([[-0.68732464, -2.6372068 ]], dtype=float32)

time = 7426	action = 0	current_phase = 0	next_phase = 1	reward = 0.066868	array([[-1.3469099, -2.7201219]], dtype=float32)

time = 7431	action = 0	current_phase = 0	next_phase = 1	reward = -0.050938	array([[-1.820261, -2.247148]], dtype=float32)

time = 7436	action = 1	current_phase = 0	next_phase = 1	reward = -2.325451	array([[-3.4126856, -3.167048 ]], dtype=float32)

time = 7444	action = 1	current_phase = 1	next_phase = 0	reward = -0.779555	array([[-2.7400677, -1.2469202]], dtype=float32)

time = 7452	action = 0	current_phase = 0	next_phase = 1	reward = 0.025930	array([[-0.5433154, -2.5930424]], dtype=float32)

time = 7457	action = 0	current_phase = 0	next_phase = 1	reward = 0.078173	array([[-1.5796063, -2.7798133]], dtype=float32)

time = 7462	action = 0	current_phase = 0	next_phase = 1	reward = -0.201547	array([[-2.432511 , -2.5027232]], dtype=float32)

time = 7467	action = 1	current_phase = 0	next_phase = 1	reward = -2.614001	array([[-3.721566 , -3.3807566]], dtype=float32)

time = 7475	action = 1	current_phase = 1	next_phase = 0	reward = -1.194780	array([[-2.9303834, -1.2688421]], dtype=float32)

time = 7483	action = 0	current_phase = 0	next_phase = 1	reward = 0.309953	array([[-0.6680238, -2.8044965]], dtype=float32)

time = 7488	action = 0	current_phase = 0	next_phase = 1	reward = 0.076869	array([[-1.6129589, -2.826295 ]], dtype=float32)

time = 7493	action = 0	current_phase = 0	next_phase = 1	reward = -0.332725	array([[-2.265985, -2.998363]], dtype=float32)

time = 7498	action = 1	current_phase = 0	next_phase = 1	reward = -2.158194	array([[-3.824075 , -3.5140634]], dtype=float32)

time = 7506	action = 1	current_phase = 1	next_phase = 0	reward = -1.078091	array([[-2.8665593, -1.4907901]], dtype=float32)

time = 7514	action = 0	current_phase = 0	next_phase = 1	reward = 0.045367	array([[-0.9105735, -2.781463 ]], dtype=float32)

time = 7519	action = 0	current_phase = 0	next_phase = 1	reward = 0.080400	array([[-2.1002257, -2.3548105]], dtype=float32)

time = 7524	action = 1	current_phase = 0	next_phase = 1	reward = -1.081402	array([[-3.1870632, -2.6050625]], dtype=float32)

time = 7532	action = 1	current_phase = 1	next_phase = 0	reward = -0.755360	array([[-2.4483166, -1.5898807]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.1466 - val_loss: 0.0424

Epoch 2/50

 - 3s - loss: 0.1319 - val_loss: 0.0513

Epoch 3/50

 - 3s - loss: 0.1453 - val_loss: 0.0566

Epoch 4/50

 - 3s - loss: 0.1187 - val_loss: 0.0556

Epoch 5/50

 - 3s - loss: 0.1424 - val_loss: 0.0478

Epoch 6/50

 - 3s - loss: 0.1321 - val_loss: 0.0525

Epoch 7/50

 - 3s - loss: 0.1406 - val_loss: 0.0478

Epoch 8/50

 - 3s - loss: 0.1207 - val_loss: 0.0456

Epoch 9/50

 - 3s - loss: 0.1305 - val_loss: 0.0533

Epoch 10/50

 - 3s - loss: 0.1196 - val_loss: 0.0493

Epoch 11/50

 - 3s - loss: 0.1125 - val_loss: 0.0519

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 488, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 476, after forget

time = 7540	action = 0	current_phase = 0	next_phase = 1	reward = -0.022518	array([[-0.6174071, -2.584219 ]], dtype=float32)

time = 7545	action = 0	current_phase = 0	next_phase = 1	reward = -0.228581	array([[-1.6951457, -2.9067264]], dtype=float32)

time = 7550	action = 1	current_phase = 0	next_phase = 1	reward = -1.033391	array([[-2.5793922, -2.4886804]], dtype=float32)

time = 7558	action = 1	current_phase = 1	next_phase = 0	reward = -0.703317	array([[-2.832913 , -1.2502548]], dtype=float32)

time = 7566	action = 0	current_phase = 0	next_phase = 1	reward = -0.067132	array([[-0.6997998, -2.836996 ]], dtype=float32)

time = 7571	action = 0	current_phase = 0	next_phase = 1	reward = -0.002964	array([[-0.7538447, -2.8005152]], dtype=float32)

time = 7576	action = 0	current_phase = 0	next_phase = 1	reward = 0.061639	array([[-1.8999581, -2.9306502]], dtype=float32)

time = 7581	action = 1	current_phase = 0	next_phase = 1	reward = -1.439615	array([[-2.435697 , -2.3152616]], dtype=float32)

time = 7589	action = 1	current_phase = 1	next_phase = 0	reward = -0.851315	array([[-2.482243 , -1.0053291]], dtype=float32)

time = 7597	action = 0	current_phase = 0	next_phase = 1	reward = -0.087459	array([[-0.18935013, -2.825097  ]], dtype=float32)

time = 7602	action = 0	current_phase = 0	next_phase = 1	reward = -0.001457	array([[-0.57104707, -3.0382094 ]], dtype=float32)

time = 7607	action = 0	current_phase = 0	next_phase = 1	reward = 0.072747	array([[-0.93138134, -2.885839  ]], dtype=float32)

time = 7612	action = 1	current_phase = 0	next_phase = 1	reward = -1.542885	array([[-2.5434017, -2.449627 ]], dtype=float32)

time = 7620	action = 1	current_phase = 1	next_phase = 0	reward = -0.995488	array([[-2.544261 , -1.0496643]], dtype=float32)

time = 7628	action = 0	current_phase = 0	next_phase = 1	reward = -0.059005	array([[-0.38968498, -2.5992675 ]], dtype=float32)

time = 7633	action = 0	current_phase = 0	next_phase = 1	reward = 0.017995	array([[-0.638512, -3.14782 ]], dtype=float32)

time = 7638	action = 0	current_phase = 0	next_phase = 1	reward = 0.089175	array([[-1.5316501, -2.7540946]], dtype=float32)

time = 7643	action = 0	current_phase = 0	next_phase = 1	reward = -0.436509	array([[-2.5414102, -2.6670456]], dtype=float32)

time = 7648	action = 1	current_phase = 0	next_phase = 1	reward = -2.156147	array([[-4.0021973, -3.3335962]], dtype=float32)

time = 7656	action = 1	current_phase = 1	next_phase = 0	reward = -1.010269	array([[-2.890326 , -1.2238183]], dtype=float32)

time = 7664	action = 0	current_phase = 0	next_phase = 1	reward = 0.041395	array([[-0.86842537, -2.8679981 ]], dtype=float32)

time = 7669	action = 0	current_phase = 0	next_phase = 1	reward = 0.055304	array([[-2.3221312, -2.439837 ]], dtype=float32)

time = 7674	action = 1	current_phase = 0	next_phase = 1	reward = -1.178472	array([[-2.9312482, -2.6015267]], dtype=float32)

time = 7682	action = 1	current_phase = 1	next_phase = 0	reward = -1.018764	array([[-2.5575552, -1.5737442]], dtype=float32)

time = 7690	action = 0	current_phase = 0	next_phase = 1	reward = 0.267391	array([[-0.5405493, -2.615139 ]], dtype=float32)

time = 7695	action = 0	current_phase = 0	next_phase = 1	reward = 0.050960	array([[-1.6929779, -2.919866 ]], dtype=float32)

time = 7700	action = 1	current_phase = 0	next_phase = 1	reward = -1.324843	array([[-3.026577 , -2.5822334]], dtype=float32)

time = 7708	action = 1	current_phase = 1	next_phase = 0	reward = -0.699299	array([[-2.8630905, -1.1030273]], dtype=float32)

time = 7716	action = 0	current_phase = 0	next_phase = 1	reward = -0.073528	array([[-0.8798706, -2.9294267]], dtype=float32)

time = 7721	action = 0	current_phase = 0	next_phase = 1	reward = 0.007346	array([[-0.49073926, -2.6567655 ]], dtype=float32)

time = 7726	action = 0	current_phase = 0	next_phase = 1	reward = 0.067733	array([[-1.9318848, -2.831933 ]], dtype=float32)

time = 7731	action = 1	current_phase = 0	next_phase = 1	reward = -1.513225	array([[-2.7398114, -2.2903674]], dtype=float32)

time = 7739	action = 1	current_phase = 1	next_phase = 0	reward = -0.793746	array([[-2.4787736, -0.9586771]], dtype=float32)

time = 7747	action = 0	current_phase = 0	next_phase = 1	reward = -0.064732	array([[-0.40789875, -2.7333846 ]], dtype=float32)

time = 7752	action = 0	current_phase = 0	next_phase = 1	reward = 0.004707	array([[-0.8413408, -2.9555779]], dtype=float32)

time = 7757	action = 0	current_phase = 0	next_phase = 1	reward = 0.069604	array([[-1.9455447, -2.8769903]], dtype=float32)

time = 7762	action = 1	current_phase = 0	next_phase = 1	reward = -1.534616	array([[-2.50518  , -2.4907923]], dtype=float32)

time = 7770	action = 1	current_phase = 1	next_phase = 0	reward = -1.286632	array([[-2.7493641, -1.077638 ]], dtype=float32)

time = 7778	action = 0	current_phase = 0	next_phase = 1	reward = 0.250777	array([[-0.22638911, -2.6060395 ]], dtype=float32)

time = 7783	action = 0	current_phase = 0	next_phase = 1	reward = 0.020809	array([[-1.077643 , -3.0514178]], dtype=float32)

time = 7788	action = 0	current_phase = 0	next_phase = 1	reward = 0.071738	array([[-1.5462272, -2.7256327]], dtype=float32)

time = 7793	action = 1	current_phase = 0	next_phase = 1	reward = -1.885313	array([[-2.9068596, -2.67944  ]], dtype=float32)

time = 7801	action = 1	current_phase = 1	next_phase = 0	reward = -1.038217	array([[-2.9669194, -1.7134107]], dtype=float32)

time = 7809	action = 0	current_phase = 0	next_phase = 1	reward = -0.019027	array([[ 0.10808575, -2.3203135 ]], dtype=float32)

time = 7814	action = 0	current_phase = 0	next_phase = 1	reward = 0.065261	array([[-0.8270761, -2.8185859]], dtype=float32)

time = 7819	action = 0	current_phase = 0	next_phase = 1	reward = 0.066956	array([[-1.7535992, -2.292716 ]], dtype=float32)

time = 7824	action = 1	current_phase = 0	next_phase = 1	reward = -1.237718	array([[-3.1238809, -2.558805 ]], dtype=float32)

time = 7832	action = 1	current_phase = 1	next_phase = 0	reward = -0.771298	array([[-2.6267657, -1.5414762]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.1277 - val_loss: 0.0693

Epoch 2/50

 - 3s - loss: 0.1101 - val_loss: 0.0664

Epoch 3/50

 - 3s - loss: 0.1150 - val_loss: 0.0756

Epoch 4/50

 - 3s - loss: 0.1221 - val_loss: 0.0705

Epoch 5/50

 - 4s - loss: 0.1144 - val_loss: 0.0654

Epoch 6/50

 - 4s - loss: 0.1089 - val_loss: 0.0588

Epoch 7/50

 - 4s - loss: 0.1144 - val_loss: 0.0654

Epoch 8/50

 - 4s - loss: 0.0941 - val_loss: 0.0903

Epoch 9/50

 - 4s - loss: 0.1012 - val_loss: 0.0750

Epoch 10/50

 - 5s - loss: 0.1218 - val_loss: 0.0725

Epoch 11/50

 - 4s - loss: 0.1129 - val_loss: 0.0716

Epoch 12/50

 - 4s - loss: 0.0960 - val_loss: 0.0737

Epoch 13/50

 - 4s - loss: 0.0926 - val_loss: 0.0777

Epoch 14/50

 - 3s - loss: 0.1010 - val_loss: 0.0832

Epoch 15/50

 - 3s - loss: 0.1102 - val_loss: 0.0916

Epoch 16/50

 - 4s - loss: 0.0912 - val_loss: 0.0901

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 498, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 486, after forget

time = 7840	action = 0	current_phase = 0	next_phase = 1	reward = -0.028227	array([[-1.0167775, -2.6461987]], dtype=float32)

time = 7845	action = 0	current_phase = 0	next_phase = 1	reward = -0.246133	array([[-1.770782, -2.915527]], dtype=float32)

time = 7850	action = 0	current_phase = 0	next_phase = 1	reward = 0.011678	array([[-2.2872875, -2.4434626]], dtype=float32)

time = 7855	action = 1	current_phase = 0	next_phase = 1	reward = -1.676469	array([[-3.2957284, -2.9669325]], dtype=float32)

time = 7863	action = 1	current_phase = 1	next_phase = 0	reward = -0.711800	array([[-2.6356454, -1.44165  ]], dtype=float32)

time = 7871	action = 0	current_phase = 0	next_phase = 1	reward = -0.002397	array([[-1.2731179, -2.825463 ]], dtype=float32)

time = 7876	action = 0	current_phase = 0	next_phase = 1	reward = 0.065293	array([[-1.5887749, -2.8484948]], dtype=float32)

time = 7881	action = 1	current_phase = 0	next_phase = 1	reward = -1.459293	array([[-3.126798 , -2.4397163]], dtype=float32)

time = 7889	action = 1	current_phase = 1	next_phase = 0	reward = -0.803222	array([[-2.8207128, -1.0866455]], dtype=float32)

time = 7897	action = 0	current_phase = 0	next_phase = 1	reward = -0.074738	array([[-1.0284867, -2.8744705]], dtype=float32)

time = 7902	action = 0	current_phase = 0	next_phase = 1	reward = 0.003844	array([[-0.47900885, -3.1025875 ]], dtype=float32)

time = 7907	action = 0	current_phase = 0	next_phase = 1	reward = 0.064470	array([[-1.7830405, -2.870596 ]], dtype=float32)

time = 7912	action = 1	current_phase = 0	next_phase = 1	reward = -1.586982	array([[-2.638023, -2.473291]], dtype=float32)

time = 7920	action = 1	current_phase = 1	next_phase = 0	reward = -0.952417	array([[-2.6451526 , -0.95915216]], dtype=float32)

time = 7928	action = 0	current_phase = 0	next_phase = 1	reward = -0.064550	array([[-0.92176497, -2.8170578 ]], dtype=float32)

time = 7933	action = 0	current_phase = 0	next_phase = 1	reward = 0.024019	array([[-1.3940604, -2.9362257]], dtype=float32)

time = 7938	action = 0	current_phase = 0	next_phase = 1	reward = 0.073469	array([[-1.975091, -2.975513]], dtype=float32)

time = 7943	action = 1	current_phase = 0	next_phase = 1	reward = -1.314591	array([[-2.814347, -2.729047]], dtype=float32)

time = 7951	action = 1	current_phase = 1	next_phase = 0	reward = -1.551344	array([[-2.6420403, -1.503073 ]], dtype=float32)

time = 7959	action = 0	current_phase = 0	next_phase = 1	reward = 0.252984	array([[-0.81831026, -2.592342  ]], dtype=float32)

time = 7964	action = 0	current_phase = 0	next_phase = 1	reward = 0.032839	array([[-1.0589018, -2.8101635]], dtype=float32)

time = 7969	action = 0	current_phase = 0	next_phase = 1	reward = 0.077190	array([[-2.3296092, -2.460386 ]], dtype=float32)

time = 7974	action = 1	current_phase = 0	next_phase = 1	reward = -1.060620	array([[-3.100414, -2.489051]], dtype=float32)

time = 7982	action = 1	current_phase = 1	next_phase = 0	reward = -0.755304	array([[-2.740722 , -1.4521915]], dtype=float32)

time = 7990	action = 0	current_phase = 0	next_phase = 1	reward = -0.023412	array([[-1.1978385, -2.7802975]], dtype=float32)

time = 7995	action = 0	current_phase = 0	next_phase = 1	reward = 0.046208	array([[-1.7474902, -2.8949945]], dtype=float32)

time = 8000	action = 1	current_phase = 0	next_phase = 1	reward = -1.429371	array([[-3.317692, -2.59712 ]], dtype=float32)

time = 8008	action = 1	current_phase = 1	next_phase = 0	reward = -0.726435	array([[-2.9139986, -1.1802183]], dtype=float32)

time = 8016	action = 0	current_phase = 0	next_phase = 1	reward = -0.080479	array([[-1.0547941, -2.803556 ]], dtype=float32)

time = 8021	action = 0	current_phase = 0	next_phase = 1	reward = -0.004490	array([[-1.228112 , -2.8528137]], dtype=float32)

time = 8026	action = 0	current_phase = 0	next_phase = 1	reward = 0.039914	array([[-1.5279801, -2.8965518]], dtype=float32)

time = 8031	action = 1	current_phase = 0	next_phase = 1	reward = -1.481483	array([[-2.6826422, -2.4610126]], dtype=float32)

time = 8039	action = 1	current_phase = 1	next_phase = 0	reward = -0.771157	array([[-2.526088 , -1.0071259]], dtype=float32)

time = 8047	action = 0	current_phase = 0	next_phase = 1	reward = -0.060480	array([[-0.9895072, -2.8888822]], dtype=float32)

time = 8052	action = 0	current_phase = 0	next_phase = 1	reward = 0.007370	array([[-1.0835474, -2.8808014]], dtype=float32)

time = 8057	action = 0	current_phase = 0	next_phase = 1	reward = 0.057705	array([[-1.873096 , -2.8727305]], dtype=float32)

time = 8062	action = 1	current_phase = 0	next_phase = 1	reward = -1.565890	array([[-2.7340553, -2.524789 ]], dtype=float32)

time = 8070	action = 1	current_phase = 1	next_phase = 0	reward = -0.925237	array([[-2.6829548, -1.1278986]], dtype=float32)

time = 8078	action = 0	current_phase = 0	next_phase = 1	reward = -0.040916	array([[-0.9849086, -2.6777248]], dtype=float32)

time = 8083	action = 0	current_phase = 0	next_phase = 1	reward = 0.017414	array([[-1.3007032, -2.9770455]], dtype=float32)

time = 8088	action = 0	current_phase = 0	next_phase = 1	reward = 0.059982	array([[-1.5664951, -2.8617   ]], dtype=float32)

time = 8093	action = 1	current_phase = 0	next_phase = 1	reward = -1.858622	array([[-2.9110906, -2.4178286]], dtype=float32)

time = 8101	action = 1	current_phase = 1	next_phase = 0	reward = -0.982032	array([[-2.8778398, -1.3260442]], dtype=float32)

time = 8109	action = 0	current_phase = 0	next_phase = 1	reward = -0.037586	array([[-0.52204335, -2.5534873 ]], dtype=float32)

time = 8114	action = 0	current_phase = 0	next_phase = 1	reward = 0.025049	array([[-0.7146687, -2.909209 ]], dtype=float32)

time = 8119	action = 0	current_phase = 0	next_phase = 1	reward = 0.070473	array([[-1.8528404, -2.5826244]], dtype=float32)

time = 8124	action = 1	current_phase = 0	next_phase = 1	reward = -1.013643	array([[-3.010493 , -2.5234733]], dtype=float32)

time = 8132	action = 1	current_phase = 1	next_phase = 0	reward = -0.703103	array([[-2.6555407, -1.6332121]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 4s - loss: 0.1372 - val_loss: 0.0714

Epoch 2/50

 - 4s - loss: 0.1292 - val_loss: 0.0618

Epoch 3/50

 - 4s - loss: 0.1150 - val_loss: 0.0672

Epoch 4/50

 - 5s - loss: 0.1165 - val_loss: 0.0668

Epoch 5/50

 - 5s - loss: 0.1296 - val_loss: 0.0845

Epoch 6/50

 - 4s - loss: 0.1098 - val_loss: 0.0687

Epoch 7/50

 - 5s - loss: 0.1215 - val_loss: 0.0599

Epoch 8/50

 - 4s - loss: 0.1205 - val_loss: 0.0714

Epoch 9/50

 - 4s - loss: 0.1046 - val_loss: 0.0634

Epoch 10/50

 - 4s - loss: 0.1285 - val_loss: 0.0654

Epoch 11/50

 - 4s - loss: 0.0995 - val_loss: 0.0629

Epoch 12/50

 - 5s - loss: 0.1070 - val_loss: 0.0651

Epoch 13/50

 - 5s - loss: 0.0929 - val_loss: 0.0703

Epoch 14/50

 - 5s - loss: 0.0944 - val_loss: 0.0830

Epoch 15/50

 - 5s - loss: 0.0954 - val_loss: 0.0750

Epoch 16/50

 - 4s - loss: 0.0946 - val_loss: 0.0786

Epoch 17/50

 - 4s - loss: 0.1067 - val_loss: 0.0820

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 508, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 496, after forget

time = 8140	action = 0	current_phase = 0	next_phase = 1	reward = -0.012173	array([[-1.375421, -2.86235 ]], dtype=float32)

time = 8145	action = 0	current_phase = 0	next_phase = 1	reward = 0.063793	array([[-2.0111063, -2.882757 ]], dtype=float32)

time = 8150	action = 1	current_phase = 0	next_phase = 1	reward = -1.362017	array([[-2.7588673, -2.4849913]], dtype=float32)

time = 8158	action = 1	current_phase = 1	next_phase = 0	reward = -0.697657	array([[-2.7386818, -1.1291715]], dtype=float32)

time = 8166	action = 0	current_phase = 0	next_phase = 1	reward = -0.081465	array([[-1.2241349, -3.0030222]], dtype=float32)

time = 8171	action = 0	current_phase = 0	next_phase = 1	reward = 0.008486	array([[-1.154763 , -2.9187078]], dtype=float32)

time = 8176	action = 0	current_phase = 0	next_phase = 1	reward = 0.063750	array([[-1.4426053, -2.830352 ]], dtype=float32)

time = 8181	action = 1	current_phase = 0	next_phase = 1	reward = -1.513502	array([[-2.7276587, -2.410867 ]], dtype=float32)

time = 8189	action = 1	current_phase = 1	next_phase = 0	reward = -0.786186	array([[-2.6413705, -1.0913374]], dtype=float32)

time = 8197	action = 0	current_phase = 0	next_phase = 1	reward = -0.062013	array([[-1.1158497, -2.934425 ]], dtype=float32)

time = 8202	action = 0	current_phase = 0	next_phase = 1	reward = -0.003320	array([[-1.3276236, -2.9745681]], dtype=float32)

time = 8207	action = 0	current_phase = 0	next_phase = 1	reward = 0.060962	array([[-1.5925539, -2.7797809]], dtype=float32)

time = 8212	action = 1	current_phase = 0	next_phase = 1	reward = -1.650533	array([[-2.8518515, -2.5123463]], dtype=float32)

time = 8220	action = 1	current_phase = 1	next_phase = 0	reward = -1.006881	array([[-2.7013516, -1.0182426]], dtype=float32)

time = 8228	action = 0	current_phase = 0	next_phase = 1	reward = -0.042603	array([[-0.90657604, -2.7828853 ]], dtype=float32)

time = 8233	action = 0	current_phase = 0	next_phase = 1	reward = 0.027280	array([[-1.2956576, -3.028354 ]], dtype=float32)

time = 8238	action = 0	current_phase = 0	next_phase = 1	reward = 0.080199	array([[-2.1840405, -3.0408463]], dtype=float32)

time = 8243	action = 1	current_phase = 0	next_phase = 1	reward = -1.814947	array([[-2.9068923, -2.6261525]], dtype=float32)

time = 8251	action = 1	current_phase = 1	next_phase = 0	reward = -1.040242	array([[-2.8389373, -1.6015825]], dtype=float32)

time = 8259	action = 0	current_phase = 0	next_phase = 1	reward = -0.037510	array([[-0.6101991, -2.6811926]], dtype=float32)

time = 8264	action = 0	current_phase = 0	next_phase = 1	reward = 0.026837	array([[-1.3790693, -2.8717954]], dtype=float32)

time = 8269	action = 0	current_phase = 0	next_phase = 1	reward = 0.059055	array([[-1.5316178, -2.2382493]], dtype=float32)

time = 8274	action = 1	current_phase = 0	next_phase = 1	reward = -1.035514	array([[-3.0898535, -2.446573 ]], dtype=float32)

time = 8282	action = 1	current_phase = 1	next_phase = 0	reward = -1.014122	array([[-2.3083837, -1.5075701]], dtype=float32)

time = 8290	action = 0	current_phase = 0	next_phase = 1	reward = 0.259968	array([[-0.80201614, -2.6690102 ]], dtype=float32)

time = 8295	action = 0	current_phase = 0	next_phase = 1	reward = 0.040148	array([[-1.6795132, -2.944373 ]], dtype=float32)

time = 8300	action = 1	current_phase = 0	next_phase = 1	reward = -1.331099	array([[-2.9930367, -2.4217875]], dtype=float32)

time = 8308	action = 1	current_phase = 1	next_phase = 0	reward = -0.984091	array([[-2.9062614, -1.1140809]], dtype=float32)

time = 8316	action = 0	current_phase = 0	next_phase = 1	reward = 0.214863	array([[-1.2381413, -2.882542 ]], dtype=float32)

time = 8321	action = 0	current_phase = 0	next_phase = 1	reward = -0.011872	array([[-1.5201414, -2.9244673]], dtype=float32)

time = 8326	action = 0	current_phase = 0	next_phase = 1	reward = 0.046359	array([[-1.8641165, -2.92567  ]], dtype=float32)

time = 8331	action = 0	current_phase = 0	next_phase = 1	reward = -0.036285	array([[-2.3069575, -2.4631393]], dtype=float32)

time = 8336	action = 1	current_phase = 0	next_phase = 1	reward = -2.281646	array([[-3.9722738, -2.7918096]], dtype=float32)

time = 8344	action = 1	current_phase = 1	next_phase = 0	reward = -0.813610	array([[-2.7772462, -1.4383037]], dtype=float32)

time = 8352	action = 0	current_phase = 0	next_phase = 1	reward = -0.010699	array([[-1.634614 , -2.9053319]], dtype=float32)

time = 8357	action = 0	current_phase = 0	next_phase = 1	reward = 0.062240	array([[-2.0324554, -2.9289567]], dtype=float32)

time = 8362	action = 1	current_phase = 0	next_phase = 1	reward = -1.450432	array([[-2.9963715, -2.4866257]], dtype=float32)

time = 8370	action = 1	current_phase = 1	next_phase = 0	reward = -0.940449	array([[-2.7386851, -1.2559319]], dtype=float32)

time = 8378	action = 0	current_phase = 0	next_phase = 1	reward = -0.057813	array([[-0.7662217, -2.7282245]], dtype=float32)

time = 8383	action = 0	current_phase = 0	next_phase = 1	reward = 0.023641	array([[-1.1073023, -2.89553  ]], dtype=float32)

time = 8388	action = 0	current_phase = 0	next_phase = 1	reward = 0.070265	array([[-2.1500556, -2.9067106]], dtype=float32)

time = 8393	action = 1	current_phase = 0	next_phase = 1	reward = -1.274087	array([[-3.0770807, -2.5911038]], dtype=float32)

time = 8401	action = 1	current_phase = 1	next_phase = 0	reward = -1.436272	array([[-2.5887961, -1.4929417]], dtype=float32)

time = 8409	action = 0	current_phase = 0	next_phase = 1	reward = 0.249353	array([[-1.4685812, -2.8080475]], dtype=float32)

time = 8414	action = 0	current_phase = 0	next_phase = 1	reward = 0.022257	array([[-1.5567884, -2.972617 ]], dtype=float32)

time = 8419	action = 1	current_phase = 0	next_phase = 1	reward = -0.656163	array([[-2.4441242, -2.4271367]], dtype=float32)

time = 8427	action = 1	current_phase = 1	next_phase = 0	reward = -0.582161	array([[-2.413134 , -1.6127332]], dtype=float32)

time = 8435	action = 0	current_phase = 0	next_phase = 1	reward = -0.095616	array([[-1.1749928, -2.9263575]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 4s - loss: 0.1129 - val_loss: 0.0572

Epoch 2/50

 - 4s - loss: 0.1238 - val_loss: 0.0727

Epoch 3/50

 - 4s - loss: 0.1052 - val_loss: 0.0620

Epoch 4/50

 - 4s - loss: 0.1257 - val_loss: 0.0607

Epoch 5/50

 - 5s - loss: 0.1131 - val_loss: 0.0630

Epoch 6/50

 - 4s - loss: 0.0998 - val_loss: 0.0601

Epoch 7/50

 - 4s - loss: 0.1009 - val_loss: 0.0527

Epoch 8/50

 - 4s - loss: 0.1143 - val_loss: 0.0623

Epoch 9/50

 - 4s - loss: 0.0875 - val_loss: 0.0789

Epoch 10/50

 - 4s - loss: 0.1277 - val_loss: 0.0564

Epoch 11/50

 - 4s - loss: 0.0901 - val_loss: 0.0560

Epoch 12/50

 - 4s - loss: 0.0979 - val_loss: 0.0596

Epoch 13/50

 - 4s - loss: 0.1017 - val_loss: 0.0571

Epoch 14/50

 - 5s - loss: 0.0862 - val_loss: 0.0548

Epoch 15/50

 - 4s - loss: 0.0832 - val_loss: 0.0656

Epoch 16/50

 - 4s - loss: 0.1059 - val_loss: 0.0622

Epoch 17/50

 - 4s - loss: 0.0975 - val_loss: 0.0549

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 518, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 506, after forget

time = 8440	action = 0	current_phase = 0	next_phase = 1	reward = -0.012403	array([[-1.6145109, -2.8794422]], dtype=float32)

time = 8445	action = 0	current_phase = 0	next_phase = 1	reward = 0.057127	array([[-2.7442648, -2.9920743]], dtype=float32)

time = 8450	action = 0	current_phase = 0	next_phase = 1	reward = -0.264564	array([[-2.2657602, -2.3569446]], dtype=float32)

time = 8455	action = 1	current_phase = 0	next_phase = 1	reward = -1.789704	array([[-3.5578084, -2.6671789]], dtype=float32)

time = 8463	action = 1	current_phase = 1	next_phase = 0	reward = -0.744279	array([[-2.8996654, -1.2213533]], dtype=float32)

time = 8471	action = 0	current_phase = 0	next_phase = 1	reward = -0.009086	array([[-1.4033186, -2.9436898]], dtype=float32)

time = 8476	action = 0	current_phase = 0	next_phase = 1	reward = 0.058703	array([[-1.7893237, -2.915017 ]], dtype=float32)

time = 8481	action = 1	current_phase = 0	next_phase = 1	reward = -1.388487	array([[-3.0225651, -2.5477235]], dtype=float32)

time = 8489	action = 1	current_phase = 1	next_phase = 0	reward = -0.839506	array([[-2.8464751, -1.1913073]], dtype=float32)

time = 8497	action = 0	current_phase = 0	next_phase = 1	reward = -0.055358	array([[-1.0001402, -2.8284624]], dtype=float32)

time = 8502	action = 0	current_phase = 0	next_phase = 1	reward = -0.004019	array([[-1.2776345, -2.929542 ]], dtype=float32)

time = 8507	action = 0	current_phase = 0	next_phase = 1	reward = 0.072044	array([[-1.7049626, -2.8281176]], dtype=float32)

time = 8512	action = 1	current_phase = 0	next_phase = 1	reward = -1.657099	array([[-2.8295074, -2.4957888]], dtype=float32)

time = 8520	action = 1	current_phase = 1	next_phase = 0	reward = -0.941349	array([[-2.688874 , -1.0600653]], dtype=float32)

time = 8528	action = 0	current_phase = 0	next_phase = 1	reward = -0.038894	array([[-0.5059744, -2.6304245]], dtype=float32)

time = 8533	action = 0	current_phase = 0	next_phase = 1	reward = 0.017419	array([[-1.6511357, -2.9969761]], dtype=float32)

time = 8538	action = 0	current_phase = 0	next_phase = 1	reward = 0.065042	array([[-2.0735292, -2.8180664]], dtype=float32)

time = 8543	action = 1	current_phase = 0	next_phase = 1	reward = -1.826734	array([[-2.9906614, -2.7342198]], dtype=float32)

time = 8551	action = 1	current_phase = 1	next_phase = 0	reward = -1.332603	array([[-2.7438126, -1.3233198]], dtype=float32)

time = 8559	action = 0	current_phase = 0	next_phase = 1	reward = 0.259946	array([[-0.8662523, -2.721442 ]], dtype=float32)

time = 8564	action = 0	current_phase = 0	next_phase = 1	reward = 0.029843	array([[-1.2195162, -2.8344204]], dtype=float32)

time = 8569	action = 0	current_phase = 0	next_phase = 1	reward = 0.054193	array([[-2.2375793, -2.5058122]], dtype=float32)

time = 8574	action = 1	current_phase = 0	next_phase = 1	reward = -1.139937	array([[-2.7379313, -2.6484513]], dtype=float32)

time = 8582	action = 1	current_phase = 1	next_phase = 0	reward = -1.263474	array([[-2.6952782, -1.3698791]], dtype=float32)

time = 8590	action = 0	current_phase = 0	next_phase = 1	reward = 0.535396	array([[-1.1852473, -2.8138   ]], dtype=float32)

time = 8595	action = 0	current_phase = 0	next_phase = 1	reward = 0.045156	array([[-1.8555707, -2.9765646]], dtype=float32)

time = 8600	action = 1	current_phase = 0	next_phase = 1	reward = -1.364437	array([[-3.0806396, -2.5656762]], dtype=float32)

time = 8608	action = 1	current_phase = 1	next_phase = 0	reward = -0.700315	array([[-3.0791657, -0.9206233]], dtype=float32)

time = 8616	action = 0	current_phase = 0	next_phase = 1	reward = -0.083367	array([[-1.142168 , -2.9409451]], dtype=float32)

time = 8621	action = 0	current_phase = 0	next_phase = 1	reward = -0.005685	array([[-1.3858206, -2.9274204]], dtype=float32)

time = 8626	action = 0	current_phase = 0	next_phase = 1	reward = 0.059796	array([[-1.6302807, -2.8560407]], dtype=float32)

time = 8631	action = 1	current_phase = 0	next_phase = 1	reward = -1.488555	array([[-2.4629774, -2.3729544]], dtype=float32)

time = 8639	action = 1	current_phase = 1	next_phase = 0	reward = -1.088887	array([[-2.623344  , -0.89924073]], dtype=float32)

time = 8647	action = 0	current_phase = 0	next_phase = 1	reward = 0.210994	array([[-0.21901107, -2.8602622 ]], dtype=float32)

time = 8652	action = 0	current_phase = 0	next_phase = 1	reward = 0.000979	array([[-1.1594025, -2.9813955]], dtype=float32)

time = 8657	action = 0	current_phase = 0	next_phase = 1	reward = 0.069987	array([[-1.8534933, -2.8606417]], dtype=float32)

time = 8662	action = 1	current_phase = 0	next_phase = 1	reward = -1.538782	array([[-2.626451 , -2.4207714]], dtype=float32)

time = 8670	action = 1	current_phase = 1	next_phase = 0	reward = -0.949216	array([[-2.5989456, -0.9941268]], dtype=float32)

time = 8678	action = 0	current_phase = 0	next_phase = 1	reward = -0.055787	array([[-0.5767815, -2.6021004]], dtype=float32)

time = 8683	action = 0	current_phase = 0	next_phase = 1	reward = 0.018984	array([[-1.6096396, -2.9940798]], dtype=float32)

time = 8688	action = 0	current_phase = 0	next_phase = 1	reward = 0.075584	array([[-2.060272, -2.858662]], dtype=float32)

time = 8693	action = 1	current_phase = 0	next_phase = 1	reward = -1.906659	array([[-2.9236276, -2.7520616]], dtype=float32)

time = 8701	action = 1	current_phase = 1	next_phase = 0	reward = -1.004402	array([[-2.8201733, -1.4578245]], dtype=float32)

time = 8709	action = 0	current_phase = 0	next_phase = 1	reward = -0.055194	array([[-0.7467296, -2.6372385]], dtype=float32)

time = 8714	action = 0	current_phase = 0	next_phase = 1	reward = 0.001990	array([[-0.841783 , -3.0742786]], dtype=float32)

time = 8719	action = 0	current_phase = 0	next_phase = 1	reward = 0.070324	array([[-2.207277, -2.422379]], dtype=float32)

time = 8724	action = 1	current_phase = 0	next_phase = 1	reward = -1.010007	array([[-2.7307515, -2.4792404]], dtype=float32)

time = 8732	action = 1	current_phase = 1	next_phase = 0	reward = -0.754019	array([[-2.3984914, -1.3539156]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 4s - loss: 0.1301 - val_loss: 0.0552

Epoch 2/50

 - 4s - loss: 0.1265 - val_loss: 0.0688

Epoch 3/50

 - 4s - loss: 0.1115 - val_loss: 0.0573

Epoch 4/50

 - 4s - loss: 0.0909 - val_loss: 0.0585

Epoch 5/50

 - 4s - loss: 0.1063 - val_loss: 0.0644

Epoch 6/50

 - 4s - loss: 0.0923 - val_loss: 0.0592

Epoch 7/50

 - 4s - loss: 0.1203 - val_loss: 0.0615

Epoch 8/50

 - 4s - loss: 0.1198 - val_loss: 0.0644

Epoch 9/50

 - 4s - loss: 0.0873 - val_loss: 0.0819

Epoch 10/50

 - 4s - loss: 0.1362 - val_loss: 0.0688

Epoch 11/50

 - 4s - loss: 0.1023 - val_loss: 0.0681

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 528, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 516, after forget

time = 8740	action = 0	current_phase = 0	next_phase = 1	reward = -0.299518	array([[-1.3827502, -2.8891017]], dtype=float32)

time = 8745	action = 0	current_phase = 0	next_phase = 1	reward = 0.340101	array([[-1.7173115, -2.930464 ]], dtype=float32)

time = 8750	action = 1	current_phase = 0	next_phase = 1	reward = -1.412787	array([[-2.7317338, -2.6736705]], dtype=float32)

time = 8758	action = 1	current_phase = 1	next_phase = 0	reward = -0.712890	array([[-2.8624282, -1.0126808]], dtype=float32)

time = 8766	action = 0	current_phase = 0	next_phase = 1	reward = -0.092552	array([[-1.3101887, -2.9987583]], dtype=float32)

time = 8771	action = 0	current_phase = 0	next_phase = 1	reward = -0.010813	array([[-1.3650737, -2.9267807]], dtype=float32)

time = 8776	action = 0	current_phase = 0	next_phase = 1	reward = 0.048188	array([[-1.7778986, -2.9117644]], dtype=float32)

time = 8781	action = 0	current_phase = 0	next_phase = 1	reward = -0.040610	array([[-2.239613 , -2.3485036]], dtype=float32)

time = 8786	action = 1	current_phase = 0	next_phase = 1	reward = -2.200107	array([[-3.6959162, -2.934191 ]], dtype=float32)

time = 8794	action = 1	current_phase = 1	next_phase = 0	reward = -0.727818	array([[-2.8777103, -1.4904135]], dtype=float32)

time = 8802	action = 0	current_phase = 0	next_phase = 1	reward = 0.007271	array([[-1.4709718, -2.9587078]], dtype=float32)

time = 8807	action = 0	current_phase = 0	next_phase = 1	reward = 0.072713	array([[-2.3343358, -2.9662395]], dtype=float32)

time = 8812	action = 1	current_phase = 0	next_phase = 1	reward = -1.668570	array([[-3.1803312, -2.6057615]], dtype=float32)

time = 8820	action = 1	current_phase = 1	next_phase = 0	reward = -0.886196	array([[-2.7141101, -1.4726242]], dtype=float32)

time = 8828	action = 0	current_phase = 0	next_phase = 1	reward = -0.043805	array([[-0.5410019, -2.7023737]], dtype=float32)

time = 8833	action = 0	current_phase = 0	next_phase = 1	reward = 0.027690	array([[-1.4215297, -2.9601264]], dtype=float32)

time = 8838	action = 0	current_phase = 0	next_phase = 1	reward = 0.077555	array([[-2.1065874, -2.896251 ]], dtype=float32)

time = 8843	action = 1	current_phase = 0	next_phase = 1	reward = -1.330009	array([[-2.9577813, -2.5162258]], dtype=float32)

time = 8851	action = 1	current_phase = 1	next_phase = 0	reward = -1.420799	array([[-2.6226187, -1.3994803]], dtype=float32)

time = 8859	action = 0	current_phase = 0	next_phase = 1	reward = 0.271980	array([[-0.8732542, -2.791545 ]], dtype=float32)

time = 8864	action = 0	current_phase = 0	next_phase = 1	reward = 0.060808	array([[-1.4572467, -2.8782194]], dtype=float32)

time = 8869	action = 0	current_phase = 0	next_phase = 1	reward = 0.318715	array([[-2.6485958, -2.6570466]], dtype=float32)

time = 8874	action = 1	current_phase = 0	next_phase = 1	reward = -1.550866	array([[-3.0411215, -2.6720855]], dtype=float32)

time = 8882	action = 1	current_phase = 1	next_phase = 0	reward = -0.739641	array([[-2.8192406, -1.3830708]], dtype=float32)

time = 8890	action = 0	current_phase = 0	next_phase = 1	reward = -0.016951	array([[-1.3768141, -2.8901784]], dtype=float32)

time = 8895	action = 0	current_phase = 0	next_phase = 1	reward = -0.242078	array([[-1.895425 , -2.9413617]], dtype=float32)

time = 8900	action = 1	current_phase = 0	next_phase = 1	reward = -1.099116	array([[-2.9981022, -2.3391385]], dtype=float32)

time = 8908	action = 1	current_phase = 1	next_phase = 0	reward = -0.702799	array([[-2.9060912, -1.048717 ]], dtype=float32)

time = 8916	action = 0	current_phase = 0	next_phase = 1	reward = -0.094163	array([[-1.1036123, -2.986256 ]], dtype=float32)

time = 8921	action = 0	current_phase = 0	next_phase = 1	reward = -0.006199	array([[-1.1289902, -3.0345302]], dtype=float32)

time = 8926	action = 0	current_phase = 0	next_phase = 1	reward = 0.066349	array([[-1.7496588, -2.8984225]], dtype=float32)

time = 8931	action = 1	current_phase = 0	next_phase = 1	reward = -1.381757	array([[-2.5312243, -2.272192 ]], dtype=float32)

time = 8939	action = 1	current_phase = 1	next_phase = 0	reward = -0.827584	array([[-2.5910044, -0.9642757]], dtype=float32)

time = 8947	action = 0	current_phase = 0	next_phase = 1	reward = -0.049297	array([[-0.25099063, -2.9502175 ]], dtype=float32)

time = 8952	action = 0	current_phase = 0	next_phase = 1	reward = 0.008429	array([[-1.2443162, -2.9792519]], dtype=float32)

time = 8957	action = 0	current_phase = 0	next_phase = 1	reward = 0.085999	array([[-1.8367974, -2.758199 ]], dtype=float32)

time = 8962	action = 1	current_phase = 0	next_phase = 1	reward = -1.601427	array([[-2.8197565, -2.4323933]], dtype=float32)

time = 8970	action = 1	current_phase = 1	next_phase = 0	reward = -1.024530	array([[-2.6704512, -1.1355634]], dtype=float32)

time = 8978	action = 0	current_phase = 0	next_phase = 1	reward = -0.072982	array([[-0.50606513, -2.7195342 ]], dtype=float32)

time = 8983	action = 0	current_phase = 0	next_phase = 1	reward = 0.006482	array([[-1.7563556, -2.9824488]], dtype=float32)

time = 8988	action = 0	current_phase = 0	next_phase = 1	reward = 0.071280	array([[-1.7287074, -2.9100194]], dtype=float32)

time = 8993	action = 1	current_phase = 0	next_phase = 1	reward = -1.798175	array([[-2.879696 , -2.5215085]], dtype=float32)

time = 9001	action = 1	current_phase = 1	next_phase = 0	reward = -0.964384	array([[-2.7979064, -1.3695455]], dtype=float32)

time = 9009	action = 0	current_phase = 0	next_phase = 1	reward = -0.024271	array([[-0.67953634, -2.725765  ]], dtype=float32)

time = 9014	action = 0	current_phase = 0	next_phase = 1	reward = 0.034445	array([[-1.1965241, -3.082656 ]], dtype=float32)

time = 9019	action = 0	current_phase = 0	next_phase = 1	reward = 0.064036	array([[-1.8370247, -2.4757607]], dtype=float32)

time = 9024	action = 1	current_phase = 0	next_phase = 1	reward = -1.090854	array([[-3.1357856, -2.5303464]], dtype=float32)

time = 9032	action = 1	current_phase = 1	next_phase = 0	reward = -0.741968	array([[-2.5778441, -1.4193015]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 4s - loss: 0.1161 - val_loss: 0.0433

Epoch 2/50

 - 4s - loss: 0.1001 - val_loss: 0.0494

Epoch 3/50

 - 4s - loss: 0.1047 - val_loss: 0.0481

Epoch 4/50

 - 4s - loss: 0.0897 - val_loss: 0.0516

Epoch 5/50

 - 4s - loss: 0.1072 - val_loss: 0.0450

Epoch 6/50

 - 4s - loss: 0.1003 - val_loss: 0.0504

Epoch 7/50

 - 4s - loss: 0.0989 - val_loss: 0.0500

Epoch 8/50

 - 4s - loss: 0.1047 - val_loss: 0.0514

Epoch 9/50

 - 4s - loss: 0.1051 - val_loss: 0.0522

Epoch 10/50

 - 4s - loss: 0.0853 - val_loss: 0.0535

Epoch 11/50

 - 4s - loss: 0.1135 - val_loss: 0.0668

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 538, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 526, after forget

time = 9040	action = 0	current_phase = 0	next_phase = 1	reward = -0.012733	array([[-0.89181346, -2.7941847 ]], dtype=float32)

time = 9045	action = 0	current_phase = 0	next_phase = 1	reward = -0.218412	array([[-1.699738 , -2.9807425]], dtype=float32)

time = 9050	action = 1	current_phase = 0	next_phase = 1	reward = -1.155658	array([[-2.8770149, -2.450306 ]], dtype=float32)

time = 9058	action = 1	current_phase = 1	next_phase = 0	reward = -0.694911	array([[-2.7023005, -1.1912698]], dtype=float32)

time = 9066	action = 0	current_phase = 0	next_phase = 1	reward = -0.067901	array([[-0.6059325, -2.8972547]], dtype=float32)

time = 9071	action = 0	current_phase = 0	next_phase = 1	reward = 0.000240	array([[-1.473054 , -3.0013232]], dtype=float32)

time = 9076	action = 0	current_phase = 0	next_phase = 1	reward = 0.056996	array([[-1.654932 , -2.9602835]], dtype=float32)

time = 9081	action = 1	current_phase = 0	next_phase = 1	reward = -1.503256	array([[-2.558681 , -2.3375604]], dtype=float32)

time = 9089	action = 1	current_phase = 1	next_phase = 0	reward = -0.772088	array([[-2.5483375, -0.9094455]], dtype=float32)

time = 9097	action = 0	current_phase = 0	next_phase = 1	reward = -0.065062	array([[-0.42346096, -2.8172276 ]], dtype=float32)

time = 9102	action = 0	current_phase = 0	next_phase = 1	reward = 0.014840	array([[-0.61878955, -3.1952462 ]], dtype=float32)

time = 9107	action = 0	current_phase = 0	next_phase = 1	reward = 0.073445	array([[-1.821283, -2.976917]], dtype=float32)

time = 9112	action = 1	current_phase = 0	next_phase = 1	reward = -1.699342	array([[-3.0766523, -2.435639 ]], dtype=float32)

time = 9120	action = 1	current_phase = 1	next_phase = 0	reward = -1.359429	array([[-2.690168 , -1.1393014]], dtype=float32)

time = 9128	action = 0	current_phase = 0	next_phase = 1	reward = 0.251452	array([[-0.35187054, -2.8422515 ]], dtype=float32)

time = 9133	action = 0	current_phase = 0	next_phase = 1	reward = 0.030466	array([[-1.3258417, -3.0435932]], dtype=float32)

time = 9138	action = 0	current_phase = 0	next_phase = 1	reward = 0.079361	array([[-2.3200805, -2.5826771]], dtype=float32)

time = 9143	action = 1	current_phase = 0	next_phase = 1	reward = -1.344643	array([[-2.9681988, -2.8711274]], dtype=float32)

time = 9151	action = 1	current_phase = 1	next_phase = 0	reward = -1.493148	array([[-2.4467845, -1.4723037]], dtype=float32)

time = 9159	action = 0	current_phase = 0	next_phase = 1	reward = 0.243853	array([[-0.56210816, -2.668821  ]], dtype=float32)

time = 9164	action = 0	current_phase = 0	next_phase = 1	reward = 0.017271	array([[-1.2834964, -2.863866 ]], dtype=float32)

time = 9169	action = 0	current_phase = 0	next_phase = 1	reward = 0.074841	array([[-2.3511987, -2.4687302]], dtype=float32)

time = 9174	action = 1	current_phase = 0	next_phase = 1	reward = -1.022939	array([[-3.0033453, -2.5910397]], dtype=float32)

time = 9182	action = 1	current_phase = 1	next_phase = 0	reward = -1.031238	array([[-2.5054426, -1.2964984]], dtype=float32)

time = 9190	action = 0	current_phase = 0	next_phase = 1	reward = 0.252850	array([[-0.8177755, -2.7434216]], dtype=float32)

time = 9195	action = 0	current_phase = 0	next_phase = 1	reward = -0.242858	array([[-1.7605317, -2.9902792]], dtype=float32)

time = 9200	action = 1	current_phase = 0	next_phase = 1	reward = -1.034965	array([[-2.3325741, -2.2994971]], dtype=float32)

time = 9208	action = 1	current_phase = 1	next_phase = 0	reward = -0.709829	array([[-2.6296167, -1.3325632]], dtype=float32)

time = 9216	action = 0	current_phase = 0	next_phase = 1	reward = -0.069346	array([[-0.6140584, -2.891999 ]], dtype=float32)

time = 9221	action = 0	current_phase = 0	next_phase = 1	reward = 0.002713	array([[-1.2454965, -2.9412632]], dtype=float32)

time = 9226	action = 0	current_phase = 0	next_phase = 1	reward = 0.066757	array([[-1.8038363, -2.9925199]], dtype=float32)

time = 9231	action = 1	current_phase = 0	next_phase = 1	reward = -1.389050	array([[-2.5999343, -2.4616303]], dtype=float32)

time = 9239	action = 1	current_phase = 1	next_phase = 0	reward = -0.783951	array([[-2.6536374, -0.9215296]], dtype=float32)

time = 9247	action = 0	current_phase = 0	next_phase = 1	reward = -0.064476	array([[-0.47816002, -2.8256416 ]], dtype=float32)

time = 9252	action = 0	current_phase = 0	next_phase = 1	reward = 0.000727	array([[-1.3826038, -3.0034525]], dtype=float32)

time = 9257	action = 0	current_phase = 0	next_phase = 1	reward = 0.066873	array([[-1.7484088, -2.8834434]], dtype=float32)

time = 9262	action = 1	current_phase = 0	next_phase = 1	reward = -1.588561	array([[-3.0618877, -2.4944048]], dtype=float32)

time = 9270	action = 1	current_phase = 1	next_phase = 0	reward = -1.006108	array([[-2.597066 , -0.9737768]], dtype=float32)

time = 9278	action = 0	current_phase = 0	next_phase = 1	reward = -0.051551	array([[-0.5771893, -2.8342826]], dtype=float32)

time = 9283	action = 0	current_phase = 0	next_phase = 1	reward = 0.035950	array([[-1.4589207, -3.065258 ]], dtype=float32)

time = 9288	action = 0	current_phase = 0	next_phase = 1	reward = 0.092753	array([[-2.0305028, -2.8027153]], dtype=float32)

time = 9293	action = 1	current_phase = 0	next_phase = 1	reward = -1.396532	array([[-3.4036326, -2.7069345]], dtype=float32)

time = 9301	action = 1	current_phase = 1	next_phase = 0	reward = -1.199482	array([[-2.7024603, -1.4322381]], dtype=float32)

time = 9309	action = 0	current_phase = 0	next_phase = 1	reward = -0.047428	array([[-0.5711683, -2.5795693]], dtype=float32)

time = 9314	action = 0	current_phase = 0	next_phase = 1	reward = 0.016088	array([[-1.6006767, -2.9932618]], dtype=float32)

time = 9319	action = 0	current_phase = 0	next_phase = 1	reward = 0.043990	array([[-2.1135616, -2.5636334]], dtype=float32)

time = 9324	action = 1	current_phase = 0	next_phase = 1	reward = -1.006591	array([[-3.3540154, -2.5483584]], dtype=float32)

time = 9332	action = 1	current_phase = 1	next_phase = 0	reward = -0.966339	array([[-1.9426568, -1.4130571]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.1165 - val_loss: 0.0720

Epoch 2/50

 - 3s - loss: 0.1341 - val_loss: 0.0551

Epoch 3/50

 - 3s - loss: 0.1097 - val_loss: 0.0709

Epoch 4/50

 - 3s - loss: 0.1142 - val_loss: 0.0678

Epoch 5/50

 - 3s - loss: 0.1010 - val_loss: 0.0643

Epoch 6/50

 - 3s - loss: 0.0951 - val_loss: 0.0637

Epoch 7/50

 - 3s - loss: 0.0921 - val_loss: 0.0792

Epoch 8/50

 - 3s - loss: 0.0887 - val_loss: 0.0722

Epoch 9/50

 - 3s - loss: 0.1066 - val_loss: 0.0649

Epoch 10/50

 - 3s - loss: 0.0962 - val_loss: 0.0645

Epoch 11/50

 - 3s - loss: 0.0909 - val_loss: 0.0675

Epoch 12/50

 - 3s - loss: 0.0920 - val_loss: 0.0772

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 548, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 536, after forget

time = 9340	action = 0	current_phase = 0	next_phase = 1	reward = 0.257029	array([[-1.4025787, -2.9167223]], dtype=float32)

time = 9345	action = 0	current_phase = 0	next_phase = 1	reward = -0.234912	array([[-1.8267362, -2.8474708]], dtype=float32)

time = 9350	action = 1	current_phase = 0	next_phase = 1	reward = -1.037104	array([[-3.1371427, -2.47298  ]], dtype=float32)

time = 9358	action = 1	current_phase = 1	next_phase = 0	reward = -0.707394	array([[-2.9726017, -1.1743109]], dtype=float32)

time = 9366	action = 0	current_phase = 0	next_phase = 1	reward = -0.090730	array([[-1.4935951, -3.0689163]], dtype=float32)

time = 9371	action = 0	current_phase = 0	next_phase = 1	reward = -0.008597	array([[-1.5942394, -2.955163 ]], dtype=float32)

time = 9376	action = 0	current_phase = 0	next_phase = 1	reward = 0.059018	array([[-1.8555604, -2.939564 ]], dtype=float32)

time = 9381	action = 1	current_phase = 0	next_phase = 1	reward = -1.456104	array([[-2.792423, -2.390071]], dtype=float32)

time = 9389	action = 1	current_phase = 1	next_phase = 0	reward = -0.774668	array([[-2.5679932, -1.0560226]], dtype=float32)

time = 9397	action = 0	current_phase = 0	next_phase = 1	reward = -0.081442	array([[-1.1361165, -3.022757 ]], dtype=float32)

time = 9402	action = 0	current_phase = 0	next_phase = 1	reward = -0.011780	array([[-1.5021772, -2.9460027]], dtype=float32)

time = 9407	action = 0	current_phase = 0	next_phase = 1	reward = 0.049451	array([[-1.9933479, -2.9898603]], dtype=float32)

time = 9412	action = 1	current_phase = 0	next_phase = 1	reward = -1.537136	array([[-3.1290665, -2.422353 ]], dtype=float32)

time = 9420	action = 1	current_phase = 1	next_phase = 0	reward = -0.821617	array([[-2.7743564, -1.3324138]], dtype=float32)

time = 9428	action = 0	current_phase = 0	next_phase = 1	reward = -0.033789	array([[-0.9311327, -2.8949404]], dtype=float32)

time = 9433	action = 0	current_phase = 0	next_phase = 1	reward = 0.040130	array([[-1.4207568, -3.1126604]], dtype=float32)

time = 9438	action = 0	current_phase = 0	next_phase = 1	reward = 0.074034	array([[-2.3133702, -2.8132653]], dtype=float32)

time = 9443	action = 1	current_phase = 0	next_phase = 1	reward = -2.050418	array([[-3.4590695, -2.5514643]], dtype=float32)

time = 9451	action = 1	current_phase = 1	next_phase = 0	reward = -1.088643	array([[-2.8825822, -1.6558505]], dtype=float32)

time = 9459	action = 0	current_phase = 0	next_phase = 1	reward = -0.035912	array([[-0.81180465, -2.6113002 ]], dtype=float32)

time = 9464	action = 0	current_phase = 0	next_phase = 1	reward = 0.044948	array([[-1.5867734, -2.8839712]], dtype=float32)

time = 9469	action = 0	current_phase = 0	next_phase = 1	reward = 0.062219	array([[-2.2581878, -2.3505492]], dtype=float32)

time = 9474	action = 1	current_phase = 0	next_phase = 1	reward = -1.062104	array([[-3.4578443, -2.5322976]], dtype=float32)

time = 9482	action = 1	current_phase = 1	next_phase = 0	reward = -0.758829	array([[-2.2883472, -1.5925597]], dtype=float32)

time = 9490	action = 0	current_phase = 0	next_phase = 1	reward = -0.022598	array([[-1.0983179, -2.7385676]], dtype=float32)

time = 9495	action = 0	current_phase = 0	next_phase = 1	reward = 0.042393	array([[-1.9815345, -3.0166016]], dtype=float32)

time = 9500	action = 1	current_phase = 0	next_phase = 1	reward = -1.381432	array([[-3.3439531, -2.386141 ]], dtype=float32)

time = 9508	action = 1	current_phase = 1	next_phase = 0	reward = -0.708078	array([[-2.9351416, -1.4532534]], dtype=float32)

time = 9516	action = 0	current_phase = 0	next_phase = 1	reward = -0.095058	array([[-1.4074478, -3.0384727]], dtype=float32)

time = 9521	action = 0	current_phase = 0	next_phase = 1	reward = -0.023425	array([[-1.328434 , -3.0503085]], dtype=float32)

time = 9526	action = 0	current_phase = 0	next_phase = 1	reward = 0.062714	array([[-2.0530493, -2.8788335]], dtype=float32)

time = 9531	action = 1	current_phase = 0	next_phase = 1	reward = -1.426337	array([[-2.9143205, -2.7052665]], dtype=float32)

time = 9539	action = 1	current_phase = 1	next_phase = 0	reward = -0.826947	array([[-2.6904192, -1.1124952]], dtype=float32)

time = 9547	action = 0	current_phase = 0	next_phase = 1	reward = -0.077324	array([[-1.0378133, -2.958917 ]], dtype=float32)

time = 9552	action = 0	current_phase = 0	next_phase = 1	reward = -0.004479	array([[-1.6906631, -2.9882367]], dtype=float32)

time = 9557	action = 0	current_phase = 0	next_phase = 1	reward = 0.065628	array([[-2.1831768, -2.922037 ]], dtype=float32)

time = 9562	action = 1	current_phase = 0	next_phase = 1	reward = -1.589647	array([[-3.3634942, -2.4259074]], dtype=float32)

time = 9570	action = 1	current_phase = 1	next_phase = 0	reward = -1.016167	array([[-2.592474 , -1.3944998]], dtype=float32)

time = 9578	action = 0	current_phase = 0	next_phase = 1	reward = -0.074044	array([[-1.041537 , -2.8548868]], dtype=float32)

time = 9583	action = 0	current_phase = 0	next_phase = 1	reward = 0.005915	array([[-1.5660323, -2.973355 ]], dtype=float32)

time = 9588	action = 0	current_phase = 0	next_phase = 1	reward = 0.079287	array([[-1.7615023, -2.9423196]], dtype=float32)

time = 9593	action = 1	current_phase = 0	next_phase = 1	reward = -1.257402	array([[-3.3993957, -2.5081232]], dtype=float32)

time = 9601	action = 1	current_phase = 1	next_phase = 0	reward = -1.485774	array([[-2.6415896, -1.7140783]], dtype=float32)

time = 9609	action = 0	current_phase = 0	next_phase = 1	reward = 0.270758	array([[-1.0756295, -2.8165345]], dtype=float32)

time = 9614	action = 0	current_phase = 0	next_phase = 1	reward = 0.047124	array([[-1.7962452, -2.9714165]], dtype=float32)

time = 9619	action = 0	current_phase = 0	next_phase = 1	reward = 0.062273	array([[-2.3336825, -2.3781784]], dtype=float32)

time = 9624	action = 1	current_phase = 0	next_phase = 1	reward = -1.128549	array([[-3.3749347, -2.6093287]], dtype=float32)

time = 9632	action = 1	current_phase = 1	next_phase = 0	reward = -1.024763	array([[-2.0485697, -1.6374576]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.1057 - val_loss: 0.0442

Epoch 2/50

 - 3s - loss: 0.1180 - val_loss: 0.0438

Epoch 3/50

 - 3s - loss: 0.1105 - val_loss: 0.0439

Epoch 4/50

 - 3s - loss: 0.1132 - val_loss: 0.0488

Epoch 5/50

 - 3s - loss: 0.1295 - val_loss: 0.0504

Epoch 6/50

 - 3s - loss: 0.1095 - val_loss: 0.0498

Epoch 7/50

 - 3s - loss: 0.0956 - val_loss: 0.0552

Epoch 8/50

 - 3s - loss: 0.1052 - val_loss: 0.0690

Epoch 9/50

 - 3s - loss: 0.0815 - val_loss: 0.0562

Epoch 10/50

 - 3s - loss: 0.1327 - val_loss: 0.0519

Epoch 11/50

 - 3s - loss: 0.0850 - val_loss: 0.0499

Epoch 12/50

 - 3s - loss: 0.0930 - val_loss: 0.0488

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 558, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 546, after forget

time = 9640	action = 0	current_phase = 0	next_phase = 1	reward = 0.263526	array([[-1.4879631, -2.8888364]], dtype=float32)

time = 9645	action = 0	current_phase = 0	next_phase = 1	reward = 0.042644	array([[-1.79004  , -2.9922495]], dtype=float32)

time = 9650	action = 1	current_phase = 0	next_phase = 1	reward = -1.311502	array([[-3.133845 , -2.5261142]], dtype=float32)

time = 9658	action = 1	current_phase = 1	next_phase = 0	reward = -0.695851	array([[-2.851021 , -1.4252918]], dtype=float32)

time = 9666	action = 0	current_phase = 0	next_phase = 1	reward = -0.076056	array([[-1.6386315, -3.0645356]], dtype=float32)

time = 9671	action = 0	current_phase = 0	next_phase = 1	reward = 0.000468	array([[-1.5442562, -2.9624114]], dtype=float32)

time = 9676	action = 0	current_phase = 0	next_phase = 1	reward = 0.061333	array([[-1.9146618, -2.9829278]], dtype=float32)

time = 9681	action = 1	current_phase = 0	next_phase = 1	reward = -1.393038	array([[-2.8945465, -2.330701 ]], dtype=float32)

time = 9689	action = 1	current_phase = 1	next_phase = 0	reward = -0.826684	array([[-2.658204 , -1.1052494]], dtype=float32)

time = 9697	action = 0	current_phase = 0	next_phase = 1	reward = -0.070413	array([[-1.2358841, -3.0243344]], dtype=float32)

time = 9702	action = 0	current_phase = 0	next_phase = 1	reward = 0.015658	array([[-1.5966871, -3.0004287]], dtype=float32)

time = 9707	action = 0	current_phase = 0	next_phase = 1	reward = 0.078149	array([[-1.9118841, -2.745281 ]], dtype=float32)

time = 9712	action = 1	current_phase = 0	next_phase = 1	reward = -1.666502	array([[-3.177627 , -2.5415852]], dtype=float32)

time = 9720	action = 1	current_phase = 1	next_phase = 0	reward = -1.213983	array([[-2.7248664, -1.197649 ]], dtype=float32)

time = 9728	action = 0	current_phase = 0	next_phase = 1	reward = 0.254214	array([[-1.1733768, -2.9081101]], dtype=float32)

time = 9733	action = 0	current_phase = 0	next_phase = 1	reward = 0.039505	array([[-1.5193274, -2.9108825]], dtype=float32)

time = 9738	action = 0	current_phase = 0	next_phase = 1	reward = 0.072422	array([[-2.0998397, -2.883819 ]], dtype=float32)

time = 9743	action = 1	current_phase = 0	next_phase = 1	reward = -1.945636	array([[-3.7268884, -2.6085813]], dtype=float32)

time = 9751	action = 1	current_phase = 1	next_phase = 0	reward = -1.367714	array([[-3.1607294, -1.634728 ]], dtype=float32)

time = 9759	action = 0	current_phase = 0	next_phase = 1	reward = 0.266940	array([[-1.1074109, -2.7493873]], dtype=float32)

time = 9764	action = 0	current_phase = 0	next_phase = 1	reward = 0.028639	array([[-1.6992745, -2.805646 ]], dtype=float32)

time = 9769	action = 0	current_phase = 0	next_phase = 1	reward = 0.049132	array([[-1.9549088, -2.5401309]], dtype=float32)

time = 9774	action = 1	current_phase = 0	next_phase = 1	reward = -1.075026	array([[-3.3523586, -2.5737095]], dtype=float32)

time = 9782	action = 1	current_phase = 1	next_phase = 0	reward = -0.907219	array([[-2.725554, -1.692219]], dtype=float32)

time = 9790	action = 0	current_phase = 0	next_phase = 1	reward = 0.275196	array([[-1.382626 , -2.8595734]], dtype=float32)

time = 9795	action = 0	current_phase = 0	next_phase = 1	reward = 0.050356	array([[-1.9422162, -3.0066562]], dtype=float32)

time = 9800	action = 1	current_phase = 0	next_phase = 1	reward = -1.329730	array([[-2.9810758, -2.41793  ]], dtype=float32)

time = 9808	action = 1	current_phase = 1	next_phase = 0	reward = -1.003257	array([[-2.9170594, -1.4018736]], dtype=float32)

time = 9816	action = 0	current_phase = 0	next_phase = 1	reward = 0.211649	array([[-1.1124084, -2.8240473]], dtype=float32)

time = 9821	action = 0	current_phase = 0	next_phase = 1	reward = -0.015205	array([[-1.3864665, -2.9909782]], dtype=float32)

time = 9826	action = 0	current_phase = 0	next_phase = 1	reward = 0.042817	array([[-1.8255286, -2.9877708]], dtype=float32)

time = 9831	action = 1	current_phase = 0	next_phase = 1	reward = -1.388997	array([[-2.9814875, -2.6294112]], dtype=float32)

time = 9839	action = 1	current_phase = 1	next_phase = 0	reward = -0.763887	array([[-2.682734 , -1.0083928]], dtype=float32)

time = 9847	action = 0	current_phase = 0	next_phase = 1	reward = -0.078269	array([[-1.230616 , -2.9259095]], dtype=float32)

time = 9852	action = 0	current_phase = 0	next_phase = 1	reward = -0.004855	array([[-1.5934885, -2.999736 ]], dtype=float32)

time = 9857	action = 0	current_phase = 0	next_phase = 1	reward = 0.068691	array([[-2.1285453, -2.927502 ]], dtype=float32)

time = 9862	action = 1	current_phase = 0	next_phase = 1	reward = -1.592281	array([[-3.156626 , -2.5365129]], dtype=float32)

time = 9870	action = 1	current_phase = 1	next_phase = 0	reward = -0.933115	array([[-2.8138685, -1.1503669]], dtype=float32)

time = 9878	action = 0	current_phase = 0	next_phase = 1	reward = -0.050362	array([[-1.0877395, -2.8413708]], dtype=float32)

time = 9883	action = 0	current_phase = 0	next_phase = 1	reward = 0.018554	array([[-1.6598791, -3.0628197]], dtype=float32)

time = 9888	action = 0	current_phase = 0	next_phase = 1	reward = 0.068063	array([[-2.1146135, -2.6265616]], dtype=float32)

time = 9893	action = 1	current_phase = 0	next_phase = 1	reward = -1.927501	array([[-3.5833933, -2.8873134]], dtype=float32)

time = 9901	action = 1	current_phase = 1	next_phase = 0	reward = -1.030596	array([[-2.9178424, -1.494622 ]], dtype=float32)

time = 9909	action = 0	current_phase = 0	next_phase = 1	reward = -0.034684	array([[-1.0981089, -2.695209 ]], dtype=float32)

time = 9914	action = 0	current_phase = 0	next_phase = 1	reward = 0.027510	array([[-1.6325902, -2.8979993]], dtype=float32)

time = 9919	action = 0	current_phase = 0	next_phase = 1	reward = 0.067104	array([[-2.1018262, -2.7489352]], dtype=float32)

time = 9924	action = 1	current_phase = 0	next_phase = 1	reward = -1.001137	array([[-3.298494 , -2.8407328]], dtype=float32)

time = 9932	action = 1	current_phase = 1	next_phase = 0	reward = -0.764986	array([[-2.8941474, -1.6979876]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0882 - val_loss: 0.0430

Epoch 2/50

 - 3s - loss: 0.0934 - val_loss: 0.0471

Epoch 3/50

 - 3s - loss: 0.1114 - val_loss: 0.0485

Epoch 4/50

 - 3s - loss: 0.0850 - val_loss: 0.0497

Epoch 5/50

 - 3s - loss: 0.1053 - val_loss: 0.0506

Epoch 6/50

 - 3s - loss: 0.0924 - val_loss: 0.0496

Epoch 7/50

 - 3s - loss: 0.0901 - val_loss: 0.0481

Epoch 8/50

 - 3s - loss: 0.0843 - val_loss: 0.0454

Epoch 9/50

 - 3s - loss: 0.0912 - val_loss: 0.0443

Epoch 10/50

 - 3s - loss: 0.0857 - val_loss: 0.0467

Epoch 11/50

 - 3s - loss: 0.0836 - val_loss: 0.0471

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 568, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 556, after forget

time = 9940	action = 0	current_phase = 0	next_phase = 1	reward = -0.296175	array([[-1.3946351, -2.898903 ]], dtype=float32)

time = 9945	action = 0	current_phase = 0	next_phase = 1	reward = 0.335095	array([[-1.64707 , -2.968529]], dtype=float32)

time = 9950	action = 1	current_phase = 0	next_phase = 1	reward = -1.300404	array([[-3.0535429, -2.4271493]], dtype=float32)

time = 9958	action = 1	current_phase = 1	next_phase = 0	reward = -0.705841	array([[-2.9763799, -1.2550175]], dtype=float32)

time = 9966	action = 0	current_phase = 0	next_phase = 1	reward = -0.074514	array([[-1.3670032, -3.0372412]], dtype=float32)

time = 9971	action = 0	current_phase = 0	next_phase = 1	reward = 0.007158	array([[-1.5958333, -3.010245 ]], dtype=float32)

time = 9976	action = 0	current_phase = 0	next_phase = 1	reward = 0.059717	array([[-1.7573745, -2.992111 ]], dtype=float32)

time = 9981	action = 1	current_phase = 0	next_phase = 1	reward = -1.434592	array([[-2.9926147, -2.260946 ]], dtype=float32)

time = 9989	action = 1	current_phase = 1	next_phase = 0	reward = -0.761375	array([[-2.7856498, -1.2126932]], dtype=float32)

time = 9997	action = 0	current_phase = 0	next_phase = 1	reward = -0.078428	array([[-1.1107864, -2.9168208]], dtype=float32)

time = 10002	action = 0	current_phase = 0	next_phase = 1	reward = 0.006627	array([[-1.2427521, -3.1167166]], dtype=float32)

time = 10007	action = 0	current_phase = 0	next_phase = 1	reward = 0.061513	array([[-1.8743646, -2.9128115]], dtype=float32)

time = 10012	action = 1	current_phase = 0	next_phase = 1	reward = -1.646933	array([[-3.1520255, -2.4769707]], dtype=float32)

time = 10020	action = 1	current_phase = 1	next_phase = 0	reward = -1.298351	array([[-2.760736 , -1.3398461]], dtype=float32)

time = 10028	action = 0	current_phase = 0	next_phase = 1	reward = 0.244336	array([[-0.9977744, -2.87769  ]], dtype=float32)

time = 10033	action = 0	current_phase = 0	next_phase = 1	reward = 0.026844	array([[-1.5604386, -3.0762684]], dtype=float32)

time = 10038	action = 0	current_phase = 0	next_phase = 1	reward = 0.071568	array([[-2.0901628, -3.0290956]], dtype=float32)

time = 10043	action = 1	current_phase = 0	next_phase = 1	reward = -1.871303	array([[-3.5708008, -2.5544796]], dtype=float32)

time = 10051	action = 1	current_phase = 1	next_phase = 0	reward = -1.437235	array([[-2.8289428, -1.8174181]], dtype=float32)

time = 10059	action = 0	current_phase = 0	next_phase = 1	reward = 0.264278	array([[-1.0088909, -2.7011404]], dtype=float32)

time = 10064	action = 0	current_phase = 0	next_phase = 1	reward = 0.029644	array([[-1.5027031, -2.9421191]], dtype=float32)

time = 10069	action = 0	current_phase = 0	next_phase = 1	reward = 0.051692	array([[-2.3087144, -2.3940444]], dtype=float32)

time = 10074	action = 1	current_phase = 0	next_phase = 1	reward = -1.080994	array([[-3.4331164, -2.326599 ]], dtype=float32)

time = 10082	action = 1	current_phase = 1	next_phase = 0	reward = -1.012913	array([[-2.7443657, -1.732494 ]], dtype=float32)

time = 10090	action = 0	current_phase = 0	next_phase = 1	reward = 0.266679	array([[-1.1734742, -2.7646415]], dtype=float32)

time = 10095	action = 0	current_phase = 0	next_phase = 1	reward = -0.232358	array([[-1.7069767, -3.0244057]], dtype=float32)

time = 10100	action = 1	current_phase = 0	next_phase = 1	reward = -1.097115	array([[-3.0690055, -2.264278 ]], dtype=float32)

time = 10108	action = 1	current_phase = 1	next_phase = 0	reward = -0.980474	array([[-2.9879966, -1.2460068]], dtype=float32)

time = 10116	action = 0	current_phase = 0	next_phase = 1	reward = 0.216960	array([[-1.0775757, -2.9035013]], dtype=float32)

time = 10121	action = 0	current_phase = 0	next_phase = 1	reward = -0.004938	array([[-1.516939 , -2.9726586]], dtype=float32)

time = 10126	action = 0	current_phase = 0	next_phase = 1	reward = 0.057102	array([[-1.8848772, -2.9645588]], dtype=float32)

time = 10131	action = 0	current_phase = 0	next_phase = 1	reward = -0.078412	array([[-2.431399 , -2.4587996]], dtype=float32)

time = 10136	action = 1	current_phase = 0	next_phase = 1	reward = -2.364341	array([[-3.8842225, -3.1784887]], dtype=float32)

time = 10144	action = 1	current_phase = 1	next_phase = 0	reward = -0.708775	array([[-2.8345249, -1.7626536]], dtype=float32)

time = 10152	action = 0	current_phase = 0	next_phase = 1	reward = 0.004629	array([[-1.5642171, -2.9709542]], dtype=float32)

time = 10157	action = 0	current_phase = 0	next_phase = 1	reward = 0.063103	array([[-2.0350792, -3.1106117]], dtype=float32)

time = 10162	action = 1	current_phase = 0	next_phase = 1	reward = -1.725872	array([[-3.2534416, -2.57721  ]], dtype=float32)

time = 10170	action = 1	current_phase = 1	next_phase = 0	reward = -0.949257	array([[-2.7988505, -1.2186345]], dtype=float32)

time = 10178	action = 0	current_phase = 0	next_phase = 1	reward = -0.053808	array([[-0.92132294, -3.1925883 ]], dtype=float32)

time = 10183	action = 0	current_phase = 0	next_phase = 1	reward = 0.020391	array([[-1.366794 , -3.0031362]], dtype=float32)

time = 10188	action = 0	current_phase = 0	next_phase = 1	reward = 0.078779	array([[-1.9029486, -2.856164 ]], dtype=float32)

time = 10193	action = 1	current_phase = 0	next_phase = 1	reward = -1.886177	array([[-3.6161385, -2.622417 ]], dtype=float32)

time = 10201	action = 1	current_phase = 1	next_phase = 0	reward = -1.347214	array([[-2.9049904, -1.5795107]], dtype=float32)

time = 10209	action = 0	current_phase = 0	next_phase = 1	reward = 0.263510	array([[-1.0710665, -2.7387495]], dtype=float32)

time = 10214	action = 0	current_phase = 0	next_phase = 1	reward = 0.038331	array([[-1.5393765, -2.9188335]], dtype=float32)

time = 10219	action = 0	current_phase = 0	next_phase = 1	reward = 0.063324	array([[-2.202363 , -2.4961672]], dtype=float32)

time = 10224	action = 1	current_phase = 0	next_phase = 1	reward = -1.019940	array([[-3.4347465, -2.4890366]], dtype=float32)

time = 10232	action = 1	current_phase = 1	next_phase = 0	reward = -0.703799	array([[-2.7590122, -1.8220239]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0989 - val_loss: 0.0359

Epoch 2/50

 - 3s - loss: 0.0831 - val_loss: 0.0408

Epoch 3/50

 - 3s - loss: 0.0890 - val_loss: 0.0406

Epoch 4/50

 - 3s - loss: 0.0852 - val_loss: 0.0398

Epoch 5/50

 - 3s - loss: 0.0850 - val_loss: 0.0394

Epoch 6/50

 - 3s - loss: 0.0789 - val_loss: 0.0492

Epoch 7/50

 - 3s - loss: 0.0766 - val_loss: 0.0539

Epoch 8/50

 - 3s - loss: 0.0803 - val_loss: 0.0393

Epoch 9/50

 - 3s - loss: 0.0754 - val_loss: 0.0425

Epoch 10/50

 - 3s - loss: 0.0757 - val_loss: 0.0568

Epoch 11/50

 - 3s - loss: 0.0732 - val_loss: 0.0403

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 578, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 566, after forget

time = 10240	action = 0	current_phase = 0	next_phase = 1	reward = -0.031770	array([[-1.3123288, -2.8356059]], dtype=float32)

time = 10245	action = 0	current_phase = 0	next_phase = 1	reward = 0.045005	array([[-1.8489611, -3.0456839]], dtype=float32)

time = 10250	action = 1	current_phase = 0	next_phase = 1	reward = -1.295868	array([[-3.0514886, -2.4106708]], dtype=float32)

time = 10258	action = 1	current_phase = 1	next_phase = 0	reward = -0.705049	array([[-2.8076658, -1.479398 ]], dtype=float32)

time = 10266	action = 0	current_phase = 0	next_phase = 1	reward = -0.072529	array([[-1.2234738, -2.9941447]], dtype=float32)

time = 10271	action = 0	current_phase = 0	next_phase = 1	reward = -0.001185	array([[-1.422034 , -2.9583368]], dtype=float32)

time = 10276	action = 0	current_phase = 0	next_phase = 1	reward = 0.063242	array([[-1.9193807, -2.8982143]], dtype=float32)

time = 10281	action = 1	current_phase = 0	next_phase = 1	reward = -1.449927	array([[-3.2556784, -2.3629615]], dtype=float32)

time = 10289	action = 1	current_phase = 1	next_phase = 0	reward = -0.786011	array([[-2.6565413, -1.1971372]], dtype=float32)

time = 10297	action = 0	current_phase = 0	next_phase = 1	reward = -0.065676	array([[-0.68302965, -3.1180432 ]], dtype=float32)

time = 10302	action = 0	current_phase = 0	next_phase = 1	reward = -0.006627	array([[-1.6130826, -3.0120416]], dtype=float32)

time = 10307	action = 0	current_phase = 0	next_phase = 1	reward = 0.068342	array([[-1.8751608, -2.9575539]], dtype=float32)

time = 10312	action = 1	current_phase = 0	next_phase = 1	reward = -1.709364	array([[-3.4038408, -2.3754127]], dtype=float32)

time = 10320	action = 1	current_phase = 1	next_phase = 0	reward = -1.001851	array([[-2.7131844, -1.3414992]], dtype=float32)

time = 10328	action = 0	current_phase = 0	next_phase = 1	reward = -0.070059	array([[-1.1725961, -2.9958258]], dtype=float32)

time = 10333	action = 0	current_phase = 0	next_phase = 1	reward = 0.006633	array([[-1.5890698, -3.0376816]], dtype=float32)

time = 10338	action = 0	current_phase = 0	next_phase = 1	reward = 0.077092	array([[-2.1188583, -2.9170837]], dtype=float32)

time = 10343	action = 1	current_phase = 0	next_phase = 1	reward = -1.305208	array([[-3.4933426, -2.500316 ]], dtype=float32)

time = 10351	action = 1	current_phase = 1	next_phase = 0	reward = -1.335742	array([[-2.7157266, -1.9779875]], dtype=float32)

time = 10359	action = 0	current_phase = 0	next_phase = 1	reward = 0.247912	array([[-1.2762089, -2.8012204]], dtype=float32)

time = 10364	action = 0	current_phase = 0	next_phase = 1	reward = 0.053597	array([[-1.5230272, -2.9825013]], dtype=float32)

time = 10369	action = 0	current_phase = 0	next_phase = 1	reward = 0.051652	array([[-2.0732322, -2.7556074]], dtype=float32)

time = 10374	action = 1	current_phase = 0	next_phase = 1	reward = -1.140327	array([[-3.429517 , -2.5441136]], dtype=float32)

time = 10382	action = 1	current_phase = 1	next_phase = 0	reward = -0.745290	array([[-2.7505717, -1.7123858]], dtype=float32)

time = 10390	action = 0	current_phase = 0	next_phase = 1	reward = -0.019755	array([[-1.383096, -2.892853]], dtype=float32)

time = 10395	action = 0	current_phase = 0	next_phase = 1	reward = 0.047908	array([[-1.8132737, -3.0170116]], dtype=float32)

time = 10400	action = 1	current_phase = 0	next_phase = 1	reward = -1.315256	array([[-2.9760187, -2.430318 ]], dtype=float32)

time = 10408	action = 1	current_phase = 1	next_phase = 0	reward = -0.646862	array([[-2.6962066, -1.4318386]], dtype=float32)

time = 10416	action = 0	current_phase = 0	next_phase = 1	reward = -0.086916	array([[-1.3864496, -3.0047045]], dtype=float32)

time = 10421	action = 0	current_phase = 0	next_phase = 1	reward = -0.022071	array([[-1.5530447, -2.9799676]], dtype=float32)

time = 10426	action = 0	current_phase = 0	next_phase = 1	reward = 0.046837	array([[-1.7642839, -2.8828018]], dtype=float32)

time = 10431	action = 1	current_phase = 0	next_phase = 1	reward = -1.445079	array([[-2.9213512, -2.4712095]], dtype=float32)

time = 10439	action = 1	current_phase = 1	next_phase = 0	reward = -0.761352	array([[-2.6671214, -1.1578226]], dtype=float32)

time = 10447	action = 0	current_phase = 0	next_phase = 1	reward = -0.052654	array([[-1.2442755, -3.0061972]], dtype=float32)

time = 10452	action = 0	current_phase = 0	next_phase = 1	reward = 0.001883	array([[-1.3146527, -3.1231804]], dtype=float32)

time = 10457	action = 0	current_phase = 0	next_phase = 1	reward = 0.078279	array([[-1.7605171, -2.863796 ]], dtype=float32)

time = 10462	action = 1	current_phase = 0	next_phase = 1	reward = -1.611006	array([[-3.5101502, -2.3543806]], dtype=float32)

time = 10470	action = 1	current_phase = 1	next_phase = 0	reward = -1.362562	array([[-2.7475147, -1.4360386]], dtype=float32)

time = 10478	action = 0	current_phase = 0	next_phase = 1	reward = 0.220879	array([[-1.1619642, -3.000745 ]], dtype=float32)

time = 10483	action = 0	current_phase = 0	next_phase = 1	reward = -0.002744	array([[-1.63926  , -3.0881288]], dtype=float32)

time = 10488	action = 0	current_phase = 0	next_phase = 1	reward = 0.073473	array([[-2.0489497, -2.7267597]], dtype=float32)

time = 10493	action = 1	current_phase = 0	next_phase = 1	reward = -1.780770	array([[-3.533508 , -2.3757606]], dtype=float32)

time = 10501	action = 1	current_phase = 1	next_phase = 0	reward = -1.137577	array([[-2.8264754, -1.8217595]], dtype=float32)

time = 10509	action = 0	current_phase = 0	next_phase = 1	reward = -0.052197	array([[-1.1230053, -2.6725519]], dtype=float32)

time = 10514	action = 0	current_phase = 0	next_phase = 1	reward = 0.026807	array([[-1.6308389, -2.9973722]], dtype=float32)

time = 10519	action = 0	current_phase = 0	next_phase = 1	reward = 0.069590	array([[-1.7008166, -2.2093396]], dtype=float32)

time = 10524	action = 1	current_phase = 0	next_phase = 1	reward = -1.023006	array([[-3.723054 , -2.3483982]], dtype=float32)

time = 10532	action = 1	current_phase = 1	next_phase = 0	reward = -0.746661	array([[-2.9010272, -1.7191133]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0981 - val_loss: 0.0401

Epoch 2/50

 - 3s - loss: 0.0908 - val_loss: 0.0412

Epoch 3/50

 - 3s - loss: 0.0929 - val_loss: 0.0350

Epoch 4/50

 - 3s - loss: 0.0930 - val_loss: 0.0477

Epoch 5/50

 - 3s - loss: 0.0847 - val_loss: 0.0532

Epoch 6/50

 - 3s - loss: 0.0822 - val_loss: 0.0402

Epoch 7/50

 - 3s - loss: 0.0685 - val_loss: 0.0455

Epoch 8/50

 - 3s - loss: 0.0789 - val_loss: 0.0411

Epoch 9/50

 - 3s - loss: 0.0869 - val_loss: 0.0408

Epoch 10/50

 - 3s - loss: 0.0759 - val_loss: 0.0489

Epoch 11/50

 - 3s - loss: 0.0764 - val_loss: 0.0596

Epoch 12/50

 - 3s - loss: 0.0751 - val_loss: 0.0438

Epoch 13/50

 - 3s - loss: 0.0715 - val_loss: 0.0556

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 588, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 576, after forget

time = 10540	action = 0	current_phase = 0	next_phase = 1	reward = -0.016201	array([[-1.5566924, -2.8899956]], dtype=float32)

time = 10545	action = 0	current_phase = 0	next_phase = 1	reward = -0.218647	array([[-1.8345762, -2.9872057]], dtype=float32)

time = 10550	action = 1	current_phase = 0	next_phase = 1	reward = -1.142059	array([[-2.6943803, -2.5284956]], dtype=float32)

time = 10558	action = 1	current_phase = 1	next_phase = 0	reward = -0.701012	array([[-2.8592696, -1.35158  ]], dtype=float32)

time = 10566	action = 0	current_phase = 0	next_phase = 1	reward = -0.082989	array([[-1.3933731, -3.055085 ]], dtype=float32)

time = 10571	action = 0	current_phase = 0	next_phase = 1	reward = -0.017833	array([[-1.7582022, -3.001934 ]], dtype=float32)

time = 10576	action = 0	current_phase = 0	next_phase = 1	reward = 0.041042	array([[-2.014326 , -3.0060868]], dtype=float32)

time = 10581	action = 1	current_phase = 0	next_phase = 1	reward = -1.452799	array([[-3.1241488, -2.2661514]], dtype=float32)

time = 10589	action = 1	current_phase = 1	next_phase = 0	reward = -0.775041	array([[-2.7140844, -1.2977301]], dtype=float32)

time = 10597	action = 0	current_phase = 0	next_phase = 1	reward = -0.057965	array([[-1.3310934, -3.0740619]], dtype=float32)

time = 10602	action = 0	current_phase = 0	next_phase = 1	reward = 0.001050	array([[-1.8152604, -2.99638  ]], dtype=float32)

time = 10607	action = 0	current_phase = 0	next_phase = 1	reward = 0.071257	array([[-1.9517355, -2.8935978]], dtype=float32)

time = 10612	action = 1	current_phase = 0	next_phase = 1	reward = -1.599694	array([[-3.6824846, -2.5746949]], dtype=float32)

time = 10620	action = 1	current_phase = 1	next_phase = 0	reward = -0.907881	array([[-2.8334599, -1.6186745]], dtype=float32)

time = 10628	action = 0	current_phase = 0	next_phase = 1	reward = -0.059958	array([[-1.2408681, -2.9763567]], dtype=float32)

time = 10633	action = 0	current_phase = 0	next_phase = 1	reward = 0.015986	array([[-1.7144022, -3.1476698]], dtype=float32)

time = 10638	action = 0	current_phase = 0	next_phase = 1	reward = 0.071781	array([[-2.2274146, -2.9109223]], dtype=float32)

time = 10643	action = 1	current_phase = 0	next_phase = 1	reward = -1.356250	array([[-3.4074178, -2.5083277]], dtype=float32)

time = 10651	action = 1	current_phase = 1	next_phase = 0	reward = -1.435657	array([[-2.700204, -1.761769]], dtype=float32)

time = 10659	action = 0	current_phase = 0	next_phase = 1	reward = 0.250438	array([[-1.4488612, -2.931115 ]], dtype=float32)

time = 10664	action = 0	current_phase = 0	next_phase = 1	reward = 0.037515	array([[-1.7102205, -2.9422781]], dtype=float32)

time = 10669	action = 1	current_phase = 0	next_phase = 1	reward = -0.687785	array([[-2.4734898, -2.314249 ]], dtype=float32)

time = 10677	action = 1	current_phase = 1	next_phase = 0	reward = -0.648266	array([[-2.6577342, -1.9992204]], dtype=float32)

time = 10685	action = 0	current_phase = 0	next_phase = 1	reward = -0.390835	array([[-1.368593 , -3.0176754]], dtype=float32)

time = 10690	action = 0	current_phase = 0	next_phase = 1	reward = -0.021168	array([[-1.699955 , -2.8331685]], dtype=float32)

time = 10695	action = 0	current_phase = 0	next_phase = 1	reward = 0.040071	array([[-2.2199059, -2.9768953]], dtype=float32)

time = 10700	action = 1	current_phase = 0	next_phase = 1	reward = -1.106738	array([[-3.1777897, -2.3503165]], dtype=float32)

time = 10708	action = 1	current_phase = 1	next_phase = 0	reward = -0.710623	array([[-2.8218331, -1.3530284]], dtype=float32)

time = 10716	action = 0	current_phase = 0	next_phase = 1	reward = -0.084110	array([[-1.3957654, -2.9644253]], dtype=float32)

time = 10721	action = 0	current_phase = 0	next_phase = 1	reward = -0.006137	array([[-1.6986041, -2.972055 ]], dtype=float32)

time = 10726	action = 0	current_phase = 0	next_phase = 1	reward = 0.064371	array([[-2.0958562, -2.8955307]], dtype=float32)

time = 10731	action = 1	current_phase = 0	next_phase = 1	reward = -1.498559	array([[-3.2924929, -2.4537687]], dtype=float32)

time = 10739	action = 1	current_phase = 1	next_phase = 0	reward = -0.822466	array([[-2.8334928, -1.3226594]], dtype=float32)

time = 10747	action = 0	current_phase = 0	next_phase = 1	reward = -0.081159	array([[-1.5502748, -3.0165884]], dtype=float32)

time = 10752	action = 0	current_phase = 0	next_phase = 1	reward = -0.004198	array([[-1.5222973, -3.0504599]], dtype=float32)

time = 10757	action = 0	current_phase = 0	next_phase = 1	reward = 0.061872	array([[-2.0505478, -2.8334548]], dtype=float32)

time = 10762	action = 1	current_phase = 0	next_phase = 1	reward = -1.603332	array([[-3.4659686, -2.4363916]], dtype=float32)

time = 10770	action = 1	current_phase = 1	next_phase = 0	reward = -0.889657	array([[-2.786305, -1.47157 ]], dtype=float32)

time = 10778	action = 0	current_phase = 0	next_phase = 1	reward = -0.044424	array([[-1.1681848, -3.086486 ]], dtype=float32)

time = 10783	action = 0	current_phase = 0	next_phase = 1	reward = 0.029080	array([[-1.7556648, -3.0564847]], dtype=float32)

time = 10788	action = 0	current_phase = 0	next_phase = 1	reward = 0.078935	array([[-2.1551256, -2.824946 ]], dtype=float32)

time = 10793	action = 1	current_phase = 0	next_phase = 1	reward = -1.957259	array([[-3.9083185, -2.7106032]], dtype=float32)

time = 10801	action = 1	current_phase = 1	next_phase = 0	reward = -1.382639	array([[-2.8796585, -1.6801692]], dtype=float32)

time = 10809	action = 0	current_phase = 0	next_phase = 1	reward = 0.270846	array([[-1.3989408, -2.8366318]], dtype=float32)

time = 10814	action = 0	current_phase = 0	next_phase = 1	reward = 0.035806	array([[-1.73975  , -3.0489028]], dtype=float32)

time = 10819	action = 1	current_phase = 0	next_phase = 1	reward = -0.645853	array([[-2.3805766, -2.2124758]], dtype=float32)

time = 10827	action = 1	current_phase = 1	next_phase = 0	reward = -1.149941	array([[-2.1735296, -1.4940293]], dtype=float32)

time = 10835	action = 0	current_phase = 0	next_phase = 1	reward = 0.461633	array([[-1.2297277, -3.005643 ]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 5s - loss: 0.1181 - val_loss: 0.0865

Epoch 2/50

 - 4s - loss: 0.1092 - val_loss: 0.0704

Epoch 3/50

 - 4s - loss: 0.1117 - val_loss: 0.0753

Epoch 4/50

 - 4s - loss: 0.0924 - val_loss: 0.0705

Epoch 5/50

 - 4s - loss: 0.0971 - val_loss: 0.0797

Epoch 6/50

 - 4s - loss: 0.1044 - val_loss: 0.0889

Epoch 7/50

 - 4s - loss: 0.1052 - val_loss: 0.0910

Epoch 8/50

 - 3s - loss: 0.0776 - val_loss: 0.0709

Epoch 9/50

 - 3s - loss: 0.0942 - val_loss: 0.0819

Epoch 10/50

 - 3s - loss: 0.0951 - val_loss: 0.0839

Epoch 11/50

 - 3s - loss: 0.0790 - val_loss: 0.0753

Epoch 12/50

 - 3s - loss: 0.0939 - val_loss: 0.0862

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 598, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 586, after forget

time = 10840	action = 0	current_phase = 0	next_phase = 1	reward = -0.297686	array([[-1.6744297, -2.784433 ]], dtype=float32)

time = 10845	action = 0	current_phase = 0	next_phase = 1	reward = 0.052031	array([[-1.8457673, -2.9130673]], dtype=float32)

time = 10850	action = 1	current_phase = 0	next_phase = 1	reward = -1.083402	array([[-2.6134052, -2.3816333]], dtype=float32)

time = 10858	action = 1	current_phase = 1	next_phase = 0	reward = -0.723186	array([[-3.0822353, -1.6394604]], dtype=float32)

time = 10866	action = 0	current_phase = 0	next_phase = 1	reward = -0.090661	array([[-1.5983744, -2.9976234]], dtype=float32)

time = 10871	action = 0	current_phase = 0	next_phase = 1	reward = -0.002922	array([[-1.7382808, -2.9467854]], dtype=float32)

time = 10876	action = 0	current_phase = 0	next_phase = 1	reward = 0.055989	array([[-1.8435271, -2.9408798]], dtype=float32)

time = 10881	action = 1	current_phase = 0	next_phase = 1	reward = -1.489957	array([[-3.3963237, -2.3891182]], dtype=float32)

time = 10889	action = 1	current_phase = 1	next_phase = 0	reward = -0.849683	array([[-3.0629556, -1.8789337]], dtype=float32)

time = 10897	action = 0	current_phase = 0	next_phase = 1	reward = -0.068751	array([[-1.7577817, -3.0681353]], dtype=float32)

time = 10902	action = 0	current_phase = 0	next_phase = 1	reward = 0.001913	array([[-1.7198253, -2.9575863]], dtype=float32)

time = 10907	action = 0	current_phase = 0	next_phase = 1	reward = 0.054423	array([[-2.1586359, -3.0222626]], dtype=float32)

time = 10912	action = 1	current_phase = 0	next_phase = 1	reward = -1.591630	array([[-3.3500152, -2.4967456]], dtype=float32)

time = 10920	action = 1	current_phase = 1	next_phase = 0	reward = -0.951520	array([[-2.9805155, -1.9334528]], dtype=float32)

time = 10928	action = 0	current_phase = 0	next_phase = 1	reward = -0.051462	array([[-1.5009313, -2.9366179]], dtype=float32)

time = 10933	action = 0	current_phase = 0	next_phase = 1	reward = 0.032821	array([[-1.7231997, -3.0430942]], dtype=float32)

time = 10938	action = 0	current_phase = 0	next_phase = 1	reward = 0.081175	array([[-2.1074653, -2.6598806]], dtype=float32)

time = 10943	action = 1	current_phase = 0	next_phase = 1	reward = -1.880726	array([[-3.7283597, -2.802775 ]], dtype=float32)

time = 10951	action = 1	current_phase = 1	next_phase = 0	reward = -1.040605	array([[-3.0352578, -2.2809894]], dtype=float32)

time = 10959	action = 0	current_phase = 0	next_phase = 1	reward = -0.041819	array([[-1.5600119, -2.856032 ]], dtype=float32)

time = 10964	action = 0	current_phase = 0	next_phase = 1	reward = 0.015844	array([[-1.6860723, -2.8404093]], dtype=float32)

time = 10969	action = 0	current_phase = 0	next_phase = 1	reward = 0.067601	array([[-2.201907, -2.298442]], dtype=float32)

time = 10974	action = 1	current_phase = 0	next_phase = 1	reward = -1.140138	array([[-3.7930274, -2.5504942]], dtype=float32)

time = 10982	action = 1	current_phase = 1	next_phase = 0	reward = -1.045176	array([[-3.3661199, -2.6000385]], dtype=float32)

time = 10990	action = 0	current_phase = 0	next_phase = 1	reward = 0.247156	array([[-1.7376533, -2.9141173]], dtype=float32)

time = 10995	action = 0	current_phase = 0	next_phase = 1	reward = -0.247220	array([[-1.8051133, -3.0504546]], dtype=float32)

time = 11000	action = 1	current_phase = 0	next_phase = 1	reward = -1.028916	array([[-2.509978 , -2.3200283]], dtype=float32)

time = 11008	action = 1	current_phase = 1	next_phase = 0	reward = -0.702324	array([[-2.9660926, -1.6843119]], dtype=float32)

time = 11016	action = 0	current_phase = 0	next_phase = 1	reward = -0.077056	array([[-1.7252848, -2.9717565]], dtype=float32)

time = 11021	action = 0	current_phase = 0	next_phase = 1	reward = -0.007901	array([[-1.7352397, -2.9804082]], dtype=float32)

time = 11026	action = 0	current_phase = 0	next_phase = 1	reward = 0.052216	array([[-1.8579717, -2.9767547]], dtype=float32)

time = 11031	action = 1	current_phase = 0	next_phase = 1	reward = -1.402636	array([[-3.430487 , -2.4562693]], dtype=float32)

time = 11039	action = 1	current_phase = 1	next_phase = 0	reward = -0.843419	array([[-2.8950505, -1.8093051]], dtype=float32)

time = 11047	action = 0	current_phase = 0	next_phase = 1	reward = -0.078726	array([[-1.70999  , -3.1078238]], dtype=float32)

time = 11052	action = 0	current_phase = 0	next_phase = 1	reward = -0.000150	array([[-1.6939054, -2.9788432]], dtype=float32)

time = 11057	action = 0	current_phase = 0	next_phase = 1	reward = 0.071573	array([[-1.8783848, -2.8407989]], dtype=float32)

time = 11062	action = 1	current_phase = 0	next_phase = 1	reward = -1.579136	array([[-3.9248075, -2.736679 ]], dtype=float32)

time = 11070	action = 1	current_phase = 1	next_phase = 0	reward = -0.961957	array([[-3.0537248, -2.1669385]], dtype=float32)

time = 11078	action = 0	current_phase = 0	next_phase = 1	reward = -0.047554	array([[-1.5469937, -3.0427089]], dtype=float32)

time = 11083	action = 0	current_phase = 0	next_phase = 1	reward = 0.024250	array([[-1.7250764, -3.0405054]], dtype=float32)

time = 11088	action = 0	current_phase = 0	next_phase = 1	reward = 0.073823	array([[-2.0911243, -2.7451568]], dtype=float32)

time = 11093	action = 1	current_phase = 0	next_phase = 1	reward = -1.264318	array([[-3.801718 , -2.8579254]], dtype=float32)

time = 11101	action = 1	current_phase = 1	next_phase = 0	reward = -1.492072	array([[-2.925075 , -2.3409624]], dtype=float32)

time = 11109	action = 0	current_phase = 0	next_phase = 1	reward = 0.255119	array([[-1.6112082, -2.9359179]], dtype=float32)

time = 11114	action = 0	current_phase = 0	next_phase = 1	reward = 0.050124	array([[-1.6108272, -2.8863025]], dtype=float32)

time = 11119	action = 0	current_phase = 0	next_phase = 1	reward = 0.058661	array([[-2.2332342, -2.4273508]], dtype=float32)

time = 11124	action = 1	current_phase = 0	next_phase = 1	reward = -1.232360	array([[-3.4647439, -2.6429434]], dtype=float32)

time = 11132	action = 1	current_phase = 1	next_phase = 0	reward = -0.748260	array([[-3.265143 , -2.5694537]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0870 - val_loss: 0.0631

Epoch 2/50

 - 3s - loss: 0.0885 - val_loss: 0.0616

Epoch 3/50

 - 3s - loss: 0.0763 - val_loss: 0.0589

Epoch 4/50

 - 4s - loss: 0.0792 - val_loss: 0.0682

Epoch 5/50

 - 3s - loss: 0.0686 - val_loss: 0.0741

Epoch 6/50

 - 3s - loss: 0.0773 - val_loss: 0.0666

Epoch 7/50

 - 3s - loss: 0.0701 - val_loss: 0.0754

Epoch 8/50

 - 3s - loss: 0.0749 - val_loss: 0.0768

Epoch 9/50

 - 3s - loss: 0.0705 - val_loss: 0.0755

Epoch 10/50

 - 3s - loss: 0.0634 - val_loss: 0.0689

Epoch 11/50

 - 3s - loss: 0.0763 - val_loss: 0.0727

Epoch 12/50

 - 3s - loss: 0.0736 - val_loss: 0.0741

Epoch 13/50

 - 3s - loss: 0.0685 - val_loss: 0.0802

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 608, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 596, after forget

time = 11140	action = 0	current_phase = 0	next_phase = 1	reward = -0.290225	array([[-1.6604681, -2.8294775]], dtype=float32)

time = 11145	action = 0	current_phase = 0	next_phase = 1	reward = 0.337235	array([[-1.8131077, -2.9116333]], dtype=float32)

time = 11150	action = 1	current_phase = 0	next_phase = 1	reward = -1.377087	array([[-3.3450847, -2.5550973]], dtype=float32)

time = 11158	action = 1	current_phase = 1	next_phase = 0	reward = -0.712697	array([[-2.869103 , -2.0216846]], dtype=float32)

time = 11166	action = 0	current_phase = 0	next_phase = 1	reward = -0.092970	array([[-1.606758, -3.010524]], dtype=float32)

time = 11171	action = 0	current_phase = 0	next_phase = 1	reward = 0.001379	array([[-1.6695919, -2.9337175]], dtype=float32)

time = 11176	action = 0	current_phase = 0	next_phase = 1	reward = 0.075402	array([[-1.8928959, -2.9042208]], dtype=float32)

time = 11181	action = 1	current_phase = 0	next_phase = 1	reward = -1.388652	array([[-3.2941635, -2.6097865]], dtype=float32)

time = 11189	action = 1	current_phase = 1	next_phase = 0	reward = -1.468671	array([[-3.0321198, -1.7750518]], dtype=float32)

time = 11197	action = 0	current_phase = 0	next_phase = 1	reward = 0.514340	array([[-1.4051808, -2.9951544]], dtype=float32)

time = 11202	action = 0	current_phase = 0	next_phase = 1	reward = 0.013003	array([[-1.7019558, -2.9630282]], dtype=float32)

time = 11207	action = 0	current_phase = 0	next_phase = 1	reward = 0.065331	array([[-1.8913431, -2.8760269]], dtype=float32)

time = 11212	action = 1	current_phase = 0	next_phase = 1	reward = -1.667079	array([[-3.6498053, -2.6153102]], dtype=float32)

time = 11220	action = 1	current_phase = 1	next_phase = 0	reward = -1.001163	array([[-2.9031453, -1.8401275]], dtype=float32)

time = 11228	action = 0	current_phase = 0	next_phase = 1	reward = -0.048026	array([[-1.30796  , -2.8724911]], dtype=float32)

time = 11233	action = 0	current_phase = 0	next_phase = 1	reward = 0.030857	array([[-1.8338108, -3.0342112]], dtype=float32)

time = 11238	action = 0	current_phase = 0	next_phase = 1	reward = 0.069836	array([[-2.1326916, -3.0686133]], dtype=float32)

time = 11243	action = 1	current_phase = 0	next_phase = 1	reward = -1.844989	array([[-4.179431 , -3.1893942]], dtype=float32)

time = 11251	action = 1	current_phase = 1	next_phase = 0	reward = -0.870578	array([[-3.1075125, -2.2039504]], dtype=float32)

time = 11259	action = 0	current_phase = 0	next_phase = 1	reward = -0.040957	array([[-1.1510999, -2.872284 ]], dtype=float32)

time = 11264	action = 0	current_phase = 0	next_phase = 1	reward = 0.022077	array([[-1.710744 , -2.8705537]], dtype=float32)

time = 11269	action = 0	current_phase = 0	next_phase = 1	reward = 0.062517	array([[-2.0361328, -2.399524 ]], dtype=float32)

time = 11274	action = 1	current_phase = 0	next_phase = 1	reward = -0.960824	array([[-3.4295459, -2.63587  ]], dtype=float32)

time = 11282	action = 1	current_phase = 1	next_phase = 0	reward = -0.973543	array([[-3.0109406, -2.294694 ]], dtype=float32)

time = 11290	action = 0	current_phase = 0	next_phase = 1	reward = 0.269592	array([[-1.5332395, -2.7579405]], dtype=float32)

time = 11295	action = 0	current_phase = 0	next_phase = 1	reward = 0.057112	array([[-1.8278365, -2.9165215]], dtype=float32)

time = 11300	action = 1	current_phase = 0	next_phase = 1	reward = -1.383257	array([[-2.8608952, -2.5231035]], dtype=float32)

time = 11308	action = 1	current_phase = 1	next_phase = 0	reward = -0.703216	array([[-2.860684 , -1.8845124]], dtype=float32)

time = 11316	action = 0	current_phase = 0	next_phase = 1	reward = -0.090862	array([[-1.5800968, -2.9864295]], dtype=float32)

time = 11321	action = 0	current_phase = 0	next_phase = 1	reward = 0.013867	array([[-1.7152979, -2.9580653]], dtype=float32)

time = 11326	action = 0	current_phase = 0	next_phase = 1	reward = 0.066681	array([[-1.923212 , -2.7919283]], dtype=float32)

time = 11331	action = 1	current_phase = 0	next_phase = 1	reward = -1.393233	array([[-3.3900952, -2.5942311]], dtype=float32)

time = 11339	action = 1	current_phase = 1	next_phase = 0	reward = -0.836647	array([[-3.0720048, -1.9561839]], dtype=float32)

time = 11347	action = 0	current_phase = 0	next_phase = 1	reward = -0.050860	array([[-1.4586301, -3.193283 ]], dtype=float32)

time = 11352	action = 0	current_phase = 0	next_phase = 1	reward = 0.031590	array([[-1.7270864, -2.9719474]], dtype=float32)

time = 11357	action = 0	current_phase = 0	next_phase = 1	reward = 0.077700	array([[-2.1843457, -2.9431956]], dtype=float32)

time = 11362	action = 1	current_phase = 0	next_phase = 1	reward = -1.630270	array([[-3.8036294, -2.7985427]], dtype=float32)

time = 11370	action = 1	current_phase = 1	next_phase = 0	reward = -0.992238	array([[-2.9993172, -1.8668442]], dtype=float32)

time = 11378	action = 0	current_phase = 0	next_phase = 1	reward = -0.050160	array([[-1.3435216, -3.083802 ]], dtype=float32)

time = 11383	action = 0	current_phase = 0	next_phase = 1	reward = 0.014791	array([[-1.830179 , -3.0054457]], dtype=float32)

time = 11388	action = 0	current_phase = 0	next_phase = 1	reward = 0.075600	array([[-2.2661726, -2.977865 ]], dtype=float32)

time = 11393	action = 1	current_phase = 0	next_phase = 1	reward = -1.884495	array([[-3.8192961, -2.9818206]], dtype=float32)

time = 11401	action = 1	current_phase = 1	next_phase = 0	reward = -1.126991	array([[-3.04513 , -2.206731]], dtype=float32)

time = 11409	action = 0	current_phase = 0	next_phase = 1	reward = -0.033262	array([[-1.3980906, -2.8177211]], dtype=float32)

time = 11414	action = 0	current_phase = 0	next_phase = 1	reward = 0.040201	array([[-1.6721894, -2.8771474]], dtype=float32)

time = 11419	action = 0	current_phase = 0	next_phase = 1	reward = 0.055832	array([[-2.1444306, -2.3870893]], dtype=float32)

time = 11424	action = 1	current_phase = 0	next_phase = 1	reward = -1.098600	array([[-3.7597106, -2.685254 ]], dtype=float32)

time = 11432	action = 1	current_phase = 1	next_phase = 0	reward = -1.309386	array([[-3.2896848, -2.2520828]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0874 - val_loss: 0.0451

Epoch 2/50

 - 3s - loss: 0.0815 - val_loss: 0.0536

Epoch 3/50

 - 3s - loss: 0.1140 - val_loss: 0.0691

Epoch 4/50

 - 3s - loss: 0.0822 - val_loss: 0.0623

Epoch 5/50

 - 4s - loss: 0.0752 - val_loss: 0.0437

Epoch 6/50

 - 3s - loss: 0.0786 - val_loss: 0.0516

Epoch 7/50

 - 4s - loss: 0.0723 - val_loss: 0.0613

Epoch 8/50

 - 3s - loss: 0.0750 - val_loss: 0.0553

Epoch 9/50

 - 3s - loss: 0.0708 - val_loss: 0.0411

Epoch 10/50

 - 3s - loss: 0.0818 - val_loss: 0.0494

Epoch 11/50

 - 4s - loss: 0.0772 - val_loss: 0.0553

Epoch 12/50

 - 3s - loss: 0.0759 - val_loss: 0.0472

Epoch 13/50

 - 5s - loss: 0.0753 - val_loss: 0.0469

Epoch 14/50

 - 5s - loss: 0.0695 - val_loss: 0.0489

Epoch 15/50

 - 5s - loss: 0.0710 - val_loss: 0.0458

Epoch 16/50

 - 4s - loss: 0.0991 - val_loss: 0.0527

Epoch 17/50

 - 4s - loss: 0.0667 - val_loss: 0.0458

Epoch 18/50

 - 3s - loss: 0.0673 - val_loss: 0.0505

Epoch 19/50

 - 4s - loss: 0.0681 - val_loss: 0.0577

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 618, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 606, after forget

time = 11440	action = 0	current_phase = 0	next_phase = 1	reward = 0.269722	array([[-1.2326498, -2.7560315]], dtype=float32)

time = 11445	action = 0	current_phase = 0	next_phase = 1	reward = 0.055507	array([[-1.5950825, -2.9183817]], dtype=float32)

time = 11450	action = 1	current_phase = 0	next_phase = 1	reward = -1.033193	array([[-2.753438 , -2.3959498]], dtype=float32)

time = 11458	action = 1	current_phase = 1	next_phase = 0	reward = -0.710135	array([[-2.8819203, -2.065422 ]], dtype=float32)

time = 11466	action = 0	current_phase = 0	next_phase = 1	reward = -0.091087	array([[-1.534203, -3.035173]], dtype=float32)

time = 11471	action = 0	current_phase = 0	next_phase = 1	reward = -0.014327	array([[-1.6130538, -2.9816513]], dtype=float32)

time = 11476	action = 0	current_phase = 0	next_phase = 1	reward = 0.051055	array([[-1.7344929, -2.9149857]], dtype=float32)

time = 11481	action = 1	current_phase = 0	next_phase = 1	reward = -1.375071	array([[-3.2168884, -2.5516577]], dtype=float32)

time = 11489	action = 1	current_phase = 1	next_phase = 0	reward = -0.833915	array([[-3.31427  , -1.8777239]], dtype=float32)

time = 11497	action = 0	current_phase = 0	next_phase = 1	reward = -0.067226	array([[-1.3756226, -3.0405445]], dtype=float32)

time = 11502	action = 0	current_phase = 0	next_phase = 1	reward = 0.010245	array([[-1.6517744, -2.981104 ]], dtype=float32)

time = 11507	action = 0	current_phase = 0	next_phase = 1	reward = 0.080693	array([[-1.9286695, -2.871479 ]], dtype=float32)

time = 11512	action = 1	current_phase = 0	next_phase = 1	reward = -1.668222	array([[-3.5872278, -2.759756 ]], dtype=float32)

time = 11520	action = 1	current_phase = 1	next_phase = 0	reward = -1.248176	array([[-2.9860005, -2.0351043]], dtype=float32)

time = 11528	action = 0	current_phase = 0	next_phase = 1	reward = 0.252591	array([[-1.3079767, -2.9848948]], dtype=float32)

time = 11533	action = 0	current_phase = 0	next_phase = 1	reward = 0.027930	array([[-1.6481314, -3.0538673]], dtype=float32)

time = 11538	action = 0	current_phase = 0	next_phase = 1	reward = 0.073880	array([[-2.0481644, -2.6732593]], dtype=float32)

time = 11543	action = 1	current_phase = 0	next_phase = 1	reward = -1.348304	array([[-3.7678263, -2.9813113]], dtype=float32)

time = 11551	action = 1	current_phase = 1	next_phase = 0	reward = -1.027796	array([[-3.1486638, -2.6081986]], dtype=float32)

time = 11559	action = 0	current_phase = 0	next_phase = 1	reward = -0.025787	array([[-1.3821691, -2.9289231]], dtype=float32)

time = 11564	action = 0	current_phase = 0	next_phase = 1	reward = 0.043209	array([[-1.806062 , -3.0446734]], dtype=float32)

time = 11569	action = 0	current_phase = 0	next_phase = 1	reward = 0.060452	array([[-2.248197 , -2.3519998]], dtype=float32)

time = 11574	action = 1	current_phase = 0	next_phase = 1	reward = -1.182883	array([[-3.503945, -2.782154]], dtype=float32)

time = 11582	action = 1	current_phase = 1	next_phase = 0	reward = -0.749135	array([[-2.9086833, -2.1582446]], dtype=float32)

time = 11590	action = 0	current_phase = 0	next_phase = 1	reward = -0.010484	array([[-1.565613, -2.912943]], dtype=float32)

time = 11595	action = 0	current_phase = 0	next_phase = 1	reward = 0.062575	array([[-1.6717778, -2.9177957]], dtype=float32)

time = 11600	action = 1	current_phase = 0	next_phase = 1	reward = -1.386459	array([[-3.0643504, -2.2888584]], dtype=float32)

time = 11608	action = 1	current_phase = 1	next_phase = 0	reward = -0.702891	array([[-3.0458713, -1.9141881]], dtype=float32)

time = 11616	action = 0	current_phase = 0	next_phase = 1	reward = -0.080638	array([[-1.4477828, -2.987133 ]], dtype=float32)

time = 11621	action = 0	current_phase = 0	next_phase = 1	reward = 0.003266	array([[-1.5188509, -2.882462 ]], dtype=float32)

time = 11626	action = 0	current_phase = 0	next_phase = 1	reward = 0.060975	array([[-1.8230342, -2.9239025]], dtype=float32)

time = 11631	action = 1	current_phase = 0	next_phase = 1	reward = -1.495617	array([[-3.5504384, -2.461038 ]], dtype=float32)

time = 11639	action = 1	current_phase = 1	next_phase = 0	reward = -0.839244	array([[-3.1700475, -1.9220788]], dtype=float32)

time = 11647	action = 0	current_phase = 0	next_phase = 1	reward = -0.072764	array([[-1.3240474, -3.1487393]], dtype=float32)

time = 11652	action = 0	current_phase = 0	next_phase = 1	reward = 0.003630	array([[-1.6492283, -2.9582415]], dtype=float32)

time = 11657	action = 0	current_phase = 0	next_phase = 1	reward = 0.059643	array([[-1.7287334, -2.8832498]], dtype=float32)

time = 11662	action = 1	current_phase = 0	next_phase = 1	reward = -1.527077	array([[-3.5629616, -2.5860345]], dtype=float32)

time = 11670	action = 1	current_phase = 1	next_phase = 0	reward = -0.950074	array([[-3.1288598, -1.9573548]], dtype=float32)

time = 11678	action = 0	current_phase = 0	next_phase = 1	reward = -0.070746	array([[-1.4072921, -3.1110892]], dtype=float32)

time = 11683	action = 0	current_phase = 0	next_phase = 1	reward = 0.006394	array([[-1.6208744, -3.0602093]], dtype=float32)

time = 11688	action = 0	current_phase = 0	next_phase = 1	reward = 0.074586	array([[-1.8179352, -2.840846 ]], dtype=float32)

time = 11693	action = 1	current_phase = 0	next_phase = 1	reward = -1.842379	array([[-3.458415 , -2.7386818]], dtype=float32)

time = 11701	action = 1	current_phase = 1	next_phase = 0	reward = -1.396808	array([[-3.2174833, -2.422833 ]], dtype=float32)

time = 11709	action = 0	current_phase = 0	next_phase = 1	reward = 0.260229	array([[-1.1980715, -2.8213496]], dtype=float32)

time = 11714	action = 0	current_phase = 0	next_phase = 1	reward = 0.038018	array([[-1.5001669, -2.807218 ]], dtype=float32)

time = 11719	action = 0	current_phase = 0	next_phase = 1	reward = 0.059034	array([[-2.0025854, -2.4112964]], dtype=float32)

time = 11724	action = 1	current_phase = 0	next_phase = 1	reward = -1.028941	array([[-3.6716166, -2.6490693]], dtype=float32)

time = 11732	action = 1	current_phase = 1	next_phase = 0	reward = -0.745891	array([[-3.0394998, -2.277353 ]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 4s - loss: 0.0773 - val_loss: 0.0585

Epoch 2/50

 - 3s - loss: 0.0791 - val_loss: 0.0527

Epoch 3/50

 - 3s - loss: 0.0757 - val_loss: 0.0636

Epoch 4/50

 - 3s - loss: 0.0765 - val_loss: 0.0541

Epoch 5/50

 - 3s - loss: 0.0768 - val_loss: 0.0556

Epoch 6/50

 - 3s - loss: 0.0738 - val_loss: 0.0534

Epoch 7/50

 - 3s - loss: 0.0869 - val_loss: 0.0642

Epoch 8/50

 - 3s - loss: 0.0856 - val_loss: 0.0640

Epoch 9/50

 - 3s - loss: 0.0763 - val_loss: 0.0583

Epoch 10/50

 - 3s - loss: 0.0588 - val_loss: 0.0665

Epoch 11/50

 - 3s - loss: 0.0858 - val_loss: 0.0665

Epoch 12/50

 - 3s - loss: 0.0612 - val_loss: 0.0557

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 628, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 616, after forget

time = 11740	action = 0	current_phase = 0	next_phase = 1	reward = -0.023054	array([[-1.4171952, -2.7434242]], dtype=float32)

time = 11745	action = 0	current_phase = 0	next_phase = 1	reward = 0.043616	array([[-1.7450243, -2.9436386]], dtype=float32)

time = 11750	action = 1	current_phase = 0	next_phase = 1	reward = -1.305390	array([[-2.895087 , -2.4111154]], dtype=float32)

time = 11758	action = 1	current_phase = 1	next_phase = 0	reward = -0.703070	array([[-3.1946967, -1.9129573]], dtype=float32)

time = 11766	action = 0	current_phase = 0	next_phase = 1	reward = -0.074698	array([[-1.6083492, -3.0497196]], dtype=float32)

time = 11771	action = 0	current_phase = 0	next_phase = 1	reward = -0.002455	array([[-1.6254959, -2.9179316]], dtype=float32)

time = 11776	action = 0	current_phase = 0	next_phase = 1	reward = 0.061831	array([[-1.8172296, -2.8509018]], dtype=float32)

time = 11781	action = 1	current_phase = 0	next_phase = 1	reward = -1.507264	array([[-3.8362205, -2.5819607]], dtype=float32)

time = 11789	action = 1	current_phase = 1	next_phase = 0	reward = -0.824387	array([[-3.2758598, -1.9170996]], dtype=float32)

time = 11797	action = 0	current_phase = 0	next_phase = 1	reward = -0.072632	array([[-1.4357213, -2.9991088]], dtype=float32)

time = 11802	action = 0	current_phase = 0	next_phase = 1	reward = 0.002155	array([[-1.6798317, -2.9819343]], dtype=float32)

time = 11807	action = 0	current_phase = 0	next_phase = 1	reward = 0.080628	array([[-2.0372548, -2.9158301]], dtype=float32)

time = 11812	action = 1	current_phase = 0	next_phase = 1	reward = -1.598161	array([[-3.729549, -2.632619]], dtype=float32)

time = 11820	action = 1	current_phase = 1	next_phase = 0	reward = -0.884974	array([[-3.1774976, -1.9967422]], dtype=float32)

time = 11828	action = 0	current_phase = 0	next_phase = 1	reward = -0.059268	array([[-1.2811955, -3.01014  ]], dtype=float32)

time = 11833	action = 0	current_phase = 0	next_phase = 1	reward = 0.014285	array([[-1.7685325, -3.1153166]], dtype=float32)

time = 11838	action = 0	current_phase = 0	next_phase = 1	reward = 0.070148	array([[-2.3224008, -3.034855 ]], dtype=float32)

time = 11843	action = 1	current_phase = 0	next_phase = 1	reward = -1.868992	array([[-3.6202657, -2.9259892]], dtype=float32)

time = 11851	action = 1	current_phase = 1	next_phase = 0	reward = -1.274176	array([[-3.2685492, -2.3000896]], dtype=float32)

time = 11859	action = 0	current_phase = 0	next_phase = 1	reward = 0.260602	array([[-1.4356635, -2.8838365]], dtype=float32)

time = 11864	action = 0	current_phase = 0	next_phase = 1	reward = 0.035016	array([[-1.673209 , -2.7983696]], dtype=float32)

time = 11869	action = 0	current_phase = 0	next_phase = 1	reward = 0.047445	array([[-1.9931605, -2.2464585]], dtype=float32)

time = 11874	action = 1	current_phase = 0	next_phase = 1	reward = -1.065445	array([[-3.442371 , -2.5165431]], dtype=float32)

time = 11882	action = 1	current_phase = 1	next_phase = 0	reward = -0.988765	array([[-3.108939 , -2.2523196]], dtype=float32)

time = 11890	action = 0	current_phase = 0	next_phase = 1	reward = 0.243828	array([[-1.4176809, -2.808739 ]], dtype=float32)

time = 11895	action = 0	current_phase = 0	next_phase = 1	reward = 0.038817	array([[-1.6834642, -2.9310622]], dtype=float32)

time = 11900	action = 1	current_phase = 0	next_phase = 1	reward = -1.298629	array([[-2.953177 , -2.2435293]], dtype=float32)

time = 11908	action = 1	current_phase = 1	next_phase = 0	reward = -0.688969	array([[-3.1427915, -1.941191 ]], dtype=float32)

time = 11916	action = 0	current_phase = 0	next_phase = 1	reward = -0.085038	array([[-1.4972941, -3.0059276]], dtype=float32)

time = 11921	action = 0	current_phase = 0	next_phase = 1	reward = -0.018740	array([[-1.6880163, -2.839452 ]], dtype=float32)

time = 11926	action = 0	current_phase = 0	next_phase = 1	reward = 0.040347	array([[-2.0381613, -2.8644152]], dtype=float32)

time = 11931	action = 1	current_phase = 0	next_phase = 1	reward = -1.566666	array([[-3.0507903, -2.6243074]], dtype=float32)

time = 11939	action = 1	current_phase = 1	next_phase = 0	reward = -1.053640	array([[-3.2349777, -1.8803322]], dtype=float32)

time = 11947	action = 0	current_phase = 0	next_phase = 1	reward = 0.233791	array([[-1.3480642, -2.9839647]], dtype=float32)

time = 11952	action = 0	current_phase = 0	next_phase = 1	reward = 0.013467	array([[-1.6309278, -3.0005603]], dtype=float32)

time = 11957	action = 0	current_phase = 0	next_phase = 1	reward = 0.053644	array([[-1.7672724, -2.9206717]], dtype=float32)

time = 11962	action = 1	current_phase = 0	next_phase = 1	reward = -1.609089	array([[-3.5643444, -2.8822145]], dtype=float32)

time = 11970	action = 1	current_phase = 1	next_phase = 0	reward = -1.282211	array([[-3.1828554, -1.9161855]], dtype=float32)

time = 11978	action = 0	current_phase = 0	next_phase = 1	reward = 0.251148	array([[-1.2919146, -3.1369777]], dtype=float32)

time = 11983	action = 0	current_phase = 0	next_phase = 1	reward = 0.046544	array([[-2.1193542, -3.139123 ]], dtype=float32)

time = 11988	action = 0	current_phase = 0	next_phase = 1	reward = 0.081312	array([[-2.005856, -2.97056 ]], dtype=float32)

time = 11993	action = 1	current_phase = 0	next_phase = 1	reward = -1.369482	array([[-3.8213851, -2.8828852]], dtype=float32)

time = 12001	action = 1	current_phase = 1	next_phase = 0	reward = -1.502771	array([[-3.0626698, -2.2984734]], dtype=float32)

time = 12009	action = 0	current_phase = 0	next_phase = 1	reward = 0.248564	array([[-1.3197165, -2.837522 ]], dtype=float32)

time = 12014	action = 0	current_phase = 0	next_phase = 1	reward = 0.038675	array([[-1.6793497, -3.0117047]], dtype=float32)

time = 12019	action = 0	current_phase = 0	next_phase = 1	reward = 0.069400	array([[-1.9569842, -2.5958946]], dtype=float32)

time = 12024	action = 1	current_phase = 0	next_phase = 1	reward = -1.063914	array([[-3.4683294, -2.6828842]], dtype=float32)

time = 12032	action = 1	current_phase = 1	next_phase = 0	reward = -1.038073	array([[-3.1570358, -2.116672 ]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0714 - val_loss: 0.0715

Epoch 2/50

 - 3s - loss: 0.0908 - val_loss: 0.0401

Epoch 3/50

 - 4s - loss: 0.0765 - val_loss: 0.0597

Epoch 4/50

 - 4s - loss: 0.0976 - val_loss: 0.0463

Epoch 5/50

 - 5s - loss: 0.0697 - val_loss: 0.0374

Epoch 6/50

 - 4s - loss: 0.0840 - val_loss: 0.0475

Epoch 7/50

 - 4s - loss: 0.0708 - val_loss: 0.0629

Epoch 8/50

 - 3s - loss: 0.0708 - val_loss: 0.0466

Epoch 9/50

 - 3s - loss: 0.0687 - val_loss: 0.0549

Epoch 10/50

 - 4s - loss: 0.0793 - val_loss: 0.0478

Epoch 11/50

 - 3s - loss: 0.0770 - val_loss: 0.0421

Epoch 12/50

 - 3s - loss: 0.0620 - val_loss: 0.0418

Epoch 13/50

 - 4s - loss: 0.0651 - val_loss: 0.0835

Epoch 14/50

 - 3s - loss: 0.0652 - val_loss: 0.0450

Epoch 15/50

 - 4s - loss: 0.0592 - val_loss: 0.0554

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 638, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 626, after forget

time = 12040	action = 0	current_phase = 0	next_phase = 1	reward = -0.028020	array([[-1.5075439, -2.86158  ]], dtype=float32)

time = 12045	action = 0	current_phase = 0	next_phase = 1	reward = 0.035560	array([[-1.6181777, -2.8450541]], dtype=float32)

time = 12050	action = 1	current_phase = 0	next_phase = 1	reward = -1.038577	array([[-2.6497636, -2.372222 ]], dtype=float32)

time = 12058	action = 1	current_phase = 1	next_phase = 0	reward = -0.704301	array([[-3.3930113, -1.6821835]], dtype=float32)

time = 12066	action = 0	current_phase = 0	next_phase = 1	reward = -0.074103	array([[-1.6171995, -3.0451527]], dtype=float32)

time = 12071	action = 0	current_phase = 0	next_phase = 1	reward = -0.012316	array([[-1.5996503, -2.8560896]], dtype=float32)

time = 12076	action = 0	current_phase = 0	next_phase = 1	reward = 0.052276	array([[-1.7908106, -2.8752213]], dtype=float32)

time = 12081	action = 1	current_phase = 0	next_phase = 1	reward = -1.381432	array([[-3.397011, -2.488347]], dtype=float32)

time = 12089	action = 1	current_phase = 1	next_phase = 0	reward = -0.838250	array([[-3.3838203, -1.8082666]], dtype=float32)

time = 12097	action = 0	current_phase = 0	next_phase = 1	reward = -0.078362	array([[-1.3456361, -3.0775971]], dtype=float32)

time = 12102	action = 0	current_phase = 0	next_phase = 1	reward = -0.000341	array([[-1.6873102, -2.9873543]], dtype=float32)

time = 12107	action = 0	current_phase = 0	next_phase = 1	reward = 0.058010	array([[-2.0067272, -2.9167795]], dtype=float32)

time = 12112	action = 1	current_phase = 0	next_phase = 1	reward = -1.523538	array([[-3.5088673, -2.7362537]], dtype=float32)

time = 12120	action = 1	current_phase = 1	next_phase = 0	reward = -0.943180	array([[-3.345505 , -1.9366425]], dtype=float32)

time = 12128	action = 0	current_phase = 0	next_phase = 1	reward = -0.065411	array([[-1.318582 , -3.0718336]], dtype=float32)

time = 12133	action = 0	current_phase = 0	next_phase = 1	reward = 0.010248	array([[-1.720451, -3.050025]], dtype=float32)

time = 12138	action = 0	current_phase = 0	next_phase = 1	reward = 0.053654	array([[-1.9295042, -2.975263 ]], dtype=float32)

time = 12143	action = 1	current_phase = 0	next_phase = 1	reward = -1.899647	array([[-3.432441, -2.938603]], dtype=float32)

time = 12151	action = 1	current_phase = 1	next_phase = 0	reward = -0.961929	array([[-3.3188426, -2.253335 ]], dtype=float32)

time = 12159	action = 0	current_phase = 0	next_phase = 1	reward = -0.028904	array([[-1.3292141, -2.7580462]], dtype=float32)

time = 12164	action = 0	current_phase = 0	next_phase = 1	reward = 0.036604	array([[-1.5796447, -2.7547498]], dtype=float32)

time = 12169	action = 0	current_phase = 0	next_phase = 1	reward = 0.055429	array([[-2.1415608, -2.2456422]], dtype=float32)

time = 12174	action = 1	current_phase = 0	next_phase = 1	reward = -1.136389	array([[-3.7911766, -2.756096 ]], dtype=float32)

time = 12182	action = 1	current_phase = 1	next_phase = 0	reward = -0.752841	array([[-3.353686 , -2.3445992]], dtype=float32)

time = 12190	action = 0	current_phase = 0	next_phase = 1	reward = -0.022991	array([[-1.552973 , -2.8932514]], dtype=float32)

time = 12195	action = 0	current_phase = 0	next_phase = 1	reward = -0.235260	array([[-1.804478 , -2.8627086]], dtype=float32)

time = 12200	action = 1	current_phase = 0	next_phase = 1	reward = -1.044007	array([[-2.7437289, -2.5933938]], dtype=float32)

time = 12208	action = 1	current_phase = 1	next_phase = 0	reward = -0.698464	array([[-3.1935482, -1.9725178]], dtype=float32)

time = 12216	action = 0	current_phase = 0	next_phase = 1	reward = -0.072356	array([[-1.4939282, -2.9929671]], dtype=float32)

time = 12221	action = 0	current_phase = 0	next_phase = 1	reward = -0.005293	array([[-1.6745361, -2.9547968]], dtype=float32)

time = 12226	action = 0	current_phase = 0	next_phase = 1	reward = 0.059590	array([[-1.7597578, -2.9021893]], dtype=float32)

time = 12231	action = 1	current_phase = 0	next_phase = 1	reward = -1.337181	array([[-3.3905842, -2.5313692]], dtype=float32)

time = 12239	action = 1	current_phase = 1	next_phase = 0	reward = -0.837258	array([[-3.3551004, -1.8187978]], dtype=float32)

time = 12247	action = 0	current_phase = 0	next_phase = 1	reward = -0.085103	array([[-1.3893733, -3.0517054]], dtype=float32)

time = 12252	action = 0	current_phase = 0	next_phase = 1	reward = 0.012328	array([[-1.5833265, -2.9534564]], dtype=float32)

time = 12257	action = 0	current_phase = 0	next_phase = 1	reward = 0.069998	array([[-1.7508178, -2.9015293]], dtype=float32)

time = 12262	action = 1	current_phase = 0	next_phase = 1	reward = -1.708373	array([[-3.4636495, -2.753541 ]], dtype=float32)

time = 12270	action = 1	current_phase = 1	next_phase = 0	reward = -0.970987	array([[-3.3162768, -1.9882158]], dtype=float32)

time = 12278	action = 0	current_phase = 0	next_phase = 1	reward = -0.044591	array([[-1.3492897, -3.0273776]], dtype=float32)

time = 12283	action = 0	current_phase = 0	next_phase = 1	reward = 0.026642	array([[-1.648027, -3.033155]], dtype=float32)

time = 12288	action = 0	current_phase = 0	next_phase = 1	reward = 0.069642	array([[-2.019188 , -2.9687338]], dtype=float32)

time = 12293	action = 1	current_phase = 0	next_phase = 1	reward = -1.866617	array([[-3.8531728, -3.0758352]], dtype=float32)

time = 12301	action = 1	current_phase = 1	next_phase = 0	reward = -1.028843	array([[-3.4148834, -2.3990097]], dtype=float32)

time = 12309	action = 0	current_phase = 0	next_phase = 1	reward = -0.035316	array([[-1.3229544, -2.8986425]], dtype=float32)

time = 12314	action = 0	current_phase = 0	next_phase = 1	reward = 0.026412	array([[-1.5720136, -2.8034754]], dtype=float32)

time = 12319	action = 0	current_phase = 0	next_phase = 1	reward = 0.071476	array([[-1.969177 , -2.4941502]], dtype=float32)

time = 12324	action = 1	current_phase = 0	next_phase = 1	reward = -1.214690	array([[-3.6584196, -2.588142 ]], dtype=float32)

time = 12332	action = 1	current_phase = 1	next_phase = 0	reward = -0.744385	array([[-3.4587984, -2.303127 ]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0882 - val_loss: 0.0552

Epoch 2/50

 - 3s - loss: 0.0917 - val_loss: 0.0633

Epoch 3/50

 - 3s - loss: 0.0922 - val_loss: 0.0498

Epoch 4/50

 - 3s - loss: 0.0900 - val_loss: 0.0563

Epoch 5/50

 - 3s - loss: 0.0731 - val_loss: 0.0632

Epoch 6/50

 - 4s - loss: 0.0675 - val_loss: 0.0677

Epoch 7/50

 - 3s - loss: 0.0732 - val_loss: 0.0707

Epoch 8/50

 - 3s - loss: 0.0786 - val_loss: 0.0684

Epoch 9/50

 - 3s - loss: 0.0774 - val_loss: 0.0556

Epoch 10/50

 - 3s - loss: 0.0725 - val_loss: 0.0589

Epoch 11/50

 - 3s - loss: 0.0666 - val_loss: 0.0532

Epoch 12/50

 - 3s - loss: 0.0706 - val_loss: 0.0539

Epoch 13/50

 - 3s - loss: 0.0707 - val_loss: 0.0727

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 648, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 636, after forget

time = 12340	action = 0	current_phase = 0	next_phase = 1	reward = -0.022487	array([[-1.6163585, -2.9157438]], dtype=float32)

time = 12345	action = 0	current_phase = 0	next_phase = 1	reward = -0.234130	array([[-1.9535971, -3.0090108]], dtype=float32)

time = 12350	action = 1	current_phase = 0	next_phase = 1	reward = -1.141870	array([[-3.165612, -2.867916]], dtype=float32)

time = 12358	action = 1	current_phase = 1	next_phase = 0	reward = -0.708317	array([[-3.3832936, -1.8542589]], dtype=float32)

time = 12366	action = 0	current_phase = 0	next_phase = 1	reward = -0.076902	array([[-1.4412069, -3.1330395]], dtype=float32)

time = 12371	action = 0	current_phase = 0	next_phase = 1	reward = -0.013663	array([[-1.6296728, -2.9236856]], dtype=float32)

time = 12376	action = 0	current_phase = 0	next_phase = 1	reward = 0.054367	array([[-1.9459074, -2.982995 ]], dtype=float32)

time = 12381	action = 1	current_phase = 0	next_phase = 1	reward = -1.441681	array([[-3.5999303, -2.957436 ]], dtype=float32)

time = 12389	action = 1	current_phase = 1	next_phase = 0	reward = -0.759093	array([[-3.4892912, -1.7663777]], dtype=float32)

time = 12397	action = 0	current_phase = 0	next_phase = 1	reward = -0.072827	array([[-1.4434936, -3.077951 ]], dtype=float32)

time = 12402	action = 0	current_phase = 0	next_phase = 1	reward = -0.007459	array([[-1.7700491, -2.966556 ]], dtype=float32)

time = 12407	action = 0	current_phase = 0	next_phase = 1	reward = 0.062425	array([[-2.2084138, -3.013929 ]], dtype=float32)

time = 12412	action = 1	current_phase = 0	next_phase = 1	reward = -1.530388	array([[-3.7286649, -3.1243324]], dtype=float32)

time = 12420	action = 1	current_phase = 1	next_phase = 0	reward = -0.901308	array([[-3.4656093, -1.8679965]], dtype=float32)

time = 12428	action = 0	current_phase = 0	next_phase = 1	reward = -0.063113	array([[-1.3549879, -3.074615 ]], dtype=float32)

time = 12433	action = 0	current_phase = 0	next_phase = 1	reward = 0.021385	array([[-1.8093381, -3.1334414]], dtype=float32)

time = 12438	action = 0	current_phase = 0	next_phase = 1	reward = 0.068961	array([[-2.0498278, -2.947401 ]], dtype=float32)

time = 12443	action = 1	current_phase = 0	next_phase = 1	reward = -1.370335	array([[-3.7324343, -3.2593107]], dtype=float32)

time = 12451	action = 1	current_phase = 1	next_phase = 0	reward = -1.391945	array([[-3.2305338, -2.2163782]], dtype=float32)

time = 12459	action = 0	current_phase = 0	next_phase = 1	reward = 0.237717	array([[-1.4090059, -2.6150541]], dtype=float32)

time = 12464	action = 0	current_phase = 0	next_phase = 1	reward = 0.011019	array([[-1.6249118, -2.9899087]], dtype=float32)

time = 12469	action = 0	current_phase = 0	next_phase = 1	reward = 0.071450	array([[-2.1285014, -2.623073 ]], dtype=float32)

time = 12474	action = 1	current_phase = 0	next_phase = 1	reward = -0.950560	array([[-3.5989811, -2.9800496]], dtype=float32)

time = 12482	action = 1	current_phase = 1	next_phase = 0	reward = -0.755027	array([[-3.1222517, -2.1262026]], dtype=float32)

time = 12490	action = 0	current_phase = 0	next_phase = 1	reward = -0.300151	array([[-1.3599813, -2.8928776]], dtype=float32)

time = 12495	action = 0	current_phase = 0	next_phase = 1	reward = 0.332613	array([[-1.8499668, -2.9462895]], dtype=float32)

time = 12500	action = 1	current_phase = 0	next_phase = 1	reward = -1.249262	array([[-3.280376 , -2.7345238]], dtype=float32)

time = 12508	action = 1	current_phase = 1	next_phase = 0	reward = -0.704085	array([[-3.2398925, -1.9093546]], dtype=float32)

time = 12516	action = 0	current_phase = 0	next_phase = 1	reward = -0.099591	array([[-1.4833864, -3.0262232]], dtype=float32)

time = 12521	action = 0	current_phase = 0	next_phase = 1	reward = -0.030882	array([[-1.6505525, -2.9691834]], dtype=float32)

time = 12526	action = 0	current_phase = 0	next_phase = 1	reward = 0.033637	array([[-1.9389548, -2.9893742]], dtype=float32)

time = 12531	action = 1	current_phase = 0	next_phase = 1	reward = -1.485257	array([[-3.240756 , -2.5874343]], dtype=float32)

time = 12539	action = 1	current_phase = 1	next_phase = 0	reward = -0.803906	array([[-3.5239773, -1.810786 ]], dtype=float32)

time = 12547	action = 0	current_phase = 0	next_phase = 1	reward = -0.055954	array([[-1.4903858, -3.104105 ]], dtype=float32)

time = 12552	action = 0	current_phase = 0	next_phase = 1	reward = 0.012087	array([[-1.6717105, -2.9514132]], dtype=float32)

time = 12557	action = 0	current_phase = 0	next_phase = 1	reward = 0.077675	array([[-2.226214 , -2.9670177]], dtype=float32)

time = 12562	action = 1	current_phase = 0	next_phase = 1	reward = -1.609270	array([[-3.6876242, -3.1326194]], dtype=float32)

time = 12570	action = 1	current_phase = 1	next_phase = 0	reward = -0.997116	array([[-3.3771064, -1.8227489]], dtype=float32)

time = 12578	action = 0	current_phase = 0	next_phase = 1	reward = -0.056600	array([[-1.5130825, -3.1003885]], dtype=float32)

time = 12583	action = 0	current_phase = 0	next_phase = 1	reward = 0.017980	array([[-1.7558775, -3.1242652]], dtype=float32)

time = 12588	action = 0	current_phase = 0	next_phase = 1	reward = 0.081081	array([[-2.3212337, -2.9691377]], dtype=float32)

time = 12593	action = 1	current_phase = 0	next_phase = 1	reward = -1.324618	array([[-3.8780336, -3.3414788]], dtype=float32)

time = 12601	action = 1	current_phase = 1	next_phase = 0	reward = -1.496715	array([[-3.2750475, -2.1972141]], dtype=float32)

time = 12609	action = 0	current_phase = 0	next_phase = 1	reward = 0.256700	array([[-1.3676043, -2.8839583]], dtype=float32)

time = 12614	action = 0	current_phase = 0	next_phase = 1	reward = 0.029551	array([[-1.7669597, -2.8768544]], dtype=float32)

time = 12619	action = 0	current_phase = 0	next_phase = 1	reward = 0.055772	array([[-2.473283 , -2.7297163]], dtype=float32)

time = 12624	action = 1	current_phase = 0	next_phase = 1	reward = -1.007660	array([[-3.3444595, -2.7629852]], dtype=float32)

time = 12632	action = 1	current_phase = 1	next_phase = 0	reward = -0.773625	array([[-3.2450702, -2.0198178]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0949 - val_loss: 0.0423

Epoch 2/50

 - 3s - loss: 0.0952 - val_loss: 0.0471

Epoch 3/50

 - 3s - loss: 0.0874 - val_loss: 0.0389

Epoch 4/50

 - 3s - loss: 0.0841 - val_loss: 0.0406

Epoch 5/50

 - 3s - loss: 0.0834 - val_loss: 0.0378

Epoch 6/50

 - 3s - loss: 0.0859 - val_loss: 0.0478

Epoch 7/50

 - 3s - loss: 0.0957 - val_loss: 0.0434

Epoch 8/50

 - 3s - loss: 0.0750 - val_loss: 0.0441

Epoch 9/50

 - 3s - loss: 0.0720 - val_loss: 0.0445

Epoch 10/50

 - 3s - loss: 0.0682 - val_loss: 0.0451

Epoch 11/50

 - 3s - loss: 0.0811 - val_loss: 0.0540

Epoch 12/50

 - 3s - loss: 0.0700 - val_loss: 0.0447

Epoch 13/50

 - 3s - loss: 0.0767 - val_loss: 0.0532

Epoch 14/50

 - 3s - loss: 0.0636 - val_loss: 0.0486

Epoch 15/50

 - 3s - loss: 0.0660 - val_loss: 0.0497

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 658, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 646, after forget

time = 12640	action = 0	current_phase = 0	next_phase = 1	reward = -0.027343	array([[-1.4257967, -2.855154 ]], dtype=float32)

time = 12645	action = 0	current_phase = 0	next_phase = 1	reward = 0.051178	array([[-1.8092675, -3.0841718]], dtype=float32)

time = 12650	action = 0	current_phase = 0	next_phase = 1	reward = 0.045205	array([[-2.6314716, -2.8145442]], dtype=float32)

time = 12655	action = 1	current_phase = 0	next_phase = 1	reward = -1.851025	array([[-3.7638905, -3.574266 ]], dtype=float32)

time = 12663	action = 1	current_phase = 1	next_phase = 0	reward = -0.751590	array([[-3.5593274, -2.163406 ]], dtype=float32)

time = 12671	action = 0	current_phase = 0	next_phase = 1	reward = -0.000303	array([[-1.413404 , -2.9257424]], dtype=float32)

time = 12676	action = 0	current_phase = 0	next_phase = 1	reward = 0.071102	array([[-1.7984177, -2.9982574]], dtype=float32)

time = 12681	action = 1	current_phase = 0	next_phase = 1	reward = -1.451372	array([[-3.3924181, -2.9009883]], dtype=float32)

time = 12689	action = 1	current_phase = 1	next_phase = 0	reward = -0.776491	array([[-3.3764007, -1.9257816]], dtype=float32)

time = 12697	action = 0	current_phase = 0	next_phase = 1	reward = -0.050727	array([[-1.5584581, -3.157934 ]], dtype=float32)

time = 12702	action = 0	current_phase = 0	next_phase = 1	reward = 0.021745	array([[-1.6438209, -3.0531864]], dtype=float32)

time = 12707	action = 0	current_phase = 0	next_phase = 1	reward = 0.082544	array([[-2.079258, -3.109816]], dtype=float32)

time = 12712	action = 1	current_phase = 0	next_phase = 1	reward = -1.683664	array([[-3.6639738, -3.2017143]], dtype=float32)

time = 12720	action = 1	current_phase = 1	next_phase = 0	reward = -0.946281	array([[-3.5852416, -2.184452 ]], dtype=float32)

time = 12728	action = 0	current_phase = 0	next_phase = 1	reward = -0.059067	array([[-1.4841024, -3.041945 ]], dtype=float32)

time = 12733	action = 0	current_phase = 0	next_phase = 1	reward = 0.013602	array([[-1.7252127, -3.0813267]], dtype=float32)

time = 12738	action = 0	current_phase = 0	next_phase = 1	reward = 0.065784	array([[-2.2908401, -3.059735 ]], dtype=float32)

time = 12743	action = 1	current_phase = 0	next_phase = 1	reward = -1.837154	array([[-3.9453528, -3.6930733]], dtype=float32)

time = 12751	action = 1	current_phase = 1	next_phase = 0	reward = -1.030278	array([[-3.5735323, -2.2959983]], dtype=float32)

time = 12759	action = 0	current_phase = 0	next_phase = 1	reward = -0.030704	array([[-1.2877517, -2.9430578]], dtype=float32)

time = 12764	action = 0	current_phase = 0	next_phase = 1	reward = 0.034586	array([[-1.5419722, -2.96641  ]], dtype=float32)

time = 12769	action = 0	current_phase = 0	next_phase = 1	reward = 0.053852	array([[-2.0925074, -2.7718592]], dtype=float32)

time = 12774	action = 1	current_phase = 0	next_phase = 1	reward = -1.009750	array([[-3.72041  , -2.9438417]], dtype=float32)

time = 12782	action = 1	current_phase = 1	next_phase = 0	reward = -0.747414	array([[-3.3978624, -2.2431202]], dtype=float32)

time = 12790	action = 0	current_phase = 0	next_phase = 1	reward = -0.028050	array([[-1.4077456, -2.85056  ]], dtype=float32)

time = 12795	action = 0	current_phase = 0	next_phase = 1	reward = 0.029108	array([[-1.8561139, -3.004755 ]], dtype=float32)

time = 12800	action = 1	current_phase = 0	next_phase = 1	reward = -1.253643	array([[-3.4911094, -3.227196 ]], dtype=float32)

time = 12808	action = 1	current_phase = 1	next_phase = 0	reward = -0.705685	array([[-3.3062434, -1.9877945]], dtype=float32)

time = 12816	action = 0	current_phase = 0	next_phase = 1	reward = -0.071089	array([[-1.3346081, -2.989812 ]], dtype=float32)

time = 12821	action = 0	current_phase = 0	next_phase = 1	reward = -0.008906	array([[-1.5644083, -3.0188787]], dtype=float32)

time = 12826	action = 0	current_phase = 0	next_phase = 1	reward = 0.060385	array([[-1.9576235, -3.0071948]], dtype=float32)

time = 12831	action = 1	current_phase = 0	next_phase = 1	reward = -1.370594	array([[-3.5674033, -2.944494 ]], dtype=float32)

time = 12839	action = 1	current_phase = 1	next_phase = 0	reward = -0.818195	array([[-3.6009457, -1.9585721]], dtype=float32)

time = 12847	action = 0	current_phase = 0	next_phase = 1	reward = -0.074741	array([[-1.501924 , -3.1219616]], dtype=float32)

time = 12852	action = 0	current_phase = 0	next_phase = 1	reward = -0.003655	array([[-1.5300206, -2.9881585]], dtype=float32)

time = 12857	action = 0	current_phase = 0	next_phase = 1	reward = 0.066464	array([[-2.3468819, -3.0078738]], dtype=float32)

time = 12862	action = 1	current_phase = 0	next_phase = 1	reward = -1.590449	array([[-3.7494183, -3.2089782]], dtype=float32)

time = 12870	action = 1	current_phase = 1	next_phase = 0	reward = -1.190809	array([[-3.6346362, -2.0258586]], dtype=float32)

time = 12878	action = 0	current_phase = 0	next_phase = 1	reward = 0.252311	array([[-1.2375747, -3.000902 ]], dtype=float32)

time = 12883	action = 0	current_phase = 0	next_phase = 1	reward = 0.025956	array([[-1.737282 , -3.1484797]], dtype=float32)

time = 12888	action = 0	current_phase = 0	next_phase = 1	reward = 0.071902	array([[-2.2208734, -2.9649222]], dtype=float32)

time = 12893	action = 1	current_phase = 0	next_phase = 1	reward = -1.362854	array([[-3.999816 , -3.3510275]], dtype=float32)

time = 12901	action = 1	current_phase = 1	next_phase = 0	reward = -1.031503	array([[-3.3959014, -2.370503 ]], dtype=float32)

time = 12909	action = 0	current_phase = 0	next_phase = 1	reward = -0.032730	array([[-1.2113225, -2.76054  ]], dtype=float32)

time = 12914	action = 0	current_phase = 0	next_phase = 1	reward = 0.023254	array([[-1.6245394, -3.0569825]], dtype=float32)

time = 12919	action = 0	current_phase = 0	next_phase = 1	reward = 0.070514	array([[-2.2859166, -2.7651355]], dtype=float32)

time = 12924	action = 1	current_phase = 0	next_phase = 1	reward = -1.160526	array([[-3.6331105, -3.0135024]], dtype=float32)

time = 12932	action = 1	current_phase = 1	next_phase = 0	reward = -0.761441	array([[-3.747428 , -2.3848138]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0935 - val_loss: 0.0425

Epoch 2/50

 - 3s - loss: 0.0877 - val_loss: 0.0399

Epoch 3/50

 - 3s - loss: 0.0905 - val_loss: 0.0417

Epoch 4/50

 - 3s - loss: 0.0771 - val_loss: 0.0484

Epoch 5/50

 - 3s - loss: 0.0743 - val_loss: 0.0376

Epoch 6/50

 - 3s - loss: 0.0717 - val_loss: 0.0454

Epoch 7/50

 - 3s - loss: 0.1118 - val_loss: 0.0507

Epoch 8/50

 - 3s - loss: 0.0706 - val_loss: 0.0509

Epoch 9/50

 - 3s - loss: 0.0783 - val_loss: 0.0476

Epoch 10/50

 - 3s - loss: 0.0721 - val_loss: 0.0380

Epoch 11/50

 - 3s - loss: 0.0784 - val_loss: 0.0454

Epoch 12/50

 - 3s - loss: 0.0670 - val_loss: 0.0457

Epoch 13/50

 - 3s - loss: 0.0910 - val_loss: 0.0418

Epoch 14/50

 - 3s - loss: 0.0808 - val_loss: 0.0425

Epoch 15/50

 - 3s - loss: 0.0779 - val_loss: 0.0446

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 668, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 656, after forget

time = 12940	action = 0	current_phase = 0	next_phase = 1	reward = -0.031352	array([[-1.3239808, -2.8464642]], dtype=float32)

time = 12945	action = 0	current_phase = 0	next_phase = 1	reward = -0.251651	array([[-1.7910922, -3.0218973]], dtype=float32)

time = 12950	action = 1	current_phase = 0	next_phase = 1	reward = -1.043791	array([[-3.0124998, -2.610806 ]], dtype=float32)

time = 12958	action = 1	current_phase = 1	next_phase = 0	reward = -0.689167	array([[-3.5007248, -1.7937027]], dtype=float32)

time = 12966	action = 0	current_phase = 0	next_phase = 1	reward = -0.079433	array([[-1.3156672, -3.0077806]], dtype=float32)

time = 12971	action = 0	current_phase = 0	next_phase = 1	reward = -0.013892	array([[-1.4651418, -3.0049918]], dtype=float32)

time = 12976	action = 0	current_phase = 0	next_phase = 1	reward = 0.052991	array([[-1.8413827, -3.032181 ]], dtype=float32)

time = 12981	action = 1	current_phase = 0	next_phase = 1	reward = -1.494998	array([[-3.3399456, -3.0404594]], dtype=float32)

time = 12989	action = 1	current_phase = 1	next_phase = 0	reward = -0.779743	array([[-3.5278735, -1.8297489]], dtype=float32)

time = 12997	action = 0	current_phase = 0	next_phase = 1	reward = -0.067724	array([[-1.3364129, -3.083043 ]], dtype=float32)

time = 13002	action = 0	current_phase = 0	next_phase = 1	reward = 0.008763	array([[-1.3255   , -3.0898535]], dtype=float32)

time = 13007	action = 0	current_phase = 0	next_phase = 1	reward = 0.075170	array([[-2.12955  , -3.0732613]], dtype=float32)

time = 13012	action = 1	current_phase = 0	next_phase = 1	reward = -1.607657	array([[-3.6904218, -3.1005924]], dtype=float32)

time = 13020	action = 1	current_phase = 1	next_phase = 0	reward = -0.998658	array([[-3.5582008, -2.0314488]], dtype=float32)

time = 13028	action = 0	current_phase = 0	next_phase = 1	reward = -0.032886	array([[-1.0954876, -3.0858603]], dtype=float32)

time = 13033	action = 0	current_phase = 0	next_phase = 1	reward = 0.051404	array([[-1.5767989, -3.051772 ]], dtype=float32)

time = 13038	action = 0	current_phase = 0	next_phase = 1	reward = 0.071157	array([[-2.1414924, -2.8234017]], dtype=float32)

time = 13043	action = 1	current_phase = 0	next_phase = 1	reward = -1.913796	array([[-4.0618887, -3.4038823]], dtype=float32)

time = 13051	action = 1	current_phase = 1	next_phase = 0	reward = -1.397673	array([[-3.673468, -2.342277]], dtype=float32)

time = 13059	action = 0	current_phase = 0	next_phase = 1	reward = 0.263318	array([[-1.1222245, -2.8945508]], dtype=float32)

time = 13064	action = 0	current_phase = 0	next_phase = 1	reward = 0.049806	array([[-1.4923346, -2.962777 ]], dtype=float32)

time = 13069	action = 0	current_phase = 0	next_phase = 1	reward = 0.064074	array([[-2.2262702, -2.6706927]], dtype=float32)

time = 13074	action = 1	current_phase = 0	next_phase = 1	reward = -1.125692	array([[-3.6803935, -2.8319447]], dtype=float32)

time = 13082	action = 1	current_phase = 1	next_phase = 0	reward = -0.689035	array([[-3.3057456, -2.0148757]], dtype=float32)

time = 13090	action = 0	current_phase = 0	next_phase = 1	reward = -0.282853	array([[-1.2110478, -2.8823357]], dtype=float32)

time = 13095	action = 0	current_phase = 0	next_phase = 1	reward = 0.336981	array([[-1.724797, -2.947305]], dtype=float32)

time = 13100	action = 1	current_phase = 0	next_phase = 1	reward = -1.388900	array([[-3.1041365, -2.8493447]], dtype=float32)

time = 13108	action = 1	current_phase = 1	next_phase = 0	reward = -0.710197	array([[-3.335249, -1.877512]], dtype=float32)

time = 13116	action = 0	current_phase = 0	next_phase = 1	reward = -0.080592	array([[-1.2687377, -3.0480578]], dtype=float32)

time = 13121	action = 0	current_phase = 0	next_phase = 1	reward = -0.005992	array([[-1.370326, -2.980544]], dtype=float32)

time = 13126	action = 0	current_phase = 0	next_phase = 1	reward = 0.061764	array([[-1.8950751, -2.9773552]], dtype=float32)

time = 13131	action = 1	current_phase = 0	next_phase = 1	reward = -1.442937	array([[-3.327399 , -2.9371057]], dtype=float32)

time = 13139	action = 1	current_phase = 1	next_phase = 0	reward = -0.777817	array([[-3.5102148, -1.8166566]], dtype=float32)

time = 13147	action = 0	current_phase = 0	next_phase = 1	reward = -0.083304	array([[-1.3074113, -3.0399146]], dtype=float32)

time = 13152	action = 0	current_phase = 0	next_phase = 1	reward = -0.020042	array([[-0.833078 , -3.0830832]], dtype=float32)

time = 13157	action = 0	current_phase = 0	next_phase = 1	reward = 0.054114	array([[-1.7886696, -3.0060704]], dtype=float32)

time = 13162	action = 1	current_phase = 0	next_phase = 1	reward = -1.509454	array([[-3.5689845, -2.8751915]], dtype=float32)

time = 13170	action = 1	current_phase = 1	next_phase = 0	reward = -0.886870	array([[-3.4723167, -1.9277782]], dtype=float32)

time = 13178	action = 0	current_phase = 0	next_phase = 1	reward = -0.050097	array([[-1.2082295, -3.0159404]], dtype=float32)

time = 13183	action = 0	current_phase = 0	next_phase = 1	reward = 0.016169	array([[-1.5446613, -3.0960643]], dtype=float32)

time = 13188	action = 0	current_phase = 0	next_phase = 1	reward = 0.071226	array([[-2.2488496, -2.741853 ]], dtype=float32)

time = 13193	action = 1	current_phase = 0	next_phase = 1	reward = -1.828177	array([[-4.0138464, -3.371712 ]], dtype=float32)

time = 13201	action = 1	current_phase = 1	next_phase = 0	reward = -1.020391	array([[-3.5554256, -2.0470057]], dtype=float32)

time = 13209	action = 0	current_phase = 0	next_phase = 1	reward = -0.045697	array([[-1.1320808, -2.9380882]], dtype=float32)

time = 13214	action = 0	current_phase = 0	next_phase = 1	reward = 0.012232	array([[-1.4650699, -2.9983673]], dtype=float32)

time = 13219	action = 0	current_phase = 0	next_phase = 1	reward = 0.069509	array([[-1.9221649, -2.649319 ]], dtype=float32)

time = 13224	action = 1	current_phase = 0	next_phase = 1	reward = -1.001904	array([[-3.738003, -2.749534]], dtype=float32)

time = 13232	action = 1	current_phase = 1	next_phase = 0	reward = -0.764086	array([[-3.5229592, -2.1276398]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0724 - val_loss: 0.0386

Epoch 2/50

 - 3s - loss: 0.0768 - val_loss: 0.0365

Epoch 3/50

 - 3s - loss: 0.0692 - val_loss: 0.0452

Epoch 4/50

 - 3s - loss: 0.0632 - val_loss: 0.0415

Epoch 5/50

 - 3s - loss: 0.0686 - val_loss: 0.0542

Epoch 6/50

 - 3s - loss: 0.0988 - val_loss: 0.0584

Epoch 7/50

 - 3s - loss: 0.0571 - val_loss: 0.0487

Epoch 8/50

 - 3s - loss: 0.0568 - val_loss: 0.0548

Epoch 9/50

 - 3s - loss: 0.0558 - val_loss: 0.0523

Epoch 10/50

 - 3s - loss: 0.0597 - val_loss: 0.0557

Epoch 11/50

 - 3s - loss: 0.0626 - val_loss: 0.0560

Epoch 12/50

 - 3s - loss: 0.0573 - val_loss: 0.0545

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 678, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 666, after forget

time = 13240	action = 0	current_phase = 0	next_phase = 1	reward = -0.028320	array([[-1.3318028, -2.9329545]], dtype=float32)

time = 13245	action = 0	current_phase = 0	next_phase = 1	reward = 0.039419	array([[-1.8337319, -2.97447  ]], dtype=float32)

time = 13250	action = 1	current_phase = 0	next_phase = 1	reward = -1.251203	array([[-3.0085685, -2.8533065]], dtype=float32)

time = 13258	action = 1	current_phase = 1	next_phase = 0	reward = -0.729437	array([[-3.2897758, -1.9733005]], dtype=float32)

time = 13266	action = 0	current_phase = 0	next_phase = 1	reward = -0.084723	array([[-1.275445, -2.963433]], dtype=float32)

time = 13271	action = 0	current_phase = 0	next_phase = 1	reward = -0.006319	array([[-1.4997973, -2.9356525]], dtype=float32)

time = 13276	action = 0	current_phase = 0	next_phase = 1	reward = 0.052492	array([[-1.9236779, -3.035727 ]], dtype=float32)

time = 13281	action = 1	current_phase = 0	next_phase = 1	reward = -1.567775	array([[-3.669152 , -2.9886382]], dtype=float32)

time = 13289	action = 1	current_phase = 1	next_phase = 0	reward = -0.791803	array([[-3.660895 , -1.9071667]], dtype=float32)

time = 13297	action = 0	current_phase = 0	next_phase = 1	reward = -0.072280	array([[-1.3727883, -3.0528076]], dtype=float32)

time = 13302	action = 0	current_phase = 0	next_phase = 1	reward = -0.004957	array([[-1.4800553, -2.9529884]], dtype=float32)

time = 13307	action = 0	current_phase = 0	next_phase = 1	reward = 0.075639	array([[-1.9267716, -2.9885287]], dtype=float32)

time = 13312	action = 1	current_phase = 0	next_phase = 1	reward = -1.687873	array([[-3.491166 , -3.1619575]], dtype=float32)

time = 13320	action = 1	current_phase = 1	next_phase = 0	reward = -1.305012	array([[-3.6510487, -2.2157068]], dtype=float32)

time = 13328	action = 0	current_phase = 0	next_phase = 1	reward = 0.234201	array([[-1.2140404, -3.004931 ]], dtype=float32)

time = 13333	action = 0	current_phase = 0	next_phase = 1	reward = 0.014967	array([[-1.7966728, -3.1040428]], dtype=float32)

time = 13338	action = 0	current_phase = 0	next_phase = 1	reward = 0.076555	array([[-2.2935524, -3.1575258]], dtype=float32)

time = 13343	action = 1	current_phase = 0	next_phase = 1	reward = -1.874989	array([[-3.9449406, -3.64383  ]], dtype=float32)

time = 13351	action = 1	current_phase = 1	next_phase = 0	reward = -1.701762	array([[-3.5206814, -2.1514254]], dtype=float32)

time = 13359	action = 0	current_phase = 0	next_phase = 1	reward = 0.551919	array([[-1.1820046, -2.9631448]], dtype=float32)

time = 13364	action = 0	current_phase = 0	next_phase = 1	reward = 0.013817	array([[-1.6019802, -3.0284832]], dtype=float32)

time = 13369	action = 0	current_phase = 0	next_phase = 1	reward = 0.070524	array([[-1.8007686, -2.7274268]], dtype=float32)

time = 13374	action = 1	current_phase = 0	next_phase = 1	reward = -0.896523	array([[-3.7455862, -3.056683 ]], dtype=float32)

time = 13382	action = 1	current_phase = 1	next_phase = 0	reward = -0.654977	array([[-3.3887706, -2.1823463]], dtype=float32)

time = 13390	action = 0	current_phase = 0	next_phase = 1	reward = -0.032011	array([[-1.3809068, -2.835323 ]], dtype=float32)

time = 13395	action = 0	current_phase = 0	next_phase = 1	reward = -0.236426	array([[-1.8773117, -3.0341718]], dtype=float32)

time = 13400	action = 1	current_phase = 0	next_phase = 1	reward = -1.023691	array([[-2.7507267, -2.6911216]], dtype=float32)

time = 13408	action = 1	current_phase = 1	next_phase = 0	reward = -0.699363	array([[-3.4701257, -1.9465553]], dtype=float32)

time = 13416	action = 0	current_phase = 0	next_phase = 1	reward = -0.083610	array([[-1.3450048, -3.0274317]], dtype=float32)

time = 13421	action = 0	current_phase = 0	next_phase = 1	reward = -0.013055	array([[-1.4623411, -2.92075  ]], dtype=float32)

time = 13426	action = 0	current_phase = 0	next_phase = 1	reward = 0.062609	array([[-1.8956635, -3.010763 ]], dtype=float32)

time = 13431	action = 1	current_phase = 0	next_phase = 1	reward = -1.489912	array([[-3.286456, -2.945169]], dtype=float32)

time = 13439	action = 1	current_phase = 1	next_phase = 0	reward = -0.797678	array([[-3.600123 , -1.9113617]], dtype=float32)

time = 13447	action = 0	current_phase = 0	next_phase = 1	reward = -0.066245	array([[-1.4128416, -3.0632348]], dtype=float32)

time = 13452	action = 0	current_phase = 0	next_phase = 1	reward = 0.029290	array([[-1.5629902, -3.0151455]], dtype=float32)

time = 13457	action = 0	current_phase = 0	next_phase = 1	reward = 0.085104	array([[-2.0076253, -3.0150392]], dtype=float32)

time = 13462	action = 1	current_phase = 0	next_phase = 1	reward = -1.570836	array([[-3.5878146, -3.3365219]], dtype=float32)

time = 13470	action = 1	current_phase = 1	next_phase = 0	reward = -1.198252	array([[-3.6329875, -2.0333872]], dtype=float32)

time = 13478	action = 0	current_phase = 0	next_phase = 1	reward = 0.234503	array([[-1.3131313, -3.0197277]], dtype=float32)

time = 13483	action = 0	current_phase = 0	next_phase = 1	reward = 0.021235	array([[-1.7256522, -3.091429 ]], dtype=float32)

time = 13488	action = 0	current_phase = 0	next_phase = 1	reward = 0.081265	array([[-2.0932624, -3.0439236]], dtype=float32)

time = 13493	action = 1	current_phase = 0	next_phase = 1	reward = -1.883353	array([[-3.9006767, -3.6940663]], dtype=float32)

time = 13501	action = 1	current_phase = 1	next_phase = 0	reward = -1.317143	array([[-3.5903134, -2.2583656]], dtype=float32)

time = 13509	action = 0	current_phase = 0	next_phase = 1	reward = 0.272317	array([[-1.2852138, -2.8606193]], dtype=float32)

time = 13514	action = 0	current_phase = 0	next_phase = 1	reward = 0.043506	array([[-1.6010135, -2.9207125]], dtype=float32)

time = 13519	action = 0	current_phase = 0	next_phase = 1	reward = 0.053166	array([[-2.053149 , -2.4839041]], dtype=float32)

time = 13524	action = 1	current_phase = 0	next_phase = 1	reward = -1.249758	array([[-3.723646 , -3.1237178]], dtype=float32)

time = 13532	action = 1	current_phase = 1	next_phase = 0	reward = -1.041642	array([[-3.6706216, -2.3155446]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0941 - val_loss: 0.0434

Epoch 2/50

 - 3s - loss: 0.0694 - val_loss: 0.0352

Epoch 3/50

 - 3s - loss: 0.0657 - val_loss: 0.0374

Epoch 4/50

 - 3s - loss: 0.0826 - val_loss: 0.0401

Epoch 5/50

 - 3s - loss: 0.0667 - val_loss: 0.0536

Epoch 6/50

 - 3s - loss: 0.0659 - val_loss: 0.0623

Epoch 7/50

 - 3s - loss: 0.0618 - val_loss: 0.0552

Epoch 8/50

 - 3s - loss: 0.0596 - val_loss: 0.0415

Epoch 9/50

 - 3s - loss: 0.0627 - val_loss: 0.0476

Epoch 10/50

 - 3s - loss: 0.0656 - val_loss: 0.0489

Epoch 11/50

 - 3s - loss: 0.0621 - val_loss: 0.0440

Epoch 12/50

 - 3s - loss: 0.0647 - val_loss: 0.0566

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 688, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 676, after forget

time = 13540	action = 0	current_phase = 0	next_phase = 1	reward = 0.260425	array([[-1.3452307, -2.887968 ]], dtype=float32)

time = 13545	action = 0	current_phase = 0	next_phase = 1	reward = 0.035225	array([[-1.7454485, -2.9896655]], dtype=float32)

time = 13550	action = 1	current_phase = 0	next_phase = 1	reward = -1.319069	array([[-3.0918279, -2.7652235]], dtype=float32)

time = 13558	action = 1	current_phase = 1	next_phase = 0	reward = -0.709733	array([[-3.4016943, -1.9671814]], dtype=float32)

time = 13566	action = 0	current_phase = 0	next_phase = 1	reward = -0.082411	array([[-1.4117157, -3.0268323]], dtype=float32)

time = 13571	action = 0	current_phase = 0	next_phase = 1	reward = -0.005348	array([[-1.4487195, -2.9689531]], dtype=float32)

time = 13576	action = 0	current_phase = 0	next_phase = 1	reward = 0.058924	array([[-1.9541932, -3.0449693]], dtype=float32)

time = 13581	action = 1	current_phase = 0	next_phase = 1	reward = -1.502873	array([[-3.3921378, -3.0667377]], dtype=float32)

time = 13589	action = 1	current_phase = 1	next_phase = 0	reward = -0.760422	array([[-3.617254 , -1.8847731]], dtype=float32)

time = 13597	action = 0	current_phase = 0	next_phase = 1	reward = -0.062257	array([[-1.3754961, -3.1357949]], dtype=float32)

time = 13602	action = 0	current_phase = 0	next_phase = 1	reward = 0.013008	array([[-1.6296892, -2.9703963]], dtype=float32)

time = 13607	action = 0	current_phase = 0	next_phase = 1	reward = 0.068223	array([[-2.0663457, -3.0353625]], dtype=float32)

time = 13612	action = 1	current_phase = 0	next_phase = 1	reward = -1.623589	array([[-3.566544, -3.046169]], dtype=float32)

time = 13620	action = 1	current_phase = 1	next_phase = 0	reward = -0.972789	array([[-3.655409 , -2.0079775]], dtype=float32)

time = 13628	action = 0	current_phase = 0	next_phase = 1	reward = -0.049759	array([[-1.2547059, -3.2986042]], dtype=float32)

time = 13633	action = 0	current_phase = 0	next_phase = 1	reward = 0.032592	array([[-1.8073273, -3.072231 ]], dtype=float32)

time = 13638	action = 0	current_phase = 0	next_phase = 1	reward = 0.079307	array([[-2.2511373, -2.9816232]], dtype=float32)

time = 13643	action = 1	current_phase = 0	next_phase = 1	reward = -0.782501	array([[-4.0321856, -3.5605707]], dtype=float32)

time = 13651	action = 1	current_phase = 1	next_phase = 0	reward = -1.232869	array([[-3.2441502, -2.1966214]], dtype=float32)

time = 13659	action = 0	current_phase = 0	next_phase = 1	reward = -0.038544	array([[-1.3487514, -2.9814887]], dtype=float32)

time = 13664	action = 0	current_phase = 0	next_phase = 1	reward = 0.021570	array([[-1.3859618, -2.9534633]], dtype=float32)

time = 13669	action = 0	current_phase = 0	next_phase = 1	reward = 0.040182	array([[-2.3219838, -2.3698626]], dtype=float32)

time = 13674	action = 1	current_phase = 0	next_phase = 1	reward = -1.090140	array([[-3.4140038, -2.9254363]], dtype=float32)

time = 13682	action = 1	current_phase = 1	next_phase = 0	reward = -1.243994	array([[-3.2613304, -2.0931087]], dtype=float32)

time = 13690	action = 0	current_phase = 0	next_phase = 1	reward = 0.271948	array([[-1.3052564, -2.9519582]], dtype=float32)

time = 13695	action = 0	current_phase = 0	next_phase = 1	reward = 0.341140	array([[-1.8594224, -2.9807336]], dtype=float32)

time = 13700	action = 1	current_phase = 0	next_phase = 1	reward = -1.255533	array([[-2.6166918, -2.509381 ]], dtype=float32)

time = 13708	action = 1	current_phase = 1	next_phase = 0	reward = -1.002802	array([[-3.3885236, -1.9277507]], dtype=float32)

time = 13716	action = 0	current_phase = 0	next_phase = 1	reward = 0.205823	array([[-1.3719058, -3.0380392]], dtype=float32)

time = 13721	action = 0	current_phase = 0	next_phase = 1	reward = 0.002859	array([[-1.5016762, -2.948826 ]], dtype=float32)

time = 13726	action = 0	current_phase = 0	next_phase = 1	reward = 0.069478	array([[-1.9257863, -3.0127983]], dtype=float32)

time = 13731	action = 1	current_phase = 0	next_phase = 1	reward = -1.514499	array([[-3.3552907, -2.8832502]], dtype=float32)

time = 13739	action = 1	current_phase = 1	next_phase = 0	reward = -0.880259	array([[-3.685982 , -1.9462702]], dtype=float32)

time = 13747	action = 0	current_phase = 0	next_phase = 1	reward = -0.051534	array([[-1.3080233, -3.1051936]], dtype=float32)

time = 13752	action = 0	current_phase = 0	next_phase = 1	reward = 0.019801	array([[-1.5637578, -3.009075 ]], dtype=float32)

time = 13757	action = 0	current_phase = 0	next_phase = 1	reward = 0.059407	array([[-2.1899562, -3.0038252]], dtype=float32)

time = 13762	action = 1	current_phase = 0	next_phase = 1	reward = -1.665534	array([[-3.6058023, -3.323868 ]], dtype=float32)

time = 13770	action = 1	current_phase = 1	next_phase = 0	reward = -1.356507	array([[-3.5248723, -1.9244212]], dtype=float32)

time = 13778	action = 0	current_phase = 0	next_phase = 1	reward = 0.236771	array([[-1.261694 , -3.0526776]], dtype=float32)

time = 13783	action = 0	current_phase = 0	next_phase = 1	reward = 0.016604	array([[-1.6724713, -2.9973507]], dtype=float32)

time = 13788	action = 0	current_phase = 0	next_phase = 1	reward = 0.075747	array([[-2.5061855, -3.0786035]], dtype=float32)

time = 13793	action = 1	current_phase = 0	next_phase = 1	reward = -1.853852	array([[-3.8141327, -3.5284393]], dtype=float32)

time = 13801	action = 1	current_phase = 1	next_phase = 0	reward = -1.345390	array([[-3.5912144, -2.1131778]], dtype=float32)

time = 13809	action = 0	current_phase = 0	next_phase = 1	reward = 0.254300	array([[-1.2143757, -3.0466743]], dtype=float32)

time = 13814	action = 0	current_phase = 0	next_phase = 1	reward = 0.027995	array([[-1.61112 , -2.993457]], dtype=float32)

time = 13819	action = 0	current_phase = 0	next_phase = 1	reward = 0.075783	array([[-2.0184112, -2.5296416]], dtype=float32)

time = 13824	action = 1	current_phase = 0	next_phase = 1	reward = -1.120299	array([[-3.966731 , -3.1228673]], dtype=float32)

time = 13832	action = 1	current_phase = 1	next_phase = 0	reward = -0.746791	array([[-3.5926216, -2.0059328]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.1174 - val_loss: 0.0629

Epoch 2/50

 - 3s - loss: 0.1182 - val_loss: 0.0668

Epoch 3/50

 - 3s - loss: 0.1113 - val_loss: 0.0650

Epoch 4/50

 - 3s - loss: 0.0898 - val_loss: 0.0755

Epoch 5/50

 - 3s - loss: 0.0878 - val_loss: 0.0632

Epoch 6/50

 - 3s - loss: 0.0917 - val_loss: 0.0690

Epoch 7/50

 - 3s - loss: 0.0745 - val_loss: 0.0672

Epoch 8/50

 - 3s - loss: 0.0736 - val_loss: 0.0610

Epoch 9/50

 - 3s - loss: 0.0781 - val_loss: 0.0630

Epoch 10/50

 - 3s - loss: 0.0687 - val_loss: 0.0753

Epoch 11/50

 - 3s - loss: 0.0807 - val_loss: 0.0599

Epoch 12/50

 - 3s - loss: 0.0815 - val_loss: 0.0602

Epoch 13/50

 - 3s - loss: 0.0834 - val_loss: 0.0685

Epoch 14/50

 - 3s - loss: 0.0898 - val_loss: 0.0603

Epoch 15/50

 - 3s - loss: 0.0758 - val_loss: 0.0574

Epoch 16/50

 - 3s - loss: 0.0939 - val_loss: 0.0671

Epoch 17/50

 - 3s - loss: 0.0688 - val_loss: 0.0715

Epoch 18/50

 - 3s - loss: 0.0802 - val_loss: 0.0584

Epoch 19/50

 - 3s - loss: 0.0736 - val_loss: 0.0675

Epoch 20/50

 - 3s - loss: 0.0846 - val_loss: 0.0651

Epoch 21/50

 - 3s - loss: 0.0658 - val_loss: 0.0726

Epoch 22/50

 - 3s - loss: 0.0824 - val_loss: 0.0624

Epoch 23/50

 - 3s - loss: 0.0720 - val_loss: 0.0850

Epoch 24/50

 - 3s - loss: 0.0629 - val_loss: 0.0659

Epoch 25/50

 - 4s - loss: 0.0689 - val_loss: 0.0714

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 698, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 686, after forget

time = 13840	action = 0	current_phase = 0	next_phase = 1	reward = -0.018347	array([[-1.3389387, -2.9082804]], dtype=float32)

time = 13845	action = 0	current_phase = 0	next_phase = 1	reward = 0.045847	array([[-2.2386508, -3.0652893]], dtype=float32)

time = 13850	action = 1	current_phase = 0	next_phase = 1	reward = -1.440823	array([[-2.9151545, -2.666536 ]], dtype=float32)

time = 13858	action = 1	current_phase = 1	next_phase = 0	reward = -0.727837	array([[-3.6667194, -1.9592952]], dtype=float32)

time = 13866	action = 0	current_phase = 0	next_phase = 1	reward = -0.072681	array([[-1.2720159, -2.999453 ]], dtype=float32)

time = 13871	action = 0	current_phase = 0	next_phase = 1	reward = 0.004398	array([[-1.5929201, -2.957476 ]], dtype=float32)

time = 13876	action = 0	current_phase = 0	next_phase = 1	reward = 0.065309	array([[-2.3172226, -3.0508106]], dtype=float32)

time = 13881	action = 1	current_phase = 0	next_phase = 1	reward = -1.444110	array([[-3.637267 , -3.0205688]], dtype=float32)

time = 13889	action = 1	current_phase = 1	next_phase = 0	reward = -0.790349	array([[-3.7089698, -2.0176153]], dtype=float32)

time = 13897	action = 0	current_phase = 0	next_phase = 1	reward = -0.084343	array([[-1.3139939, -3.0143669]], dtype=float32)

time = 13902	action = 0	current_phase = 0	next_phase = 1	reward = 0.009007	array([[-1.5763798, -2.959247 ]], dtype=float32)

time = 13907	action = 0	current_phase = 0	next_phase = 1	reward = 0.066306	array([[-2.2053013, -3.0430617]], dtype=float32)

time = 13912	action = 1	current_phase = 0	next_phase = 1	reward = -1.516921	array([[-3.7061212, -3.074648 ]], dtype=float32)

time = 13920	action = 1	current_phase = 1	next_phase = 0	reward = -0.996035	array([[-3.720544 , -2.1171417]], dtype=float32)

time = 13928	action = 0	current_phase = 0	next_phase = 1	reward = -0.057653	array([[-1.1941403, -3.0197175]], dtype=float32)

time = 13933	action = 0	current_phase = 0	next_phase = 1	reward = 0.028372	array([[-1.9845341, -3.1214514]], dtype=float32)

time = 13938	action = 0	current_phase = 0	next_phase = 1	reward = 0.079715	array([[-2.6101172, -2.987786 ]], dtype=float32)

time = 13943	action = 1	current_phase = 0	next_phase = 1	reward = -1.875224	array([[-3.9571664, -3.4950666]], dtype=float32)

time = 13951	action = 1	current_phase = 1	next_phase = 0	reward = -1.351600	array([[-3.5048096, -2.1091318]], dtype=float32)

time = 13959	action = 0	current_phase = 0	next_phase = 1	reward = 0.250801	array([[-1.164756, -2.943808]], dtype=float32)

time = 13964	action = 0	current_phase = 0	next_phase = 1	reward = 0.025226	array([[-1.6292996, -2.9505994]], dtype=float32)

time = 13969	action = 0	current_phase = 0	next_phase = 1	reward = 0.068902	array([[-2.3255227, -2.8433793]], dtype=float32)

time = 13974	action = 1	current_phase = 0	next_phase = 1	reward = -0.997051	array([[-3.7475026, -2.877997 ]], dtype=float32)

time = 13982	action = 1	current_phase = 1	next_phase = 0	reward = -1.332614	array([[-3.341783, -2.118504]], dtype=float32)

time = 13990	action = 0	current_phase = 0	next_phase = 1	reward = -0.018956	array([[-1.223102 , -2.9281075]], dtype=float32)

time = 13995	action = 0	current_phase = 0	next_phase = 1	reward = 0.607072	array([[-1.9021782, -2.993769 ]], dtype=float32)

time = 14000	action = 1	current_phase = 0	next_phase = 1	reward = -1.270717	array([[-3.1051898, -2.6869826]], dtype=float32)

time = 14008	action = 1	current_phase = 1	next_phase = 0	reward = -0.702481	array([[-3.2205877, -2.0667117]], dtype=float32)

time = 14016	action = 0	current_phase = 0	next_phase = 1	reward = -0.095576	array([[-1.332102 , -2.9796104]], dtype=float32)

time = 14021	action = 0	current_phase = 0	next_phase = 1	reward = -0.002951	array([[-1.7213012, -2.9984617]], dtype=float32)

time = 14026	action = 0	current_phase = 0	next_phase = 1	reward = 0.063569	array([[-2.129703 , -3.0221558]], dtype=float32)

time = 14031	action = 1	current_phase = 0	next_phase = 1	reward = -1.440646	array([[-3.61583  , -2.9511087]], dtype=float32)

time = 14039	action = 1	current_phase = 1	next_phase = 0	reward = -0.775785	array([[-3.6728098, -2.074309 ]], dtype=float32)

time = 14047	action = 0	current_phase = 0	next_phase = 1	reward = -0.066362	array([[-1.2778563, -3.040621 ]], dtype=float32)

time = 14052	action = 0	current_phase = 0	next_phase = 1	reward = -0.000940	array([[-1.7622157, -3.0384817]], dtype=float32)

time = 14057	action = 0	current_phase = 0	next_phase = 1	reward = 0.086676	array([[-2.2522924, -3.0214622]], dtype=float32)

time = 14062	action = 1	current_phase = 0	next_phase = 1	reward = -1.717888	array([[-3.6660762, -3.249794 ]], dtype=float32)

time = 14070	action = 1	current_phase = 1	next_phase = 0	reward = -0.956499	array([[-3.647281 , -2.2024693]], dtype=float32)

time = 14078	action = 0	current_phase = 0	next_phase = 1	reward = -0.058766	array([[-1.3076601, -3.0080996]], dtype=float32)

time = 14083	action = 0	current_phase = 0	next_phase = 1	reward = 0.014530	array([[-1.7778311, -3.0576355]], dtype=float32)

time = 14088	action = 0	current_phase = 0	next_phase = 1	reward = 0.070142	array([[-3.0993328, -3.2585163]], dtype=float32)

time = 14093	action = 1	current_phase = 0	next_phase = 1	reward = -1.927215	array([[-3.8289232, -3.3972101]], dtype=float32)

time = 14101	action = 1	current_phase = 1	next_phase = 0	reward = -1.001092	array([[-3.6439316, -2.5586996]], dtype=float32)

time = 14109	action = 0	current_phase = 0	next_phase = 1	reward = -0.050402	array([[-1.1464715, -2.975211 ]], dtype=float32)

time = 14114	action = 0	current_phase = 0	next_phase = 1	reward = 0.003653	array([[-1.8102791, -2.9404535]], dtype=float32)

time = 14119	action = 0	current_phase = 0	next_phase = 1	reward = 0.072725	array([[-2.7156851, -2.761172 ]], dtype=float32)

time = 14124	action = 1	current_phase = 0	next_phase = 1	reward = -1.608749	array([[-3.9839818, -3.0076377]], dtype=float32)

time = 14132	action = 1	current_phase = 1	next_phase = 0	reward = -1.222673	array([[-3.9107394, -2.261338 ]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0844 - val_loss: 0.0309

Epoch 2/50

 - 3s - loss: 0.0812 - val_loss: 0.0333

Epoch 3/50

 - 3s - loss: 0.0802 - val_loss: 0.0317

Epoch 4/50

 - 3s - loss: 0.0741 - val_loss: 0.0468

Epoch 5/50

 - 3s - loss: 0.1010 - val_loss: 0.0378

Epoch 6/50

 - 3s - loss: 0.0741 - val_loss: 0.0373

Epoch 7/50

 - 3s - loss: 0.0749 - val_loss: 0.0388

Epoch 8/50

 - 3s - loss: 0.0752 - val_loss: 0.0386

Epoch 9/50

 - 3s - loss: 0.0693 - val_loss: 0.0357

Epoch 10/50

 - 3s - loss: 0.0651 - val_loss: 0.0445

Epoch 11/50

 - 3s - loss: 0.0702 - val_loss: 0.0405

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 708, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 696, after forget

time = 14140	action = 0	current_phase = 0	next_phase = 1	reward = 0.249798	array([[-1.2349062, -2.804626 ]], dtype=float32)

time = 14145	action = 0	current_phase = 0	next_phase = 1	reward = 0.323192	array([[-1.8186475, -3.008726 ]], dtype=float32)

time = 14150	action = 1	current_phase = 0	next_phase = 1	reward = -1.256772	array([[-3.247208 , -2.8244708]], dtype=float32)

time = 14158	action = 1	current_phase = 1	next_phase = 0	reward = -0.704431	array([[-3.3962617, -1.8844397]], dtype=float32)

time = 14166	action = 0	current_phase = 0	next_phase = 1	reward = -0.089062	array([[-1.2008367, -3.0236602]], dtype=float32)

time = 14171	action = 0	current_phase = 0	next_phase = 1	reward = 0.000871	array([[-1.6287876, -2.9643705]], dtype=float32)

time = 14176	action = 0	current_phase = 0	next_phase = 1	reward = 0.076586	array([[-2.3934078, -3.040372 ]], dtype=float32)

time = 14181	action = 1	current_phase = 0	next_phase = 1	reward = -1.497102	array([[-3.6703997, -3.1015797]], dtype=float32)

time = 14189	action = 1	current_phase = 1	next_phase = 0	reward = -0.830326	array([[-3.6950684, -1.9739716]], dtype=float32)

time = 14197	action = 0	current_phase = 0	next_phase = 1	reward = -0.066065	array([[-1.2108006, -3.0764608]], dtype=float32)

time = 14202	action = 0	current_phase = 0	next_phase = 1	reward = 0.013480	array([[-1.7289424, -2.9949996]], dtype=float32)

time = 14207	action = 0	current_phase = 0	next_phase = 1	reward = 0.075219	array([[-2.5418572, -3.0124369]], dtype=float32)

time = 14212	action = 1	current_phase = 0	next_phase = 1	reward = -1.554300	array([[-3.6906953, -3.226338 ]], dtype=float32)

time = 14220	action = 1	current_phase = 1	next_phase = 0	reward = -1.006115	array([[-3.6859803, -2.0369058]], dtype=float32)

time = 14228	action = 0	current_phase = 0	next_phase = 1	reward = -0.056007	array([[-1.2118839, -3.0064754]], dtype=float32)

time = 14233	action = 0	current_phase = 0	next_phase = 1	reward = 0.024251	array([[-1.8324739, -3.0729146]], dtype=float32)

time = 14238	action = 0	current_phase = 0	next_phase = 1	reward = 0.093029	array([[-2.673686, -3.05024 ]], dtype=float32)

time = 14243	action = 1	current_phase = 0	next_phase = 1	reward = -0.784644	array([[-3.8625765, -3.2609038]], dtype=float32)

time = 14251	action = 1	current_phase = 1	next_phase = 0	reward = -1.310805	array([[-3.2916393, -2.1465502]], dtype=float32)

time = 14259	action = 0	current_phase = 0	next_phase = 1	reward = -0.036031	array([[-1.2481973, -2.9049675]], dtype=float32)

time = 14264	action = 0	current_phase = 0	next_phase = 1	reward = 0.042503	array([[-1.5882725, -2.9641185]], dtype=float32)

time = 14269	action = 0	current_phase = 0	next_phase = 1	reward = 0.060852	array([[-2.3312862, -2.9180074]], dtype=float32)

time = 14274	action = 1	current_phase = 0	next_phase = 1	reward = -1.114698	array([[-3.5862498, -2.8817368]], dtype=float32)

time = 14282	action = 1	current_phase = 1	next_phase = 0	reward = -0.999150	array([[-3.4797437, -1.8893431]], dtype=float32)

time = 14290	action = 0	current_phase = 0	next_phase = 1	reward = 0.245804	array([[-1.2927583, -2.9541576]], dtype=float32)

time = 14295	action = 0	current_phase = 0	next_phase = 1	reward = -0.241571	array([[-2.168266, -2.993097]], dtype=float32)

time = 14300	action = 1	current_phase = 0	next_phase = 1	reward = -0.983237	array([[-3.150652 , -2.7461789]], dtype=float32)

time = 14308	action = 1	current_phase = 1	next_phase = 0	reward = -0.697551	array([[-3.5403697, -1.8618053]], dtype=float32)

time = 14316	action = 0	current_phase = 0	next_phase = 1	reward = -0.089105	array([[-1.2467055, -2.9767041]], dtype=float32)

time = 14321	action = 0	current_phase = 0	next_phase = 1	reward = -0.025041	array([[-1.6637033, -2.958994 ]], dtype=float32)

time = 14326	action = 0	current_phase = 0	next_phase = 1	reward = 0.046167	array([[-2.3176723, -3.0699842]], dtype=float32)

time = 14331	action = 1	current_phase = 0	next_phase = 1	reward = -1.432931	array([[-3.7261982, -2.8901842]], dtype=float32)

time = 14339	action = 1	current_phase = 1	next_phase = 0	reward = -0.754478	array([[-3.702857 , -1.9655161]], dtype=float32)

time = 14347	action = 0	current_phase = 0	next_phase = 1	reward = -0.056056	array([[-1.2952347, -3.044236 ]], dtype=float32)

time = 14352	action = 0	current_phase = 0	next_phase = 1	reward = 0.018307	array([[-1.7167177, -2.9916043]], dtype=float32)

time = 14357	action = 0	current_phase = 0	next_phase = 1	reward = 0.078090	array([[-2.4265623, -3.0307918]], dtype=float32)

time = 14362	action = 1	current_phase = 0	next_phase = 1	reward = -1.606983	array([[-3.7098908, -3.185638 ]], dtype=float32)

time = 14370	action = 1	current_phase = 1	next_phase = 0	reward = -0.958761	array([[-3.6904483, -2.0004618]], dtype=float32)

time = 14378	action = 0	current_phase = 0	next_phase = 1	reward = -0.045591	array([[-1.1704783, -2.9997606]], dtype=float32)

time = 14383	action = 0	current_phase = 0	next_phase = 1	reward = 0.027155	array([[-1.8974307, -3.0963058]], dtype=float32)

time = 14388	action = 0	current_phase = 0	next_phase = 1	reward = 0.074252	array([[-2.610538 , -3.0139475]], dtype=float32)

time = 14393	action = 1	current_phase = 0	next_phase = 1	reward = -1.886792	array([[-3.9670634, -3.4406986]], dtype=float32)

time = 14401	action = 1	current_phase = 1	next_phase = 0	reward = -1.072653	array([[-3.7603838, -2.3118043]], dtype=float32)

time = 14409	action = 0	current_phase = 0	next_phase = 1	reward = -0.024159	array([[-1.1911974, -2.9769883]], dtype=float32)

time = 14414	action = 0	current_phase = 0	next_phase = 1	reward = 0.054425	array([[-1.8139747, -2.9554236]], dtype=float32)

time = 14419	action = 1	current_phase = 0	next_phase = 1	reward = -0.678115	array([[-2.5630798, -2.5254858]], dtype=float32)

time = 14427	action = 1	current_phase = 1	next_phase = 0	reward = -0.658898	array([[-3.457187 , -1.9462129]], dtype=float32)

time = 14435	action = 0	current_phase = 0	next_phase = 1	reward = -0.108594	array([[-1.2706043, -3.0511308]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0667 - val_loss: 0.0403

Epoch 2/50

 - 3s - loss: 0.0824 - val_loss: 0.0456

Epoch 3/50

 - 3s - loss: 0.0693 - val_loss: 0.0484

Epoch 4/50

 - 3s - loss: 0.0802 - val_loss: 0.0458

Epoch 5/50

 - 3s - loss: 0.0837 - val_loss: 0.0530

Epoch 6/50

 - 3s - loss: 0.0693 - val_loss: 0.0485

Epoch 7/50

 - 3s - loss: 0.0644 - val_loss: 0.0560

Epoch 8/50

 - 3s - loss: 0.0580 - val_loss: 0.0649

Epoch 9/50

 - 3s - loss: 0.0656 - val_loss: 0.0480

Epoch 10/50

 - 3s - loss: 0.0696 - val_loss: 0.0510

Epoch 11/50

 - 3s - loss: 0.0662 - val_loss: 0.0502

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 718, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 706, after forget

time = 14440	action = 0	current_phase = 0	next_phase = 1	reward = -0.020698	array([[-1.334592 , -2.7883596]], dtype=float32)

time = 14445	action = 0	current_phase = 0	next_phase = 1	reward = 0.055817	array([[-2.1185246, -3.0080118]], dtype=float32)

time = 14450	action = 1	current_phase = 0	next_phase = 1	reward = -1.320533	array([[-3.4628155, -2.7843032]], dtype=float32)

time = 14458	action = 1	current_phase = 1	next_phase = 0	reward = -0.708190	array([[-3.6910331, -1.9206957]], dtype=float32)

time = 14466	action = 0	current_phase = 0	next_phase = 1	reward = -0.071295	array([[-1.1643395, -2.9725122]], dtype=float32)

time = 14471	action = 0	current_phase = 0	next_phase = 1	reward = 0.002717	array([[-1.4006859, -2.909289 ]], dtype=float32)

time = 14476	action = 0	current_phase = 0	next_phase = 1	reward = 0.063054	array([[-2.172796, -3.017994]], dtype=float32)

time = 14481	action = 1	current_phase = 0	next_phase = 1	reward = -1.506681	array([[-3.726682 , -3.0205426]], dtype=float32)

time = 14489	action = 1	current_phase = 1	next_phase = 0	reward = -0.785962	array([[-3.7484732, -2.0248528]], dtype=float32)

time = 14497	action = 0	current_phase = 0	next_phase = 1	reward = -0.091082	array([[-1.1324096, -2.9766788]], dtype=float32)

time = 14502	action = 0	current_phase = 0	next_phase = 1	reward = -0.008973	array([[-1.4750235, -2.9311447]], dtype=float32)

time = 14507	action = 0	current_phase = 0	next_phase = 1	reward = 0.054676	array([[-2.2007403, -3.0237474]], dtype=float32)

time = 14512	action = 1	current_phase = 0	next_phase = 1	reward = -1.622079	array([[-3.590206 , -3.0886059]], dtype=float32)

time = 14520	action = 1	current_phase = 1	next_phase = 0	reward = -0.911367	array([[-3.622805 , -2.0260997]], dtype=float32)

time = 14528	action = 0	current_phase = 0	next_phase = 1	reward = -0.048241	array([[-1.08908  , -2.9979095]], dtype=float32)

time = 14533	action = 0	current_phase = 0	next_phase = 1	reward = 0.028888	array([[-1.6551039, -3.038261 ]], dtype=float32)

time = 14538	action = 0	current_phase = 0	next_phase = 1	reward = 0.080090	array([[-2.4183958, -3.0561934]], dtype=float32)

time = 14543	action = 1	current_phase = 0	next_phase = 1	reward = -0.775774	array([[-3.7958922, -3.2006235]], dtype=float32)

time = 14551	action = 1	current_phase = 1	next_phase = 0	reward = -1.299406	array([[-3.252556 , -2.1062353]], dtype=float32)

time = 14559	action = 0	current_phase = 0	next_phase = 1	reward = -0.040351	array([[-1.1448848, -2.9369488]], dtype=float32)

time = 14564	action = 0	current_phase = 0	next_phase = 1	reward = 0.035875	array([[-1.4398988, -2.9240403]], dtype=float32)

time = 14569	action = 0	current_phase = 0	next_phase = 1	reward = 0.061350	array([[-2.1309526, -2.4310684]], dtype=float32)

time = 14574	action = 1	current_phase = 0	next_phase = 1	reward = -1.007428	array([[-3.4005144, -2.7489996]], dtype=float32)

time = 14582	action = 1	current_phase = 1	next_phase = 0	reward = -0.759161	array([[-3.4021707, -1.9112942]], dtype=float32)

time = 14590	action = 0	current_phase = 0	next_phase = 1	reward = -0.022790	array([[-1.239484 , -2.8715835]], dtype=float32)

time = 14595	action = 0	current_phase = 0	next_phase = 1	reward = 0.054561	array([[-2.0645158, -2.9358602]], dtype=float32)

time = 14600	action = 1	current_phase = 0	next_phase = 1	reward = -1.324032	array([[-2.9073937, -2.7161207]], dtype=float32)

time = 14608	action = 1	current_phase = 1	next_phase = 0	reward = -0.705528	array([[-3.543114 , -1.8046322]], dtype=float32)

time = 14616	action = 0	current_phase = 0	next_phase = 1	reward = -0.071533	array([[-1.1568916, -2.900837 ]], dtype=float32)

time = 14621	action = 0	current_phase = 0	next_phase = 1	reward = 0.007656	array([[-1.4445506, -2.892848 ]], dtype=float32)

time = 14626	action = 0	current_phase = 0	next_phase = 1	reward = 0.062441	array([[-2.127838, -3.008576]], dtype=float32)

time = 14631	action = 1	current_phase = 0	next_phase = 1	reward = -1.490788	array([[-3.5428977, -2.9906893]], dtype=float32)

time = 14639	action = 1	current_phase = 1	next_phase = 0	reward = -0.792840	array([[-3.7583947, -2.0598094]], dtype=float32)

time = 14647	action = 0	current_phase = 0	next_phase = 1	reward = -0.071952	array([[-1.2194369, -2.9855337]], dtype=float32)

time = 14652	action = 0	current_phase = 0	next_phase = 1	reward = 0.005665	array([[-1.4550717, -2.9267821]], dtype=float32)

time = 14657	action = 0	current_phase = 0	next_phase = 1	reward = 0.073475	array([[-2.377223, -3.027442]], dtype=float32)

time = 14662	action = 1	current_phase = 0	next_phase = 1	reward = -1.583473	array([[-3.6568747, -3.1137013]], dtype=float32)

time = 14670	action = 1	current_phase = 1	next_phase = 0	reward = -0.894035	array([[-3.6649485, -2.0489416]], dtype=float32)

time = 14678	action = 0	current_phase = 0	next_phase = 1	reward = -0.042142	array([[-1.055176 , -3.1033144]], dtype=float32)

time = 14683	action = 0	current_phase = 0	next_phase = 1	reward = 0.026993	array([[-1.7981154, -3.0238466]], dtype=float32)

time = 14688	action = 0	current_phase = 0	next_phase = 1	reward = 0.068566	array([[-2.4029179, -3.0052981]], dtype=float32)

time = 14693	action = 1	current_phase = 0	next_phase = 1	reward = -1.342398	array([[-3.840374 , -3.1865258]], dtype=float32)

time = 14701	action = 1	current_phase = 1	next_phase = 0	reward = -1.432155	array([[-3.4042192, -2.2365496]], dtype=float32)

time = 14709	action = 0	current_phase = 0	next_phase = 1	reward = 0.265367	array([[-1.2141534, -2.88689  ]], dtype=float32)

time = 14714	action = 0	current_phase = 0	next_phase = 1	reward = 0.042614	array([[-1.5817835, -2.9173846]], dtype=float32)

time = 14719	action = 0	current_phase = 0	next_phase = 1	reward = 0.054969	array([[-2.416905 , -2.6581407]], dtype=float32)

time = 14724	action = 1	current_phase = 0	next_phase = 1	reward = -1.153443	array([[-3.4265385, -2.7038093]], dtype=float32)

time = 14732	action = 1	current_phase = 1	next_phase = 0	reward = -1.027802	array([[-3.3248148, -1.9744496]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0855 - val_loss: 0.0304

Epoch 2/50

 - 3s - loss: 0.0875 - val_loss: 0.0310

Epoch 3/50

 - 3s - loss: 0.0815 - val_loss: 0.0458

Epoch 4/50

 - 3s - loss: 0.0898 - val_loss: 0.0351

Epoch 5/50

 - 3s - loss: 0.0746 - val_loss: 0.0385

Epoch 6/50

 - 3s - loss: 0.0727 - val_loss: 0.0345

Epoch 7/50

 - 3s - loss: 0.0755 - val_loss: 0.0368

Epoch 8/50

 - 3s - loss: 0.0730 - val_loss: 0.0366

Epoch 9/50

 - 3s - loss: 0.0704 - val_loss: 0.0354

Epoch 10/50

 - 4s - loss: 0.0737 - val_loss: 0.0319

Epoch 11/50

 - 4s - loss: 0.0767 - val_loss: 0.0341

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 728, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 716, after forget

time = 14740	action = 0	current_phase = 0	next_phase = 1	reward = 0.269340	array([[-1.3650737, -2.9174707]], dtype=float32)

time = 14745	action = 0	current_phase = 0	next_phase = 1	reward = -0.224479	array([[-2.0997365, -2.9545386]], dtype=float32)

time = 14750	action = 1	current_phase = 0	next_phase = 1	reward = -1.044518	array([[-3.145736 , -2.8274288]], dtype=float32)

time = 14758	action = 1	current_phase = 1	next_phase = 0	reward = -0.716679	array([[-3.570136 , -1.9285746]], dtype=float32)

time = 14766	action = 0	current_phase = 0	next_phase = 1	reward = -0.091401	array([[-1.2278745, -2.94261  ]], dtype=float32)

time = 14771	action = 0	current_phase = 0	next_phase = 1	reward = -0.011091	array([[-1.5062108, -2.897713 ]], dtype=float32)

time = 14776	action = 0	current_phase = 0	next_phase = 1	reward = 0.062470	array([[-2.1809862, -3.0167682]], dtype=float32)

time = 14781	action = 1	current_phase = 0	next_phase = 1	reward = -1.487047	array([[-3.8930268, -2.962083 ]], dtype=float32)

time = 14789	action = 1	current_phase = 1	next_phase = 0	reward = -0.776491	array([[-3.678188 , -1.8948964]], dtype=float32)

time = 14797	action = 0	current_phase = 0	next_phase = 1	reward = -0.057319	array([[-1.2114779, -3.0120742]], dtype=float32)

time = 14802	action = 0	current_phase = 0	next_phase = 1	reward = 0.016820	array([[-1.5659443, -2.9894602]], dtype=float32)

time = 14807	action = 0	current_phase = 0	next_phase = 1	reward = 0.070115	array([[-2.315732, -3.032634]], dtype=float32)

time = 14812	action = 1	current_phase = 0	next_phase = 1	reward = -1.673152	array([[-3.7466946, -3.206793 ]], dtype=float32)

time = 14820	action = 1	current_phase = 1	next_phase = 0	reward = -0.905767	array([[-3.5803614, -1.9266391]], dtype=float32)

time = 14828	action = 0	current_phase = 0	next_phase = 1	reward = -0.084532	array([[-1.1019007, -3.0403254]], dtype=float32)

time = 14833	action = 0	current_phase = 0	next_phase = 1	reward = -0.011546	array([[-1.644129 , -3.0198362]], dtype=float32)

time = 14838	action = 0	current_phase = 0	next_phase = 1	reward = 0.068955	array([[-2.8766131, -3.1178277]], dtype=float32)

time = 14843	action = 1	current_phase = 0	next_phase = 1	reward = -1.847678	array([[-3.926523, -3.322164]], dtype=float32)

time = 14851	action = 1	current_phase = 1	next_phase = 0	reward = -1.326772	array([[-3.969162 , -2.3817258]], dtype=float32)

time = 14859	action = 0	current_phase = 0	next_phase = 1	reward = 0.271041	array([[-1.1795256, -2.9020562]], dtype=float32)

time = 14864	action = 0	current_phase = 0	next_phase = 1	reward = 0.039271	array([[-1.5839682, -2.9053385]], dtype=float32)

time = 14869	action = 1	current_phase = 0	next_phase = 1	reward = -0.704666	array([[-2.6303406, -2.5532627]], dtype=float32)

time = 14877	action = 1	current_phase = 1	next_phase = 0	reward = -0.939925	array([[-3.048682 , -1.8601463]], dtype=float32)

time = 14885	action = 0	current_phase = 0	next_phase = 1	reward = 0.165569	array([[-1.2517909, -2.9623523]], dtype=float32)

time = 14890	action = 0	current_phase = 0	next_phase = 1	reward = -0.307585	array([[-1.3688945, -2.8637207]], dtype=float32)

time = 14895	action = 0	current_phase = 0	next_phase = 1	reward = 0.318130	array([[-1.9493937, -2.9220126]], dtype=float32)

time = 14900	action = 1	current_phase = 0	next_phase = 1	reward = -1.203194	array([[-3.1179204, -2.7484612]], dtype=float32)

time = 14908	action = 1	current_phase = 1	next_phase = 0	reward = -0.693363	array([[-3.445806 , -1.8113694]], dtype=float32)

time = 14916	action = 0	current_phase = 0	next_phase = 1	reward = -0.087468	array([[-1.2561256, -2.9616988]], dtype=float32)

time = 14921	action = 0	current_phase = 0	next_phase = 1	reward = -0.017001	array([[-1.5459278, -2.9089777]], dtype=float32)

time = 14926	action = 0	current_phase = 0	next_phase = 1	reward = 0.045133	array([[-2.097431 , -3.0129461]], dtype=float32)

time = 14931	action = 1	current_phase = 0	next_phase = 1	reward = -1.496339	array([[-3.6182234, -2.9511342]], dtype=float32)

time = 14939	action = 1	current_phase = 1	next_phase = 0	reward = -0.768933	array([[-3.6617794, -1.7594476]], dtype=float32)

time = 14947	action = 0	current_phase = 0	next_phase = 1	reward = -0.069826	array([[-1.2249458, -2.9906242]], dtype=float32)

time = 14952	action = 0	current_phase = 0	next_phase = 1	reward = 0.008324	array([[-1.5737724, -2.9422038]], dtype=float32)

time = 14957	action = 0	current_phase = 0	next_phase = 1	reward = 0.073292	array([[-2.4392295, -3.0119362]], dtype=float32)

time = 14962	action = 1	current_phase = 0	next_phase = 1	reward = -1.652258	array([[-3.709091, -3.145925]], dtype=float32)

time = 14970	action = 1	current_phase = 1	next_phase = 0	reward = -0.945462	array([[-3.6132655, -2.0123098]], dtype=float32)

time = 14978	action = 0	current_phase = 0	next_phase = 1	reward = -0.045716	array([[-1.1705576, -2.9704964]], dtype=float32)

time = 14983	action = 0	current_phase = 0	next_phase = 1	reward = 0.031260	array([[-1.6308115, -3.057448 ]], dtype=float32)

time = 14988	action = 1	current_phase = 0	next_phase = 1	reward = -1.838539	array([[-2.9891002, -2.975814 ]], dtype=float32)

time = 14996	action = 1	current_phase = 1	next_phase = 0	reward = -0.831204	array([[-3.5954561, -1.8396332]], dtype=float32)

time = 15004	action = 0	current_phase = 0	next_phase = 1	reward = -0.109540	array([[-1.2548361, -3.0094028]], dtype=float32)

time = 15009	action = 0	current_phase = 0	next_phase = 1	reward = -0.035933	array([[-0.99507594, -2.824143  ]], dtype=float32)

time = 15014	action = 0	current_phase = 0	next_phase = 1	reward = 0.033045	array([[-1.8099992, -2.9504564]], dtype=float32)

time = 15019	action = 1	current_phase = 0	next_phase = 1	reward = -0.661499	array([[-2.7200823, -2.4530954]], dtype=float32)

time = 15027	action = 1	current_phase = 1	next_phase = 0	reward = -1.144309	array([[-3.5186915, -2.1146746]], dtype=float32)

time = 15035	action = 0	current_phase = 0	next_phase = 1	reward = 0.465454	array([[-1.2463474, -2.9653556]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 4s - loss: 0.0920 - val_loss: 0.0320

Epoch 2/50

 - 4s - loss: 0.0762 - val_loss: 0.0478

Epoch 3/50

 - 3s - loss: 0.0794 - val_loss: 0.0493

Epoch 4/50

 - 4s - loss: 0.0738 - val_loss: 0.0440

Epoch 5/50

 - 4s - loss: 0.0815 - val_loss: 0.0393

Epoch 6/50

 - 3s - loss: 0.0827 - val_loss: 0.0389

Epoch 7/50

 - 4s - loss: 0.0662 - val_loss: 0.0425

Epoch 8/50

 - 3s - loss: 0.0766 - val_loss: 0.0494

Epoch 9/50

 - 3s - loss: 0.0985 - val_loss: 0.0365

Epoch 10/50

 - 3s - loss: 0.1035 - val_loss: 0.0439

Epoch 11/50

 - 3s - loss: 0.0809 - val_loss: 0.0466

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 738, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 726, after forget

time = 15040	action = 0	current_phase = 0	next_phase = 1	reward = -0.288636	array([[-1.5328069, -2.7384765]], dtype=float32)

time = 15045	action = 0	current_phase = 0	next_phase = 1	reward = 0.344935	array([[-2.184647, -3.030949]], dtype=float32)

time = 15050	action = 1	current_phase = 0	next_phase = 1	reward = -1.318581	array([[-3.5587451, -2.955553 ]], dtype=float32)

time = 15058	action = 1	current_phase = 1	next_phase = 0	reward = -0.711130	array([[-3.7681804, -1.8669704]], dtype=float32)

time = 15066	action = 0	current_phase = 0	next_phase = 1	reward = -0.088676	array([[-1.3469136, -3.049047 ]], dtype=float32)

time = 15071	action = 0	current_phase = 0	next_phase = 1	reward = -0.010054	array([[-1.4353771, -2.8911307]], dtype=float32)

time = 15076	action = 0	current_phase = 0	next_phase = 1	reward = 0.046198	array([[-2.1306953, -2.9825563]], dtype=float32)

time = 15081	action = 1	current_phase = 0	next_phase = 1	reward = -1.446930	array([[-3.663613 , -3.0067706]], dtype=float32)

time = 15089	action = 1	current_phase = 1	next_phase = 0	reward = -1.072676	array([[-3.7755976, -1.920008 ]], dtype=float32)

time = 15097	action = 0	current_phase = 0	next_phase = 1	reward = 0.219183	array([[-1.2051173, -3.0226715]], dtype=float32)

time = 15102	action = 0	current_phase = 0	next_phase = 1	reward = -0.000730	array([[-1.5132415, -2.9256988]], dtype=float32)

time = 15107	action = 0	current_phase = 0	next_phase = 1	reward = 0.062688	array([[-2.2346811, -3.0102408]], dtype=float32)

time = 15112	action = 1	current_phase = 0	next_phase = 1	reward = -1.475654	array([[-3.8511689, -3.2575293]], dtype=float32)

time = 15120	action = 1	current_phase = 1	next_phase = 0	reward = -1.288017	array([[-3.8057554, -2.1368313]], dtype=float32)

time = 15128	action = 0	current_phase = 0	next_phase = 1	reward = 0.245443	array([[-1.1278253, -3.0156333]], dtype=float32)

time = 15133	action = 0	current_phase = 0	next_phase = 1	reward = 0.021961	array([[-1.6492991, -3.1478593]], dtype=float32)

time = 15138	action = 0	current_phase = 0	next_phase = 1	reward = 0.071714	array([[-2.4688482, -3.0442245]], dtype=float32)

time = 15143	action = 1	current_phase = 0	next_phase = 1	reward = -2.002803	array([[-4.217449 , -3.5774257]], dtype=float32)

time = 15151	action = 1	current_phase = 1	next_phase = 0	reward = -1.139188	array([[-3.8703969, -2.1613758]], dtype=float32)

time = 15159	action = 0	current_phase = 0	next_phase = 1	reward = -0.030281	array([[-1.2238958, -2.8720086]], dtype=float32)

time = 15164	action = 0	current_phase = 0	next_phase = 1	reward = 0.034114	array([[-1.6228821, -2.9140933]], dtype=float32)

time = 15169	action = 1	current_phase = 0	next_phase = 1	reward = -0.756422	array([[-2.3339262, -2.2779558]], dtype=float32)

time = 15177	action = 1	current_phase = 1	next_phase = 0	reward = -0.654027	array([[-3.5502558, -1.8310766]], dtype=float32)

time = 15185	action = 0	current_phase = 0	next_phase = 1	reward = -0.383009	array([[-1.3169892, -3.0616202]], dtype=float32)

time = 15190	action = 0	current_phase = 0	next_phase = 1	reward = 0.262438	array([[-1.2432516, -2.7270868]], dtype=float32)

time = 15195	action = 0	current_phase = 0	next_phase = 1	reward = 0.044488	array([[-2.0757442, -3.0421755]], dtype=float32)

time = 15200	action = 1	current_phase = 0	next_phase = 1	reward = -1.357713	array([[-3.2660773, -3.0031247]], dtype=float32)

time = 15208	action = 1	current_phase = 1	next_phase = 0	reward = -0.722686	array([[-3.8132925, -1.8524992]], dtype=float32)

time = 15216	action = 0	current_phase = 0	next_phase = 1	reward = -0.076265	array([[-1.2904962, -2.9628685]], dtype=float32)

time = 15221	action = 0	current_phase = 0	next_phase = 1	reward = -0.009794	array([[-1.4017786, -2.8829017]], dtype=float32)

time = 15226	action = 0	current_phase = 0	next_phase = 1	reward = 0.050261	array([[-2.1206384, -3.0161448]], dtype=float32)

time = 15231	action = 1	current_phase = 0	next_phase = 1	reward = -1.559342	array([[-3.792992 , -3.1974535]], dtype=float32)

time = 15239	action = 1	current_phase = 1	next_phase = 0	reward = -1.059052	array([[-3.854615 , -1.9938208]], dtype=float32)

time = 15247	action = 0	current_phase = 0	next_phase = 1	reward = 0.215713	array([[-1.2469289, -3.002742 ]], dtype=float32)

time = 15252	action = 0	current_phase = 0	next_phase = 1	reward = 0.006683	array([[-1.5918477, -2.9054668]], dtype=float32)

time = 15257	action = 0	current_phase = 0	next_phase = 1	reward = 0.072215	array([[-2.3095033, -3.0311344]], dtype=float32)

time = 15262	action = 1	current_phase = 0	next_phase = 1	reward = -1.662593	array([[-3.7967196, -3.1576185]], dtype=float32)

time = 15270	action = 1	current_phase = 1	next_phase = 0	reward = -0.965588	array([[-3.76469  , -2.1381674]], dtype=float32)

time = 15278	action = 0	current_phase = 0	next_phase = 1	reward = -0.051766	array([[-1.150882, -3.002034]], dtype=float32)

time = 15283	action = 0	current_phase = 0	next_phase = 1	reward = 0.015779	array([[-1.6981864, -2.9931443]], dtype=float32)

time = 15288	action = 0	current_phase = 0	next_phase = 1	reward = 0.080941	array([[-2.3771799, -3.0946913]], dtype=float32)

time = 15293	action = 1	current_phase = 0	next_phase = 1	reward = -1.323767	array([[-3.9301803, -3.2895768]], dtype=float32)

time = 15301	action = 1	current_phase = 1	next_phase = 0	reward = -1.433132	array([[-3.7064018, -2.436474 ]], dtype=float32)

time = 15309	action = 0	current_phase = 0	next_phase = 1	reward = 0.252344	array([[-1.1993914, -2.9127476]], dtype=float32)

time = 15314	action = 0	current_phase = 0	next_phase = 1	reward = 0.054202	array([[-1.5683202, -2.9396973]], dtype=float32)

time = 15319	action = 0	current_phase = 0	next_phase = 1	reward = 0.059940	array([[-2.1885607, -2.6832721]], dtype=float32)

time = 15324	action = 1	current_phase = 0	next_phase = 1	reward = -1.027016	array([[-3.552741, -2.946461]], dtype=float32)

time = 15332	action = 1	current_phase = 1	next_phase = 0	reward = -0.737129	array([[-3.4809313, -1.9252542]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0926 - val_loss: 0.0493

Epoch 2/50

 - 3s - loss: 0.1134 - val_loss: 0.0453

Epoch 3/50

 - 3s - loss: 0.1072 - val_loss: 0.0493

Epoch 4/50

 - 3s - loss: 0.0834 - val_loss: 0.0510

Epoch 5/50

 - 3s - loss: 0.0956 - val_loss: 0.0507

Epoch 6/50

 - 3s - loss: 0.0667 - val_loss: 0.0496

Epoch 7/50

 - 4s - loss: 0.0840 - val_loss: 0.0547

Epoch 8/50

 - 3s - loss: 0.0765 - val_loss: 0.0617

Epoch 9/50

 - 4s - loss: 0.0716 - val_loss: 0.0490

Epoch 10/50

 - 4s - loss: 0.0818 - val_loss: 0.0611

Epoch 11/50

 - 4s - loss: 0.0746 - val_loss: 0.0594

Epoch 12/50

 - 4s - loss: 0.0773 - val_loss: 0.0476

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 748, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 736, after forget

time = 15340	action = 0	current_phase = 0	next_phase = 1	reward = -0.293881	array([[-1.5623317, -2.8791237]], dtype=float32)

time = 15345	action = 0	current_phase = 0	next_phase = 1	reward = 0.333291	array([[-2.2343094, -2.9556503]], dtype=float32)

time = 15350	action = 1	current_phase = 0	next_phase = 1	reward = -1.310202	array([[-3.1998067, -2.7574978]], dtype=float32)

time = 15358	action = 1	current_phase = 1	next_phase = 0	reward = -0.654911	array([[-3.4862814, -1.8263501]], dtype=float32)

time = 15366	action = 0	current_phase = 0	next_phase = 1	reward = -0.079700	array([[-1.3650286, -2.9886503]], dtype=float32)

time = 15371	action = 0	current_phase = 0	next_phase = 1	reward = 0.013625	array([[-1.7129968, -2.909758 ]], dtype=float32)

time = 15376	action = 0	current_phase = 0	next_phase = 1	reward = 0.062360	array([[-2.29807  , -3.0161958]], dtype=float32)

time = 15381	action = 1	current_phase = 0	next_phase = 1	reward = -1.510942	array([[-4.0445657, -2.9475856]], dtype=float32)

time = 15389	action = 1	current_phase = 1	next_phase = 0	reward = -0.773207	array([[-3.690656 , -1.8506951]], dtype=float32)

time = 15397	action = 0	current_phase = 0	next_phase = 1	reward = -0.067971	array([[-1.3616085, -2.9518518]], dtype=float32)

time = 15402	action = 0	current_phase = 0	next_phase = 1	reward = 0.007546	array([[-1.9394667, -2.9748259]], dtype=float32)

time = 15407	action = 0	current_phase = 0	next_phase = 1	reward = 0.067296	array([[-2.6726384, -3.0966563]], dtype=float32)

time = 15412	action = 1	current_phase = 0	next_phase = 1	reward = -1.601110	array([[-3.950973 , -3.1712947]], dtype=float32)

time = 15420	action = 1	current_phase = 1	next_phase = 0	reward = -1.040170	array([[-3.7649388, -1.8606277]], dtype=float32)

time = 15428	action = 0	current_phase = 0	next_phase = 1	reward = -0.045670	array([[-1.3520614, -2.9619718]], dtype=float32)

time = 15433	action = 0	current_phase = 0	next_phase = 1	reward = 0.024323	array([[-2.0162654, -3.0929809]], dtype=float32)

time = 15438	action = 0	current_phase = 0	next_phase = 1	reward = 0.071985	array([[-2.8464549, -2.9043808]], dtype=float32)

time = 15443	action = 1	current_phase = 0	next_phase = 1	reward = -2.002474	array([[-4.273401 , -3.3036814]], dtype=float32)

time = 15451	action = 1	current_phase = 1	next_phase = 0	reward = -1.445527	array([[-4.0557375, -2.2073653]], dtype=float32)

time = 15459	action = 0	current_phase = 0	next_phase = 1	reward = 0.270299	array([[-1.337609 , -2.8022184]], dtype=float32)

time = 15464	action = 0	current_phase = 0	next_phase = 1	reward = 0.055654	array([[-1.7331833, -2.9457636]], dtype=float32)

time = 15469	action = 0	current_phase = 0	next_phase = 1	reward = 0.055418	array([[-2.5771732, -2.6270962]], dtype=float32)

time = 15474	action = 1	current_phase = 0	next_phase = 1	reward = -1.083389	array([[-3.8503869, -2.9338503]], dtype=float32)

time = 15482	action = 1	current_phase = 1	next_phase = 0	reward = -0.987520	array([[-3.6410003, -1.9797655]], dtype=float32)

time = 15490	action = 0	current_phase = 0	next_phase = 1	reward = -0.007929	array([[-1.5188155, -2.8342576]], dtype=float32)

time = 15495	action = 0	current_phase = 0	next_phase = 1	reward = 0.330655	array([[-2.1977801, -2.955501 ]], dtype=float32)

time = 15500	action = 1	current_phase = 0	next_phase = 1	reward = -1.318000	array([[-3.1316576, -2.833373 ]], dtype=float32)

time = 15508	action = 1	current_phase = 1	next_phase = 0	reward = -0.704016	array([[-3.65231  , -1.7229222]], dtype=float32)

time = 15516	action = 0	current_phase = 0	next_phase = 1	reward = -0.086719	array([[-1.3672352, -2.9139047]], dtype=float32)

time = 15521	action = 0	current_phase = 0	next_phase = 1	reward = -0.005485	array([[-1.7282355, -2.927401 ]], dtype=float32)

time = 15526	action = 0	current_phase = 0	next_phase = 1	reward = 0.059886	array([[-2.3070664, -3.010448 ]], dtype=float32)

time = 15531	action = 1	current_phase = 0	next_phase = 1	reward = -1.375557	array([[-3.9456985, -3.0042195]], dtype=float32)

time = 15539	action = 1	current_phase = 1	next_phase = 0	reward = -0.769834	array([[-3.7144022, -1.8302544]], dtype=float32)

time = 15547	action = 0	current_phase = 0	next_phase = 1	reward = -0.058282	array([[-1.3841453, -2.9670439]], dtype=float32)

time = 15552	action = 0	current_phase = 0	next_phase = 1	reward = 0.031904	array([[-1.9114025, -3.001974 ]], dtype=float32)

time = 15557	action = 0	current_phase = 0	next_phase = 1	reward = 0.078783	array([[-2.5814865, -3.1167202]], dtype=float32)

time = 15562	action = 1	current_phase = 0	next_phase = 1	reward = -1.672510	array([[-3.9653752, -3.0526724]], dtype=float32)

time = 15570	action = 1	current_phase = 1	next_phase = 0	reward = -0.971563	array([[-3.7899103, -1.9822085]], dtype=float32)

time = 15578	action = 0	current_phase = 0	next_phase = 1	reward = -0.062651	array([[-1.4206028, -2.9310808]], dtype=float32)

time = 15583	action = 0	current_phase = 0	next_phase = 1	reward = 0.027755	array([[-2.0318594, -3.0945473]], dtype=float32)

time = 15588	action = 0	current_phase = 0	next_phase = 1	reward = 0.091760	array([[-2.7711258, -3.2509103]], dtype=float32)

time = 15593	action = 1	current_phase = 0	next_phase = 1	reward = -1.267975	array([[-4.096947, -3.347652]], dtype=float32)

time = 15601	action = 1	current_phase = 1	next_phase = 0	reward = -1.126841	array([[-3.7935638, -2.2768788]], dtype=float32)

time = 15609	action = 0	current_phase = 0	next_phase = 1	reward = -0.007278	array([[-1.3681208, -2.9136515]], dtype=float32)

time = 15614	action = 0	current_phase = 0	next_phase = 1	reward = 0.070502	array([[-1.7608485, -2.877347 ]], dtype=float32)

time = 15619	action = 1	current_phase = 0	next_phase = 1	reward = -0.748850	array([[-3.0620391, -2.7381072]], dtype=float32)

time = 15627	action = 1	current_phase = 1	next_phase = 0	reward = -0.651017	array([[-3.6144545, -1.6843132]], dtype=float32)

time = 15635	action = 0	current_phase = 0	next_phase = 1	reward = -0.371676	array([[-1.3717864, -2.9679418]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0890 - val_loss: 0.0585

Epoch 2/50

 - 3s - loss: 0.0804 - val_loss: 0.0454

Epoch 3/50

 - 3s - loss: 0.0810 - val_loss: 0.0397

Epoch 4/50

 - 3s - loss: 0.0886 - val_loss: 0.0595

Epoch 5/50

 - 3s - loss: 0.0857 - val_loss: 0.0412

Epoch 6/50

 - 3s - loss: 0.0781 - val_loss: 0.0620

Epoch 7/50

 - 5s - loss: 0.0884 - val_loss: 0.0409

Epoch 8/50

 - 3s - loss: 0.0836 - val_loss: 0.0468

Epoch 9/50

 - 3s - loss: 0.0723 - val_loss: 0.0453

Epoch 10/50

 - 3s - loss: 0.0809 - val_loss: 0.0384

Epoch 11/50

 - 4s - loss: 0.0772 - val_loss: 0.0423

Epoch 12/50

 - 4s - loss: 0.1013 - val_loss: 0.0387

Epoch 13/50

 - 4s - loss: 0.0696 - val_loss: 0.0461

Epoch 14/50

 - 4s - loss: 0.0698 - val_loss: 0.0441

Epoch 15/50

 - 4s - loss: 0.0664 - val_loss: 0.0462

Epoch 16/50

 - 3s - loss: 0.0768 - val_loss: 0.0505

Epoch 17/50

 - 3s - loss: 0.1173 - val_loss: 0.0388

Epoch 18/50

 - 3s - loss: 0.0843 - val_loss: 0.0455

Epoch 19/50

 - 3s - loss: 0.0702 - val_loss: 0.0489

Epoch 20/50

 - 3s - loss: 0.0572 - val_loss: 0.0511

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 758, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 746, after forget

time = 15640	action = 0	current_phase = 0	next_phase = 1	reward = -0.009632	array([[-1.6242802, -2.8699732]], dtype=float32)

time = 15645	action = 0	current_phase = 0	next_phase = 1	reward = 0.339986	array([[-2.2538762, -3.0039573]], dtype=float32)

time = 15650	action = 1	current_phase = 0	next_phase = 1	reward = -1.429066	array([[-3.6161165, -3.1149673]], dtype=float32)

time = 15658	action = 1	current_phase = 1	next_phase = 0	reward = -0.714305	array([[-3.743098 , -2.0135274]], dtype=float32)

time = 15666	action = 0	current_phase = 0	next_phase = 1	reward = -0.090766	array([[-1.3685011, -2.9045925]], dtype=float32)

time = 15671	action = 0	current_phase = 0	next_phase = 1	reward = -0.012230	array([[-1.7842026, -2.906384 ]], dtype=float32)

time = 15676	action = 0	current_phase = 0	next_phase = 1	reward = 0.058840	array([[-2.2975788, -2.9971452]], dtype=float32)

time = 15681	action = 1	current_phase = 0	next_phase = 1	reward = -1.489878	array([[-3.9952056, -3.1521869]], dtype=float32)

time = 15689	action = 1	current_phase = 1	next_phase = 0	reward = -0.837407	array([[-3.5951612, -1.9714973]], dtype=float32)

time = 15697	action = 0	current_phase = 0	next_phase = 1	reward = -0.078982	array([[-1.3151803, -2.9420476]], dtype=float32)

time = 15702	action = 0	current_phase = 0	next_phase = 1	reward = -0.004853	array([[-1.9635735, -2.9677773]], dtype=float32)

time = 15707	action = 0	current_phase = 0	next_phase = 1	reward = 0.056000	array([[-2.6440296, -3.0706306]], dtype=float32)

time = 15712	action = 1	current_phase = 0	next_phase = 1	reward = -1.537815	array([[-4.096907 , -3.2332768]], dtype=float32)

time = 15720	action = 1	current_phase = 1	next_phase = 0	reward = -1.200449	array([[-3.6914983, -2.0482697]], dtype=float32)

time = 15728	action = 0	current_phase = 0	next_phase = 1	reward = 0.239316	array([[-1.3231767, -2.934198 ]], dtype=float32)

time = 15733	action = 0	current_phase = 0	next_phase = 1	reward = 0.015302	array([[-1.9955971, -3.0485287]], dtype=float32)

time = 15738	action = 0	current_phase = 0	next_phase = 1	reward = 0.073504	array([[-2.6368697, -3.0585551]], dtype=float32)

time = 15743	action = 1	current_phase = 0	next_phase = 1	reward = -1.268014	array([[-4.067493 , -3.4577818]], dtype=float32)

time = 15751	action = 1	current_phase = 1	next_phase = 0	reward = -1.433981	array([[-3.6350038, -2.1619558]], dtype=float32)

time = 15759	action = 0	current_phase = 0	next_phase = 1	reward = 0.243737	array([[-1.2733661, -2.8585744]], dtype=float32)

time = 15764	action = 0	current_phase = 0	next_phase = 1	reward = 0.026963	array([[-1.8601363, -2.955604 ]], dtype=float32)

time = 15769	action = 1	current_phase = 0	next_phase = 1	reward = -0.715991	array([[-2.8430033, -2.5885677]], dtype=float32)

time = 15777	action = 1	current_phase = 1	next_phase = 0	reward = -1.204895	array([[-3.2919228, -2.142953 ]], dtype=float32)

time = 15785	action = 0	current_phase = 0	next_phase = 1	reward = 0.465211	array([[-1.3036582, -2.9632874]], dtype=float32)

time = 15790	action = 0	current_phase = 0	next_phase = 1	reward = -0.027862	array([[-1.5621252, -2.7483306]], dtype=float32)

time = 15795	action = 0	current_phase = 0	next_phase = 1	reward = -0.231652	array([[-2.214427 , -3.0302162]], dtype=float32)

time = 15800	action = 1	current_phase = 0	next_phase = 1	reward = -1.098290	array([[-3.3822122, -3.0147448]], dtype=float32)

time = 15808	action = 1	current_phase = 1	next_phase = 0	reward = -0.715998	array([[-3.7621968, -1.8980937]], dtype=float32)

time = 15816	action = 0	current_phase = 0	next_phase = 1	reward = -0.079153	array([[-1.3410687, -2.921835 ]], dtype=float32)

time = 15821	action = 0	current_phase = 0	next_phase = 1	reward = -0.008066	array([[-1.8046403, -2.9247756]], dtype=float32)

time = 15826	action = 0	current_phase = 0	next_phase = 1	reward = 0.061153	array([[-2.3701518, -2.9934998]], dtype=float32)

time = 15831	action = 1	current_phase = 0	next_phase = 1	reward = -1.486049	array([[-4.0402784, -2.9430203]], dtype=float32)

time = 15839	action = 1	current_phase = 1	next_phase = 0	reward = -0.819211	array([[-3.6448257, -1.9581434]], dtype=float32)

time = 15847	action = 0	current_phase = 0	next_phase = 1	reward = -0.062347	array([[-1.362658, -2.966105]], dtype=float32)

time = 15852	action = 0	current_phase = 0	next_phase = 1	reward = 0.013238	array([[-1.9291232, -2.9711566]], dtype=float32)

time = 15857	action = 0	current_phase = 0	next_phase = 1	reward = 0.068090	array([[-2.5785048, -3.0377803]], dtype=float32)

time = 15862	action = 1	current_phase = 0	next_phase = 1	reward = -1.677374	array([[-4.023142, -3.15983 ]], dtype=float32)

time = 15870	action = 1	current_phase = 1	next_phase = 0	reward = -0.988453	array([[-3.6811728, -1.9935858]], dtype=float32)

time = 15878	action = 0	current_phase = 0	next_phase = 1	reward = -0.070874	array([[-1.3747227, -2.89038  ]], dtype=float32)

time = 15883	action = 0	current_phase = 0	next_phase = 1	reward = -0.027123	array([[-2.0932329, -3.0074143]], dtype=float32)

time = 15888	action = 0	current_phase = 0	next_phase = 1	reward = 0.080661	array([[-2.5053253, -2.950973 ]], dtype=float32)

time = 15893	action = 1	current_phase = 0	next_phase = 1	reward = -1.887295	array([[-4.115556 , -3.2143936]], dtype=float32)

time = 15901	action = 1	current_phase = 1	next_phase = 0	reward = -1.089256	array([[-3.8478463, -2.3256152]], dtype=float32)

time = 15909	action = 0	current_phase = 0	next_phase = 1	reward = -0.033313	array([[-1.2966369, -2.8184905]], dtype=float32)

time = 15914	action = 0	current_phase = 0	next_phase = 1	reward = 0.034088	array([[-1.8231382, -2.8926392]], dtype=float32)

time = 15919	action = 0	current_phase = 0	next_phase = 1	reward = 0.061918	array([[-2.5862918, -2.626727 ]], dtype=float32)

time = 15924	action = 1	current_phase = 0	next_phase = 1	reward = -1.076899	array([[-4.223686, -2.91124 ]], dtype=float32)

time = 15932	action = 1	current_phase = 1	next_phase = 0	reward = -0.982749	array([[-3.723794, -2.098467]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0812 - val_loss: 0.0545

Epoch 2/50

 - 3s - loss: 0.0805 - val_loss: 0.0334

Epoch 3/50

 - 3s - loss: 0.0689 - val_loss: 0.0425

Epoch 4/50

 - 3s - loss: 0.0802 - val_loss: 0.0353

Epoch 5/50

 - 3s - loss: 0.0718 - val_loss: 0.0480

Epoch 6/50

 - 4s - loss: 0.0660 - val_loss: 0.0389

Epoch 7/50

 - 5s - loss: 0.0623 - val_loss: 0.0370

Epoch 8/50

 - 4s - loss: 0.0707 - val_loss: 0.0441

Epoch 9/50

 - 4s - loss: 0.0631 - val_loss: 0.0434

Epoch 10/50

 - 4s - loss: 0.0677 - val_loss: 0.0377

Epoch 11/50

 - 4s - loss: 0.0555 - val_loss: 0.0571

Epoch 12/50

 - 4s - loss: 0.0592 - val_loss: 0.0370

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 768, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 756, after forget

time = 15940	action = 0	current_phase = 0	next_phase = 1	reward = 0.257443	array([[-1.4278955, -2.8183105]], dtype=float32)

time = 15945	action = 0	current_phase = 0	next_phase = 1	reward = 0.044319	array([[-2.0099192, -2.9289715]], dtype=float32)

time = 15950	action = 1	current_phase = 0	next_phase = 1	reward = -1.275962	array([[-3.444963 , -2.8620367]], dtype=float32)

time = 15958	action = 1	current_phase = 1	next_phase = 0	reward = -0.639528	array([[-3.600318 , -1.8095007]], dtype=float32)

time = 15966	action = 0	current_phase = 0	next_phase = 1	reward = -0.092271	array([[-1.2651261, -2.9400609]], dtype=float32)

time = 15971	action = 0	current_phase = 0	next_phase = 1	reward = -0.021973	array([[-1.8170545, -2.8957498]], dtype=float32)

time = 15976	action = 0	current_phase = 0	next_phase = 1	reward = 0.059157	array([[-2.3284838, -2.9965427]], dtype=float32)

time = 15981	action = 1	current_phase = 0	next_phase = 1	reward = -1.493650	array([[-4.2192545, -2.985521 ]], dtype=float32)

time = 15989	action = 1	current_phase = 1	next_phase = 0	reward = -0.855753	array([[-3.7472773, -1.8985564]], dtype=float32)

time = 15997	action = 0	current_phase = 0	next_phase = 1	reward = -0.072773	array([[-1.3027941, -2.9155831]], dtype=float32)

time = 16002	action = 0	current_phase = 0	next_phase = 1	reward = -0.013139	array([[-1.8104664, -2.9717066]], dtype=float32)

time = 16007	action = 0	current_phase = 0	next_phase = 1	reward = 0.072161	array([[-2.3395436, -3.0176582]], dtype=float32)

time = 16012	action = 1	current_phase = 0	next_phase = 1	reward = -1.631200	array([[-4.1654687, -2.9962897]], dtype=float32)

time = 16020	action = 1	current_phase = 1	next_phase = 0	reward = -0.953872	array([[-3.723103 , -1.9318225]], dtype=float32)

time = 16028	action = 0	current_phase = 0	next_phase = 1	reward = -0.047643	array([[-1.1581763, -2.9890087]], dtype=float32)

time = 16033	action = 0	current_phase = 0	next_phase = 1	reward = 0.041262	array([[-1.8973494, -3.0482867]], dtype=float32)

time = 16038	action = 0	current_phase = 0	next_phase = 1	reward = 0.072744	array([[-2.6775706, -2.8492386]], dtype=float32)

time = 16043	action = 1	current_phase = 0	next_phase = 1	reward = -2.001401	array([[-4.2674003, -3.118174 ]], dtype=float32)

time = 16051	action = 1	current_phase = 1	next_phase = 0	reward = -1.398437	array([[-3.8481727, -2.1018636]], dtype=float32)

time = 16059	action = 0	current_phase = 0	next_phase = 1	reward = 0.263283	array([[-1.245378, -2.855978]], dtype=float32)

time = 16064	action = 0	current_phase = 0	next_phase = 1	reward = 0.038112	array([[-1.7033587, -2.9233253]], dtype=float32)

time = 16069	action = 1	current_phase = 0	next_phase = 1	reward = -0.713335	array([[-2.419138 , -2.2850542]], dtype=float32)

time = 16077	action = 1	current_phase = 1	next_phase = 0	reward = -1.213325	array([[-3.159626 , -2.0673642]], dtype=float32)

time = 16085	action = 0	current_phase = 0	next_phase = 1	reward = 0.455284	array([[-1.1563472, -3.1190364]], dtype=float32)

time = 16090	action = 0	current_phase = 0	next_phase = 1	reward = -0.312859	array([[-1.5747821, -2.8503406]], dtype=float32)

time = 16095	action = 0	current_phase = 0	next_phase = 1	reward = 0.026375	array([[-2.1888733, -2.9917178]], dtype=float32)

time = 16100	action = 1	current_phase = 0	next_phase = 1	reward = -1.040141	array([[-3.8038602, -2.7967165]], dtype=float32)

time = 16108	action = 1	current_phase = 1	next_phase = 0	reward = -0.686636	array([[-3.5851068, -1.9350746]], dtype=float32)

time = 16116	action = 0	current_phase = 0	next_phase = 1	reward = -0.079870	array([[-1.32077  , -2.9345968]], dtype=float32)

time = 16121	action = 0	current_phase = 0	next_phase = 1	reward = -0.010038	array([[-1.785445, -2.889988]], dtype=float32)

time = 16126	action = 0	current_phase = 0	next_phase = 1	reward = 0.059177	array([[-2.30895  , -3.0163696]], dtype=float32)

time = 16131	action = 1	current_phase = 0	next_phase = 1	reward = -1.512259	array([[-4.0768433, -3.039019 ]], dtype=float32)

time = 16139	action = 1	current_phase = 1	next_phase = 0	reward = -0.764854	array([[-3.6711082, -1.8687773]], dtype=float32)

time = 16147	action = 0	current_phase = 0	next_phase = 1	reward = -0.075523	array([[-1.276841 , -2.9177387]], dtype=float32)

time = 16152	action = 0	current_phase = 0	next_phase = 1	reward = -0.005711	array([[-1.9640037, -2.9877186]], dtype=float32)

time = 16157	action = 0	current_phase = 0	next_phase = 1	reward = 0.063176	array([[-2.5798178, -3.0563707]], dtype=float32)

time = 16162	action = 1	current_phase = 0	next_phase = 1	reward = -1.521492	array([[-4.1291027, -3.0850499]], dtype=float32)

time = 16170	action = 1	current_phase = 1	next_phase = 0	reward = -1.207243	array([[-3.7073216, -2.019137 ]], dtype=float32)

time = 16178	action = 0	current_phase = 0	next_phase = 1	reward = 0.220706	array([[-1.2899534, -2.9353197]], dtype=float32)

time = 16183	action = 0	current_phase = 0	next_phase = 1	reward = 0.024849	array([[-1.9661858, -3.0134747]], dtype=float32)

time = 16188	action = 0	current_phase = 0	next_phase = 1	reward = 0.076278	array([[-2.4540415, -2.9867704]], dtype=float32)

time = 16193	action = 1	current_phase = 0	next_phase = 1	reward = -1.855325	array([[-4.126788, -3.18931 ]], dtype=float32)

time = 16201	action = 1	current_phase = 1	next_phase = 0	reward = -1.086875	array([[-3.7842073, -2.1096702]], dtype=float32)

time = 16209	action = 0	current_phase = 0	next_phase = 1	reward = -0.037840	array([[-1.2330118, -2.842369 ]], dtype=float32)

time = 16214	action = 0	current_phase = 0	next_phase = 1	reward = 0.018997	array([[-1.628654 , -2.8760202]], dtype=float32)

time = 16219	action = 1	current_phase = 0	next_phase = 1	reward = -0.647453	array([[-2.561488, -2.353332]], dtype=float32)

time = 16227	action = 1	current_phase = 1	next_phase = 0	reward = -0.902572	array([[-3.0645328, -2.0736508]], dtype=float32)

time = 16235	action = 0	current_phase = 0	next_phase = 1	reward = 0.187729	array([[-1.2345033, -2.9884212]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0746 - val_loss: 0.0323

Epoch 2/50

 - 3s - loss: 0.0676 - val_loss: 0.0309

Epoch 3/50

 - 3s - loss: 0.0816 - val_loss: 0.0450

Epoch 4/50

 - 3s - loss: 0.0712 - val_loss: 0.0325

Epoch 5/50

 - 3s - loss: 0.0588 - val_loss: 0.0329

Epoch 6/50

 - 3s - loss: 0.0773 - val_loss: 0.0354

Epoch 7/50

 - 5s - loss: 0.0693 - val_loss: 0.0405

Epoch 8/50

 - 5s - loss: 0.0602 - val_loss: 0.0303

Epoch 9/50

 - 4s - loss: 0.0583 - val_loss: 0.0340

Epoch 10/50

 - 4s - loss: 0.0719 - val_loss: 0.0476

Epoch 11/50

 - 4s - loss: 0.0592 - val_loss: 0.0418

Epoch 12/50

 - 4s - loss: 0.0631 - val_loss: 0.0322

Epoch 13/50

 - 4s - loss: 0.0549 - val_loss: 0.0324

Epoch 14/50

 - 4s - loss: 0.0626 - val_loss: 0.0444

Epoch 15/50

 - 4s - loss: 0.0561 - val_loss: 0.0378

Epoch 16/50

 - 4s - loss: 0.0580 - val_loss: 0.0429

Epoch 17/50

 - 4s - loss: 0.0581 - val_loss: 0.0360

Epoch 18/50

 - 3s - loss: 0.0583 - val_loss: 0.0380

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 778, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 766, after forget

time = 16240	action = 0	current_phase = 0	next_phase = 1	reward = -0.042459	array([[-1.5305583, -2.7937162]], dtype=float32)

time = 16245	action = 0	current_phase = 0	next_phase = 1	reward = 0.041056	array([[-2.3014226, -2.9838347]], dtype=float32)

time = 16250	action = 1	current_phase = 0	next_phase = 1	reward = -1.389490	array([[-3.6487622, -2.9917407]], dtype=float32)

time = 16258	action = 1	current_phase = 1	next_phase = 0	reward = -0.689842	array([[-3.794435 , -1.8576443]], dtype=float32)

time = 16266	action = 0	current_phase = 0	next_phase = 1	reward = -0.080354	array([[-1.2493113, -2.903566 ]], dtype=float32)

time = 16271	action = 0	current_phase = 0	next_phase = 1	reward = 0.010848	array([[-1.7357312, -2.8937228]], dtype=float32)

time = 16276	action = 0	current_phase = 0	next_phase = 1	reward = 0.066727	array([[-2.4046316, -3.0071468]], dtype=float32)

time = 16281	action = 1	current_phase = 0	next_phase = 1	reward = -1.494085	array([[-3.948691, -3.083716]], dtype=float32)

time = 16289	action = 1	current_phase = 1	next_phase = 0	reward = -0.790336	array([[-3.7819045, -1.8055804]], dtype=float32)

time = 16297	action = 0	current_phase = 0	next_phase = 1	reward = -0.062897	array([[-1.2133209, -2.967555 ]], dtype=float32)

time = 16302	action = 0	current_phase = 0	next_phase = 1	reward = 0.021300	array([[-1.8662869, -2.9881184]], dtype=float32)

time = 16307	action = 0	current_phase = 0	next_phase = 1	reward = 0.071101	array([[-2.3284783, -2.9737122]], dtype=float32)

time = 16312	action = 1	current_phase = 0	next_phase = 1	reward = -1.665217	array([[-4.1081295, -3.2544892]], dtype=float32)

time = 16320	action = 1	current_phase = 1	next_phase = 0	reward = -1.005215	array([[-3.7196927, -1.931756 ]], dtype=float32)

time = 16328	action = 0	current_phase = 0	next_phase = 1	reward = -0.064012	array([[-1.3857898, -2.8807704]], dtype=float32)

time = 16333	action = 0	current_phase = 0	next_phase = 1	reward = 0.011955	array([[-2.0045288, -3.0772564]], dtype=float32)

time = 16338	action = 0	current_phase = 0	next_phase = 1	reward = 0.084488	array([[-2.6084385, -2.908946 ]], dtype=float32)

time = 16343	action = 1	current_phase = 0	next_phase = 1	reward = -1.871406	array([[-4.3028216, -3.5955188]], dtype=float32)

time = 16351	action = 1	current_phase = 1	next_phase = 0	reward = -1.647800	array([[-3.9046218, -2.1987758]], dtype=float32)

time = 16359	action = 0	current_phase = 0	next_phase = 1	reward = 0.566838	array([[-1.1943954, -2.8947315]], dtype=float32)

time = 16364	action = 0	current_phase = 0	next_phase = 1	reward = 0.055562	array([[-1.8246851, -2.8574722]], dtype=float32)

time = 16369	action = 0	current_phase = 0	next_phase = 1	reward = 0.067206	array([[-2.2588415, -2.5309289]], dtype=float32)

time = 16374	action = 1	current_phase = 0	next_phase = 1	reward = -1.087536	array([[-4.1514487, -2.8852804]], dtype=float32)

time = 16382	action = 1	current_phase = 1	next_phase = 0	reward = -0.756129	array([[-3.6137662, -2.095704 ]], dtype=float32)

time = 16390	action = 0	current_phase = 0	next_phase = 1	reward = -0.029349	array([[-1.5255432, -2.8042867]], dtype=float32)

time = 16395	action = 0	current_phase = 0	next_phase = 1	reward = 0.040182	array([[-2.174552 , -2.8848503]], dtype=float32)

time = 16400	action = 1	current_phase = 0	next_phase = 1	reward = -1.321069	array([[-3.3800807, -2.6768813]], dtype=float32)

time = 16408	action = 1	current_phase = 1	next_phase = 0	reward = -0.705988	array([[-3.8239858, -1.81798  ]], dtype=float32)

time = 16416	action = 0	current_phase = 0	next_phase = 1	reward = -0.086745	array([[-1.2090884, -2.8877678]], dtype=float32)

time = 16421	action = 0	current_phase = 0	next_phase = 1	reward = -0.011960	array([[-1.7390858, -2.885639 ]], dtype=float32)

time = 16426	action = 0	current_phase = 0	next_phase = 1	reward = 0.056041	array([[-2.3312244, -2.9848638]], dtype=float32)

time = 16431	action = 1	current_phase = 0	next_phase = 1	reward = -1.434011	array([[-4.1097264, -3.0414002]], dtype=float32)

time = 16439	action = 1	current_phase = 1	next_phase = 0	reward = -0.755847	array([[-3.6501741, -1.8608426]], dtype=float32)

time = 16447	action = 0	current_phase = 0	next_phase = 1	reward = -0.049813	array([[-1.2524979, -2.9544818]], dtype=float32)

time = 16452	action = 0	current_phase = 0	next_phase = 1	reward = 0.014178	array([[-2.0327554, -2.945513 ]], dtype=float32)

time = 16457	action = 0	current_phase = 0	next_phase = 1	reward = 0.069677	array([[-2.4858797, -3.0798976]], dtype=float32)

time = 16462	action = 1	current_phase = 0	next_phase = 1	reward = -1.725010	array([[-4.100044, -3.230189]], dtype=float32)

time = 16470	action = 1	current_phase = 1	next_phase = 0	reward = -0.999324	array([[-3.7797143, -1.9246173]], dtype=float32)

time = 16478	action = 0	current_phase = 0	next_phase = 1	reward = -0.046842	array([[-1.3286753, -2.9049566]], dtype=float32)

time = 16483	action = 0	current_phase = 0	next_phase = 1	reward = 0.028281	array([[-2.0676374, -3.0222242]], dtype=float32)

time = 16488	action = 0	current_phase = 0	next_phase = 1	reward = 0.076819	array([[-2.7172427, -3.0341694]], dtype=float32)

time = 16493	action = 1	current_phase = 0	next_phase = 1	reward = -1.825826	array([[-4.2768307, -3.6884997]], dtype=float32)

time = 16501	action = 1	current_phase = 1	next_phase = 0	reward = -1.341623	array([[-3.9134905, -2.1551342]], dtype=float32)

time = 16509	action = 0	current_phase = 0	next_phase = 1	reward = 0.267481	array([[-1.2551668, -2.8401704]], dtype=float32)

time = 16514	action = 0	current_phase = 0	next_phase = 1	reward = 0.042661	array([[-1.8650347, -2.9690144]], dtype=float32)

time = 16519	action = 0	current_phase = 0	next_phase = 1	reward = 0.047207	array([[-2.2202668, -2.6346076]], dtype=float32)

time = 16524	action = 1	current_phase = 0	next_phase = 1	reward = -1.138925	array([[-3.6784272, -3.0254285]], dtype=float32)

time = 16532	action = 1	current_phase = 1	next_phase = 0	reward = -1.028177	array([[-3.7006505, -2.0902803]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0634 - val_loss: 0.0318

Epoch 2/50

 - 3s - loss: 0.0907 - val_loss: 0.0459

Epoch 3/50

 - 3s - loss: 0.0693 - val_loss: 0.0394

Epoch 4/50

 - 3s - loss: 0.0599 - val_loss: 0.0393

Epoch 5/50

 - 3s - loss: 0.0567 - val_loss: 0.0401

Epoch 6/50

 - 3s - loss: 0.0615 - val_loss: 0.0370

Epoch 7/50

 - 3s - loss: 0.0527 - val_loss: 0.0542

Epoch 8/50

 - 3s - loss: 0.0662 - val_loss: 0.0449

Epoch 9/50

 - 4s - loss: 0.0599 - val_loss: 0.0380

Epoch 10/50

 - 3s - loss: 0.0581 - val_loss: 0.0423

Epoch 11/50

 - 3s - loss: 0.0605 - val_loss: 0.0544

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 788, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 776, after forget

time = 16540	action = 0	current_phase = 0	next_phase = 1	reward = -0.027561	array([[-1.3592353, -2.7877991]], dtype=float32)

time = 16545	action = 0	current_phase = 0	next_phase = 1	reward = 0.057168	array([[-2.021965, -2.922785]], dtype=float32)

time = 16550	action = 0	current_phase = 0	next_phase = 1	reward = 0.033518	array([[-2.6513066, -2.6632974]], dtype=float32)

time = 16555	action = 1	current_phase = 0	next_phase = 1	reward = -1.836330	array([[-3.6535826, -3.4049194]], dtype=float32)

time = 16563	action = 1	current_phase = 1	next_phase = 0	reward = -0.755246	array([[-3.6081953, -1.982909 ]], dtype=float32)

time = 16571	action = 0	current_phase = 0	next_phase = 1	reward = -0.017341	array([[-1.621593 , -2.8535895]], dtype=float32)

time = 16576	action = 0	current_phase = 0	next_phase = 1	reward = 0.053911	array([[-2.1451728, -2.8883135]], dtype=float32)

time = 16581	action = 1	current_phase = 0	next_phase = 1	reward = -1.443888	array([[-3.5947027, -2.867699 ]], dtype=float32)

time = 16589	action = 1	current_phase = 1	next_phase = 0	reward = -0.896490	array([[-3.7844481, -1.852037 ]], dtype=float32)

time = 16597	action = 0	current_phase = 0	next_phase = 1	reward = -0.066028	array([[-1.3004627, -2.93902  ]], dtype=float32)

time = 16602	action = 0	current_phase = 0	next_phase = 1	reward = 0.000701	array([[-1.9272997, -2.8963082]], dtype=float32)

time = 16607	action = 0	current_phase = 0	next_phase = 1	reward = 0.051483	array([[-2.2790895, -2.979791 ]], dtype=float32)

time = 16612	action = 1	current_phase = 0	next_phase = 1	reward = -1.642254	array([[-3.9893675, -3.2104244]], dtype=float32)

time = 16620	action = 1	current_phase = 1	next_phase = 0	reward = -1.295166	array([[-3.705597 , -1.8716184]], dtype=float32)

time = 16628	action = 0	current_phase = 0	next_phase = 1	reward = 0.253262	array([[-1.2546849, -2.8976524]], dtype=float32)

time = 16633	action = 0	current_phase = 0	next_phase = 1	reward = 0.044589	array([[-1.9765382, -3.0201554]], dtype=float32)

time = 16638	action = 0	current_phase = 0	next_phase = 1	reward = 0.084038	array([[-2.485129 , -3.1630805]], dtype=float32)

time = 16643	action = 1	current_phase = 0	next_phase = 1	reward = -1.393256	array([[-4.011787 , -3.3285353]], dtype=float32)

time = 16651	action = 1	current_phase = 1	next_phase = 0	reward = -1.471292	array([[-3.784771 , -2.2858803]], dtype=float32)

time = 16659	action = 0	current_phase = 0	next_phase = 1	reward = 0.179393	array([[-1.2054098, -2.8625953]], dtype=float32)

time = 16664	action = 0	current_phase = 0	next_phase = 1	reward = 0.052915	array([[-1.5791407, -2.8511198]], dtype=float32)

time = 16669	action = 0	current_phase = 0	next_phase = 1	reward = 0.076955	array([[-1.8285842, -2.7876801]], dtype=float32)

time = 16674	action = 1	current_phase = 0	next_phase = 1	reward = -1.075236	array([[-3.98417  , -2.8327136]], dtype=float32)

time = 16682	action = 1	current_phase = 1	next_phase = 0	reward = -0.750635	array([[-3.6236749, -1.9992054]], dtype=float32)

time = 16690	action = 0	current_phase = 0	next_phase = 1	reward = -0.011303	array([[-1.4063563, -2.7935758]], dtype=float32)

time = 16695	action = 0	current_phase = 0	next_phase = 1	reward = 0.053994	array([[-2.1922798, -2.9110534]], dtype=float32)

time = 16700	action = 1	current_phase = 0	next_phase = 1	reward = -1.376676	array([[-3.1128798, -2.8061643]], dtype=float32)

time = 16708	action = 1	current_phase = 1	next_phase = 0	reward = -0.708805	array([[-3.8476634, -1.6960019]], dtype=float32)

time = 16716	action = 0	current_phase = 0	next_phase = 1	reward = -0.085439	array([[-1.2629526, -2.9266865]], dtype=float32)

time = 16721	action = 0	current_phase = 0	next_phase = 1	reward = -0.008660	array([[-1.8111162, -2.8710964]], dtype=float32)

time = 16726	action = 0	current_phase = 0	next_phase = 1	reward = 0.055128	array([[-2.173347 , -2.9441054]], dtype=float32)

time = 16731	action = 1	current_phase = 0	next_phase = 1	reward = -1.491866	array([[-3.8733644, -3.1587214]], dtype=float32)

time = 16739	action = 1	current_phase = 1	next_phase = 0	reward = -0.837273	array([[-3.7128487, -1.8462983]], dtype=float32)

time = 16747	action = 0	current_phase = 0	next_phase = 1	reward = -0.069321	array([[-1.2577981, -2.921915 ]], dtype=float32)

time = 16752	action = 0	current_phase = 0	next_phase = 1	reward = 0.022503	array([[-1.8215401, -2.9155953]], dtype=float32)

time = 16757	action = 0	current_phase = 0	next_phase = 1	reward = 0.078542	array([[-2.4084983, -3.032241 ]], dtype=float32)

time = 16762	action = 1	current_phase = 0	next_phase = 1	reward = -1.611892	array([[-4.0367026, -3.1637826]], dtype=float32)

time = 16770	action = 1	current_phase = 1	next_phase = 0	reward = -1.270074	array([[-3.7817764, -1.9779949]], dtype=float32)

time = 16778	action = 0	current_phase = 0	next_phase = 1	reward = 0.231529	array([[-1.3002417, -2.8918798]], dtype=float32)

time = 16783	action = 0	current_phase = 0	next_phase = 1	reward = 0.004336	array([[-1.9681822, -2.9685733]], dtype=float32)

time = 16788	action = 0	current_phase = 0	next_phase = 1	reward = 0.058973	array([[-2.51021  , -3.2495558]], dtype=float32)

time = 16793	action = 1	current_phase = 0	next_phase = 1	reward = -1.306851	array([[-4.108165 , -3.3081245]], dtype=float32)

time = 16801	action = 1	current_phase = 1	next_phase = 0	reward = -1.191608	array([[-3.7358646, -2.145928 ]], dtype=float32)

time = 16809	action = 0	current_phase = 0	next_phase = 1	reward = -0.057169	array([[-1.2227978, -2.8475978]], dtype=float32)

time = 16814	action = 0	current_phase = 0	next_phase = 1	reward = 0.017949	array([[-1.7157929, -2.8860636]], dtype=float32)

time = 16819	action = 1	current_phase = 0	next_phase = 1	reward = -1.238324	array([[-2.5292087, -2.4464583]], dtype=float32)

time = 16827	action = 1	current_phase = 1	next_phase = 0	reward = -0.771405	array([[-3.6875453, -1.9124682]], dtype=float32)

time = 16835	action = 0	current_phase = 0	next_phase = 1	reward = -0.087903	array([[-1.2968974, -2.9664586]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0867 - val_loss: 0.0485

Epoch 2/50

 - 3s - loss: 0.0740 - val_loss: 0.0519

Epoch 3/50

 - 4s - loss: 0.0786 - val_loss: 0.0474

Epoch 4/50

 - 3s - loss: 0.0781 - val_loss: 0.0459

Epoch 5/50

 - 3s - loss: 0.0598 - val_loss: 0.0489

Epoch 6/50

 - 3s - loss: 0.0651 - val_loss: 0.0551

Epoch 7/50

 - 3s - loss: 0.0780 - val_loss: 0.0450

Epoch 8/50

 - 3s - loss: 0.0687 - val_loss: 0.0447

Epoch 9/50

 - 3s - loss: 0.0734 - val_loss: 0.0449

Epoch 10/50

 - 3s - loss: 0.0603 - val_loss: 0.0474

Epoch 11/50

 - 3s - loss: 0.0575 - val_loss: 0.0454

Epoch 12/50

 - 3s - loss: 0.0719 - val_loss: 0.0504

Epoch 13/50

 - 3s - loss: 0.0681 - val_loss: 0.0531

Epoch 14/50

 - 3s - loss: 0.0668 - val_loss: 0.0513

Epoch 15/50

 - 3s - loss: 0.0558 - val_loss: 0.0526

Epoch 16/50

 - 3s - loss: 0.0587 - val_loss: 0.0478

Epoch 17/50

 - 3s - loss: 0.0539 - val_loss: 0.0498

Epoch 18/50

 - 4s - loss: 0.0573 - val_loss: 0.0515

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 798, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 786, after forget

time = 16840	action = 0	current_phase = 0	next_phase = 1	reward = 0.266691	array([[-1.571858 , -2.8357213]], dtype=float32)

time = 16845	action = 0	current_phase = 0	next_phase = 1	reward = 0.035002	array([[-2.1263387, -3.020095 ]], dtype=float32)

time = 16850	action = 1	current_phase = 0	next_phase = 1	reward = -1.319998	array([[-3.0401049, -2.7277582]], dtype=float32)

time = 16858	action = 1	current_phase = 1	next_phase = 0	reward = -0.709961	array([[-3.777338 , -1.6188737]], dtype=float32)

time = 16866	action = 0	current_phase = 0	next_phase = 1	reward = -0.090536	array([[-1.438798, -2.917294]], dtype=float32)

time = 16871	action = 0	current_phase = 0	next_phase = 1	reward = -0.019455	array([[-1.6926161, -2.8684516]], dtype=float32)

time = 16876	action = 0	current_phase = 0	next_phase = 1	reward = 0.052854	array([[-2.2172947, -2.9756305]], dtype=float32)

time = 16881	action = 1	current_phase = 0	next_phase = 1	reward = -1.377438	array([[-3.865839 , -3.0483205]], dtype=float32)

time = 16889	action = 1	current_phase = 1	next_phase = 0	reward = -0.768233	array([[-3.6858544, -1.7997276]], dtype=float32)

time = 16897	action = 0	current_phase = 0	next_phase = 1	reward = -0.072736	array([[-1.4761013, -2.9856396]], dtype=float32)

time = 16902	action = 0	current_phase = 0	next_phase = 1	reward = -0.003171	array([[-1.7984226, -2.8817627]], dtype=float32)

time = 16907	action = 0	current_phase = 0	next_phase = 1	reward = 0.066563	array([[-2.304669 , -2.9728668]], dtype=float32)

time = 16912	action = 1	current_phase = 0	next_phase = 1	reward = -1.636213	array([[-3.9353728, -3.1892579]], dtype=float32)

time = 16920	action = 1	current_phase = 1	next_phase = 0	reward = -0.997950	array([[-3.7401023, -1.8720253]], dtype=float32)

time = 16928	action = 0	current_phase = 0	next_phase = 1	reward = -0.050862	array([[-1.4035094, -2.9550984]], dtype=float32)

time = 16933	action = 0	current_phase = 0	next_phase = 1	reward = 0.018403	array([[-1.7287083, -2.9881313]], dtype=float32)

time = 16938	action = 0	current_phase = 0	next_phase = 1	reward = 0.069729	array([[-2.7947755, -2.8145537]], dtype=float32)

time = 16943	action = 1	current_phase = 0	next_phase = 1	reward = -1.820888	array([[-4.0248632, -3.2670324]], dtype=float32)

time = 16951	action = 1	current_phase = 1	next_phase = 0	reward = -1.377759	array([[-3.9196434, -2.1159787]], dtype=float32)

time = 16959	action = 0	current_phase = 0	next_phase = 1	reward = 0.259036	array([[-1.3824013, -2.864082 ]], dtype=float32)

time = 16964	action = 0	current_phase = 0	next_phase = 1	reward = 0.028424	array([[-1.7414516, -2.9406297]], dtype=float32)

time = 16969	action = 1	current_phase = 0	next_phase = 1	reward = -0.626866	array([[-2.3063972, -2.2638736]], dtype=float32)

time = 16977	action = 1	current_phase = 1	next_phase = 0	reward = -1.162421	array([[-3.0125756, -2.0048146]], dtype=float32)

time = 16985	action = 0	current_phase = 0	next_phase = 1	reward = 0.460325	array([[-1.3682348, -3.020717 ]], dtype=float32)

time = 16990	action = 0	current_phase = 0	next_phase = 1	reward = -0.312796	array([[-1.6131507, -2.743241 ]], dtype=float32)

time = 16995	action = 0	current_phase = 0	next_phase = 1	reward = -0.231767	array([[-2.227785, -2.963545]], dtype=float32)

time = 17000	action = 1	current_phase = 0	next_phase = 1	reward = -0.744962	array([[-3.302939 , -2.6944444]], dtype=float32)

time = 17008	action = 1	current_phase = 1	next_phase = 0	reward = -0.703685	array([[-3.6714911, -1.9206488]], dtype=float32)

time = 17016	action = 0	current_phase = 0	next_phase = 1	reward = -0.068344	array([[-1.4068053, -2.9089072]], dtype=float32)

time = 17021	action = 0	current_phase = 0	next_phase = 1	reward = 0.000845	array([[-1.7365233, -2.8687665]], dtype=float32)

time = 17026	action = 0	current_phase = 0	next_phase = 1	reward = 0.068294	array([[-2.3174999, -2.974318 ]], dtype=float32)

time = 17031	action = 1	current_phase = 0	next_phase = 1	reward = -1.290109	array([[-3.878824 , -2.9739187]], dtype=float32)

time = 17039	action = 1	current_phase = 1	next_phase = 0	reward = -1.173516	array([[-3.658629 , -1.8436197]], dtype=float32)

time = 17047	action = 0	current_phase = 0	next_phase = 1	reward = 0.234494	array([[-1.4240952, -2.9447458]], dtype=float32)

time = 17052	action = 0	current_phase = 0	next_phase = 1	reward = 0.019193	array([[-1.9106112, -2.9353077]], dtype=float32)

time = 17057	action = 0	current_phase = 0	next_phase = 1	reward = 0.067931	array([[-2.4278615, -2.9678972]], dtype=float32)

time = 17062	action = 1	current_phase = 0	next_phase = 1	reward = -1.694735	array([[-3.992305 , -3.1457064]], dtype=float32)

time = 17070	action = 1	current_phase = 1	next_phase = 0	reward = -1.263624	array([[-3.7160482, -1.8311856]], dtype=float32)

time = 17078	action = 0	current_phase = 0	next_phase = 1	reward = 0.238038	array([[-1.3450693, -2.925783 ]], dtype=float32)

time = 17083	action = 0	current_phase = 0	next_phase = 1	reward = 0.000247	array([[-1.7317568, -3.183082 ]], dtype=float32)

time = 17088	action = 0	current_phase = 0	next_phase = 1	reward = 0.068451	array([[-2.5187123, -2.814734 ]], dtype=float32)

time = 17093	action = 1	current_phase = 0	next_phase = 1	reward = -1.912224	array([[-4.021255 , -3.3856564]], dtype=float32)

time = 17101	action = 1	current_phase = 1	next_phase = 0	reward = -0.983087	array([[-3.9665532, -2.0707102]], dtype=float32)

time = 17109	action = 0	current_phase = 0	next_phase = 1	reward = -0.036438	array([[-1.3744291, -2.8436768]], dtype=float32)

time = 17114	action = 0	current_phase = 0	next_phase = 1	reward = 0.293498	array([[-1.8197688, -2.9317808]], dtype=float32)

time = 17119	action = 0	current_phase = 0	next_phase = 1	reward = -0.214676	array([[-2.5664399, -2.6683726]], dtype=float32)

time = 17124	action = 1	current_phase = 0	next_phase = 1	reward = -1.017775	array([[-3.9990103, -2.9556594]], dtype=float32)

time = 17132	action = 1	current_phase = 1	next_phase = 0	reward = -1.031519	array([[-3.6503158, -2.0682695]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0876 - val_loss: 0.0316

Epoch 2/50

 - 3s - loss: 0.0795 - val_loss: 0.0328

Epoch 3/50

 - 3s - loss: 0.0660 - val_loss: 0.0315

Epoch 4/50

 - 3s - loss: 0.0717 - val_loss: 0.0344

Epoch 5/50

 - 3s - loss: 0.0666 - val_loss: 0.0341

Epoch 6/50

 - 4s - loss: 0.0570 - val_loss: 0.0315

Epoch 7/50

 - 3s - loss: 0.0639 - val_loss: 0.0311

Epoch 8/50

 - 3s - loss: 0.0623 - val_loss: 0.0301

Epoch 9/50

 - 3s - loss: 0.0621 - val_loss: 0.0339

Epoch 10/50

 - 3s - loss: 0.0701 - val_loss: 0.0312

Epoch 11/50

 - 4s - loss: 0.0638 - val_loss: 0.0332

Epoch 12/50

 - 3s - loss: 0.0743 - val_loss: 0.0351

Epoch 13/50

 - 3s - loss: 0.0543 - val_loss: 0.0308

Epoch 14/50

 - 4s - loss: 0.0864 - val_loss: 0.0371

Epoch 15/50

 - 4s - loss: 0.0595 - val_loss: 0.0340

Epoch 16/50

 - 4s - loss: 0.0570 - val_loss: 0.0312

Epoch 17/50

 - 4s - loss: 0.0722 - val_loss: 0.0343

Epoch 18/50

 - 4s - loss: 0.0561 - val_loss: 0.0314

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 808, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 796, after forget

time = 17140	action = 0	current_phase = 0	next_phase = 1	reward = -0.024945	array([[-1.5508838, -2.7670448]], dtype=float32)

time = 17145	action = 0	current_phase = 0	next_phase = 1	reward = 0.042862	array([[-2.136482 , -2.8850896]], dtype=float32)

time = 17150	action = 1	current_phase = 0	next_phase = 1	reward = -1.086624	array([[-2.87916  , -2.6499293]], dtype=float32)

time = 17158	action = 1	current_phase = 1	next_phase = 0	reward = -0.699434	array([[-3.7804537, -1.8957896]], dtype=float32)

time = 17166	action = 0	current_phase = 0	next_phase = 1	reward = -0.082466	array([[-1.4148304, -2.8301237]], dtype=float32)

time = 17171	action = 0	current_phase = 0	next_phase = 1	reward = -0.006630	array([[-1.649456 , -2.9024599]], dtype=float32)

time = 17176	action = 0	current_phase = 0	next_phase = 1	reward = 0.061607	array([[-2.2589574, -2.951981 ]], dtype=float32)

time = 17181	action = 1	current_phase = 0	next_phase = 1	reward = -1.475207	array([[-4.0235085, -3.0825412]], dtype=float32)

time = 17189	action = 1	current_phase = 1	next_phase = 0	reward = -1.129814	array([[-3.804438 , -1.8676788]], dtype=float32)

time = 17197	action = 0	current_phase = 0	next_phase = 1	reward = 0.225665	array([[-1.4746473, -2.9516225]], dtype=float32)

time = 17202	action = 0	current_phase = 0	next_phase = 1	reward = 0.005618	array([[-1.8783789, -2.9891555]], dtype=float32)

time = 17207	action = 0	current_phase = 0	next_phase = 1	reward = 0.061808	array([[-2.4626398, -3.0391695]], dtype=float32)

time = 17212	action = 1	current_phase = 0	next_phase = 1	reward = -1.675421	array([[-4.021551 , -3.2301257]], dtype=float32)

time = 17220	action = 1	current_phase = 1	next_phase = 0	reward = -0.848540	array([[-3.7597473, -1.8973498]], dtype=float32)

time = 17228	action = 0	current_phase = 0	next_phase = 1	reward = -0.036457	array([[-1.4940884, -2.9173965]], dtype=float32)

time = 17233	action = 0	current_phase = 0	next_phase = 1	reward = 0.026902	array([[-2.0191865, -3.014694 ]], dtype=float32)

time = 17238	action = 0	current_phase = 0	next_phase = 1	reward = 0.071618	array([[-2.7526004, -2.8318784]], dtype=float32)

time = 17243	action = 1	current_phase = 0	next_phase = 1	reward = -1.944644	array([[-4.1011653, -3.4649386]], dtype=float32)

time = 17251	action = 1	current_phase = 1	next_phase = 0	reward = -1.022071	array([[-3.945354, -2.169536]], dtype=float32)

time = 17259	action = 0	current_phase = 0	next_phase = 1	reward = -0.030850	array([[-1.4365535, -2.8236167]], dtype=float32)

time = 17264	action = 0	current_phase = 0	next_phase = 1	reward = 0.037993	array([[-1.7088419, -2.8458273]], dtype=float32)

time = 17269	action = 0	current_phase = 0	next_phase = 1	reward = 0.059752	array([[-2.414945 , -2.6326602]], dtype=float32)

time = 17274	action = 1	current_phase = 0	next_phase = 1	reward = -1.020994	array([[-3.9352229, -3.0297134]], dtype=float32)

time = 17282	action = 1	current_phase = 1	next_phase = 0	reward = -1.036799	array([[-3.5673933, -2.090831 ]], dtype=float32)

time = 17290	action = 0	current_phase = 0	next_phase = 1	reward = 0.241618	array([[-1.5872884, -2.7207701]], dtype=float32)

time = 17295	action = 0	current_phase = 0	next_phase = 1	reward = -0.253044	array([[-2.2146013, -2.9328382]], dtype=float32)

time = 17300	action = 1	current_phase = 0	next_phase = 1	reward = -0.992947	array([[-2.916409, -2.721875]], dtype=float32)

time = 17308	action = 1	current_phase = 1	next_phase = 0	reward = -0.703680	array([[-3.5272727, -1.8561248]], dtype=float32)

time = 17316	action = 0	current_phase = 0	next_phase = 1	reward = -0.088524	array([[-1.4687667, -2.9384758]], dtype=float32)

time = 17321	action = 0	current_phase = 0	next_phase = 1	reward = -0.003350	array([[-1.8266786, -2.8432024]], dtype=float32)

time = 17326	action = 0	current_phase = 0	next_phase = 1	reward = 0.072426	array([[-2.3333755, -2.956025 ]], dtype=float32)

time = 17331	action = 1	current_phase = 0	next_phase = 1	reward = -1.494247	array([[-3.9738007, -2.9708781]], dtype=float32)

time = 17339	action = 1	current_phase = 1	next_phase = 0	reward = -1.140887	array([[-3.813259 , -1.9051843]], dtype=float32)

time = 17347	action = 0	current_phase = 0	next_phase = 1	reward = 0.218041	array([[-1.4619656, -2.9012396]], dtype=float32)

time = 17352	action = 0	current_phase = 0	next_phase = 1	reward = 0.000177	array([[-1.8213866, -2.8738234]], dtype=float32)

time = 17357	action = 0	current_phase = 0	next_phase = 1	reward = 0.056385	array([[-2.3823106, -3.0122344]], dtype=float32)

time = 17362	action = 1	current_phase = 0	next_phase = 1	reward = -1.699967	array([[-4.046696 , -3.1674526]], dtype=float32)

time = 17370	action = 1	current_phase = 1	next_phase = 0	reward = -0.949530	array([[-3.717503 , -1.9384177]], dtype=float32)

time = 17378	action = 0	current_phase = 0	next_phase = 1	reward = -0.066936	array([[-1.544152, -2.903726]], dtype=float32)

time = 17383	action = 0	current_phase = 0	next_phase = 1	reward = 0.014370	array([[-2.036412 , -3.0532897]], dtype=float32)

time = 17388	action = 0	current_phase = 0	next_phase = 1	reward = 0.087238	array([[-2.4845164, -3.1506624]], dtype=float32)

time = 17393	action = 1	current_phase = 0	next_phase = 1	reward = -1.855360	array([[-3.967082 , -3.5733259]], dtype=float32)

time = 17401	action = 1	current_phase = 1	next_phase = 0	reward = -1.132832	array([[-3.9748707, -2.297161 ]], dtype=float32)

time = 17409	action = 0	current_phase = 0	next_phase = 1	reward = -0.032813	array([[-1.3610538, -2.7632852]], dtype=float32)

time = 17414	action = 0	current_phase = 0	next_phase = 1	reward = 0.035666	array([[-1.8801169, -2.876501 ]], dtype=float32)

time = 17419	action = 0	current_phase = 0	next_phase = 1	reward = 0.055274	array([[-2.3775353, -2.4407   ]], dtype=float32)

time = 17424	action = 1	current_phase = 0	next_phase = 1	reward = -1.134565	array([[-3.9624887, -2.9568913]], dtype=float32)

time = 17432	action = 1	current_phase = 1	next_phase = 0	reward = -0.754117	array([[-3.8459861, -2.135579 ]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 4s - loss: 0.0647 - val_loss: 0.0316

Epoch 2/50

 - 4s - loss: 0.0670 - val_loss: 0.0374

Epoch 3/50

 - 4s - loss: 0.0637 - val_loss: 0.0347

Epoch 4/50

 - 3s - loss: 0.0671 - val_loss: 0.0371

Epoch 5/50

 - 3s - loss: 0.0696 - val_loss: 0.0465

Epoch 6/50

 - 3s - loss: 0.0619 - val_loss: 0.0366

Epoch 7/50

 - 3s - loss: 0.0652 - val_loss: 0.0427

Epoch 8/50

 - 4s - loss: 0.0590 - val_loss: 0.0381

Epoch 9/50

 - 4s - loss: 0.0687 - val_loss: 0.0439

Epoch 10/50

 - 4s - loss: 0.0505 - val_loss: 0.0428

Epoch 11/50

 - 3s - loss: 0.0585 - val_loss: 0.0366

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 818, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 806, after forget

time = 17440	action = 0	current_phase = 0	next_phase = 1	reward = -0.290096	array([[-1.5359573, -2.808027 ]], dtype=float32)

time = 17445	action = 0	current_phase = 0	next_phase = 1	reward = 0.334779	array([[-2.0398316, -2.8616686]], dtype=float32)

time = 17450	action = 1	current_phase = 0	next_phase = 1	reward = -1.301173	array([[-2.9875524, -2.5951827]], dtype=float32)

time = 17458	action = 1	current_phase = 1	next_phase = 0	reward = -1.007586	array([[-3.7896533, -1.8303638]], dtype=float32)

time = 17466	action = 0	current_phase = 0	next_phase = 1	reward = 0.199968	array([[-1.3063483, -2.8474326]], dtype=float32)

time = 17471	action = 0	current_phase = 0	next_phase = 1	reward = -0.004516	array([[-1.7666972, -2.8401144]], dtype=float32)

time = 17476	action = 0	current_phase = 0	next_phase = 1	reward = 0.050099	array([[-2.2025805, -2.9562566]], dtype=float32)

time = 17481	action = 1	current_phase = 0	next_phase = 1	reward = -1.504513	array([[-3.911036 , -2.9768162]], dtype=float32)

time = 17489	action = 1	current_phase = 1	next_phase = 0	reward = -0.766028	array([[-3.8065214, -1.9504851]], dtype=float32)

time = 17497	action = 0	current_phase = 0	next_phase = 1	reward = -0.069254	array([[-1.3916472, -2.9225817]], dtype=float32)

time = 17502	action = 0	current_phase = 0	next_phase = 1	reward = 0.009481	array([[-1.7931535, -2.9597862]], dtype=float32)

time = 17507	action = 0	current_phase = 0	next_phase = 1	reward = 0.064235	array([[-2.4634595, -2.942143 ]], dtype=float32)

time = 17512	action = 1	current_phase = 0	next_phase = 1	reward = -1.502008	array([[-3.9802723, -3.370162 ]], dtype=float32)

time = 17520	action = 1	current_phase = 1	next_phase = 0	reward = -0.938225	array([[-3.6496296, -1.9993461]], dtype=float32)

time = 17528	action = 0	current_phase = 0	next_phase = 1	reward = -0.044087	array([[-1.415907 , -2.8788648]], dtype=float32)

time = 17533	action = 0	current_phase = 0	next_phase = 1	reward = 0.023391	array([[-1.8988843, -3.0340664]], dtype=float32)

time = 17538	action = 0	current_phase = 0	next_phase = 1	reward = 0.079707	array([[-2.5570571, -2.9414093]], dtype=float32)

time = 17543	action = 1	current_phase = 0	next_phase = 1	reward = -1.441733	array([[-4.0741715, -3.4419093]], dtype=float32)

time = 17551	action = 1	current_phase = 1	next_phase = 0	reward = -1.657784	array([[-3.6630406, -2.255873 ]], dtype=float32)

time = 17559	action = 0	current_phase = 0	next_phase = 1	reward = 0.240404	array([[-1.3876071, -2.7879908]], dtype=float32)

time = 17564	action = 0	current_phase = 0	next_phase = 1	reward = 0.049380	array([[-1.70047  , -2.8595767]], dtype=float32)

time = 17569	action = 0	current_phase = 0	next_phase = 1	reward = 0.065345	array([[-2.21959  , -2.4508467]], dtype=float32)

time = 17574	action = 1	current_phase = 0	next_phase = 1	reward = -1.137945	array([[-3.4823916, -2.830182 ]], dtype=float32)

time = 17582	action = 1	current_phase = 1	next_phase = 0	reward = -0.981068	array([[-3.5091681, -2.06742  ]], dtype=float32)

time = 17590	action = 0	current_phase = 0	next_phase = 1	reward = -0.016081	array([[-1.4853618, -2.800256 ]], dtype=float32)

time = 17595	action = 0	current_phase = 0	next_phase = 1	reward = 0.330571	array([[-2.0910058, -2.867897 ]], dtype=float32)

time = 17600	action = 1	current_phase = 0	next_phase = 1	reward = -1.369639	array([[-3.0027578, -2.7245462]], dtype=float32)

time = 17608	action = 1	current_phase = 1	next_phase = 0	reward = -0.695193	array([[-3.7842588, -1.8947103]], dtype=float32)

time = 17616	action = 0	current_phase = 0	next_phase = 1	reward = -0.080210	array([[-1.3639193, -2.926508 ]], dtype=float32)

time = 17621	action = 0	current_phase = 0	next_phase = 1	reward = -0.008725	array([[-1.832984 , -2.8621573]], dtype=float32)

time = 17626	action = 0	current_phase = 0	next_phase = 1	reward = 0.064322	array([[-2.2945337, -2.973499 ]], dtype=float32)

time = 17631	action = 1	current_phase = 0	next_phase = 1	reward = -1.441907	array([[-4.0373254, -2.9435732]], dtype=float32)

time = 17639	action = 1	current_phase = 1	next_phase = 0	reward = -0.784616	array([[-3.762189 , -1.9430897]], dtype=float32)

time = 17647	action = 0	current_phase = 0	next_phase = 1	reward = -0.061782	array([[-1.3883545, -2.959659 ]], dtype=float32)

time = 17652	action = 0	current_phase = 0	next_phase = 1	reward = 0.007659	array([[-1.9509385, -2.995644 ]], dtype=float32)

time = 17657	action = 0	current_phase = 0	next_phase = 1	reward = 0.074543	array([[-2.3180568, -2.9860353]], dtype=float32)

time = 17662	action = 1	current_phase = 0	next_phase = 1	reward = -1.648376	array([[-4.0487614, -3.172395 ]], dtype=float32)

time = 17670	action = 1	current_phase = 1	next_phase = 0	reward = -1.025320	array([[-3.7135239, -2.0279634]], dtype=float32)

time = 17678	action = 0	current_phase = 0	next_phase = 1	reward = -0.084964	array([[-1.4698801, -2.9261374]], dtype=float32)

time = 17683	action = 0	current_phase = 0	next_phase = 1	reward = 0.000776	array([[-1.9692211, -3.0510213]], dtype=float32)

time = 17688	action = 0	current_phase = 0	next_phase = 1	reward = 0.092082	array([[-2.6466212, -2.9958863]], dtype=float32)

time = 17693	action = 1	current_phase = 0	next_phase = 1	reward = -1.786384	array([[-4.0086126, -3.4441338]], dtype=float32)

time = 17701	action = 1	current_phase = 1	next_phase = 0	reward = -1.135365	array([[-3.7952485, -2.136611 ]], dtype=float32)

time = 17709	action = 0	current_phase = 0	next_phase = 1	reward = -0.033436	array([[-1.3331494, -2.7616591]], dtype=float32)

time = 17714	action = 0	current_phase = 0	next_phase = 1	reward = 0.031363	array([[-1.7995648, -2.8649638]], dtype=float32)

time = 17719	action = 1	current_phase = 0	next_phase = 1	reward = -0.687654	array([[-2.576582 , -2.5187082]], dtype=float32)

time = 17727	action = 1	current_phase = 1	next_phase = 0	reward = -0.637890	array([[-3.3704386, -2.019745 ]], dtype=float32)

time = 17735	action = 0	current_phase = 0	next_phase = 1	reward = -0.088690	array([[-1.4635704, -3.0047882]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0887 - val_loss: 0.0393

Epoch 2/50

 - 4s - loss: 0.0696 - val_loss: 0.0386

Epoch 3/50

 - 4s - loss: 0.0751 - val_loss: 0.0366

Epoch 4/50

 - 3s - loss: 0.0670 - val_loss: 0.0369

Epoch 5/50

 - 4s - loss: 0.0692 - val_loss: 0.0411

Epoch 6/50

 - 4s - loss: 0.0712 - val_loss: 0.0390

Epoch 7/50

 - 4s - loss: 0.0777 - val_loss: 0.0383

Epoch 8/50

 - 4s - loss: 0.0649 - val_loss: 0.0399

Epoch 9/50

 - 4s - loss: 0.0594 - val_loss: 0.0413

Epoch 10/50

 - 4s - loss: 0.0758 - val_loss: 0.0370

Epoch 11/50

 - 4s - loss: 0.0617 - val_loss: 0.0443

Epoch 12/50

 - 4s - loss: 0.0580 - val_loss: 0.0394

Epoch 13/50

 - 3s - loss: 0.0648 - val_loss: 0.0398

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 828, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 816, after forget

time = 17740	action = 0	current_phase = 0	next_phase = 1	reward = -0.010087	array([[-1.6815447, -2.816609 ]], dtype=float32)

time = 17745	action = 0	current_phase = 0	next_phase = 1	reward = 0.042611	array([[-2.170346 , -2.8864133]], dtype=float32)

time = 17750	action = 1	current_phase = 0	next_phase = 1	reward = -1.426276	array([[-3.3844898, -2.7372093]], dtype=float32)

time = 17758	action = 1	current_phase = 1	next_phase = 0	reward = -0.703549	array([[-3.8525302, -1.8579428]], dtype=float32)

time = 17766	action = 0	current_phase = 0	next_phase = 1	reward = -0.086973	array([[-1.3937271, -2.8568068]], dtype=float32)

time = 17771	action = 0	current_phase = 0	next_phase = 1	reward = -0.004353	array([[-1.893281 , -2.8406382]], dtype=float32)

time = 17776	action = 0	current_phase = 0	next_phase = 1	reward = 0.060167	array([[-2.3817737, -2.9442923]], dtype=float32)

time = 17781	action = 1	current_phase = 0	next_phase = 1	reward = -1.513722	array([[-3.8316715, -2.9038687]], dtype=float32)

time = 17789	action = 1	current_phase = 1	next_phase = 0	reward = -0.838439	array([[-3.7239342, -1.8301727]], dtype=float32)

time = 17797	action = 0	current_phase = 0	next_phase = 1	reward = -0.076074	array([[-1.4639785, -2.907512 ]], dtype=float32)

time = 17802	action = 0	current_phase = 0	next_phase = 1	reward = 0.000271	array([[-1.9898382, -2.8742476]], dtype=float32)

time = 17807	action = 0	current_phase = 0	next_phase = 1	reward = 0.071405	array([[-2.326229 , -2.9607232]], dtype=float32)

time = 17812	action = 1	current_phase = 0	next_phase = 1	reward = -1.632046	array([[-3.9833705, -3.0518043]], dtype=float32)

time = 17820	action = 1	current_phase = 1	next_phase = 0	reward = -1.252280	array([[-3.798548 , -2.0025368]], dtype=float32)

time = 17828	action = 0	current_phase = 0	next_phase = 1	reward = 0.237283	array([[-1.4608068, -2.8538215]], dtype=float32)

time = 17833	action = 0	current_phase = 0	next_phase = 1	reward = 0.038893	array([[-1.9369859, -3.0145001]], dtype=float32)

time = 17838	action = 0	current_phase = 0	next_phase = 1	reward = 0.079541	array([[-2.3942347, -2.9445548]], dtype=float32)

time = 17843	action = 1	current_phase = 0	next_phase = 1	reward = -1.410938	array([[-4.0820503, -3.3881352]], dtype=float32)

time = 17851	action = 1	current_phase = 1	next_phase = 0	reward = -1.143225	array([[-3.6472821, -2.0930629]], dtype=float32)

time = 17859	action = 0	current_phase = 0	next_phase = 1	reward = -0.041718	array([[-1.3905851, -2.765338 ]], dtype=float32)

time = 17864	action = 0	current_phase = 0	next_phase = 1	reward = 0.013328	array([[-1.8377967, -2.8552885]], dtype=float32)

time = 17869	action = 0	current_phase = 0	next_phase = 1	reward = 0.063581	array([[-2.2380219, -2.3204796]], dtype=float32)

time = 17874	action = 1	current_phase = 0	next_phase = 1	reward = -1.068581	array([[-3.8526547, -2.990749 ]], dtype=float32)

time = 17882	action = 1	current_phase = 1	next_phase = 0	reward = -0.754693	array([[-3.839182 , -2.0905252]], dtype=float32)

time = 17890	action = 0	current_phase = 0	next_phase = 1	reward = -0.038354	array([[-1.6232693, -2.7551847]], dtype=float32)

time = 17895	action = 0	current_phase = 0	next_phase = 1	reward = -0.256517	array([[-2.0834875, -2.8589306]], dtype=float32)

time = 17900	action = 1	current_phase = 0	next_phase = 1	reward = -0.899566	array([[-2.792047 , -2.6369772]], dtype=float32)

time = 17908	action = 1	current_phase = 1	next_phase = 0	reward = -0.709024	array([[-3.5073056, -1.8866502]], dtype=float32)

time = 17916	action = 0	current_phase = 0	next_phase = 1	reward = -0.083003	array([[-1.4204786, -2.884995 ]], dtype=float32)

time = 17921	action = 0	current_phase = 0	next_phase = 1	reward = -0.010448	array([[-1.7375913, -2.8116536]], dtype=float32)

time = 17926	action = 0	current_phase = 0	next_phase = 1	reward = 0.060650	array([[-2.3778682, -2.9443283]], dtype=float32)

time = 17931	action = 1	current_phase = 0	next_phase = 1	reward = -1.432709	array([[-3.9429705, -2.8770132]], dtype=float32)

time = 17939	action = 1	current_phase = 1	next_phase = 0	reward = -0.778816	array([[-3.7115119, -1.8164388]], dtype=float32)

time = 17947	action = 0	current_phase = 0	next_phase = 1	reward = -0.064064	array([[-1.4648858, -2.8844638]], dtype=float32)

time = 17952	action = 0	current_phase = 0	next_phase = 1	reward = 0.005433	array([[-1.9272854, -2.8745704]], dtype=float32)

time = 17957	action = 0	current_phase = 0	next_phase = 1	reward = 0.060938	array([[-2.4301066, -2.925645 ]], dtype=float32)

time = 17962	action = 1	current_phase = 0	next_phase = 1	reward = -1.669450	array([[-3.9606931, -3.3440285]], dtype=float32)

time = 17970	action = 1	current_phase = 1	next_phase = 0	reward = -0.949552	array([[-3.7615151, -2.0116415]], dtype=float32)

time = 17978	action = 0	current_phase = 0	next_phase = 1	reward = -0.042725	array([[-1.4670088, -2.8478   ]], dtype=float32)

time = 17983	action = 0	current_phase = 0	next_phase = 1	reward = 0.018572	array([[-1.9604355, -3.0057368]], dtype=float32)

time = 17988	action = 0	current_phase = 0	next_phase = 1	reward = 0.079635	array([[-2.7493985, -2.7619543]], dtype=float32)

time = 17993	action = 1	current_phase = 0	next_phase = 1	reward = -1.938955	array([[-4.028163 , -3.4727643]], dtype=float32)

time = 18001	action = 1	current_phase = 1	next_phase = 0	reward = -1.037344	array([[-3.8631842, -2.0728357]], dtype=float32)

time = 18009	action = 0	current_phase = 0	next_phase = 1	reward = -0.046131	array([[-1.3797648, -2.7888286]], dtype=float32)

time = 18014	action = 0	current_phase = 0	next_phase = 1	reward = 0.022545	array([[-1.6631078, -2.8743198]], dtype=float32)

time = 18019	action = 0	current_phase = 0	next_phase = 1	reward = 0.077172	array([[-2.1660397, -2.2347767]], dtype=float32)

time = 18024	action = 1	current_phase = 0	next_phase = 1	reward = -1.066636	array([[-3.8758624, -2.9219835]], dtype=float32)

time = 18032	action = 1	current_phase = 1	next_phase = 0	reward = -1.036446	array([[-3.7563784, -2.0513358]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 4s - loss: 0.0665 - val_loss: 0.0312

Epoch 2/50

 - 3s - loss: 0.0656 - val_loss: 0.0381

Epoch 3/50

 - 3s - loss: 0.0666 - val_loss: 0.0331

Epoch 4/50

 - 4s - loss: 0.0603 - val_loss: 0.0314

Epoch 5/50

 - 4s - loss: 0.0647 - val_loss: 0.0306

Epoch 6/50

 - 4s - loss: 0.0592 - val_loss: 0.0337

Epoch 7/50

 - 4s - loss: 0.0616 - val_loss: 0.0386

Epoch 8/50

 - 4s - loss: 0.0652 - val_loss: 0.0346

Epoch 9/50

 - 4s - loss: 0.0553 - val_loss: 0.0311

Epoch 10/50

 - 4s - loss: 0.0635 - val_loss: 0.0341

Epoch 11/50

 - 4s - loss: 0.0565 - val_loss: 0.0374

Epoch 12/50

 - 3s - loss: 0.0533 - val_loss: 0.0355

Epoch 13/50

 - 3s - loss: 0.0566 - val_loss: 0.0416

Epoch 14/50

 - 3s - loss: 0.0477 - val_loss: 0.0338

Epoch 15/50

 - 3s - loss: 0.0529 - val_loss: 0.0358

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 838, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 826, after forget

time = 18040	action = 0	current_phase = 0	next_phase = 1	reward = 0.245491	array([[-1.5874474, -2.7842977]], dtype=float32)

time = 18045	action = 0	current_phase = 0	next_phase = 1	reward = -0.522076	array([[-2.0728216, -2.878582 ]], dtype=float32)

time = 18050	action = 1	current_phase = 0	next_phase = 1	reward = -0.788577	array([[-2.9443512, -2.6457317]], dtype=float32)

time = 18058	action = 1	current_phase = 1	next_phase = 0	reward = -0.701518	array([[-3.7447863, -1.8198804]], dtype=float32)

time = 18066	action = 0	current_phase = 0	next_phase = 1	reward = -0.084372	array([[-1.4181522, -2.8843853]], dtype=float32)

time = 18071	action = 0	current_phase = 0	next_phase = 1	reward = -0.003521	array([[-1.7328115, -2.838534 ]], dtype=float32)

time = 18076	action = 0	current_phase = 0	next_phase = 1	reward = 0.061240	array([[-2.3799186, -2.998777 ]], dtype=float32)

time = 18081	action = 1	current_phase = 0	next_phase = 1	reward = -1.393526	array([[-3.9016697, -3.0658576]], dtype=float32)

time = 18089	action = 1	current_phase = 1	next_phase = 0	reward = -0.850317	array([[-3.6882048, -1.7899495]], dtype=float32)

time = 18097	action = 0	current_phase = 0	next_phase = 1	reward = -0.058638	array([[-1.4314929, -2.881133 ]], dtype=float32)

time = 18102	action = 0	current_phase = 0	next_phase = 1	reward = -0.003338	array([[-1.7961597, -2.906144 ]], dtype=float32)

time = 18107	action = 0	current_phase = 0	next_phase = 1	reward = 0.060120	array([[-2.4517832, -2.9624898]], dtype=float32)

time = 18112	action = 1	current_phase = 0	next_phase = 1	reward = -1.492692	array([[-4.092885 , -3.1494792]], dtype=float32)

time = 18120	action = 1	current_phase = 1	next_phase = 0	reward = -0.887642	array([[-3.697704 , -2.0029533]], dtype=float32)

time = 18128	action = 0	current_phase = 0	next_phase = 1	reward = -0.038611	array([[-1.4567845, -2.905252 ]], dtype=float32)

time = 18133	action = 0	current_phase = 0	next_phase = 1	reward = 0.016850	array([[-1.9263053, -3.040952 ]], dtype=float32)

time = 18138	action = 0	current_phase = 0	next_phase = 1	reward = 0.075568	array([[-2.650915, -2.886473]], dtype=float32)

time = 18143	action = 1	current_phase = 0	next_phase = 1	reward = -1.953678	array([[-4.1866374, -3.5174534]], dtype=float32)

time = 18151	action = 1	current_phase = 1	next_phase = 0	reward = -1.088608	array([[-3.8485303, -2.2280703]], dtype=float32)

time = 18159	action = 0	current_phase = 0	next_phase = 1	reward = -0.035128	array([[-1.3560635, -2.7878716]], dtype=float32)

time = 18164	action = 0	current_phase = 0	next_phase = 1	reward = 0.033569	array([[-1.9068893, -2.9818153]], dtype=float32)

time = 18169	action = 0	current_phase = 0	next_phase = 1	reward = 0.059828	array([[-2.3188498, -2.5247276]], dtype=float32)

time = 18174	action = 1	current_phase = 0	next_phase = 1	reward = -1.116864	array([[-3.9213545, -2.7799761]], dtype=float32)

time = 18182	action = 1	current_phase = 1	next_phase = 0	reward = -0.763745	array([[-3.8616617, -2.053561 ]], dtype=float32)

time = 18190	action = 0	current_phase = 0	next_phase = 1	reward = -0.019816	array([[-1.7280879, -2.8001301]], dtype=float32)

time = 18195	action = 0	current_phase = 0	next_phase = 1	reward = -0.228017	array([[-2.1657054, -2.862192 ]], dtype=float32)

time = 18200	action = 1	current_phase = 0	next_phase = 1	reward = -1.087775	array([[-3.0483716, -2.7441015]], dtype=float32)

time = 18208	action = 1	current_phase = 1	next_phase = 0	reward = -0.715302	array([[-3.7171855, -1.7989486]], dtype=float32)

time = 18216	action = 0	current_phase = 0	next_phase = 1	reward = -0.092400	array([[-1.441405 , -2.8986242]], dtype=float32)

time = 18221	action = 0	current_phase = 0	next_phase = 1	reward = -0.011056	array([[-1.7126262, -2.8234398]], dtype=float32)

time = 18226	action = 0	current_phase = 0	next_phase = 1	reward = 0.059516	array([[-2.4150908, -2.9955935]], dtype=float32)

time = 18231	action = 1	current_phase = 0	next_phase = 1	reward = -1.447378	array([[-3.8728786, -3.0284982]], dtype=float32)

time = 18239	action = 1	current_phase = 1	next_phase = 0	reward = -0.768675	array([[-3.8118849, -1.798127 ]], dtype=float32)

time = 18247	action = 0	current_phase = 0	next_phase = 1	reward = -0.068194	array([[-1.4287629, -2.9364583]], dtype=float32)

time = 18252	action = 0	current_phase = 0	next_phase = 1	reward = 0.005809	array([[-1.8832347, -2.907139 ]], dtype=float32)

time = 18257	action = 0	current_phase = 0	next_phase = 1	reward = 0.064263	array([[-2.5579636, -3.0403252]], dtype=float32)

time = 18262	action = 1	current_phase = 0	next_phase = 1	reward = -1.557242	array([[-4.0877934, -3.0868187]], dtype=float32)

time = 18270	action = 1	current_phase = 1	next_phase = 0	reward = -0.936470	array([[-3.808789 , -2.0566354]], dtype=float32)

time = 18278	action = 0	current_phase = 0	next_phase = 1	reward = -0.031807	array([[-1.4599128, -2.9010365]], dtype=float32)

time = 18283	action = 0	current_phase = 0	next_phase = 1	reward = 0.041691	array([[-2.038525, -3.04355 ]], dtype=float32)

time = 18288	action = 0	current_phase = 0	next_phase = 1	reward = 0.079488	array([[-2.6850374, -2.9015715]], dtype=float32)

time = 18293	action = 1	current_phase = 0	next_phase = 1	reward = -0.873903	array([[-4.091154 , -3.1793416]], dtype=float32)

time = 18301	action = 1	current_phase = 1	next_phase = 0	reward = -1.355675	array([[-3.507852, -2.226131]], dtype=float32)

time = 18309	action = 0	current_phase = 0	next_phase = 1	reward = -0.025126	array([[-1.3627455, -2.8183725]], dtype=float32)

time = 18314	action = 0	current_phase = 0	next_phase = 1	reward = 0.042599	array([[-1.8890994, -2.9144673]], dtype=float32)

time = 18319	action = 1	current_phase = 0	next_phase = 1	reward = -1.284473	array([[-2.6502013, -2.63407  ]], dtype=float32)

time = 18327	action = 1	current_phase = 1	next_phase = 0	reward = -1.136482	array([[-3.7896965, -2.0765727]], dtype=float32)

time = 18335	action = 0	current_phase = 0	next_phase = 1	reward = 0.481954	array([[-1.493423 , -2.9286895]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0706 - val_loss: 0.0846

Epoch 2/50

 - 3s - loss: 0.0631 - val_loss: 0.0748

Epoch 3/50

 - 3s - loss: 0.0692 - val_loss: 0.0512

Epoch 4/50

 - 3s - loss: 0.0639 - val_loss: 0.0667

Epoch 5/50

 - 3s - loss: 0.0648 - val_loss: 0.0900

Epoch 6/50

 - 3s - loss: 0.0636 - val_loss: 0.0597

Epoch 7/50

 - 3s - loss: 0.0572 - val_loss: 0.0806

Epoch 8/50

 - 3s - loss: 0.0634 - val_loss: 0.1014

Epoch 9/50

 - 3s - loss: 0.0613 - val_loss: 0.0904

Epoch 10/50

 - 3s - loss: 0.0540 - val_loss: 0.0817

Epoch 11/50

 - 3s - loss: 0.0606 - val_loss: 0.0744

Epoch 12/50

 - 3s - loss: 0.0534 - val_loss: 0.0940

Epoch 13/50

 - 3s - loss: 0.0672 - val_loss: 0.1222

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 848, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 836, after forget

time = 18340	action = 0	current_phase = 0	next_phase = 1	reward = -0.293338	array([[-1.669487 , -2.7450166]], dtype=float32)

time = 18345	action = 0	current_phase = 0	next_phase = 1	reward = 0.342270	array([[-2.13924  , -2.8917246]], dtype=float32)

time = 18350	action = 1	current_phase = 0	next_phase = 1	reward = -1.311757	array([[-3.5083435, -2.8643618]], dtype=float32)

time = 18358	action = 1	current_phase = 1	next_phase = 0	reward = -0.695737	array([[-3.81363  , -1.9024761]], dtype=float32)

time = 18366	action = 0	current_phase = 0	next_phase = 1	reward = -0.069095	array([[-1.4273522, -2.8280978]], dtype=float32)

time = 18371	action = 0	current_phase = 0	next_phase = 1	reward = 0.004750	array([[-1.8384931, -2.814443 ]], dtype=float32)

time = 18376	action = 0	current_phase = 0	next_phase = 1	reward = 0.067748	array([[-2.3185127, -2.9115767]], dtype=float32)

time = 18381	action = 1	current_phase = 0	next_phase = 1	reward = -1.454306	array([[-4.1474676, -2.8577147]], dtype=float32)

time = 18389	action = 1	current_phase = 1	next_phase = 0	reward = -0.883393	array([[-3.7935996, -1.9305766]], dtype=float32)

time = 18397	action = 0	current_phase = 0	next_phase = 1	reward = -0.069478	array([[-1.5024645, -2.8589349]], dtype=float32)

time = 18402	action = 0	current_phase = 0	next_phase = 1	reward = 0.008887	array([[-1.8938916, -2.9053726]], dtype=float32)

time = 18407	action = 0	current_phase = 0	next_phase = 1	reward = 0.070868	array([[-2.3992643, -2.934527 ]], dtype=float32)

time = 18412	action = 1	current_phase = 0	next_phase = 1	reward = -1.616650	array([[-4.129743 , -2.9846725]], dtype=float32)

time = 18420	action = 1	current_phase = 1	next_phase = 0	reward = -0.963111	array([[-3.7434611, -2.110559 ]], dtype=float32)

time = 18428	action = 0	current_phase = 0	next_phase = 1	reward = -0.050968	array([[-1.5104532, -2.8350568]], dtype=float32)

time = 18433	action = 0	current_phase = 0	next_phase = 1	reward = 0.034141	array([[-1.9187076, -2.979311 ]], dtype=float32)

time = 18438	action = 1	current_phase = 0	next_phase = 1	reward = -1.716224	array([[-2.7425497, -2.7261462]], dtype=float32)

time = 18446	action = 1	current_phase = 1	next_phase = 0	reward = -1.084078	array([[-3.8094442, -1.8966316]], dtype=float32)

time = 18454	action = 0	current_phase = 0	next_phase = 1	reward = 0.172170	array([[-1.3804505, -2.8741608]], dtype=float32)

time = 18459	action = 0	current_phase = 0	next_phase = 1	reward = -0.050648	array([[-1.6119643, -2.815837 ]], dtype=float32)

time = 18464	action = 0	current_phase = 0	next_phase = 1	reward = 0.036399	array([[-1.8680658, -3.0017452]], dtype=float32)

time = 18469	action = 1	current_phase = 0	next_phase = 1	reward = -0.637204	array([[-3.2344694, -2.7725782]], dtype=float32)

time = 18477	action = 1	current_phase = 1	next_phase = 0	reward = -0.651144	array([[-3.5688832, -2.0186377]], dtype=float32)

time = 18485	action = 0	current_phase = 0	next_phase = 1	reward = -0.367548	array([[-1.385307 , -2.8392663]], dtype=float32)

time = 18490	action = 0	current_phase = 0	next_phase = 1	reward = 0.267082	array([[-1.580594 , -2.7914953]], dtype=float32)

time = 18495	action = 0	current_phase = 0	next_phase = 1	reward = -0.225242	array([[-2.1582785, -2.904574 ]], dtype=float32)

time = 18500	action = 1	current_phase = 0	next_phase = 1	reward = -1.089072	array([[-3.4939647, -2.667646 ]], dtype=float32)

time = 18508	action = 1	current_phase = 1	next_phase = 0	reward = -0.711820	array([[-3.7969308, -1.9154176]], dtype=float32)

time = 18516	action = 0	current_phase = 0	next_phase = 1	reward = -0.092402	array([[-1.4960916, -2.8420515]], dtype=float32)

time = 18521	action = 0	current_phase = 0	next_phase = 1	reward = -0.015529	array([[-1.7817075, -2.7949414]], dtype=float32)

time = 18526	action = 0	current_phase = 0	next_phase = 1	reward = 0.046019	array([[-2.237827 , -2.9044652]], dtype=float32)

time = 18531	action = 1	current_phase = 0	next_phase = 1	reward = -1.413568	array([[-4.146309, -2.872332]], dtype=float32)

time = 18539	action = 1	current_phase = 1	next_phase = 0	reward = -0.827138	array([[-3.7116046, -1.8917899]], dtype=float32)

time = 18547	action = 0	current_phase = 0	next_phase = 1	reward = -0.073108	array([[-1.4497519, -2.8206234]], dtype=float32)

time = 18552	action = 0	current_phase = 0	next_phase = 1	reward = 0.007592	array([[-1.8807163, -2.845059 ]], dtype=float32)

time = 18557	action = 0	current_phase = 0	next_phase = 1	reward = 0.063869	array([[-2.4108794, -2.9274974]], dtype=float32)

time = 18562	action = 1	current_phase = 0	next_phase = 1	reward = -1.604148	array([[-4.1076303, -3.0703316]], dtype=float32)

time = 18570	action = 1	current_phase = 1	next_phase = 0	reward = -1.060618	array([[-3.8016317, -2.129871 ]], dtype=float32)

time = 18578	action = 0	current_phase = 0	next_phase = 1	reward = -0.046691	array([[-1.4530602, -2.8235931]], dtype=float32)

time = 18583	action = 0	current_phase = 0	next_phase = 1	reward = 0.033734	array([[-2.0146248, -3.0313935]], dtype=float32)

time = 18588	action = 0	current_phase = 0	next_phase = 1	reward = 0.083134	array([[-2.6354496, -2.8632646]], dtype=float32)

time = 18593	action = 1	current_phase = 0	next_phase = 1	reward = -1.774311	array([[-4.1867137, -3.3032389]], dtype=float32)

time = 18601	action = 1	current_phase = 1	next_phase = 0	reward = -1.383345	array([[-4.0167894, -2.2606745]], dtype=float32)

time = 18609	action = 0	current_phase = 0	next_phase = 1	reward = 0.266213	array([[-1.3520241, -2.717332 ]], dtype=float32)

time = 18614	action = 0	current_phase = 0	next_phase = 1	reward = 0.036059	array([[-1.6304216, -2.8073735]], dtype=float32)

time = 18619	action = 0	current_phase = 0	next_phase = 1	reward = 0.342016	array([[-2.2415762, -2.3351188]], dtype=float32)

time = 18624	action = 1	current_phase = 0	next_phase = 1	reward = -1.460954	array([[-4.0346637, -2.7856984]], dtype=float32)

time = 18632	action = 1	current_phase = 1	next_phase = 0	reward = -0.763285	array([[-4.012047, -2.153131]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0897 - val_loss: 0.0304

Epoch 2/50

 - 3s - loss: 0.0686 - val_loss: 0.0346

Epoch 3/50

 - 3s - loss: 0.0694 - val_loss: 0.0341

Epoch 4/50

 - 3s - loss: 0.0745 - val_loss: 0.0474

Epoch 5/50

 - 4s - loss: 0.0633 - val_loss: 0.0518

Epoch 6/50

 - 3s - loss: 0.0648 - val_loss: 0.0366

Epoch 7/50

 - 3s - loss: 0.0714 - val_loss: 0.0340

Epoch 8/50

 - 3s - loss: 0.0675 - val_loss: 0.0366

Epoch 9/50

 - 3s - loss: 0.0632 - val_loss: 0.0373

Epoch 10/50

 - 4s - loss: 0.0697 - val_loss: 0.0393

Epoch 11/50

 - 4s - loss: 0.0612 - val_loss: 0.0359

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 858, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 846, after forget

time = 18640	action = 0	current_phase = 0	next_phase = 1	reward = -0.022670	array([[-1.7770455, -2.7380867]], dtype=float32)

time = 18645	action = 0	current_phase = 0	next_phase = 1	reward = 0.051878	array([[-2.1160374, -2.893134 ]], dtype=float32)

time = 18650	action = 1	current_phase = 0	next_phase = 1	reward = -1.349653	array([[-3.2026467, -2.778922 ]], dtype=float32)

time = 18658	action = 1	current_phase = 1	next_phase = 0	reward = -0.710899	array([[-3.7977035, -1.9727354]], dtype=float32)

time = 18666	action = 0	current_phase = 0	next_phase = 1	reward = -0.084204	array([[-1.5045347, -2.8597836]], dtype=float32)

time = 18671	action = 0	current_phase = 0	next_phase = 1	reward = -0.007745	array([[-1.9724886, -2.827054 ]], dtype=float32)

time = 18676	action = 0	current_phase = 0	next_phase = 1	reward = 0.072246	array([[-2.294169, -2.893219]], dtype=float32)

time = 18681	action = 1	current_phase = 0	next_phase = 1	reward = -1.501261	array([[-4.032522, -2.940662]], dtype=float32)

time = 18689	action = 1	current_phase = 1	next_phase = 0	reward = -0.839744	array([[-3.7549534, -2.0049489]], dtype=float32)

time = 18697	action = 0	current_phase = 0	next_phase = 1	reward = -0.073955	array([[-1.5681882, -2.8709388]], dtype=float32)

time = 18702	action = 0	current_phase = 0	next_phase = 1	reward = -0.001643	array([[-2.1025515, -2.8728778]], dtype=float32)

time = 18707	action = 0	current_phase = 0	next_phase = 1	reward = 0.066492	array([[-2.3195183, -2.957391 ]], dtype=float32)

time = 18712	action = 1	current_phase = 0	next_phase = 1	reward = -1.589716	array([[-4.188893 , -3.1856327]], dtype=float32)

time = 18720	action = 1	current_phase = 1	next_phase = 0	reward = -0.949902	array([[-3.7512305, -2.2250605]], dtype=float32)

time = 18728	action = 0	current_phase = 0	next_phase = 1	reward = -0.052389	array([[-1.5541849, -2.7998126]], dtype=float32)

time = 18733	action = 0	current_phase = 0	next_phase = 1	reward = 0.029581	array([[-2.144165 , -3.0051749]], dtype=float32)

time = 18738	action = 0	current_phase = 0	next_phase = 1	reward = 0.081932	array([[-2.7492213, -2.8357618]], dtype=float32)

time = 18743	action = 1	current_phase = 0	next_phase = 1	reward = -1.411012	array([[-4.1832423, -3.261595 ]], dtype=float32)

time = 18751	action = 1	current_phase = 1	next_phase = 0	reward = -1.529496	array([[-3.7598445, -2.5424008]], dtype=float32)

time = 18759	action = 0	current_phase = 0	next_phase = 1	reward = 0.275321	array([[-1.428221 , -2.7814558]], dtype=float32)

time = 18764	action = 0	current_phase = 0	next_phase = 1	reward = 0.311891	array([[-1.837167 , -2.8385394]], dtype=float32)

time = 18769	action = 0	current_phase = 0	next_phase = 1	reward = -0.217890	array([[-2.166698 , -2.3462627]], dtype=float32)

time = 18774	action = 1	current_phase = 0	next_phase = 1	reward = -1.181036	array([[-3.982468 , -2.8607264]], dtype=float32)

time = 18782	action = 1	current_phase = 1	next_phase = 0	reward = -0.985122	array([[-3.7789943, -2.210392 ]], dtype=float32)

time = 18790	action = 0	current_phase = 0	next_phase = 1	reward = -0.032397	array([[-1.6261207, -2.7115836]], dtype=float32)

time = 18795	action = 0	current_phase = 0	next_phase = 1	reward = -0.230885	array([[-2.0666225, -2.8648083]], dtype=float32)

time = 18800	action = 1	current_phase = 0	next_phase = 1	reward = -0.732336	array([[-2.98951 , -2.489293]], dtype=float32)

time = 18808	action = 1	current_phase = 1	next_phase = 0	reward = -0.715650	array([[-3.4784276, -1.9223828]], dtype=float32)

time = 18816	action = 0	current_phase = 0	next_phase = 1	reward = -0.072526	array([[-1.5943432, -2.8941262]], dtype=float32)

time = 18821	action = 0	current_phase = 0	next_phase = 1	reward = -0.000354	array([[-2.044506 , -2.8300116]], dtype=float32)

time = 18826	action = 0	current_phase = 0	next_phase = 1	reward = 0.071545	array([[-2.396983 , -2.9282794]], dtype=float32)

time = 18831	action = 1	current_phase = 0	next_phase = 1	reward = -1.493810	array([[-4.0617642, -2.9801798]], dtype=float32)

time = 18839	action = 1	current_phase = 1	next_phase = 0	reward = -0.787490	array([[-3.6550338, -1.8994776]], dtype=float32)

time = 18847	action = 0	current_phase = 0	next_phase = 1	reward = -0.077448	array([[-1.5045273, -2.8300142]], dtype=float32)

time = 18852	action = 0	current_phase = 0	next_phase = 1	reward = 0.009460	array([[-2.1502302, -2.8916135]], dtype=float32)

time = 18857	action = 0	current_phase = 0	next_phase = 1	reward = 0.084769	array([[-2.400088 , -2.9053366]], dtype=float32)

time = 18862	action = 1	current_phase = 0	next_phase = 1	reward = -1.651288	array([[-4.19587  , -3.1978147]], dtype=float32)

time = 18870	action = 1	current_phase = 1	next_phase = 0	reward = -1.009800	array([[-3.8129997, -2.2260103]], dtype=float32)

time = 18878	action = 0	current_phase = 0	next_phase = 1	reward = -0.057747	array([[-1.5559103, -2.8214784]], dtype=float32)

time = 18883	action = 0	current_phase = 0	next_phase = 1	reward = 0.015339	array([[-2.154126 , -3.0020301]], dtype=float32)

time = 18888	action = 0	current_phase = 0	next_phase = 1	reward = 0.070174	array([[-2.5888894, -2.9128468]], dtype=float32)

time = 18893	action = 1	current_phase = 0	next_phase = 1	reward = -1.879723	array([[-4.3365307, -3.5249846]], dtype=float32)

time = 18901	action = 1	current_phase = 1	next_phase = 0	reward = -1.280346	array([[-4.12342  , -2.4273882]], dtype=float32)

time = 18909	action = 0	current_phase = 0	next_phase = 1	reward = 0.272345	array([[-1.4048592, -2.6807876]], dtype=float32)

time = 18914	action = 0	current_phase = 0	next_phase = 1	reward = 0.291013	array([[-1.7630534, -2.8023703]], dtype=float32)

time = 18919	action = 0	current_phase = 0	next_phase = 1	reward = -0.224233	array([[-2.3435788, -2.3864326]], dtype=float32)

time = 18924	action = 1	current_phase = 0	next_phase = 1	reward = -1.117013	array([[-4.155492 , -2.9043908]], dtype=float32)

time = 18932	action = 1	current_phase = 1	next_phase = 0	reward = -0.979971	array([[-3.6194227, -2.23179  ]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 4s - loss: 0.0808 - val_loss: 0.0715

Epoch 2/50

 - 4s - loss: 0.0676 - val_loss: 0.0763

Epoch 3/50

 - 5s - loss: 0.0850 - val_loss: 0.0922

Epoch 4/50

 - 5s - loss: 0.0730 - val_loss: 0.0553

Epoch 5/50

 - 4s - loss: 0.0561 - val_loss: 0.0530

Epoch 6/50

 - 3s - loss: 0.0614 - val_loss: 0.0476

Epoch 7/50

 - 3s - loss: 0.0568 - val_loss: 0.0841

Epoch 8/50

 - 4s - loss: 0.0604 - val_loss: 0.0541

Epoch 9/50

 - 4s - loss: 0.0542 - val_loss: 0.0534

Epoch 10/50

 - 4s - loss: 0.0608 - val_loss: 0.0727

Epoch 11/50

 - 4s - loss: 0.0704 - val_loss: 0.0556

Epoch 12/50

 - 4s - loss: 0.0614 - val_loss: 0.0561

Epoch 13/50

 - 4s - loss: 0.0617 - val_loss: 0.0776

Epoch 14/50

 - 4s - loss: 0.0602 - val_loss: 0.0633

Epoch 15/50

 - 4s - loss: 0.0544 - val_loss: 0.0570

Epoch 16/50

 - 4s - loss: 0.0648 - val_loss: 0.0633

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 868, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 856, after forget

time = 18940	action = 0	current_phase = 0	next_phase = 1	reward = -0.033201	array([[-1.6698554, -2.7197526]], dtype=float32)

time = 18945	action = 0	current_phase = 0	next_phase = 1	reward = 0.327868	array([[-1.9718591, -2.864163 ]], dtype=float32)

time = 18950	action = 1	current_phase = 0	next_phase = 1	reward = -1.211647	array([[-2.97365  , -2.6730747]], dtype=float32)

time = 18958	action = 1	current_phase = 1	next_phase = 0	reward = -0.699143	array([[-2.9982896, -1.8074231]], dtype=float32)

time = 18966	action = 0	current_phase = 0	next_phase = 1	reward = -0.081113	array([[-1.56243  , -2.8312085]], dtype=float32)

time = 18971	action = 0	current_phase = 0	next_phase = 1	reward = -0.014177	array([[-1.9442987, -2.8139286]], dtype=float32)

time = 18976	action = 0	current_phase = 0	next_phase = 1	reward = 0.047883	array([[-2.299034 , -2.8936265]], dtype=float32)

time = 18981	action = 1	current_phase = 0	next_phase = 1	reward = -1.330723	array([[-3.8671365, -2.9730923]], dtype=float32)

time = 18989	action = 1	current_phase = 1	next_phase = 0	reward = -0.812332	array([[-3.8190842, -2.054754 ]], dtype=float32)

time = 18997	action = 0	current_phase = 0	next_phase = 1	reward = -0.065647	array([[-1.4827833, -2.818291 ]], dtype=float32)

time = 19002	action = 0	current_phase = 0	next_phase = 1	reward = 0.010251	array([[-1.9979143, -2.9196696]], dtype=float32)

time = 19007	action = 0	current_phase = 0	next_phase = 1	reward = 0.064898	array([[-2.461053 , -2.9428337]], dtype=float32)

time = 19012	action = 1	current_phase = 0	next_phase = 1	reward = -1.591281	array([[-4.190449 , -3.1834514]], dtype=float32)

time = 19020	action = 1	current_phase = 1	next_phase = 0	reward = -0.945818	array([[-3.7775192, -2.0900252]], dtype=float32)

time = 19028	action = 0	current_phase = 0	next_phase = 1	reward = -0.071999	array([[-1.4411765, -2.795574 ]], dtype=float32)

time = 19033	action = 0	current_phase = 0	next_phase = 1	reward = 0.002390	array([[-2.127849 , -2.9920874]], dtype=float32)

time = 19038	action = 0	current_phase = 0	next_phase = 1	reward = 0.074433	array([[-2.7592123, -2.7880738]], dtype=float32)

time = 19043	action = 1	current_phase = 0	next_phase = 1	reward = -1.853825	array([[-4.2613153, -3.8341389]], dtype=float32)

time = 19051	action = 1	current_phase = 1	next_phase = 0	reward = -1.335667	array([[-4.007901 , -2.3039143]], dtype=float32)

time = 19059	action = 0	current_phase = 0	next_phase = 1	reward = 0.255078	array([[-1.3507622, -2.693084 ]], dtype=float32)

time = 19064	action = 0	current_phase = 0	next_phase = 1	reward = 0.019527	array([[-1.7044239, -2.8406246]], dtype=float32)

time = 19069	action = 0	current_phase = 0	next_phase = 1	reward = 0.063045	array([[-2.3454885, -2.7180512]], dtype=float32)

time = 19074	action = 1	current_phase = 0	next_phase = 1	reward = -1.066698	array([[-4.0805783, -2.919638 ]], dtype=float32)

time = 19082	action = 1	current_phase = 1	next_phase = 0	reward = -1.040636	array([[-3.5282922, -2.2614253]], dtype=float32)

time = 19090	action = 0	current_phase = 0	next_phase = 1	reward = -0.015431	array([[-1.6630547, -2.7127535]], dtype=float32)

time = 19095	action = 0	current_phase = 0	next_phase = 1	reward = 0.327214	array([[-2.0559464, -2.893831 ]], dtype=float32)

time = 19100	action = 1	current_phase = 0	next_phase = 1	reward = -1.379475	array([[-3.1393414, -2.8027651]], dtype=float32)

time = 19108	action = 1	current_phase = 1	next_phase = 0	reward = -0.725839	array([[-3.8160095, -1.98363  ]], dtype=float32)

time = 19116	action = 0	current_phase = 0	next_phase = 1	reward = -0.092187	array([[-1.4268911, -2.8257055]], dtype=float32)

time = 19121	action = 0	current_phase = 0	next_phase = 1	reward = -0.004870	array([[-1.917754 , -2.8038347]], dtype=float32)

time = 19126	action = 0	current_phase = 0	next_phase = 1	reward = 0.064899	array([[-2.3162735, -2.897179 ]], dtype=float32)

time = 19131	action = 1	current_phase = 0	next_phase = 1	reward = -1.379104	array([[-4.18545  , -2.9538968]], dtype=float32)

time = 19139	action = 1	current_phase = 1	next_phase = 0	reward = -0.779508	array([[-3.844945, -2.010442]], dtype=float32)

time = 19147	action = 0	current_phase = 0	next_phase = 1	reward = -0.066535	array([[-1.5235767, -2.8431594]], dtype=float32)

time = 19152	action = 0	current_phase = 0	next_phase = 1	reward = 0.007081	array([[-2.0027657, -2.876305 ]], dtype=float32)

time = 19157	action = 0	current_phase = 0	next_phase = 1	reward = 0.080187	array([[-2.3922787, -2.9712317]], dtype=float32)

time = 19162	action = 1	current_phase = 0	next_phase = 1	reward = -1.642086	array([[-4.302618 , -3.0430307]], dtype=float32)

time = 19170	action = 1	current_phase = 1	next_phase = 0	reward = -1.311142	array([[-3.7084012, -2.1154583]], dtype=float32)

time = 19178	action = 0	current_phase = 0	next_phase = 1	reward = 0.253191	array([[-1.4957299, -2.8513954]], dtype=float32)

time = 19183	action = 0	current_phase = 0	next_phase = 1	reward = 0.026562	array([[-2.0103807, -2.9828808]], dtype=float32)

time = 19188	action = 0	current_phase = 0	next_phase = 1	reward = 0.084862	array([[-2.6271439, -2.7834609]], dtype=float32)

time = 19193	action = 1	current_phase = 0	next_phase = 1	reward = -1.410848	array([[-4.191313 , -3.3777125]], dtype=float32)

time = 19201	action = 1	current_phase = 1	next_phase = 0	reward = -1.488550	array([[-3.5126028, -2.3855193]], dtype=float32)

time = 19209	action = 0	current_phase = 0	next_phase = 1	reward = 0.243243	array([[-1.370847, -2.603595]], dtype=float32)

time = 19214	action = 0	current_phase = 0	next_phase = 1	reward = 0.016697	array([[-1.7696607, -2.8256526]], dtype=float32)

time = 19219	action = 0	current_phase = 0	next_phase = 1	reward = 0.066438	array([[-2.3422184, -2.4327896]], dtype=float32)

time = 19224	action = 1	current_phase = 0	next_phase = 1	reward = -1.012498	array([[-3.7025433, -2.9518507]], dtype=float32)

time = 19232	action = 1	current_phase = 1	next_phase = 0	reward = -1.044289	array([[-3.413571, -2.193563]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0667 - val_loss: 0.0417

Epoch 2/50

 - 3s - loss: 0.0685 - val_loss: 0.0368

Epoch 3/50

 - 3s - loss: 0.0719 - val_loss: 0.0328

Epoch 4/50

 - 3s - loss: 0.0560 - val_loss: 0.0374

Epoch 5/50

 - 3s - loss: 0.0545 - val_loss: 0.0429

Epoch 6/50

 - 4s - loss: 0.0531 - val_loss: 0.0407

Epoch 7/50

 - 4s - loss: 0.0546 - val_loss: 0.0431

Epoch 8/50

 - 3s - loss: 0.0568 - val_loss: 0.0382

Epoch 9/50

 - 4s - loss: 0.0562 - val_loss: 0.0440

Epoch 10/50

 - 4s - loss: 0.0651 - val_loss: 0.0387

Epoch 11/50

 - 3s - loss: 0.0480 - val_loss: 0.0453

Epoch 12/50

 - 3s - loss: 0.0977 - val_loss: 0.0394

Epoch 13/50

 - 4s - loss: 0.0487 - val_loss: 0.0417

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 878, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 866, after forget

time = 19240	action = 0	current_phase = 0	next_phase = 1	reward = 0.242284	array([[-1.7363452, -2.7100081]], dtype=float32)

time = 19245	action = 0	current_phase = 0	next_phase = 1	reward = 0.026913	array([[-1.9295039, -2.8166723]], dtype=float32)

time = 19250	action = 1	current_phase = 0	next_phase = 1	reward = -1.393479	array([[-3.069667 , -2.6308074]], dtype=float32)

time = 19258	action = 1	current_phase = 1	next_phase = 0	reward = -0.693202	array([[-3.815824 , -2.0304465]], dtype=float32)

time = 19266	action = 0	current_phase = 0	next_phase = 1	reward = -0.077217	array([[-1.5650511, -2.7479696]], dtype=float32)

time = 19271	action = 0	current_phase = 0	next_phase = 1	reward = -0.004707	array([[-1.9570892, -2.7950525]], dtype=float32)

time = 19276	action = 0	current_phase = 0	next_phase = 1	reward = 0.064454	array([[-2.3310466, -2.8828216]], dtype=float32)

time = 19281	action = 1	current_phase = 0	next_phase = 1	reward = -1.456077	array([[-4.1662154, -2.8794727]], dtype=float32)

time = 19289	action = 1	current_phase = 1	next_phase = 0	reward = -0.833092	array([[-3.8513384, -2.0204763]], dtype=float32)

time = 19297	action = 0	current_phase = 0	next_phase = 1	reward = -0.056840	array([[-1.5699534, -2.8051362]], dtype=float32)

time = 19302	action = 0	current_phase = 0	next_phase = 1	reward = 0.019454	array([[-2.0143478, -2.876258 ]], dtype=float32)

time = 19307	action = 0	current_phase = 0	next_phase = 1	reward = 0.077541	array([[-2.3415174, -2.9502707]], dtype=float32)

time = 19312	action = 1	current_phase = 0	next_phase = 1	reward = -1.570734	array([[-4.2653503, -3.1442456]], dtype=float32)

time = 19320	action = 1	current_phase = 1	next_phase = 0	reward = -1.000122	array([[-3.7179332, -2.1230469]], dtype=float32)

time = 19328	action = 0	current_phase = 0	next_phase = 1	reward = -0.039686	array([[-1.5449661, -2.784532 ]], dtype=float32)

time = 19333	action = 0	current_phase = 0	next_phase = 1	reward = 0.025824	array([[-2.0887475, -2.9640455]], dtype=float32)

time = 19338	action = 0	current_phase = 0	next_phase = 1	reward = 0.067958	array([[-2.4162202, -2.922772 ]], dtype=float32)

time = 19343	action = 1	current_phase = 0	next_phase = 1	reward = -1.933245	array([[-4.223679 , -3.2071338]], dtype=float32)

time = 19351	action = 1	current_phase = 1	next_phase = 0	reward = -1.077294	array([[-3.8031158, -2.286539 ]], dtype=float32)

time = 19359	action = 0	current_phase = 0	next_phase = 1	reward = -0.033860	array([[-1.4678736, -2.7226171]], dtype=float32)

time = 19364	action = 0	current_phase = 0	next_phase = 1	reward = 0.030102	array([[-1.9852991, -2.8700109]], dtype=float32)

time = 19369	action = 1	current_phase = 0	next_phase = 1	reward = -0.704248	array([[-2.3863733, -2.1935744]], dtype=float32)

time = 19377	action = 1	current_phase = 1	next_phase = 0	reward = -0.601251	array([[-2.7138906, -1.8049517]], dtype=float32)

time = 19385	action = 0	current_phase = 0	next_phase = 1	reward = -0.384286	array([[-1.54813 , -2.888605]], dtype=float32)

time = 19390	action = 0	current_phase = 0	next_phase = 1	reward = 0.248550	array([[-1.7567394, -2.673122 ]], dtype=float32)

time = 19395	action = 0	current_phase = 0	next_phase = 1	reward = 0.030447	array([[-2.2954159, -2.8793192]], dtype=float32)

time = 19400	action = 1	current_phase = 0	next_phase = 1	reward = -1.251693	array([[-3.1492016, -2.3433957]], dtype=float32)

time = 19408	action = 1	current_phase = 1	next_phase = 0	reward = -0.686744	array([[-3.741315 , -2.0415664]], dtype=float32)

time = 19416	action = 0	current_phase = 0	next_phase = 1	reward = -0.093173	array([[-1.5264901, -2.8043814]], dtype=float32)

time = 19421	action = 0	current_phase = 0	next_phase = 1	reward = 0.011145	array([[-2.042484 , -2.8287501]], dtype=float32)

time = 19426	action = 0	current_phase = 0	next_phase = 1	reward = 0.079659	array([[-2.3377278, -2.893715 ]], dtype=float32)

time = 19431	action = 1	current_phase = 0	next_phase = 1	reward = -1.502600	array([[-4.1917467, -2.91817  ]], dtype=float32)

time = 19439	action = 1	current_phase = 1	next_phase = 0	reward = -0.829951	array([[-3.8173904, -2.016492 ]], dtype=float32)

time = 19447	action = 0	current_phase = 0	next_phase = 1	reward = -0.067455	array([[-1.5427817, -2.8244858]], dtype=float32)

time = 19452	action = 0	current_phase = 0	next_phase = 1	reward = 0.007995	array([[-2.0209317, -2.8355365]], dtype=float32)

time = 19457	action = 0	current_phase = 0	next_phase = 1	reward = 0.077793	array([[-2.37503  , -2.9802694]], dtype=float32)

time = 19462	action = 1	current_phase = 0	next_phase = 1	reward = -1.641183	array([[-4.194419 , -3.0900483]], dtype=float32)

time = 19470	action = 1	current_phase = 1	next_phase = 0	reward = -1.007245	array([[-3.8711133, -2.2652814]], dtype=float32)

time = 19478	action = 0	current_phase = 0	next_phase = 1	reward = -0.058612	array([[-1.5697361, -2.8095274]], dtype=float32)

time = 19483	action = 0	current_phase = 0	next_phase = 1	reward = 0.022219	array([[-2.14478  , -2.9599075]], dtype=float32)

time = 19488	action = 0	current_phase = 0	next_phase = 1	reward = 0.082399	array([[-2.50735  , -2.8741817]], dtype=float32)

time = 19493	action = 1	current_phase = 0	next_phase = 1	reward = -1.926129	array([[-4.2308655, -3.474203 ]], dtype=float32)

time = 19501	action = 1	current_phase = 1	next_phase = 0	reward = -1.047176	array([[-3.8910475, -2.3181157]], dtype=float32)

time = 19509	action = 0	current_phase = 0	next_phase = 1	reward = -0.041819	array([[-1.4499114, -2.7306356]], dtype=float32)

time = 19514	action = 0	current_phase = 0	next_phase = 1	reward = 0.038874	array([[-1.9103105, -2.8742332]], dtype=float32)

time = 19519	action = 1	current_phase = 0	next_phase = 1	reward = -0.641743	array([[-2.2976072, -2.2809901]], dtype=float32)

time = 19527	action = 1	current_phase = 1	next_phase = 0	reward = -0.924708	array([[-2.839995 , -2.1307154]], dtype=float32)

time = 19535	action = 0	current_phase = 0	next_phase = 1	reward = 0.175840	array([[-1.4713073, -2.952746 ]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.1014 - val_loss: 0.0247

Epoch 2/50

 - 3s - loss: 0.0751 - val_loss: 0.0278

Epoch 3/50

 - 4s - loss: 0.0789 - val_loss: 0.0238

Epoch 4/50

 - 4s - loss: 0.0766 - val_loss: 0.0258

Epoch 5/50

 - 4s - loss: 0.1189 - val_loss: 0.0279

Epoch 6/50

 - 4s - loss: 0.0590 - val_loss: 0.0297

Epoch 7/50

 - 3s - loss: 0.0583 - val_loss: 0.0289

Epoch 8/50

 - 3s - loss: 0.0747 - val_loss: 0.0287

Epoch 9/50

 - 4s - loss: 0.0597 - val_loss: 0.0400

Epoch 10/50

 - 3s - loss: 0.0648 - val_loss: 0.0285

Epoch 11/50

 - 4s - loss: 0.0656 - val_loss: 0.0316

Epoch 12/50

 - 4s - loss: 0.0745 - val_loss: 0.0332

Epoch 13/50

 - 4s - loss: 0.0750 - val_loss: 0.0343

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 888, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 876, after forget

time = 19540	action = 0	current_phase = 0	next_phase = 1	reward = -0.308423	array([[-1.9205189, -2.6954553]], dtype=float32)

time = 19545	action = 0	current_phase = 0	next_phase = 1	reward = 0.323973	array([[-2.198211, -2.855081]], dtype=float32)

time = 19550	action = 1	current_phase = 0	next_phase = 1	reward = -1.315896	array([[-3.6763532, -2.6279886]], dtype=float32)

time = 19558	action = 1	current_phase = 1	next_phase = 0	reward = -0.703526	array([[-3.528812 , -1.9339168]], dtype=float32)

time = 19566	action = 0	current_phase = 0	next_phase = 1	reward = -0.071291	array([[-1.427565 , -2.8013964]], dtype=float32)

time = 19571	action = 0	current_phase = 0	next_phase = 1	reward = 0.001450	array([[-2.01048  , -2.7871504]], dtype=float32)

time = 19576	action = 0	current_phase = 0	next_phase = 1	reward = 0.060419	array([[-2.3219967, -2.8698292]], dtype=float32)

time = 19581	action = 1	current_phase = 0	next_phase = 1	reward = -1.372310	array([[-4.247306 , -2.9396107]], dtype=float32)

time = 19589	action = 1	current_phase = 1	next_phase = 0	reward = -0.856397	array([[-3.8611665, -2.141537 ]], dtype=float32)

time = 19597	action = 0	current_phase = 0	next_phase = 1	reward = -0.072889	array([[-1.4517477, -2.700339 ]], dtype=float32)

time = 19602	action = 0	current_phase = 0	next_phase = 1	reward = -0.001288	array([[-1.9284787, -2.808068 ]], dtype=float32)

time = 19607	action = 0	current_phase = 0	next_phase = 1	reward = 0.067394	array([[-2.3119724, -2.8392859]], dtype=float32)

time = 19612	action = 1	current_phase = 0	next_phase = 1	reward = -1.525910	array([[-4.482205 , -3.0424376]], dtype=float32)

time = 19620	action = 1	current_phase = 1	next_phase = 0	reward = -1.006200	array([[-3.8128855, -2.228137 ]], dtype=float32)

time = 19628	action = 0	current_phase = 0	next_phase = 1	reward = -0.051841	array([[-1.4445252, -2.7821631]], dtype=float32)

time = 19633	action = 0	current_phase = 0	next_phase = 1	reward = 0.013715	array([[-1.9853265, -2.9237792]], dtype=float32)

time = 19638	action = 0	current_phase = 0	next_phase = 1	reward = 0.077919	array([[-2.6145275, -2.7904756]], dtype=float32)

time = 19643	action = 1	current_phase = 0	next_phase = 1	reward = -1.381779	array([[-4.316067 , -3.3306246]], dtype=float32)

time = 19651	action = 1	current_phase = 1	next_phase = 0	reward = -1.495365	array([[-3.5732577, -2.3515692]], dtype=float32)

time = 19659	action = 0	current_phase = 0	next_phase = 1	reward = 0.238691	array([[-1.4217356, -2.64172  ]], dtype=float32)

time = 19664	action = 0	current_phase = 0	next_phase = 1	reward = 0.016856	array([[-1.7358177, -2.7138436]], dtype=float32)

time = 19669	action = 1	current_phase = 0	next_phase = 1	reward = -1.287928	array([[-2.5353587, -2.1761577]], dtype=float32)

time = 19677	action = 1	current_phase = 1	next_phase = 0	reward = -1.120762	array([[-3.8137424, -2.1341887]], dtype=float32)

time = 19685	action = 0	current_phase = 0	next_phase = 1	reward = 0.483855	array([[-1.6184181, -2.8500323]], dtype=float32)

time = 19690	action = 0	current_phase = 0	next_phase = 1	reward = -0.287004	array([[-1.6351497, -2.676061 ]], dtype=float32)

time = 19695	action = 0	current_phase = 0	next_phase = 1	reward = 0.353513	array([[-2.2526858, -2.9083028]], dtype=float32)

time = 19700	action = 1	current_phase = 0	next_phase = 1	reward = -1.383938	array([[-3.4985082, -2.5538418]], dtype=float32)

time = 19708	action = 1	current_phase = 1	next_phase = 0	reward = -0.731051	array([[-3.6454084, -1.9242177]], dtype=float32)

time = 19716	action = 0	current_phase = 0	next_phase = 1	reward = -0.100494	array([[-1.4365022, -2.8303294]], dtype=float32)

time = 19721	action = 0	current_phase = 0	next_phase = 1	reward = 0.000261	array([[-1.9882793, -2.7758522]], dtype=float32)

time = 19726	action = 0	current_phase = 0	next_phase = 1	reward = 0.062830	array([[-2.295797 , -2.8614724]], dtype=float32)

time = 19731	action = 1	current_phase = 0	next_phase = 1	reward = -1.322942	array([[-4.327724 , -2.8436105]], dtype=float32)

time = 19739	action = 1	current_phase = 1	next_phase = 0	reward = -0.838221	array([[-3.7667596, -2.00557  ]], dtype=float32)

time = 19747	action = 0	current_phase = 0	next_phase = 1	reward = -0.067648	array([[-1.3767846, -2.759374 ]], dtype=float32)

time = 19752	action = 0	current_phase = 0	next_phase = 1	reward = 0.004953	array([[-1.8974395, -2.8243341]], dtype=float32)

time = 19757	action = 0	current_phase = 0	next_phase = 1	reward = 0.065636	array([[-2.296176 , -2.8666866]], dtype=float32)

time = 19762	action = 1	current_phase = 0	next_phase = 1	reward = -1.693660	array([[-4.4359174, -3.031959 ]], dtype=float32)

time = 19770	action = 1	current_phase = 1	next_phase = 0	reward = -0.994661	array([[-3.8449981, -2.196334 ]], dtype=float32)

time = 19778	action = 0	current_phase = 0	next_phase = 1	reward = -0.035144	array([[-1.3195463, -2.78624  ]], dtype=float32)

time = 19783	action = 0	current_phase = 0	next_phase = 1	reward = 0.031409	array([[-2.1328065, -2.9350612]], dtype=float32)

time = 19788	action = 1	current_phase = 0	next_phase = 1	reward = -1.796750	array([[-2.7006772, -2.6070175]], dtype=float32)

time = 19796	action = 1	current_phase = 1	next_phase = 0	reward = -1.188390	array([[-3.937517 , -2.1755965]], dtype=float32)

time = 19804	action = 0	current_phase = 0	next_phase = 1	reward = 0.184432	array([[-1.3590806, -2.790662 ]], dtype=float32)

time = 19809	action = 0	current_phase = 0	next_phase = 1	reward = -0.034259	array([[-1.4350209, -2.6312575]], dtype=float32)

time = 19814	action = 0	current_phase = 0	next_phase = 1	reward = 0.036185	array([[-2.0009797, -2.8321872]], dtype=float32)

time = 19819	action = 1	current_phase = 0	next_phase = 1	reward = -0.758221	array([[-2.812149 , -2.1720028]], dtype=float32)

time = 19827	action = 1	current_phase = 1	next_phase = 0	reward = -0.639641	array([[-3.753143 , -2.0383377]], dtype=float32)

time = 19835	action = 0	current_phase = 0	next_phase = 1	reward = -0.099287	array([[-1.4150169, -2.8286784]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 4s - loss: 0.0783 - val_loss: 0.0220

Epoch 2/50

 - 5s - loss: 0.0880 - val_loss: 0.0314

Epoch 3/50

 - 4s - loss: 0.0613 - val_loss: 0.0270

Epoch 4/50

 - 3s - loss: 0.0866 - val_loss: 0.0346

Epoch 5/50

 - 3s - loss: 0.0776 - val_loss: 0.0465

Epoch 6/50

 - 3s - loss: 0.0806 - val_loss: 0.0297

Epoch 7/50

 - 3s - loss: 0.0689 - val_loss: 0.0340

Epoch 8/50

 - 3s - loss: 0.0688 - val_loss: 0.0250

Epoch 9/50

 - 3s - loss: 0.0653 - val_loss: 0.0353

Epoch 10/50

 - 3s - loss: 0.0714 - val_loss: 0.0322

Epoch 11/50

 - 3s - loss: 0.0715 - val_loss: 0.0296

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 898, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 886, after forget

time = 19840	action = 0	current_phase = 0	next_phase = 1	reward = -0.040654	array([[-1.9034631, -2.8328784]], dtype=float32)

time = 19845	action = 0	current_phase = 0	next_phase = 1	reward = -0.244533	array([[-2.2999902, -2.93741  ]], dtype=float32)

time = 19850	action = 1	current_phase = 0	next_phase = 1	reward = -0.983411	array([[-3.7414963, -2.8054166]], dtype=float32)

time = 19858	action = 1	current_phase = 1	next_phase = 0	reward = -0.713394	array([[-3.6135097, -1.9509089]], dtype=float32)

time = 19866	action = 0	current_phase = 0	next_phase = 1	reward = -0.081969	array([[-1.6300281, -2.7749984]], dtype=float32)

time = 19871	action = 0	current_phase = 0	next_phase = 1	reward = -0.007192	array([[-2.080504 , -2.8340514]], dtype=float32)

time = 19876	action = 0	current_phase = 0	next_phase = 1	reward = 0.062696	array([[-2.2879763, -2.9050193]], dtype=float32)

time = 19881	action = 1	current_phase = 0	next_phase = 1	reward = -1.557502	array([[-4.527814, -3.020545]], dtype=float32)

time = 19889	action = 1	current_phase = 1	next_phase = 0	reward = -0.785287	array([[-3.8783927, -2.1077259]], dtype=float32)

time = 19897	action = 0	current_phase = 0	next_phase = 1	reward = -0.065725	array([[-1.6471643, -2.8537595]], dtype=float32)

time = 19902	action = 0	current_phase = 0	next_phase = 1	reward = 0.007092	array([[-1.9956527, -2.9140747]], dtype=float32)

time = 19907	action = 0	current_phase = 0	next_phase = 1	reward = 0.065987	array([[-2.352677 , -2.9397113]], dtype=float32)

time = 19912	action = 1	current_phase = 0	next_phase = 1	reward = -1.577789	array([[-4.420395 , -3.2728105]], dtype=float32)

time = 19920	action = 1	current_phase = 1	next_phase = 0	reward = -1.053956	array([[-3.8970623, -2.2314284]], dtype=float32)

time = 19928	action = 0	current_phase = 0	next_phase = 1	reward = -0.044427	array([[-1.6294265, -2.8405766]], dtype=float32)

time = 19933	action = 0	current_phase = 0	next_phase = 1	reward = 0.024350	array([[-2.0122764, -2.9419172]], dtype=float32)

time = 19938	action = 0	current_phase = 0	next_phase = 1	reward = 0.081696	array([[-2.675373 , -2.9540148]], dtype=float32)

time = 19943	action = 1	current_phase = 0	next_phase = 1	reward = -1.404690	array([[-4.422385, -3.323526]], dtype=float32)

time = 19951	action = 1	current_phase = 1	next_phase = 0	reward = -1.454723	array([[-3.7492285, -2.2874043]], dtype=float32)

time = 19959	action = 0	current_phase = 0	next_phase = 1	reward = 0.260854	array([[-1.5150007, -2.7556574]], dtype=float32)

time = 19964	action = 0	current_phase = 0	next_phase = 1	reward = 0.580123	array([[-2.060286 , -2.8718722]], dtype=float32)

time = 19969	action = 1	current_phase = 0	next_phase = 1	reward = -1.206704	array([[-3.0199661, -2.6415024]], dtype=float32)

time = 19977	action = 1	current_phase = 1	next_phase = 0	reward = -1.191467	array([[-3.326702 , -2.1432376]], dtype=float32)

time = 19985	action = 0	current_phase = 0	next_phase = 1	reward = 0.462441	array([[-1.5034616, -2.993926 ]], dtype=float32)

time = 19990	action = 0	current_phase = 0	next_phase = 1	reward = -0.028199	array([[-1.7578813, -2.6767254]], dtype=float32)

time = 19995	action = 0	current_phase = 0	next_phase = 1	reward = 0.032834	array([[-2.1466365, -2.893146 ]], dtype=float32)

time = 20000	action = 1	current_phase = 0	next_phase = 1	reward = -1.324760	array([[-3.5527737, -2.7938025]], dtype=float32)

time = 20008	action = 1	current_phase = 1	next_phase = 0	reward = -0.712959	array([[-3.5848622, -1.9713922]], dtype=float32)

time = 20016	action = 0	current_phase = 0	next_phase = 1	reward = -0.090719	array([[-1.561385 , -2.7801726]], dtype=float32)

time = 20021	action = 0	current_phase = 0	next_phase = 1	reward = -0.003089	array([[-1.927914 , -2.7875664]], dtype=float32)

time = 20026	action = 0	current_phase = 0	next_phase = 1	reward = 0.065161	array([[-2.3669753, -2.9063177]], dtype=float32)

time = 20031	action = 1	current_phase = 0	next_phase = 1	reward = -1.377148	array([[-4.453197 , -3.0867565]], dtype=float32)

time = 20039	action = 1	current_phase = 1	next_phase = 0	reward = -0.827232	array([[-3.848876, -2.038739]], dtype=float32)

time = 20047	action = 0	current_phase = 0	next_phase = 1	reward = -0.080422	array([[-1.6659826, -2.8036094]], dtype=float32)

time = 20052	action = 0	current_phase = 0	next_phase = 1	reward = 0.010227	array([[-2.064278 , -2.8826087]], dtype=float32)

time = 20057	action = 0	current_phase = 0	next_phase = 1	reward = 0.075649	array([[-2.680329, -3.006359]], dtype=float32)

time = 20062	action = 1	current_phase = 0	next_phase = 1	reward = -1.654602	array([[-4.4651074, -3.259853 ]], dtype=float32)

time = 20070	action = 1	current_phase = 1	next_phase = 0	reward = -0.885455	array([[-3.865553 , -2.1928935]], dtype=float32)

time = 20078	action = 0	current_phase = 0	next_phase = 1	reward = -0.039697	array([[-1.630173 , -2.7919214]], dtype=float32)

time = 20083	action = 0	current_phase = 0	next_phase = 1	reward = 0.041252	array([[-2.139503 , -3.0117617]], dtype=float32)

time = 20088	action = 0	current_phase = 0	next_phase = 1	reward = 0.082959	array([[-2.6127234, -2.8846483]], dtype=float32)

time = 20093	action = 1	current_phase = 0	next_phase = 1	reward = -1.894588	array([[-4.4260693, -3.4671628]], dtype=float32)

time = 20101	action = 1	current_phase = 1	next_phase = 0	reward = -1.389938	array([[-4.013272 , -2.4013004]], dtype=float32)

time = 20109	action = 0	current_phase = 0	next_phase = 1	reward = 0.278937	array([[-1.4609505, -2.6401565]], dtype=float32)

time = 20114	action = 0	current_phase = 0	next_phase = 1	reward = 0.041297	array([[-1.9136995, -2.8810875]], dtype=float32)

time = 20119	action = 0	current_phase = 0	next_phase = 1	reward = 0.051386	array([[-2.3275228, -2.3510532]], dtype=float32)

time = 20124	action = 1	current_phase = 0	next_phase = 1	reward = -1.146991	array([[-4.1963577, -2.9479828]], dtype=float32)

time = 20132	action = 1	current_phase = 1	next_phase = 0	reward = -0.749756	array([[-3.818739 , -2.2069035]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0672 - val_loss: 0.0303

Epoch 2/50

 - 3s - loss: 0.0698 - val_loss: 0.0298

Epoch 3/50

 - 3s - loss: 0.0672 - val_loss: 0.0303

Epoch 4/50

 - 3s - loss: 0.0621 - val_loss: 0.0346

Epoch 5/50

 - 3s - loss: 0.0622 - val_loss: 0.0340

Epoch 6/50

 - 3s - loss: 0.0627 - val_loss: 0.0296

Epoch 7/50

 - 3s - loss: 0.0587 - val_loss: 0.0336

Epoch 8/50

 - 4s - loss: 0.0573 - val_loss: 0.0299

Epoch 9/50

 - 4s - loss: 0.0602 - val_loss: 0.0352

Epoch 10/50

 - 4s - loss: 0.0517 - val_loss: 0.0403

Epoch 11/50

 - 3s - loss: 0.0599 - val_loss: 0.0327

Epoch 12/50

 - 3s - loss: 0.0567 - val_loss: 0.0318

Epoch 13/50

 - 3s - loss: 0.0568 - val_loss: 0.0357

Epoch 14/50

 - 3s - loss: 0.0594 - val_loss: 0.0321

Epoch 15/50

 - 3s - loss: 0.0667 - val_loss: 0.0381

Epoch 16/50

 - 3s - loss: 0.0626 - val_loss: 0.0316

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 908, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 896, after forget

time = 20140	action = 0	current_phase = 0	next_phase = 1	reward = -0.016912	array([[-1.6620642, -2.7239985]], dtype=float32)

time = 20145	action = 0	current_phase = 0	next_phase = 1	reward = -0.227119	array([[-1.9945332, -2.8382075]], dtype=float32)

time = 20150	action = 1	current_phase = 0	next_phase = 1	reward = -1.145569	array([[-3.4219594, -2.796512 ]], dtype=float32)

time = 20158	action = 1	current_phase = 1	next_phase = 0	reward = -0.703750	array([[-3.6882358, -1.9766026]], dtype=float32)

time = 20166	action = 0	current_phase = 0	next_phase = 1	reward = -0.079163	array([[-1.5123571, -2.8146057]], dtype=float32)

time = 20171	action = 0	current_phase = 0	next_phase = 1	reward = -0.002373	array([[-1.8998487, -2.816441 ]], dtype=float32)

time = 20176	action = 0	current_phase = 0	next_phase = 1	reward = 0.055151	array([[-2.2316265, -2.924145 ]], dtype=float32)

time = 20181	action = 1	current_phase = 0	next_phase = 1	reward = -1.494664	array([[-4.5814114, -3.0779479]], dtype=float32)

time = 20189	action = 1	current_phase = 1	next_phase = 0	reward = -1.063705	array([[-3.8495278, -1.9336667]], dtype=float32)

time = 20197	action = 0	current_phase = 0	next_phase = 1	reward = 0.220099	array([[-1.4665915, -2.8509872]], dtype=float32)

time = 20202	action = 0	current_phase = 0	next_phase = 1	reward = 0.009085	array([[-1.9494418, -2.8914723]], dtype=float32)

time = 20207	action = 0	current_phase = 0	next_phase = 1	reward = 0.079583	array([[-2.340446 , -3.0280192]], dtype=float32)

time = 20212	action = 1	current_phase = 0	next_phase = 1	reward = -1.654676	array([[-4.5233126, -3.363552 ]], dtype=float32)

time = 20220	action = 1	current_phase = 1	next_phase = 0	reward = -0.895743	array([[-3.6487904, -1.9890064]], dtype=float32)

time = 20228	action = 0	current_phase = 0	next_phase = 1	reward = -0.047469	array([[-1.465147 , -2.8482165]], dtype=float32)

time = 20233	action = 0	current_phase = 0	next_phase = 1	reward = 0.011590	array([[-2.001798 , -2.9823086]], dtype=float32)

time = 20238	action = 0	current_phase = 0	next_phase = 1	reward = 0.083125	array([[-2.7499588, -2.8604202]], dtype=float32)

time = 20243	action = 1	current_phase = 0	next_phase = 1	reward = -1.336045	array([[-4.4852104, -3.4472318]], dtype=float32)

time = 20251	action = 1	current_phase = 1	next_phase = 0	reward = -1.486309	array([[-3.7565556, -2.3337798]], dtype=float32)

time = 20259	action = 0	current_phase = 0	next_phase = 1	reward = 0.248266	array([[-1.4695464, -2.7600584]], dtype=float32)

time = 20264	action = 0	current_phase = 0	next_phase = 1	reward = 0.029588	array([[-1.6670533, -2.8189135]], dtype=float32)

time = 20269	action = 1	current_phase = 0	next_phase = 1	reward = -0.636191	array([[-2.4958653, -2.2348096]], dtype=float32)

time = 20277	action = 1	current_phase = 1	next_phase = 0	reward = -0.582944	array([[-2.7087526, -1.9160998]], dtype=float32)

time = 20285	action = 0	current_phase = 0	next_phase = 1	reward = -0.103413	array([[-1.5442348, -2.968075 ]], dtype=float32)

time = 20290	action = 0	current_phase = 0	next_phase = 1	reward = -0.018176	array([[-1.6040921, -2.781196 ]], dtype=float32)

time = 20295	action = 0	current_phase = 0	next_phase = 1	reward = 0.047076	array([[-2.0802639, -2.8922174]], dtype=float32)

time = 20300	action = 1	current_phase = 0	next_phase = 1	reward = -1.305281	array([[-3.9529676, -2.7424786]], dtype=float32)

time = 20308	action = 1	current_phase = 1	next_phase = 0	reward = -0.697583	array([[-3.8396606, -1.8619025]], dtype=float32)

time = 20316	action = 0	current_phase = 0	next_phase = 1	reward = -0.091659	array([[-1.5127382, -2.7975261]], dtype=float32)

time = 20321	action = 0	current_phase = 0	next_phase = 1	reward = -0.018025	array([[-1.8681319, -2.8378856]], dtype=float32)

time = 20326	action = 0	current_phase = 0	next_phase = 1	reward = 0.030686	array([[-2.2883942, -2.9226801]], dtype=float32)

time = 20331	action = 1	current_phase = 0	next_phase = 1	reward = -1.387650	array([[-4.4527745, -3.1386526]], dtype=float32)

time = 20339	action = 1	current_phase = 1	next_phase = 0	reward = -0.742644	array([[-3.7181444, -1.8251309]], dtype=float32)

time = 20347	action = 0	current_phase = 0	next_phase = 1	reward = -0.071939	array([[-1.5067708, -2.8387668]], dtype=float32)

time = 20352	action = 0	current_phase = 0	next_phase = 1	reward = -0.010839	array([[-1.9977636, -2.86906  ]], dtype=float32)

time = 20357	action = 0	current_phase = 0	next_phase = 1	reward = 0.056910	array([[-2.2184682, -2.967459 ]], dtype=float32)

time = 20362	action = 1	current_phase = 0	next_phase = 1	reward = -1.585978	array([[-4.53568  , -3.4091647]], dtype=float32)

time = 20370	action = 1	current_phase = 1	next_phase = 0	reward = -1.304948	array([[-3.878521 , -2.0489178]], dtype=float32)

time = 20378	action = 0	current_phase = 0	next_phase = 1	reward = 0.239015	array([[-1.5324016, -2.7984643]], dtype=float32)

time = 20383	action = 0	current_phase = 0	next_phase = 1	reward = 0.031168	array([[-1.9560632, -2.9652183]], dtype=float32)

time = 20388	action = 0	current_phase = 0	next_phase = 1	reward = 0.083964	array([[-2.662037 , -2.9775395]], dtype=float32)

time = 20393	action = 1	current_phase = 0	next_phase = 1	reward = -1.881992	array([[-4.458432 , -3.5850484]], dtype=float32)

time = 20401	action = 1	current_phase = 1	next_phase = 0	reward = -0.945591	array([[-3.9732652, -2.3289127]], dtype=float32)

time = 20409	action = 0	current_phase = 0	next_phase = 1	reward = -0.036615	array([[-1.3977416, -2.750358 ]], dtype=float32)

time = 20414	action = 0	current_phase = 0	next_phase = 1	reward = 0.023275	array([[-1.7087207, -2.8646033]], dtype=float32)

time = 20419	action = 0	current_phase = 0	next_phase = 1	reward = 0.060530	array([[-2.2006035, -2.3077023]], dtype=float32)

time = 20424	action = 1	current_phase = 0	next_phase = 1	reward = -1.056014	array([[-4.248663 , -2.9727056]], dtype=float32)

time = 20432	action = 1	current_phase = 1	next_phase = 0	reward = -0.766394	array([[-3.7393017, -2.206893 ]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.1060 - val_loss: 0.0339

Epoch 2/50

 - 4s - loss: 0.0971 - val_loss: 0.0312

Epoch 3/50

 - 3s - loss: 0.1027 - val_loss: 0.0294

Epoch 4/50

 - 3s - loss: 0.0698 - val_loss: 0.0322

Epoch 5/50

 - 3s - loss: 0.0839 - val_loss: 0.0360

Epoch 6/50

 - 3s - loss: 0.0611 - val_loss: 0.0328

Epoch 7/50

 - 3s - loss: 0.0645 - val_loss: 0.0373

Epoch 8/50

 - 3s - loss: 0.0779 - val_loss: 0.0325

Epoch 9/50

 - 3s - loss: 0.0729 - val_loss: 0.0325

Epoch 10/50

 - 3s - loss: 0.0972 - val_loss: 0.0302

Epoch 11/50

 - 3s - loss: 0.0600 - val_loss: 0.0307

Epoch 12/50

 - 3s - loss: 0.0728 - val_loss: 0.0331

Epoch 13/50

 - 3s - loss: 0.0689 - val_loss: 0.0348

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 918, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 906, after forget

time = 20440	action = 0	current_phase = 0	next_phase = 1	reward = -0.043190	array([[-1.5725431, -2.7000372]], dtype=float32)

time = 20445	action = 0	current_phase = 0	next_phase = 1	reward = 0.020258	array([[-1.8829107, -2.8861444]], dtype=float32)

time = 20450	action = 1	current_phase = 0	next_phase = 1	reward = -1.211634	array([[-3.3958821, -3.0828972]], dtype=float32)

time = 20458	action = 1	current_phase = 1	next_phase = 0	reward = -0.712042	array([[-3.4106226, -1.9432642]], dtype=float32)

time = 20466	action = 0	current_phase = 0	next_phase = 1	reward = -0.085408	array([[-1.4549334, -2.8004978]], dtype=float32)

time = 20471	action = 0	current_phase = 0	next_phase = 1	reward = 0.016750	array([[-1.909226, -2.833082]], dtype=float32)

time = 20476	action = 0	current_phase = 0	next_phase = 1	reward = 0.075916	array([[-2.1806276, -2.9264038]], dtype=float32)

time = 20481	action = 1	current_phase = 0	next_phase = 1	reward = -1.433185	array([[-4.539446, -3.043613]], dtype=float32)

time = 20489	action = 1	current_phase = 1	next_phase = 0	reward = -0.841105	array([[-3.819123 , -2.1033628]], dtype=float32)

time = 20497	action = 0	current_phase = 0	next_phase = 1	reward = -0.068275	array([[-1.5080011, -2.8486493]], dtype=float32)

time = 20502	action = 0	current_phase = 0	next_phase = 1	reward = 0.006798	array([[-1.8806243, -2.9130783]], dtype=float32)

time = 20507	action = 0	current_phase = 0	next_phase = 1	reward = 0.070413	array([[-2.283111 , -2.9596744]], dtype=float32)

time = 20512	action = 1	current_phase = 0	next_phase = 1	reward = -1.598213	array([[-4.4472837, -3.4032512]], dtype=float32)

time = 20520	action = 1	current_phase = 1	next_phase = 0	reward = -1.051479	array([[-3.796257 , -2.0986137]], dtype=float32)

time = 20528	action = 0	current_phase = 0	next_phase = 1	reward = -0.044860	array([[-1.4513023, -2.7997708]], dtype=float32)

time = 20533	action = 0	current_phase = 0	next_phase = 1	reward = 0.025956	array([[-1.987361 , -2.9934506]], dtype=float32)

time = 20538	action = 0	current_phase = 0	next_phase = 1	reward = 0.073451	array([[-2.6119938, -2.7902112]], dtype=float32)

time = 20543	action = 1	current_phase = 0	next_phase = 1	reward = -1.388593	array([[-4.4731894, -3.4030912]], dtype=float32)

time = 20551	action = 1	current_phase = 1	next_phase = 0	reward = -1.441200	array([[-3.4797049, -2.2513025]], dtype=float32)

time = 20559	action = 0	current_phase = 0	next_phase = 1	reward = 0.236874	array([[-1.3253981, -2.7356646]], dtype=float32)

time = 20564	action = 0	current_phase = 0	next_phase = 1	reward = 0.035436	array([[-1.68033  , -2.8198342]], dtype=float32)

time = 20569	action = 0	current_phase = 0	next_phase = 1	reward = 0.052240	array([[-2.3830028, -2.3955278]], dtype=float32)

time = 20574	action = 1	current_phase = 0	next_phase = 1	reward = -1.026744	array([[-4.1230183, -2.951605 ]], dtype=float32)

time = 20582	action = 1	current_phase = 1	next_phase = 0	reward = -1.243524	array([[-3.4526389, -2.1882544]], dtype=float32)

time = 20590	action = 0	current_phase = 0	next_phase = 1	reward = 0.539580	array([[-1.4488041, -2.7531161]], dtype=float32)

time = 20595	action = 0	current_phase = 0	next_phase = 1	reward = -0.227958	array([[-1.9803123, -2.834584 ]], dtype=float32)

time = 20600	action = 1	current_phase = 0	next_phase = 1	reward = -1.148317	array([[-3.249775, -2.735471]], dtype=float32)

time = 20608	action = 1	current_phase = 1	next_phase = 0	reward = -0.707766	array([[-3.6819465, -1.9405077]], dtype=float32)

time = 20616	action = 0	current_phase = 0	next_phase = 1	reward = -0.066180	array([[-1.5589595, -2.9317396]], dtype=float32)

time = 20621	action = 0	current_phase = 0	next_phase = 1	reward = -0.004488	array([[-1.7754025, -2.85357  ]], dtype=float32)

time = 20626	action = 0	current_phase = 0	next_phase = 1	reward = 0.060680	array([[-2.2015355, -2.9365048]], dtype=float32)

time = 20631	action = 1	current_phase = 0	next_phase = 1	reward = -1.478919	array([[-4.4913974, -3.1035776]], dtype=float32)

time = 20639	action = 1	current_phase = 1	next_phase = 0	reward = -0.825006	array([[-3.776808 , -1.9390575]], dtype=float32)

time = 20647	action = 0	current_phase = 0	next_phase = 1	reward = -0.064014	array([[-1.5526016, -2.863123 ]], dtype=float32)

time = 20652	action = 0	current_phase = 0	next_phase = 1	reward = 0.012467	array([[-1.9506292, -2.9176812]], dtype=float32)

time = 20657	action = 0	current_phase = 0	next_phase = 1	reward = 0.076191	array([[-2.306436 , -2.9264228]], dtype=float32)

time = 20662	action = 1	current_phase = 0	next_phase = 1	reward = -1.563303	array([[-4.407038 , -3.4419045]], dtype=float32)

time = 20670	action = 1	current_phase = 1	next_phase = 0	reward = -1.049234	array([[-3.8285956, -2.153051 ]], dtype=float32)

time = 20678	action = 0	current_phase = 0	next_phase = 1	reward = -0.054558	array([[-1.4164772, -2.8149667]], dtype=float32)

time = 20683	action = 0	current_phase = 0	next_phase = 1	reward = 0.014343	array([[-1.9296165, -3.0367758]], dtype=float32)

time = 20688	action = 0	current_phase = 0	next_phase = 1	reward = 0.079595	array([[-2.662313 , -2.7060401]], dtype=float32)

time = 20693	action = 1	current_phase = 0	next_phase = 1	reward = -1.327919	array([[-4.5928774, -3.575038 ]], dtype=float32)

time = 20701	action = 1	current_phase = 1	next_phase = 0	reward = -1.492336	array([[-3.5915766, -2.2624307]], dtype=float32)

time = 20709	action = 0	current_phase = 0	next_phase = 1	reward = 0.247684	array([[-1.3542517, -2.7232523]], dtype=float32)

time = 20714	action = 0	current_phase = 0	next_phase = 1	reward = 0.022836	array([[-1.6576326, -2.8338313]], dtype=float32)

time = 20719	action = 0	current_phase = 0	next_phase = 1	reward = 0.056013	array([[-2.4172437, -2.5770094]], dtype=float32)

time = 20724	action = 1	current_phase = 0	next_phase = 1	reward = -1.023134	array([[-4.0328746, -2.925865 ]], dtype=float32)

time = 20732	action = 1	current_phase = 1	next_phase = 0	reward = -0.693364	array([[-3.5235975, -2.1945682]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0643 - val_loss: 0.0229

Epoch 2/50

 - 4s - loss: 0.0650 - val_loss: 0.0252

Epoch 3/50

 - 3s - loss: 0.0575 - val_loss: 0.0230

Epoch 4/50

 - 3s - loss: 0.0591 - val_loss: 0.0250

Epoch 5/50

 - 3s - loss: 0.0608 - val_loss: 0.0248

Epoch 6/50

 - 3s - loss: 0.0559 - val_loss: 0.0247

Epoch 7/50

 - 3s - loss: 0.0547 - val_loss: 0.0238

Epoch 8/50

 - 3s - loss: 0.0590 - val_loss: 0.0276

Epoch 9/50

 - 3s - loss: 0.0439 - val_loss: 0.0234

Epoch 10/50

 - 3s - loss: 0.0487 - val_loss: 0.0233

Epoch 11/50

 - 3s - loss: 0.0501 - val_loss: 0.0261

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 928, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 916, after forget

time = 20740	action = 0	current_phase = 0	next_phase = 1	reward = -0.299220	array([[-1.6804128, -2.7088418]], dtype=float32)

time = 20745	action = 0	current_phase = 0	next_phase = 1	reward = 0.059622	array([[-2.024737 , -2.8442104]], dtype=float32)

time = 20750	action = 1	current_phase = 0	next_phase = 1	reward = -1.035173	array([[-3.2612948, -2.7870488]], dtype=float32)

time = 20758	action = 1	current_phase = 1	next_phase = 0	reward = -0.705523	array([[-3.3682537, -1.8808658]], dtype=float32)

time = 20766	action = 0	current_phase = 0	next_phase = 1	reward = -0.079705	array([[-1.612792, -2.842323]], dtype=float32)

time = 20771	action = 0	current_phase = 0	next_phase = 1	reward = -0.001656	array([[-1.8825767, -2.794325 ]], dtype=float32)

time = 20776	action = 0	current_phase = 0	next_phase = 1	reward = 0.066391	array([[-2.30147  , -2.9329844]], dtype=float32)

time = 20781	action = 1	current_phase = 0	next_phase = 1	reward = -1.500221	array([[-4.5897884, -3.0297666]], dtype=float32)

time = 20789	action = 1	current_phase = 1	next_phase = 0	reward = -0.767928	array([[-3.7461534, -1.9989185]], dtype=float32)

time = 20797	action = 0	current_phase = 0	next_phase = 1	reward = -0.066960	array([[-1.6321031, -2.8430598]], dtype=float32)

time = 20802	action = 0	current_phase = 0	next_phase = 1	reward = 0.013982	array([[-2.0442142, -2.8944714]], dtype=float32)

time = 20807	action = 0	current_phase = 0	next_phase = 1	reward = 0.072781	array([[-2.3889887, -2.9797802]], dtype=float32)

time = 20812	action = 1	current_phase = 0	next_phase = 1	reward = -1.657329	array([[-4.5126915, -3.2698107]], dtype=float32)

time = 20820	action = 1	current_phase = 1	next_phase = 0	reward = -0.974073	array([[-3.7119493, -2.0987859]], dtype=float32)

time = 20828	action = 0	current_phase = 0	next_phase = 1	reward = -0.052157	array([[-1.5289869, -2.8117397]], dtype=float32)

time = 20833	action = 0	current_phase = 0	next_phase = 1	reward = 0.009284	array([[-2.0427325, -2.9833953]], dtype=float32)

time = 20838	action = 0	current_phase = 0	next_phase = 1	reward = 0.075565	array([[-2.6860206, -2.8383334]], dtype=float32)

time = 20843	action = 1	current_phase = 0	next_phase = 1	reward = -1.377429	array([[-4.5386186, -3.3854396]], dtype=float32)

time = 20851	action = 1	current_phase = 1	next_phase = 0	reward = -1.491568	array([[-3.6266026, -2.220614 ]], dtype=float32)

time = 20859	action = 0	current_phase = 0	next_phase = 1	reward = 0.264700	array([[-1.4431112, -2.7437587]], dtype=float32)

time = 20864	action = 0	current_phase = 0	next_phase = 1	reward = 0.039530	array([[-1.6767044, -2.8112621]], dtype=float32)

time = 20869	action = 0	current_phase = 0	next_phase = 1	reward = 0.065606	array([[-2.3670957, -2.5278437]], dtype=float32)

time = 20874	action = 1	current_phase = 0	next_phase = 1	reward = -1.186116	array([[-4.40354  , -3.0654287]], dtype=float32)

time = 20882	action = 1	current_phase = 1	next_phase = 0	reward = -0.746700	array([[-3.7969594, -2.0916088]], dtype=float32)

time = 20890	action = 0	current_phase = 0	next_phase = 1	reward = -0.284727	array([[-1.5810976, -2.7208235]], dtype=float32)

time = 20895	action = 0	current_phase = 0	next_phase = 1	reward = 0.344968	array([[-1.9564419, -2.8380637]], dtype=float32)

time = 20900	action = 1	current_phase = 0	next_phase = 1	reward = -1.313786	array([[-3.4346418, -2.8769977]], dtype=float32)

time = 20908	action = 1	current_phase = 1	next_phase = 0	reward = -0.714465	array([[-3.6923895, -1.91626  ]], dtype=float32)

time = 20916	action = 0	current_phase = 0	next_phase = 1	reward = -0.076786	array([[-1.618323 , -2.8834736]], dtype=float32)

time = 20921	action = 0	current_phase = 0	next_phase = 1	reward = -0.010743	array([[-1.840611 , -2.7654958]], dtype=float32)

time = 20926	action = 0	current_phase = 0	next_phase = 1	reward = 0.059135	array([[-2.2610347, -2.922542 ]], dtype=float32)

time = 20931	action = 1	current_phase = 0	next_phase = 1	reward = -1.422268	array([[-4.5971074, -3.094748 ]], dtype=float32)

time = 20939	action = 1	current_phase = 1	next_phase = 0	reward = -1.127082	array([[-3.7163491, -1.961988 ]], dtype=float32)

time = 20947	action = 0	current_phase = 0	next_phase = 1	reward = 0.226281	array([[-1.5373306, -2.821307 ]], dtype=float32)

time = 20952	action = 0	current_phase = 0	next_phase = 1	reward = 0.007109	array([[-2.03524  , -2.9006643]], dtype=float32)

time = 20957	action = 0	current_phase = 0	next_phase = 1	reward = 0.062861	array([[-2.2782254, -2.9600558]], dtype=float32)

time = 20962	action = 1	current_phase = 0	next_phase = 1	reward = -1.713536	array([[-4.548027 , -3.2942786]], dtype=float32)

time = 20970	action = 1	current_phase = 1	next_phase = 0	reward = -1.263824	array([[-3.8263226, -2.165684 ]], dtype=float32)

time = 20978	action = 0	current_phase = 0	next_phase = 1	reward = 0.229485	array([[-1.5252085, -2.7986934]], dtype=float32)

time = 20983	action = 0	current_phase = 0	next_phase = 1	reward = -0.000796	array([[-2.0227118, -2.9328647]], dtype=float32)

time = 20988	action = 0	current_phase = 0	next_phase = 1	reward = 0.070800	array([[-2.688863 , -2.8365123]], dtype=float32)

time = 20993	action = 1	current_phase = 0	next_phase = 1	reward = -1.802537	array([[-4.594646 , -3.6103325]], dtype=float32)

time = 21001	action = 1	current_phase = 1	next_phase = 0	reward = -1.572000	array([[-3.802493, -2.190467]], dtype=float32)

time = 21009	action = 0	current_phase = 0	next_phase = 1	reward = 0.571155	array([[-1.4046813, -2.6774604]], dtype=float32)

time = 21014	action = 0	current_phase = 0	next_phase = 1	reward = 0.042719	array([[-1.7396715, -2.8222094]], dtype=float32)

time = 21019	action = 0	current_phase = 0	next_phase = 1	reward = 0.048522	array([[-2.229509, -2.460136]], dtype=float32)

time = 21024	action = 1	current_phase = 0	next_phase = 1	reward = -1.192976	array([[-4.4653897, -2.9320793]], dtype=float32)

time = 21032	action = 1	current_phase = 1	next_phase = 0	reward = -0.763692	array([[-3.752489 , -2.1675825]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0594 - val_loss: 0.0314

Epoch 2/50

 - 3s - loss: 0.0831 - val_loss: 0.0324

Epoch 3/50

 - 3s - loss: 0.0635 - val_loss: 0.0307

Epoch 4/50

 - 3s - loss: 0.0643 - val_loss: 0.0292

Epoch 5/50

 - 3s - loss: 0.0659 - val_loss: 0.0296

Epoch 6/50

 - 3s - loss: 0.0637 - val_loss: 0.0296

Epoch 7/50

 - 3s - loss: 0.0676 - val_loss: 0.0392

Epoch 8/50

 - 3s - loss: 0.0529 - val_loss: 0.0391

Epoch 9/50

 - 3s - loss: 0.0571 - val_loss: 0.0367

Epoch 10/50

 - 3s - loss: 0.0538 - val_loss: 0.0323

Epoch 11/50

 - 3s - loss: 0.0565 - val_loss: 0.0400

Epoch 12/50

 - 4s - loss: 0.0576 - val_loss: 0.0399

Epoch 13/50

 - 4s - loss: 0.0492 - val_loss: 0.0323

Epoch 14/50

 - 4s - loss: 0.0501 - val_loss: 0.0361

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 938, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 926, after forget

time = 21040	action = 0	current_phase = 0	next_phase = 1	reward = -0.302705	array([[-1.6205846, -2.703825 ]], dtype=float32)

time = 21045	action = 0	current_phase = 0	next_phase = 1	reward = 0.324622	array([[-1.9871695, -2.8225272]], dtype=float32)

time = 21050	action = 1	current_phase = 0	next_phase = 1	reward = -1.310387	array([[-3.3830388, -2.9004147]], dtype=float32)

time = 21058	action = 1	current_phase = 1	next_phase = 0	reward = -0.706770	array([[-3.6839838, -1.9876169]], dtype=float32)

time = 21066	action = 0	current_phase = 0	next_phase = 1	reward = -0.089727	array([[-1.4428942, -2.7890697]], dtype=float32)

time = 21071	action = 0	current_phase = 0	next_phase = 1	reward = -0.013164	array([[-1.8580096, -2.8304803]], dtype=float32)

time = 21076	action = 0	current_phase = 0	next_phase = 1	reward = 0.061374	array([[-2.198181, -2.935479]], dtype=float32)

time = 21081	action = 1	current_phase = 0	next_phase = 1	reward = -1.441146	array([[-4.6148214, -3.0739782]], dtype=float32)

time = 21089	action = 1	current_phase = 1	next_phase = 0	reward = -0.895044	array([[-3.737502 , -2.0311737]], dtype=float32)

time = 21097	action = 0	current_phase = 0	next_phase = 1	reward = -0.068702	array([[-1.5145895, -2.8263812]], dtype=float32)

time = 21102	action = 0	current_phase = 0	next_phase = 1	reward = 0.022163	array([[-1.9375839, -2.899343 ]], dtype=float32)

time = 21107	action = 0	current_phase = 0	next_phase = 1	reward = 0.071668	array([[-2.238394 , -2.9239001]], dtype=float32)

time = 21112	action = 1	current_phase = 0	next_phase = 1	reward = -1.611510	array([[-4.5754857, -3.3044431]], dtype=float32)

time = 21120	action = 1	current_phase = 1	next_phase = 0	reward = -1.003484	array([[-3.750982 , -2.1322427]], dtype=float32)

time = 21128	action = 0	current_phase = 0	next_phase = 1	reward = -0.060892	array([[-1.4730079, -2.8091686]], dtype=float32)

time = 21133	action = 0	current_phase = 0	next_phase = 1	reward = 0.020376	array([[-1.9647377, -2.9616392]], dtype=float32)

time = 21138	action = 0	current_phase = 0	next_phase = 1	reward = 0.080365	array([[-2.4998424, -2.8968928]], dtype=float32)

time = 21143	action = 1	current_phase = 0	next_phase = 1	reward = -1.916488	array([[-4.484777 , -3.2869432]], dtype=float32)

time = 21151	action = 1	current_phase = 1	next_phase = 0	reward = -1.344375	array([[-3.7309046, -2.2857513]], dtype=float32)

time = 21159	action = 0	current_phase = 0	next_phase = 1	reward = 0.265207	array([[-1.3177738, -2.7124305]], dtype=float32)

time = 21164	action = 0	current_phase = 0	next_phase = 1	reward = 0.035935	array([[-1.7888515, -2.8673863]], dtype=float32)

time = 21169	action = 0	current_phase = 0	next_phase = 1	reward = 0.060218	array([[-2.3952522, -2.5219824]], dtype=float32)

time = 21174	action = 1	current_phase = 0	next_phase = 1	reward = -1.068404	array([[-4.415048 , -2.9311087]], dtype=float32)

time = 21182	action = 1	current_phase = 1	next_phase = 0	reward = -0.753364	array([[-3.5160937, -2.25627  ]], dtype=float32)

time = 21190	action = 0	current_phase = 0	next_phase = 1	reward = -0.301157	array([[-1.6244352, -2.6266181]], dtype=float32)

time = 21195	action = 0	current_phase = 0	next_phase = 1	reward = 0.323049	array([[-2.0119588, -2.852873 ]], dtype=float32)

time = 21200	action = 1	current_phase = 0	next_phase = 1	reward = -1.206799	array([[-3.537665 , -2.8478322]], dtype=float32)

time = 21208	action = 1	current_phase = 1	next_phase = 0	reward = -0.703231	array([[-3.3069365, -1.9252923]], dtype=float32)

time = 21216	action = 0	current_phase = 0	next_phase = 1	reward = -0.088107	array([[-1.5053241, -2.8404365]], dtype=float32)

time = 21221	action = 0	current_phase = 0	next_phase = 1	reward = -0.011198	array([[-1.8903801, -2.8153667]], dtype=float32)

time = 21226	action = 0	current_phase = 0	next_phase = 1	reward = 0.064328	array([[-2.207721 , -2.9265769]], dtype=float32)

time = 21231	action = 1	current_phase = 0	next_phase = 1	reward = -1.496129	array([[-4.4939814, -3.1148043]], dtype=float32)

time = 21239	action = 1	current_phase = 1	next_phase = 0	reward = -0.850347	array([[-3.741988 , -2.0255885]], dtype=float32)

time = 21247	action = 0	current_phase = 0	next_phase = 1	reward = -0.079456	array([[-1.439259 , -2.7966864]], dtype=float32)

time = 21252	action = 0	current_phase = 0	next_phase = 1	reward = 0.003147	array([[-1.8791263, -2.8494728]], dtype=float32)

time = 21257	action = 0	current_phase = 0	next_phase = 1	reward = 0.063157	array([[-2.2924185, -2.9621596]], dtype=float32)

time = 21262	action = 1	current_phase = 0	next_phase = 1	reward = -1.595678	array([[-4.462867 , -3.2802246]], dtype=float32)

time = 21270	action = 1	current_phase = 1	next_phase = 0	reward = -0.880889	array([[-3.734212 , -2.1852808]], dtype=float32)

time = 21278	action = 0	current_phase = 0	next_phase = 1	reward = -0.039163	array([[-1.501899 , -2.7969244]], dtype=float32)

time = 21283	action = 0	current_phase = 0	next_phase = 1	reward = 0.017060	array([[-2.0044065, -2.9692914]], dtype=float32)

time = 21288	action = 0	current_phase = 0	next_phase = 1	reward = 0.075500	array([[-2.4839704, -2.8099701]], dtype=float32)

time = 21293	action = 1	current_phase = 0	next_phase = 1	reward = -1.391985	array([[-4.435263, -3.190364]], dtype=float32)

time = 21301	action = 1	current_phase = 1	next_phase = 0	reward = -1.452157	array([[-3.5395136, -2.3003602]], dtype=float32)

time = 21309	action = 0	current_phase = 0	next_phase = 1	reward = 0.246599	array([[-1.3788886, -2.7244782]], dtype=float32)

time = 21314	action = 0	current_phase = 0	next_phase = 1	reward = 0.037994	array([[-1.7530384, -2.762342 ]], dtype=float32)

time = 21319	action = 0	current_phase = 0	next_phase = 1	reward = 0.068928	array([[-2.4486754, -2.457603 ]], dtype=float32)

time = 21324	action = 1	current_phase = 0	next_phase = 1	reward = -1.068463	array([[-4.3082333, -2.999752 ]], dtype=float32)

time = 21332	action = 1	current_phase = 1	next_phase = 0	reward = -1.008288	array([[-3.4031959, -2.2342896]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.1039 - val_loss: 0.0349

Epoch 2/50

 - 3s - loss: 0.0769 - val_loss: 0.0313

Epoch 3/50

 - 3s - loss: 0.0912 - val_loss: 0.0403

Epoch 4/50

 - 3s - loss: 0.0763 - val_loss: 0.0287

Epoch 5/50

 - 3s - loss: 0.1178 - val_loss: 0.0340

Epoch 6/50

 - 3s - loss: 0.0762 - val_loss: 0.0320

Epoch 7/50

 - 3s - loss: 0.0690 - val_loss: 0.0411

Epoch 8/50

 - 3s - loss: 0.0726 - val_loss: 0.0319

Epoch 9/50

 - 3s - loss: 0.0966 - val_loss: 0.0340

Epoch 10/50

 - 3s - loss: 0.0729 - val_loss: 0.0316

Epoch 11/50

 - 3s - loss: 0.0701 - val_loss: 0.0329

Epoch 12/50

 - 3s - loss: 0.0738 - val_loss: 0.0495

Epoch 13/50

 - 3s - loss: 0.0744 - val_loss: 0.0323

Epoch 14/50

 - 3s - loss: 0.0688 - val_loss: 0.0407

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 948, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 936, after forget

time = 21340	action = 0	current_phase = 0	next_phase = 1	reward = -0.024783	array([[-1.4459227, -2.67949  ]], dtype=float32)

time = 21345	action = 0	current_phase = 0	next_phase = 1	reward = 0.323249	array([[-1.9325562, -2.778811 ]], dtype=float32)

time = 21350	action = 1	current_phase = 0	next_phase = 1	reward = -1.263187	array([[-3.323026 , -2.6997008]], dtype=float32)

time = 21358	action = 1	current_phase = 1	next_phase = 0	reward = -0.698952	array([[-3.5384607, -2.0158854]], dtype=float32)

time = 21366	action = 0	current_phase = 0	next_phase = 1	reward = -0.078794	array([[-1.3555517, -2.7451138]], dtype=float32)

time = 21371	action = 0	current_phase = 0	next_phase = 1	reward = -0.007125	array([[-1.6477442, -2.7295728]], dtype=float32)

time = 21376	action = 0	current_phase = 0	next_phase = 1	reward = 0.061656	array([[-2.309063 , -2.9139771]], dtype=float32)

time = 21381	action = 1	current_phase = 0	next_phase = 1	reward = -1.507167	array([[-4.6273823, -3.0074096]], dtype=float32)

time = 21389	action = 1	current_phase = 1	next_phase = 0	reward = -0.845240	array([[-3.72933  , -1.9455162]], dtype=float32)

time = 21397	action = 0	current_phase = 0	next_phase = 1	reward = -0.079964	array([[-1.46785 , -2.818841]], dtype=float32)

time = 21402	action = 0	current_phase = 0	next_phase = 1	reward = 0.000037	array([[-1.7472209, -2.761218 ]], dtype=float32)

time = 21407	action = 0	current_phase = 0	next_phase = 1	reward = 0.067560	array([[-2.3604484, -2.9523096]], dtype=float32)

time = 21412	action = 1	current_phase = 0	next_phase = 1	reward = -1.544244	array([[-4.641668 , -3.2243876]], dtype=float32)

time = 21420	action = 1	current_phase = 1	next_phase = 0	reward = -0.996132	array([[-3.6981225, -2.149734 ]], dtype=float32)

time = 21428	action = 0	current_phase = 0	next_phase = 1	reward = -0.051923	array([[-1.4061539, -2.749271 ]], dtype=float32)

time = 21433	action = 0	current_phase = 0	next_phase = 1	reward = 0.011754	array([[-1.9318359, -2.9292989]], dtype=float32)

time = 21438	action = 0	current_phase = 0	next_phase = 1	reward = 0.078433	array([[-2.6990871, -2.7577205]], dtype=float32)

time = 21443	action = 1	current_phase = 0	next_phase = 1	reward = -1.768555	array([[-4.609534 , -3.4697614]], dtype=float32)

time = 21451	action = 1	current_phase = 1	next_phase = 0	reward = -1.397148	array([[-3.8100958, -2.2655818]], dtype=float32)

time = 21459	action = 0	current_phase = 0	next_phase = 1	reward = 0.254567	array([[-1.2824422, -2.704295 ]], dtype=float32)

time = 21464	action = 0	current_phase = 0	next_phase = 1	reward = 0.016090	array([[-1.8069934, -2.873704 ]], dtype=float32)

time = 21469	action = 1	current_phase = 0	next_phase = 1	reward = -0.683843	array([[-2.588526 , -2.4543781]], dtype=float32)

time = 21477	action = 1	current_phase = 1	next_phase = 0	reward = -0.867833	array([[-3.0168657, -1.828538 ]], dtype=float32)

time = 21485	action = 0	current_phase = 0	next_phase = 1	reward = -0.095561	array([[-1.3519872, -2.8724046]], dtype=float32)

time = 21490	action = 0	current_phase = 0	next_phase = 1	reward = 0.254738	array([[-1.502389, -2.706777]], dtype=float32)

time = 21495	action = 0	current_phase = 0	next_phase = 1	reward = 0.026933	array([[-2.1190329, -2.8835988]], dtype=float32)

time = 21500	action = 1	current_phase = 0	next_phase = 1	reward = -1.364082	array([[-3.81094  , -2.8321638]], dtype=float32)

time = 21508	action = 1	current_phase = 1	next_phase = 0	reward = -0.721210	array([[-3.6785278, -1.966054 ]], dtype=float32)

time = 21516	action = 0	current_phase = 0	next_phase = 1	reward = -0.082612	array([[-1.445016 , -2.7878904]], dtype=float32)

time = 21521	action = 0	current_phase = 0	next_phase = 1	reward = -0.007298	array([[-1.7585658, -2.7791753]], dtype=float32)

time = 21526	action = 0	current_phase = 0	next_phase = 1	reward = 0.055963	array([[-2.3288956, -2.9213467]], dtype=float32)

time = 21531	action = 1	current_phase = 0	next_phase = 1	reward = -1.510098	array([[-4.5924797, -2.9661684]], dtype=float32)

time = 21539	action = 1	current_phase = 1	next_phase = 0	reward = -0.845672	array([[-3.7141943, -2.0790296]], dtype=float32)

time = 21547	action = 0	current_phase = 0	next_phase = 1	reward = -0.075443	array([[-1.4166435, -2.784685 ]], dtype=float32)

time = 21552	action = 0	current_phase = 0	next_phase = 1	reward = -0.016769	array([[-1.8913026, -2.8306708]], dtype=float32)

time = 21557	action = 0	current_phase = 0	next_phase = 1	reward = 0.052166	array([[-2.5663724, -2.839272 ]], dtype=float32)

time = 21562	action = 1	current_phase = 0	next_phase = 1	reward = -1.639538	array([[-4.5372324, -3.2165241]], dtype=float32)

time = 21570	action = 1	current_phase = 1	next_phase = 0	reward = -1.121355	array([[-3.6792088, -2.0044098]], dtype=float32)

time = 21578	action = 0	current_phase = 0	next_phase = 1	reward = 0.239519	array([[-1.4208449, -2.730342 ]], dtype=float32)

time = 21583	action = 0	current_phase = 0	next_phase = 1	reward = 0.031057	array([[-1.8721253, -2.8981752]], dtype=float32)

time = 21588	action = 0	current_phase = 0	next_phase = 1	reward = 0.082379	array([[-2.603464 , -2.7973275]], dtype=float32)

time = 21593	action = 1	current_phase = 0	next_phase = 1	reward = -0.791199	array([[-4.31159  , -3.2247167]], dtype=float32)

time = 21601	action = 1	current_phase = 1	next_phase = 0	reward = -1.312295	array([[-3.382513 , -2.3216898]], dtype=float32)

time = 21609	action = 0	current_phase = 0	next_phase = 1	reward = -0.052475	array([[-1.3607309, -2.7166717]], dtype=float32)

time = 21614	action = 0	current_phase = 0	next_phase = 1	reward = 0.043193	array([[-1.7936928, -2.789709 ]], dtype=float32)

time = 21619	action = 1	current_phase = 0	next_phase = 1	reward = -0.793690	array([[-2.6522982, -2.3630536]], dtype=float32)

time = 21627	action = 1	current_phase = 1	next_phase = 0	reward = -0.639191	array([[-3.6027064, -1.9893799]], dtype=float32)

time = 21635	action = 0	current_phase = 0	next_phase = 1	reward = -0.096817	array([[-1.5031128, -2.8437023]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0658 - val_loss: 0.0238

Epoch 2/50

 - 3s - loss: 0.0628 - val_loss: 0.0314

Epoch 3/50

 - 3s - loss: 0.0641 - val_loss: 0.0357

Epoch 4/50

 - 3s - loss: 0.0556 - val_loss: 0.0337

Epoch 5/50

 - 3s - loss: 0.0580 - val_loss: 0.0285

Epoch 6/50

 - 3s - loss: 0.0470 - val_loss: 0.0267

Epoch 7/50

 - 3s - loss: 0.0693 - val_loss: 0.0278

Epoch 8/50

 - 3s - loss: 0.0534 - val_loss: 0.0303

Epoch 9/50

 - 3s - loss: 0.0606 - val_loss: 0.0348

Epoch 10/50

 - 3s - loss: 0.0662 - val_loss: 0.0324

Epoch 11/50

 - 3s - loss: 0.0516 - val_loss: 0.0278

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 958, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 946, after forget

time = 21640	action = 0	current_phase = 0	next_phase = 1	reward = -0.024837	array([[-1.8311737, -2.6601899]], dtype=float32)

time = 21645	action = 0	current_phase = 0	next_phase = 1	reward = -0.232588	array([[-2.2462068, -2.923307 ]], dtype=float32)

time = 21650	action = 1	current_phase = 0	next_phase = 1	reward = -1.036757	array([[-3.5627842, -2.7129114]], dtype=float32)

time = 21658	action = 1	current_phase = 1	next_phase = 0	reward = -0.721193	array([[-3.5172138, -1.9367362]], dtype=float32)

time = 21666	action = 0	current_phase = 0	next_phase = 1	reward = -0.099340	array([[-1.5541645, -2.8187735]], dtype=float32)

time = 21671	action = 0	current_phase = 0	next_phase = 1	reward = -0.010475	array([[-1.9254683, -2.778665 ]], dtype=float32)

time = 21676	action = 0	current_phase = 0	next_phase = 1	reward = 0.068658	array([[-2.4163744, -2.9246237]], dtype=float32)

time = 21681	action = 1	current_phase = 0	next_phase = 1	reward = -1.492296	array([[-4.7846875, -3.0484107]], dtype=float32)

time = 21689	action = 1	current_phase = 1	next_phase = 0	reward = -0.832098	array([[-3.6931715, -2.0098557]], dtype=float32)

time = 21697	action = 0	current_phase = 0	next_phase = 1	reward = -0.053209	array([[-1.5742731, -2.804079 ]], dtype=float32)

time = 21702	action = 0	current_phase = 0	next_phase = 1	reward = 0.012582	array([[-1.8780311, -2.8069937]], dtype=float32)

time = 21707	action = 0	current_phase = 0	next_phase = 1	reward = 0.064182	array([[-2.5596023, -2.9261847]], dtype=float32)

time = 21712	action = 1	current_phase = 0	next_phase = 1	reward = -1.504711	array([[-4.599034 , -3.3571198]], dtype=float32)

time = 21720	action = 1	current_phase = 1	next_phase = 0	reward = -0.991261	array([[-3.7043166, -2.070999 ]], dtype=float32)

time = 21728	action = 0	current_phase = 0	next_phase = 1	reward = -0.054444	array([[-1.5257802, -2.7933936]], dtype=float32)

time = 21733	action = 0	current_phase = 0	next_phase = 1	reward = 0.019183	array([[-1.9954003, -3.0004365]], dtype=float32)

time = 21738	action = 0	current_phase = 0	next_phase = 1	reward = 0.069434	array([[-2.6267593, -2.943839 ]], dtype=float32)

time = 21743	action = 1	current_phase = 0	next_phase = 1	reward = -1.811094	array([[-4.601569 , -3.6099641]], dtype=float32)

time = 21751	action = 1	current_phase = 1	next_phase = 0	reward = -1.032748	array([[-3.9214864, -2.2494748]], dtype=float32)

time = 21759	action = 0	current_phase = 0	next_phase = 1	reward = -0.035196	array([[-1.3950963, -2.6867216]], dtype=float32)

time = 21764	action = 0	current_phase = 0	next_phase = 1	reward = 0.035518	array([[-1.8565863, -2.8424118]], dtype=float32)

time = 21769	action = 0	current_phase = 0	next_phase = 1	reward = 0.065749	array([[-2.3017032, -2.3815184]], dtype=float32)

time = 21774	action = 1	current_phase = 0	next_phase = 1	reward = -1.127255	array([[-4.561668, -3.002598]], dtype=float32)

time = 21782	action = 1	current_phase = 1	next_phase = 0	reward = -1.018963	array([[-3.5739303, -2.153882 ]], dtype=float32)

time = 21790	action = 0	current_phase = 0	next_phase = 1	reward = 0.266696	array([[-1.6863475, -2.668593 ]], dtype=float32)

time = 21795	action = 0	current_phase = 0	next_phase = 1	reward = 0.053583	array([[-2.2241645, -2.78624  ]], dtype=float32)

time = 21800	action = 1	current_phase = 0	next_phase = 1	reward = -1.373696	array([[-3.4482436, -2.8040743]], dtype=float32)

time = 21808	action = 1	current_phase = 1	next_phase = 0	reward = -0.709121	array([[-3.628663, -1.930943]], dtype=float32)

time = 21816	action = 0	current_phase = 0	next_phase = 1	reward = -0.076573	array([[-1.5712969, -2.7945387]], dtype=float32)

time = 21821	action = 0	current_phase = 0	next_phase = 1	reward = 0.001341	array([[-1.7894782, -2.7350354]], dtype=float32)

time = 21826	action = 0	current_phase = 0	next_phase = 1	reward = 0.054557	array([[-2.4161134, -2.9278448]], dtype=float32)

time = 21831	action = 1	current_phase = 0	next_phase = 1	reward = -1.397407	array([[-4.6032457, -3.12155  ]], dtype=float32)

time = 21839	action = 1	current_phase = 1	next_phase = 0	reward = -0.775213	array([[-3.685217 , -2.0056543]], dtype=float32)

time = 21847	action = 0	current_phase = 0	next_phase = 1	reward = -0.072782	array([[-1.6973766, -2.8041017]], dtype=float32)

time = 21852	action = 0	current_phase = 0	next_phase = 1	reward = 0.005538	array([[-1.9803077, -2.8500571]], dtype=float32)

time = 21857	action = 0	current_phase = 0	next_phase = 1	reward = 0.065242	array([[-2.647894, -2.935703]], dtype=float32)

time = 21862	action = 1	current_phase = 0	next_phase = 1	reward = -1.529919	array([[-4.6671104, -3.2257297]], dtype=float32)

time = 21870	action = 1	current_phase = 1	next_phase = 0	reward = -0.843678	array([[-3.6989007, -2.1241882]], dtype=float32)

time = 21878	action = 0	current_phase = 0	next_phase = 1	reward = -0.038139	array([[-1.5000019, -2.759672 ]], dtype=float32)

time = 21883	action = 0	current_phase = 0	next_phase = 1	reward = 0.035104	array([[-2.038449, -2.944864]], dtype=float32)

time = 21888	action = 1	current_phase = 0	next_phase = 1	reward = -1.785360	array([[-2.8229403, -2.805593 ]], dtype=float32)

time = 21896	action = 1	current_phase = 1	next_phase = 0	reward = -0.933346	array([[-3.851317 , -2.0500662]], dtype=float32)

time = 21904	action = 0	current_phase = 0	next_phase = 1	reward = -0.115316	array([[-1.4421229, -2.8272288]], dtype=float32)

time = 21909	action = 0	current_phase = 0	next_phase = 1	reward = -0.053814	array([[-1.4493082, -2.679046 ]], dtype=float32)

time = 21914	action = 0	current_phase = 0	next_phase = 1	reward = 0.002291	array([[-1.9514722, -2.8772452]], dtype=float32)

time = 21919	action = 1	current_phase = 0	next_phase = 1	reward = -0.653953	array([[-3.0130181, -2.400455 ]], dtype=float32)

time = 21927	action = 1	current_phase = 1	next_phase = 0	reward = -0.593654	array([[-3.3875198, -1.9818568]], dtype=float32)

time = 21935	action = 0	current_phase = 0	next_phase = 1	reward = -0.120007	array([[-1.5974082, -2.8403237]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0700 - val_loss: 0.0335

Epoch 2/50

 - 3s - loss: 0.0733 - val_loss: 0.0378

Epoch 3/50

 - 3s - loss: 0.0974 - val_loss: 0.0338

Epoch 4/50

 - 3s - loss: 0.0771 - val_loss: 0.0318

Epoch 5/50

 - 3s - loss: 0.1123 - val_loss: 0.0350

Epoch 6/50

 - 3s - loss: 0.0688 - val_loss: 0.0376

Epoch 7/50

 - 3s - loss: 0.0616 - val_loss: 0.0344

Epoch 8/50

 - 3s - loss: 0.0646 - val_loss: 0.0317

Epoch 9/50

 - 3s - loss: 0.0580 - val_loss: 0.0312

Epoch 10/50

 - 3s - loss: 0.0585 - val_loss: 0.0353

Epoch 11/50

 - 3s - loss: 0.0670 - val_loss: 0.0315

Epoch 12/50

 - 3s - loss: 0.0535 - val_loss: 0.0287

Epoch 13/50

 - 3s - loss: 0.0688 - val_loss: 0.0315

Epoch 14/50

 - 3s - loss: 0.0530 - val_loss: 0.0308

Epoch 15/50

 - 3s - loss: 0.0581 - val_loss: 0.0288

Epoch 16/50

 - 3s - loss: 0.0562 - val_loss: 0.0352

Epoch 17/50

 - 3s - loss: 0.0526 - val_loss: 0.0341

Epoch 18/50

 - 3s - loss: 0.0582 - val_loss: 0.0401

Epoch 19/50

 - 3s - loss: 0.0588 - val_loss: 0.0392

Epoch 20/50

 - 3s - loss: 0.0695 - val_loss: 0.0358

Epoch 21/50

 - 3s - loss: 0.0729 - val_loss: 0.0336

Epoch 22/50

 - 3s - loss: 0.0554 - val_loss: 0.0413

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 968, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 956, after forget

time = 21940	action = 0	current_phase = 0	next_phase = 1	reward = -0.308508	array([[-1.766758 , -2.7048674]], dtype=float32)

time = 21945	action = 0	current_phase = 0	next_phase = 1	reward = 0.326855	array([[-2.2113109, -2.933265 ]], dtype=float32)

time = 21950	action = 1	current_phase = 0	next_phase = 1	reward = -1.318520	array([[-3.6221035, -2.7879105]], dtype=float32)

time = 21958	action = 1	current_phase = 1	next_phase = 0	reward = -0.684723	array([[-3.7594075, -1.9867421]], dtype=float32)

time = 21966	action = 0	current_phase = 0	next_phase = 1	reward = -0.076251	array([[-1.4509728, -2.7898717]], dtype=float32)

time = 21971	action = 0	current_phase = 0	next_phase = 1	reward = 0.007869	array([[-1.7394114, -2.7828808]], dtype=float32)

time = 21976	action = 0	current_phase = 0	next_phase = 1	reward = 0.062983	array([[-2.3906038, -2.9299867]], dtype=float32)

time = 21981	action = 1	current_phase = 0	next_phase = 1	reward = -1.499225	array([[-4.32786  , -3.0842407]], dtype=float32)

time = 21989	action = 1	current_phase = 1	next_phase = 0	reward = -0.846222	array([[-3.7814116, -2.0545855]], dtype=float32)

time = 21997	action = 0	current_phase = 0	next_phase = 1	reward = -0.071420	array([[-1.5018895, -2.820672 ]], dtype=float32)

time = 22002	action = 0	current_phase = 0	next_phase = 1	reward = -0.008429	array([[-1.8148425, -2.7847114]], dtype=float32)

time = 22007	action = 0	current_phase = 0	next_phase = 1	reward = 0.053437	array([[-2.512609, -2.898456]], dtype=float32)

time = 22012	action = 1	current_phase = 0	next_phase = 1	reward = -1.541511	array([[-4.6293335, -3.2909627]], dtype=float32)

time = 22020	action = 1	current_phase = 1	next_phase = 0	reward = -0.946697	array([[-3.7885675, -2.134941 ]], dtype=float32)

time = 22028	action = 0	current_phase = 0	next_phase = 1	reward = -0.046177	array([[-1.4321083, -2.7364693]], dtype=float32)

time = 22033	action = 0	current_phase = 0	next_phase = 1	reward = 0.041479	array([[-1.9164213, -2.9206436]], dtype=float32)

time = 22038	action = 0	current_phase = 0	next_phase = 1	reward = 0.072818	array([[-2.2409327, -2.748731 ]], dtype=float32)

time = 22043	action = 1	current_phase = 0	next_phase = 1	reward = -1.317585	array([[-4.5704913, -3.507919 ]], dtype=float32)

time = 22051	action = 1	current_phase = 1	next_phase = 0	reward = -1.380867	array([[-3.581236, -2.215867]], dtype=float32)

time = 22059	action = 0	current_phase = 0	next_phase = 1	reward = 0.248365	array([[-1.3424726, -2.6651785]], dtype=float32)

time = 22064	action = 0	current_phase = 0	next_phase = 1	reward = 0.310880	array([[-1.7165364, -2.7687888]], dtype=float32)

time = 22069	action = 0	current_phase = 0	next_phase = 1	reward = -0.227091	array([[-2.1201968, -2.3937905]], dtype=float32)

time = 22074	action = 1	current_phase = 0	next_phase = 1	reward = -1.077289	array([[-4.1297827, -2.9821382]], dtype=float32)

time = 22082	action = 1	current_phase = 1	next_phase = 0	reward = -1.027750	array([[-3.6565294, -2.1190503]], dtype=float32)

time = 22090	action = 0	current_phase = 0	next_phase = 1	reward = 0.252249	array([[-1.5749509, -2.6744502]], dtype=float32)

time = 22095	action = 0	current_phase = 0	next_phase = 1	reward = 0.043219	array([[-2.1549191, -2.8768487]], dtype=float32)

time = 22100	action = 1	current_phase = 0	next_phase = 1	reward = -1.259714	array([[-3.3543608, -2.9003415]], dtype=float32)

time = 22108	action = 1	current_phase = 1	next_phase = 0	reward = -0.705629	array([[-3.5960045, -1.9862467]], dtype=float32)

time = 22116	action = 0	current_phase = 0	next_phase = 1	reward = -0.084059	array([[-1.4648287, -2.789991 ]], dtype=float32)

time = 22121	action = 0	current_phase = 0	next_phase = 1	reward = -0.005099	array([[-1.7173843, -2.7177286]], dtype=float32)

time = 22126	action = 0	current_phase = 0	next_phase = 1	reward = 0.054455	array([[-2.388659 , -2.9094534]], dtype=float32)

time = 22131	action = 1	current_phase = 0	next_phase = 1	reward = -1.436178	array([[-4.5769   , -3.0520444]], dtype=float32)

time = 22139	action = 1	current_phase = 1	next_phase = 0	reward = -0.766800	array([[-3.7834077, -2.032925 ]], dtype=float32)

time = 22147	action = 0	current_phase = 0	next_phase = 1	reward = -0.070909	array([[-1.4569787, -2.774509 ]], dtype=float32)

time = 22152	action = 0	current_phase = 0	next_phase = 1	reward = 0.019381	array([[-1.7418675, -2.78603  ]], dtype=float32)

time = 22157	action = 0	current_phase = 0	next_phase = 1	reward = 0.073042	array([[-2.3830032, -2.9508502]], dtype=float32)

time = 22162	action = 1	current_phase = 0	next_phase = 1	reward = -1.545737	array([[-4.5747557, -3.3606012]], dtype=float32)

time = 22170	action = 1	current_phase = 1	next_phase = 0	reward = -1.006764	array([[-3.8642259, -2.1599665]], dtype=float32)

time = 22178	action = 0	current_phase = 0	next_phase = 1	reward = -0.058810	array([[-1.3909564, -2.732    ]], dtype=float32)

time = 22183	action = 0	current_phase = 0	next_phase = 1	reward = 0.014599	array([[-1.8837566, -2.8979998]], dtype=float32)

time = 22188	action = 0	current_phase = 0	next_phase = 1	reward = 0.080813	array([[-2.476848 , -2.8636842]], dtype=float32)

time = 22193	action = 1	current_phase = 0	next_phase = 1	reward = -0.787939	array([[-4.6304317, -3.3832674]], dtype=float32)

time = 22201	action = 1	current_phase = 1	next_phase = 0	reward = -1.260922	array([[-3.442957, -2.313803]], dtype=float32)

time = 22209	action = 0	current_phase = 0	next_phase = 1	reward = -0.036755	array([[-1.4085298, -2.727035 ]], dtype=float32)

time = 22214	action = 0	current_phase = 0	next_phase = 1	reward = 0.059359	array([[-1.7864182, -2.7630172]], dtype=float32)

time = 22219	action = 0	current_phase = 0	next_phase = 1	reward = 0.055502	array([[-2.403627, -2.752523]], dtype=float32)

time = 22224	action = 1	current_phase = 0	next_phase = 1	reward = -1.240615	array([[-3.991211 , -2.9199276]], dtype=float32)

time = 22232	action = 1	current_phase = 1	next_phase = 0	reward = -1.019671	array([[-3.7097783, -2.1618066]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0781 - val_loss: 0.0257

Epoch 2/50

 - 3s - loss: 0.0581 - val_loss: 0.0259

Epoch 3/50

 - 3s - loss: 0.0541 - val_loss: 0.0209

Epoch 4/50

 - 3s - loss: 0.0569 - val_loss: 0.0223

Epoch 5/50

 - 3s - loss: 0.0499 - val_loss: 0.0275

Epoch 6/50

 - 3s - loss: 0.0558 - val_loss: 0.0213

Epoch 7/50

 - 3s - loss: 0.0518 - val_loss: 0.0221

Epoch 8/50

 - 3s - loss: 0.0540 - val_loss: 0.0230

Epoch 9/50

 - 3s - loss: 0.0487 - val_loss: 0.0289

Epoch 10/50

 - 3s - loss: 0.0519 - val_loss: 0.0283

Epoch 11/50

 - 3s - loss: 0.0532 - val_loss: 0.0253

Epoch 12/50

 - 3s - loss: 0.0432 - val_loss: 0.0300

Epoch 13/50

 - 3s - loss: 0.0544 - val_loss: 0.0277

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 978, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 966, after forget

time = 22240	action = 0	current_phase = 0	next_phase = 1	reward = 0.276483	array([[-1.5796026, -2.637986 ]], dtype=float32)

time = 22245	action = 0	current_phase = 0	next_phase = 1	reward = 0.070817	array([[-2.1646957, -2.7989929]], dtype=float32)

time = 22250	action = 1	current_phase = 0	next_phase = 1	reward = -1.372004	array([[-3.5970907, -2.90173  ]], dtype=float32)

time = 22258	action = 1	current_phase = 1	next_phase = 0	reward = -0.710069	array([[-3.644137 , -2.0581803]], dtype=float32)

time = 22266	action = 0	current_phase = 0	next_phase = 1	reward = -0.084247	array([[-1.6317925, -2.805509 ]], dtype=float32)

time = 22271	action = 0	current_phase = 0	next_phase = 1	reward = -0.012389	array([[-1.7647276, -2.7169838]], dtype=float32)

time = 22276	action = 0	current_phase = 0	next_phase = 1	reward = 0.058032	array([[-2.520489 , -2.9365044]], dtype=float32)

time = 22281	action = 1	current_phase = 0	next_phase = 1	reward = -1.435816	array([[-4.6577926, -3.0667775]], dtype=float32)

time = 22289	action = 1	current_phase = 1	next_phase = 0	reward = -0.909414	array([[-3.658401 , -1.9651845]], dtype=float32)

time = 22297	action = 0	current_phase = 0	next_phase = 1	reward = -0.070304	array([[-1.588077, -2.754649]], dtype=float32)

time = 22302	action = 0	current_phase = 0	next_phase = 1	reward = 0.010539	array([[-1.8460733, -2.7662807]], dtype=float32)

time = 22307	action = 0	current_phase = 0	next_phase = 1	reward = 0.074831	array([[-2.4486632, -2.9355404]], dtype=float32)

time = 22312	action = 1	current_phase = 0	next_phase = 1	reward = -1.600664	array([[-4.6369343, -3.3367457]], dtype=float32)

time = 22320	action = 1	current_phase = 1	next_phase = 0	reward = -1.205062	array([[-3.804955 , -2.1736002]], dtype=float32)

time = 22328	action = 0	current_phase = 0	next_phase = 1	reward = 0.248431	array([[-1.6375449, -2.784226 ]], dtype=float32)

time = 22333	action = 0	current_phase = 0	next_phase = 1	reward = 0.019936	array([[-2.0097184, -2.917986 ]], dtype=float32)

time = 22338	action = 1	current_phase = 0	next_phase = 1	reward = -1.718031	array([[-2.824644 , -2.7799344]], dtype=float32)

time = 22346	action = 1	current_phase = 1	next_phase = 0	reward = -0.927129	array([[-3.7266827, -2.0369146]], dtype=float32)

time = 22354	action = 0	current_phase = 0	next_phase = 1	reward = 0.167535	array([[-1.4222772, -2.7949715]], dtype=float32)

time = 22359	action = 0	current_phase = 0	next_phase = 1	reward = -0.313446	array([[-1.5539846, -2.753685 ]], dtype=float32)

time = 22364	action = 0	current_phase = 0	next_phase = 1	reward = 0.033436	array([[-1.1106853, -3.0509732]], dtype=float32)

time = 22369	action = 1	current_phase = 0	next_phase = 1	reward = -0.749191	array([[-3.0623226, -2.6242318]], dtype=float32)

time = 22377	action = 1	current_phase = 1	next_phase = 0	reward = -0.918736	array([[-3.6489863, -2.006844 ]], dtype=float32)

time = 22385	action = 0	current_phase = 0	next_phase = 1	reward = 0.180017	array([[-1.4976176, -2.769276 ]], dtype=float32)

time = 22390	action = 0	current_phase = 0	next_phase = 1	reward = -0.020406	array([[-1.8793666, -2.7855182]], dtype=float32)

time = 22395	action = 0	current_phase = 0	next_phase = 1	reward = 0.049876	array([[-2.431377 , -2.9299362]], dtype=float32)

time = 22400	action = 1	current_phase = 0	next_phase = 1	reward = -1.352580	array([[-3.9408655, -2.9143393]], dtype=float32)

time = 22408	action = 1	current_phase = 1	next_phase = 0	reward = -0.721884	array([[-3.691377 , -1.9774821]], dtype=float32)

time = 22416	action = 0	current_phase = 0	next_phase = 1	reward = -0.089051	array([[-1.6892747, -2.8206723]], dtype=float32)

time = 22421	action = 0	current_phase = 0	next_phase = 1	reward = -0.001732	array([[-1.7972219, -2.7197528]], dtype=float32)

time = 22426	action = 0	current_phase = 0	next_phase = 1	reward = 0.061846	array([[-2.4043467, -2.9087434]], dtype=float32)

time = 22431	action = 1	current_phase = 0	next_phase = 1	reward = -1.550154	array([[-4.6927476, -3.0299056]], dtype=float32)

time = 22439	action = 1	current_phase = 1	next_phase = 0	reward = -1.088746	array([[-3.7182333, -2.0113716]], dtype=float32)

time = 22447	action = 0	current_phase = 0	next_phase = 1	reward = 0.203348	array([[-1.533035 , -2.7420955]], dtype=float32)

time = 22452	action = 0	current_phase = 0	next_phase = 1	reward = -0.012873	array([[-1.7812436, -2.7693262]], dtype=float32)

time = 22457	action = 0	current_phase = 0	next_phase = 1	reward = 0.060312	array([[-2.4267225, -2.9411485]], dtype=float32)

time = 22462	action = 1	current_phase = 0	next_phase = 1	reward = -1.522929	array([[-4.746383 , -3.2394853]], dtype=float32)

time = 22470	action = 1	current_phase = 1	next_phase = 0	reward = -1.304493	array([[-3.7222004, -2.1884706]], dtype=float32)

time = 22478	action = 0	current_phase = 0	next_phase = 1	reward = 0.243346	array([[-1.4485688, -2.7300582]], dtype=float32)

time = 22483	action = 0	current_phase = 0	next_phase = 1	reward = 0.025723	array([[-1.987888 , -2.9121974]], dtype=float32)

time = 22488	action = 0	current_phase = 0	next_phase = 1	reward = 0.081192	array([[-2.559326 , -2.9078407]], dtype=float32)

time = 22493	action = 1	current_phase = 0	next_phase = 1	reward = -1.873727	array([[-4.6052866, -3.3490644]], dtype=float32)

time = 22501	action = 1	current_phase = 1	next_phase = 0	reward = -1.134433	array([[-3.81395 , -2.245722]], dtype=float32)

time = 22509	action = 0	current_phase = 0	next_phase = 1	reward = -0.030386	array([[-1.4105802, -2.6821337]], dtype=float32)

time = 22514	action = 0	current_phase = 0	next_phase = 1	reward = 0.031295	array([[-1.8455317, -2.8145244]], dtype=float32)

time = 22519	action = 1	current_phase = 0	next_phase = 1	reward = -0.694945	array([[-2.385119, -2.2842  ]], dtype=float32)

time = 22527	action = 1	current_phase = 1	next_phase = 0	reward = -0.585993	array([[-2.7079957, -1.8215487]], dtype=float32)

time = 22535	action = 0	current_phase = 0	next_phase = 1	reward = -0.098863	array([[-1.4607949, -2.808243 ]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0749 - val_loss: 0.0237

Epoch 2/50

 - 3s - loss: 0.0661 - val_loss: 0.0293

Epoch 3/50

 - 3s - loss: 0.0788 - val_loss: 0.0294

Epoch 4/50

 - 3s - loss: 0.0563 - val_loss: 0.0248

Epoch 5/50

 - 3s - loss: 0.0535 - val_loss: 0.0323

Epoch 6/50

 - 3s - loss: 0.0757 - val_loss: 0.0274

Epoch 7/50

 - 3s - loss: 0.0550 - val_loss: 0.0320

Epoch 8/50

 - 3s - loss: 0.0722 - val_loss: 0.0280

Epoch 9/50

 - 3s - loss: 0.0723 - val_loss: 0.0310

Epoch 10/50

 - 3s - loss: 0.0545 - val_loss: 0.0229

Epoch 11/50

 - 3s - loss: 0.0681 - val_loss: 0.0254

Epoch 12/50

 - 3s - loss: 0.0646 - val_loss: 0.0296

Epoch 13/50

 - 3s - loss: 0.0531 - val_loss: 0.0277

Epoch 14/50

 - 3s - loss: 0.0492 - val_loss: 0.0258

Epoch 15/50

 - 3s - loss: 0.0562 - val_loss: 0.0259

Epoch 16/50

 - 3s - loss: 0.0585 - val_loss: 0.0362

Epoch 17/50

 - 3s - loss: 0.0613 - val_loss: 0.0315

Epoch 18/50

 - 3s - loss: 0.0610 - val_loss: 0.0300

Epoch 19/50

 - 3s - loss: 0.0549 - val_loss: 0.0287

Epoch 20/50

 - 3s - loss: 0.0554 - val_loss: 0.0259

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 988, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 976, after forget

time = 22540	action = 0	current_phase = 0	next_phase = 1	reward = -0.015223	array([[-1.7260537, -2.6927164]], dtype=float32)

time = 22545	action = 0	current_phase = 0	next_phase = 1	reward = -0.219823	array([[-2.373132, -2.916152]], dtype=float32)

time = 22550	action = 1	current_phase = 0	next_phase = 1	reward = -1.038120	array([[-3.7714324, -2.8180413]], dtype=float32)

time = 22558	action = 1	current_phase = 1	next_phase = 0	reward = -0.692801	array([[-3.5814877, -1.9957061]], dtype=float32)

time = 22566	action = 0	current_phase = 0	next_phase = 1	reward = -0.076962	array([[-1.5288959, -2.8050265]], dtype=float32)

time = 22571	action = 0	current_phase = 0	next_phase = 1	reward = 0.006521	array([[-1.8324119, -2.759687 ]], dtype=float32)

time = 22576	action = 0	current_phase = 0	next_phase = 1	reward = 0.070303	array([[-2.4288857, -2.9291897]], dtype=float32)

time = 22581	action = 1	current_phase = 0	next_phase = 1	reward = -1.543631	array([[-4.8117723, -3.0095763]], dtype=float32)

time = 22589	action = 1	current_phase = 1	next_phase = 0	reward = -0.770647	array([[-3.7611299, -2.142366 ]], dtype=float32)

time = 22597	action = 0	current_phase = 0	next_phase = 1	reward = -0.071357	array([[-1.5028445, -2.7435875]], dtype=float32)

time = 22602	action = 0	current_phase = 0	next_phase = 1	reward = 0.015764	array([[-1.9396629, -2.8266592]], dtype=float32)

time = 22607	action = 0	current_phase = 0	next_phase = 1	reward = 0.082615	array([[-2.4253883, -2.9382186]], dtype=float32)

time = 22612	action = 1	current_phase = 0	next_phase = 1	reward = -1.733719	array([[-4.649418 , -3.1820307]], dtype=float32)

time = 22620	action = 1	current_phase = 1	next_phase = 0	reward = -1.060676	array([[-3.7388792, -2.148501 ]], dtype=float32)

time = 22628	action = 0	current_phase = 0	next_phase = 1	reward = -0.051327	array([[-1.5114685, -2.736761 ]], dtype=float32)

time = 22633	action = 0	current_phase = 0	next_phase = 1	reward = 0.026869	array([[-1.9967041, -2.897881 ]], dtype=float32)

time = 22638	action = 0	current_phase = 0	next_phase = 1	reward = 0.081351	array([[-2.5077841, -3.006153 ]], dtype=float32)

time = 22643	action = 1	current_phase = 0	next_phase = 1	reward = -1.943356	array([[-4.6741333, -3.4134502]], dtype=float32)

time = 22651	action = 1	current_phase = 1	next_phase = 0	reward = -1.439001	array([[-3.7905636, -2.3433719]], dtype=float32)

time = 22659	action = 0	current_phase = 0	next_phase = 1	reward = 0.255715	array([[-1.3022801, -2.6809607]], dtype=float32)

time = 22664	action = 0	current_phase = 0	next_phase = 1	reward = 0.034309	array([[-1.7826114, -2.7712255]], dtype=float32)

time = 22669	action = 1	current_phase = 0	next_phase = 1	reward = -0.661649	array([[-2.5176506, -2.399867 ]], dtype=float32)

time = 22677	action = 1	current_phase = 1	next_phase = 0	reward = -1.109307	array([[-2.9850817, -1.9882402]], dtype=float32)

time = 22685	action = 0	current_phase = 0	next_phase = 1	reward = 0.168245	array([[-1.4206495, -2.7811823]], dtype=float32)

time = 22690	action = 0	current_phase = 0	next_phase = 1	reward = 0.242950	array([[-1.6743445, -2.703609 ]], dtype=float32)

time = 22695	action = 0	current_phase = 0	next_phase = 1	reward = 0.022475	array([[-2.2117617, -2.915617 ]], dtype=float32)

time = 22700	action = 1	current_phase = 0	next_phase = 1	reward = -1.321259	array([[-3.983607 , -2.9957724]], dtype=float32)

time = 22708	action = 1	current_phase = 1	next_phase = 0	reward = -0.703316	array([[-3.7165246, -2.0543108]], dtype=float32)

time = 22716	action = 0	current_phase = 0	next_phase = 1	reward = -0.093576	array([[-1.5247371, -2.7439928]], dtype=float32)

time = 22721	action = 0	current_phase = 0	next_phase = 1	reward = -0.007847	array([[-1.7956893, -2.7534027]], dtype=float32)

time = 22726	action = 0	current_phase = 0	next_phase = 1	reward = 0.055491	array([[-2.3858526, -2.9200592]], dtype=float32)

time = 22731	action = 1	current_phase = 0	next_phase = 1	reward = -1.551400	array([[-4.736627, -3.035421]], dtype=float32)

time = 22739	action = 1	current_phase = 1	next_phase = 0	reward = -0.781527	array([[-3.732418 , -2.0886352]], dtype=float32)

time = 22747	action = 0	current_phase = 0	next_phase = 1	reward = -0.060362	array([[-1.5204957, -2.7557054]], dtype=float32)

time = 22752	action = 0	current_phase = 0	next_phase = 1	reward = 0.017286	array([[-1.8891475, -2.766995 ]], dtype=float32)

time = 22757	action = 0	current_phase = 0	next_phase = 1	reward = 0.070626	array([[-2.5293272, -2.9053245]], dtype=float32)

time = 22762	action = 1	current_phase = 0	next_phase = 1	reward = -1.728014	array([[-4.6834264, -3.2424212]], dtype=float32)

time = 22770	action = 1	current_phase = 1	next_phase = 0	reward = -0.881911	array([[-3.7377396, -2.1650271]], dtype=float32)

time = 22778	action = 0	current_phase = 0	next_phase = 1	reward = -0.041968	array([[-1.4904401, -2.7269895]], dtype=float32)

time = 22783	action = 0	current_phase = 0	next_phase = 1	reward = 0.008834	array([[-2.0609226, -2.9444418]], dtype=float32)

time = 22788	action = 0	current_phase = 0	next_phase = 1	reward = 0.067761	array([[-2.6609123, -2.8632255]], dtype=float32)

time = 22793	action = 1	current_phase = 0	next_phase = 1	reward = -1.878555	array([[-4.6803718, -3.5328941]], dtype=float32)

time = 22801	action = 1	current_phase = 1	next_phase = 0	reward = -1.038357	array([[-3.6560135, -2.2336235]], dtype=float32)

time = 22809	action = 0	current_phase = 0	next_phase = 1	reward = -0.037709	array([[-1.277108 , -2.6892543]], dtype=float32)

time = 22814	action = 0	current_phase = 0	next_phase = 1	reward = 0.030654	array([[-1.9217422, -2.8147497]], dtype=float32)

time = 22819	action = 0	current_phase = 0	next_phase = 1	reward = 0.049000	array([[-2.3452375, -2.377366 ]], dtype=float32)

time = 22824	action = 1	current_phase = 0	next_phase = 1	reward = -1.132527	array([[-4.531951 , -2.9507327]], dtype=float32)

time = 22832	action = 1	current_phase = 1	next_phase = 0	reward = -0.708754	array([[-3.5315504, -2.2614224]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0856 - val_loss: 0.0460

Epoch 2/50

 - 3s - loss: 0.0834 - val_loss: 0.0447

Epoch 3/50

 - 3s - loss: 0.0803 - val_loss: 0.0477

Epoch 4/50

 - 3s - loss: 0.0635 - val_loss: 0.0463

Epoch 5/50

 - 3s - loss: 0.0633 - val_loss: 0.0443

Epoch 6/50

 - 3s - loss: 0.0674 - val_loss: 0.0449

Epoch 7/50

 - 3s - loss: 0.0833 - val_loss: 0.0446

Epoch 8/50

 - 3s - loss: 0.0571 - val_loss: 0.0423

Epoch 9/50

 - 3s - loss: 0.0643 - val_loss: 0.0492

Epoch 10/50

 - 3s - loss: 0.0698 - val_loss: 0.0485

Epoch 11/50

 - 3s - loss: 0.0684 - val_loss: 0.0447

Epoch 12/50

 - 3s - loss: 0.0614 - val_loss: 0.0507

Epoch 13/50

 - 3s - loss: 0.0733 - val_loss: 0.0487

Epoch 14/50

 - 3s - loss: 0.0579 - val_loss: 0.0496

Epoch 15/50

 - 3s - loss: 0.0536 - val_loss: 0.0489

Epoch 16/50

 - 3s - loss: 0.0663 - val_loss: 0.0463

Epoch 17/50

 - 3s - loss: 0.0535 - val_loss: 0.0487

Epoch 18/50

 - 3s - loss: 0.0822 - val_loss: 0.0507

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 998, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 986, after forget

time = 22840	action = 0	current_phase = 0	next_phase = 1	reward = -0.027532	array([[-1.7088991, -2.6914074]], dtype=float32)

time = 22845	action = 0	current_phase = 0	next_phase = 1	reward = 0.048028	array([[-2.1636226, -2.8030295]], dtype=float32)

time = 22850	action = 1	current_phase = 0	next_phase = 1	reward = -1.262507	array([[-3.4021616, -2.8130817]], dtype=float32)

time = 22858	action = 1	current_phase = 1	next_phase = 0	reward = -0.709255	array([[-3.5388775, -2.0164237]], dtype=float32)

time = 22866	action = 0	current_phase = 0	next_phase = 1	reward = -0.087035	array([[-1.4732713, -2.7389693]], dtype=float32)

time = 22871	action = 0	current_phase = 0	next_phase = 1	reward = -0.001001	array([[-1.8440117, -2.7540426]], dtype=float32)

time = 22876	action = 0	current_phase = 0	next_phase = 1	reward = 0.058866	array([[-2.36873 , -2.916524]], dtype=float32)

time = 22881	action = 1	current_phase = 0	next_phase = 1	reward = -1.449407	array([[-4.761856 , -3.0333288]], dtype=float32)

time = 22889	action = 1	current_phase = 1	next_phase = 0	reward = -0.833176	array([[-3.716352 , -2.1088111]], dtype=float32)

time = 22897	action = 0	current_phase = 0	next_phase = 1	reward = -0.055946	array([[-1.4802358, -2.7438955]], dtype=float32)

time = 22902	action = 0	current_phase = 0	next_phase = 1	reward = 0.019010	array([[-1.937916 , -2.8129687]], dtype=float32)

time = 22907	action = 0	current_phase = 0	next_phase = 1	reward = 0.076042	array([[-2.394682 , -2.9313169]], dtype=float32)

time = 22912	action = 1	current_phase = 0	next_phase = 1	reward = -1.613065	array([[-4.645013 , -3.3605413]], dtype=float32)

time = 22920	action = 1	current_phase = 1	next_phase = 0	reward = -0.912186	array([[-3.8701012, -2.343713 ]], dtype=float32)

time = 22928	action = 0	current_phase = 0	next_phase = 1	reward = -0.048617	array([[-1.4886181, -2.702967 ]], dtype=float32)

time = 22933	action = 0	current_phase = 0	next_phase = 1	reward = 0.018353	array([[-1.9516814, -2.8710961]], dtype=float32)

time = 22938	action = 0	current_phase = 0	next_phase = 1	reward = 0.080890	array([[-2.6597877, -2.9048266]], dtype=float32)

time = 22943	action = 1	current_phase = 0	next_phase = 1	reward = -1.930041	array([[-4.612591, -3.360341]], dtype=float32)

time = 22951	action = 1	current_phase = 1	next_phase = 0	reward = -1.093723	array([[-3.8066542, -2.325892 ]], dtype=float32)

time = 22959	action = 0	current_phase = 0	next_phase = 1	reward = -0.044039	array([[-1.3636168, -2.667925 ]], dtype=float32)

time = 22964	action = 0	current_phase = 0	next_phase = 1	reward = 0.020000	array([[-1.8500752, -2.767432 ]], dtype=float32)

time = 22969	action = 0	current_phase = 0	next_phase = 1	reward = 0.068020	array([[-2.2969062, -2.384557 ]], dtype=float32)

time = 22974	action = 1	current_phase = 0	next_phase = 1	reward = -0.965651	array([[-4.6325216, -3.0258474]], dtype=float32)

time = 22982	action = 1	current_phase = 1	next_phase = 0	reward = -1.297327	array([[-3.5239155, -2.1258903]], dtype=float32)

time = 22990	action = 0	current_phase = 0	next_phase = 1	reward = 0.534513	array([[-1.6570957, -2.6835828]], dtype=float32)

time = 22995	action = 0	current_phase = 0	next_phase = 1	reward = 0.034139	array([[-2.145031 , -2.8264673]], dtype=float32)

time = 23000	action = 1	current_phase = 0	next_phase = 1	reward = -1.324851	array([[-3.475351, -2.846064]], dtype=float32)

time = 23008	action = 1	current_phase = 1	next_phase = 0	reward = -0.995594	array([[-3.5949907, -2.044344 ]], dtype=float32)

time = 23016	action = 0	current_phase = 0	next_phase = 1	reward = 0.194153	array([[-1.4334697, -2.7223463]], dtype=float32)

time = 23021	action = 0	current_phase = 0	next_phase = 1	reward = -0.027191	array([[-2.0300653, -2.7955227]], dtype=float32)

time = 23026	action = 0	current_phase = 0	next_phase = 1	reward = 0.040888	array([[-2.4004228, -2.9122088]], dtype=float32)

time = 23031	action = 1	current_phase = 0	next_phase = 1	reward = -1.406648	array([[-4.659112 , -3.0742886]], dtype=float32)

time = 23039	action = 1	current_phase = 1	next_phase = 0	reward = -0.738003	array([[-3.7296376, -2.108582 ]], dtype=float32)

time = 23047	action = 0	current_phase = 0	next_phase = 1	reward = -0.071601	array([[-1.5302907, -2.7466166]], dtype=float32)

time = 23052	action = 0	current_phase = 0	next_phase = 1	reward = 0.016307	array([[-1.9552233, -2.7953703]], dtype=float32)

time = 23057	action = 0	current_phase = 0	next_phase = 1	reward = 0.085029	array([[-2.3712723, -2.963147 ]], dtype=float32)

time = 23062	action = 1	current_phase = 0	next_phase = 1	reward = -1.674148	array([[-4.546881 , -3.3253129]], dtype=float32)

time = 23070	action = 1	current_phase = 1	next_phase = 0	reward = -1.311433	array([[-3.7732646, -2.2677736]], dtype=float32)

time = 23078	action = 0	current_phase = 0	next_phase = 1	reward = 0.234383	array([[-1.5880044, -2.733591 ]], dtype=float32)

time = 23083	action = 0	current_phase = 0	next_phase = 1	reward = 0.028054	array([[-1.996336 , -2.8803449]], dtype=float32)

time = 23088	action = 0	current_phase = 0	next_phase = 1	reward = 0.094169	array([[-2.6019692, -2.8565457]], dtype=float32)

time = 23093	action = 1	current_phase = 0	next_phase = 1	reward = -1.915330	array([[-4.623538 , -3.3011956]], dtype=float32)

time = 23101	action = 1	current_phase = 1	next_phase = 0	reward = -0.938883	array([[-3.7599251, -2.1845117]], dtype=float32)

time = 23109	action = 0	current_phase = 0	next_phase = 1	reward = -0.038101	array([[-1.3730851, -2.6043177]], dtype=float32)

time = 23114	action = 0	current_phase = 0	next_phase = 1	reward = 0.024642	array([[-1.7894564, -2.7663133]], dtype=float32)

time = 23119	action = 0	current_phase = 0	next_phase = 1	reward = 0.060013	array([[-2.1939962, -2.3879833]], dtype=float32)

time = 23124	action = 1	current_phase = 0	next_phase = 1	reward = -1.067551	array([[-4.678748, -3.063114]], dtype=float32)

time = 23132	action = 1	current_phase = 1	next_phase = 0	reward = -1.036366	array([[-3.6087482, -2.168734 ]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0848 - val_loss: 0.0347

Epoch 2/50

 - 3s - loss: 0.0916 - val_loss: 0.0380

Epoch 3/50

 - 3s - loss: 0.0702 - val_loss: 0.0369

Epoch 4/50

 - 3s - loss: 0.0731 - val_loss: 0.0397

Epoch 5/50

 - 3s - loss: 0.0704 - val_loss: 0.0335

Epoch 6/50

 - 3s - loss: 0.0992 - val_loss: 0.0352

Epoch 7/50

 - 3s - loss: 0.0711 - val_loss: 0.0320

Epoch 8/50

 - 3s - loss: 0.0794 - val_loss: 0.0384

Epoch 9/50

 - 3s - loss: 0.0696 - val_loss: 0.0313

Epoch 10/50

 - 3s - loss: 0.0715 - val_loss: 0.0415

Epoch 11/50

 - 3s - loss: 0.0710 - val_loss: 0.0365

Epoch 12/50

 - 3s - loss: 0.0653 - val_loss: 0.0355

Epoch 13/50

 - 3s - loss: 0.0687 - val_loss: 0.0379

Epoch 14/50

 - 3s - loss: 0.0615 - val_loss: 0.0337

Epoch 15/50

 - 3s - loss: 0.0639 - val_loss: 0.0374

Epoch 16/50

 - 3s - loss: 0.0633 - val_loss: 0.0334

Epoch 17/50

 - 3s - loss: 0.0654 - val_loss: 0.0341

Epoch 18/50

 - 3s - loss: 0.0604 - val_loss: 0.0449

Epoch 19/50

 - 3s - loss: 0.0622 - val_loss: 0.0386

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1008, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 996, after forget

time = 23140	action = 0	current_phase = 0	next_phase = 1	reward = 0.258736	array([[-1.7433656, -2.7323565]], dtype=float32)

time = 23145	action = 0	current_phase = 0	next_phase = 1	reward = -0.229689	array([[-2.1849177, -2.8707893]], dtype=float32)

time = 23150	action = 1	current_phase = 0	next_phase = 1	reward = -0.980934	array([[-3.4465733, -2.9013083]], dtype=float32)

time = 23158	action = 1	current_phase = 1	next_phase = 0	reward = -0.707601	array([[-3.3901746, -2.030171 ]], dtype=float32)

time = 23166	action = 0	current_phase = 0	next_phase = 1	reward = -0.071684	array([[-1.4593036, -2.7448192]], dtype=float32)

time = 23171	action = 0	current_phase = 0	next_phase = 1	reward = 0.002456	array([[-2.001237, -2.800189]], dtype=float32)

time = 23176	action = 0	current_phase = 0	next_phase = 1	reward = 0.055968	array([[-2.3616905, -2.990015 ]], dtype=float32)

time = 23181	action = 1	current_phase = 0	next_phase = 1	reward = -1.397645	array([[-4.792027 , -3.2939992]], dtype=float32)

time = 23189	action = 1	current_phase = 1	next_phase = 0	reward = -0.817819	array([[-3.59553  , -2.0234907]], dtype=float32)

time = 23197	action = 0	current_phase = 0	next_phase = 1	reward = -0.058649	array([[-1.5237743, -2.814763 ]], dtype=float32)

time = 23202	action = 0	current_phase = 0	next_phase = 1	reward = 0.018084	array([[-1.9967579, -2.816393 ]], dtype=float32)

time = 23207	action = 0	current_phase = 0	next_phase = 1	reward = 0.074506	array([[-2.5232778, -2.8806846]], dtype=float32)

time = 23212	action = 1	current_phase = 0	next_phase = 1	reward = -1.726229	array([[-4.6830473, -3.345005 ]], dtype=float32)

time = 23220	action = 1	current_phase = 1	next_phase = 0	reward = -0.963347	array([[-3.7074554, -2.2069042]], dtype=float32)

time = 23228	action = 0	current_phase = 0	next_phase = 1	reward = -0.045836	array([[-1.5978112, -2.749619 ]], dtype=float32)

time = 23233	action = 0	current_phase = 0	next_phase = 1	reward = 0.052430	array([[-2.0567183, -2.9315748]], dtype=float32)

time = 23238	action = 0	current_phase = 0	next_phase = 1	reward = 0.081576	array([[-2.539681, -2.895263]], dtype=float32)

time = 23243	action = 1	current_phase = 0	next_phase = 1	reward = -1.540766	array([[-4.6431723, -3.4988575]], dtype=float32)

time = 23251	action = 1	current_phase = 1	next_phase = 0	reward = -1.123326	array([[-3.358426 , -2.2838404]], dtype=float32)

time = 23259	action = 0	current_phase = 0	next_phase = 1	reward = -0.039350	array([[-1.4024315, -2.6869226]], dtype=float32)

time = 23264	action = 0	current_phase = 0	next_phase = 1	reward = 0.037313	array([[-1.780575, -2.777083]], dtype=float32)

time = 23269	action = 0	current_phase = 0	next_phase = 1	reward = 0.065957	array([[-2.2729716, -2.619419 ]], dtype=float32)

time = 23274	action = 1	current_phase = 0	next_phase = 1	reward = -0.961675	array([[-4.482355 , -3.0418384]], dtype=float32)

time = 23282	action = 1	current_phase = 1	next_phase = 0	reward = -0.666552	array([[-3.2620869, -2.1913662]], dtype=float32)

time = 23290	action = 0	current_phase = 0	next_phase = 1	reward = -0.033759	array([[-1.7390466, -2.702074 ]], dtype=float32)

time = 23295	action = 0	current_phase = 0	next_phase = 1	reward = -0.231676	array([[-2.2479951, -2.8981233]], dtype=float32)

time = 23300	action = 1	current_phase = 0	next_phase = 1	reward = -1.079793	array([[-3.190134 , -2.7682981]], dtype=float32)

time = 23308	action = 1	current_phase = 1	next_phase = 0	reward = -0.707187	array([[-3.5312514, -1.9759867]], dtype=float32)

time = 23316	action = 0	current_phase = 0	next_phase = 1	reward = -0.089194	array([[-1.4677224, -2.7548656]], dtype=float32)

time = 23321	action = 0	current_phase = 0	next_phase = 1	reward = -0.015843	array([[-1.9427179, -2.7985098]], dtype=float32)

time = 23326	action = 0	current_phase = 0	next_phase = 1	reward = 0.059611	array([[-2.39222  , -2.9496934]], dtype=float32)

time = 23331	action = 1	current_phase = 0	next_phase = 1	reward = -1.378871	array([[-4.7834773, -3.2767224]], dtype=float32)

time = 23339	action = 1	current_phase = 1	next_phase = 0	reward = -0.841096	array([[-3.5191474, -2.0492275]], dtype=float32)

time = 23347	action = 0	current_phase = 0	next_phase = 1	reward = -0.070202	array([[-1.572826, -2.799865]], dtype=float32)

time = 23352	action = 0	current_phase = 0	next_phase = 1	reward = 0.009330	array([[-2.0339398, -2.8053493]], dtype=float32)

time = 23357	action = 0	current_phase = 0	next_phase = 1	reward = 0.063726	array([[-2.4335885, -2.9865136]], dtype=float32)

time = 23362	action = 1	current_phase = 0	next_phase = 1	reward = -1.646510	array([[-4.6289463, -3.261984 ]], dtype=float32)

time = 23370	action = 1	current_phase = 1	next_phase = 0	reward = -0.972526	array([[-3.676067, -2.181881]], dtype=float32)

time = 23378	action = 0	current_phase = 0	next_phase = 1	reward = -0.054718	array([[-1.6212325, -2.7424204]], dtype=float32)

time = 23383	action = 0	current_phase = 0	next_phase = 1	reward = 0.022322	array([[-2.0411057, -2.9341297]], dtype=float32)

time = 23388	action = 0	current_phase = 0	next_phase = 1	reward = 0.090077	array([[-2.4369311, -2.8973033]], dtype=float32)

time = 23393	action = 1	current_phase = 0	next_phase = 1	reward = -0.772818	array([[-4.5687766, -3.4823542]], dtype=float32)

time = 23401	action = 1	current_phase = 1	next_phase = 0	reward = -1.286527	array([[-3.174575 , -2.3303778]], dtype=float32)

time = 23409	action = 0	current_phase = 0	next_phase = 1	reward = -0.031494	array([[-1.4453237, -2.6642685]], dtype=float32)

time = 23414	action = 0	current_phase = 0	next_phase = 1	reward = 0.043018	array([[-1.9020834, -2.8092399]], dtype=float32)

time = 23419	action = 1	current_phase = 0	next_phase = 1	reward = -0.676036	array([[-2.7362678, -2.4876862]], dtype=float32)

time = 23427	action = 1	current_phase = 1	next_phase = 0	reward = -0.925150	array([[-3.1901274, -2.08212  ]], dtype=float32)

time = 23435	action = 0	current_phase = 0	next_phase = 1	reward = -0.116764	array([[-1.4954075, -2.7878544]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0895 - val_loss: 0.0291

Epoch 2/50

 - 3s - loss: 0.0910 - val_loss: 0.0331

Epoch 3/50

 - 3s - loss: 0.0774 - val_loss: 0.0336

Epoch 4/50

 - 3s - loss: 0.0805 - val_loss: 0.0297

Epoch 5/50

 - 3s - loss: 0.0800 - val_loss: 0.0299

Epoch 6/50

 - 3s - loss: 0.0730 - val_loss: 0.0314

Epoch 7/50

 - 3s - loss: 0.0758 - val_loss: 0.0317

Epoch 8/50

 - 3s - loss: 0.0704 - val_loss: 0.0281

Epoch 9/50

 - 3s - loss: 0.0705 - val_loss: 0.0350

Epoch 10/50

 - 3s - loss: 0.0691 - val_loss: 0.0339

Epoch 11/50

 - 3s - loss: 0.0678 - val_loss: 0.0331

Epoch 12/50

 - 3s - loss: 0.0663 - val_loss: 0.0331

Epoch 13/50

 - 3s - loss: 0.0726 - val_loss: 0.0417

Epoch 14/50

 - 3s - loss: 0.0698 - val_loss: 0.0342

Epoch 15/50

 - 3s - loss: 0.0673 - val_loss: 0.0349

Epoch 16/50

 - 3s - loss: 0.0576 - val_loss: 0.0307

Epoch 17/50

 - 3s - loss: 0.0575 - val_loss: 0.0318

Epoch 18/50

 - 3s - loss: 0.0688 - val_loss: 0.0290

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1006, before forget

length of memory (state 1, action 1): 1000, after forget

time = 23440	action = 0	current_phase = 0	next_phase = 1	reward = -0.298099	array([[-1.7544866, -2.6978774]], dtype=float32)

time = 23445	action = 0	current_phase = 0	next_phase = 1	reward = 0.611056	array([[-2.353301 , -2.9423964]], dtype=float32)

time = 23450	action = 1	current_phase = 0	next_phase = 1	reward = -1.203365	array([[-3.8640711, -2.8958738]], dtype=float32)

time = 23458	action = 1	current_phase = 1	next_phase = 0	reward = -0.697103	array([[-3.44212  , -2.0588484]], dtype=float32)

time = 23466	action = 0	current_phase = 0	next_phase = 1	reward = -0.087850	array([[-1.5156765, -2.7827194]], dtype=float32)

time = 23471	action = 0	current_phase = 0	next_phase = 1	reward = -0.012345	array([[-1.9732459, -2.8394086]], dtype=float32)

time = 23476	action = 0	current_phase = 0	next_phase = 1	reward = 0.051168	array([[-2.439758 , -2.9699068]], dtype=float32)

time = 23481	action = 1	current_phase = 0	next_phase = 1	reward = -1.394864	array([[-4.722651 , -3.1647277]], dtype=float32)

time = 23489	action = 1	current_phase = 1	next_phase = 0	reward = -0.819779	array([[-3.5731635, -1.988342 ]], dtype=float32)

time = 23497	action = 0	current_phase = 0	next_phase = 1	reward = -0.075993	array([[-1.517767 , -2.7838323]], dtype=float32)

time = 23502	action = 0	current_phase = 0	next_phase = 1	reward = -0.003102	array([[-2.0809238, -2.914865 ]], dtype=float32)

time = 23507	action = 0	current_phase = 0	next_phase = 1	reward = 0.058023	array([[-2.4131634, -3.0093572]], dtype=float32)

time = 23512	action = 1	current_phase = 0	next_phase = 1	reward = -1.557029	array([[-4.735576, -3.501705]], dtype=float32)

time = 23520	action = 1	current_phase = 1	next_phase = 0	reward = -0.932093	array([[-3.7560802, -2.1381707]], dtype=float32)

time = 23528	action = 0	current_phase = 0	next_phase = 1	reward = -0.056642	array([[-1.5153193, -2.763236 ]], dtype=float32)

time = 23533	action = 0	current_phase = 0	next_phase = 1	reward = -0.003687	array([[-2.1028154, -2.927388 ]], dtype=float32)

time = 23538	action = 0	current_phase = 0	next_phase = 1	reward = 0.070905	array([[-2.5660043, -2.8357773]], dtype=float32)

time = 23543	action = 1	current_phase = 0	next_phase = 1	reward = -1.249607	array([[-4.678276, -3.316213]], dtype=float32)

time = 23551	action = 1	current_phase = 1	next_phase = 0	reward = -1.228807	array([[-3.2754397, -2.3081079]], dtype=float32)

time = 23559	action = 0	current_phase = 0	next_phase = 1	reward = -0.048632	array([[-1.3894274, -2.7145252]], dtype=float32)

time = 23564	action = 0	current_phase = 0	next_phase = 1	reward = 0.013878	array([[-1.8869274, -2.8174477]], dtype=float32)

time = 23569	action = 1	current_phase = 0	next_phase = 1	reward = -0.692563	array([[-2.5769095, -2.4118507]], dtype=float32)

time = 23577	action = 1	current_phase = 1	next_phase = 0	reward = -0.926588	array([[-3.3675485, -2.1656942]], dtype=float32)

time = 23585	action = 0	current_phase = 0	next_phase = 1	reward = 0.174937	array([[-1.406473 , -2.8961036]], dtype=float32)

time = 23590	action = 0	current_phase = 0	next_phase = 1	reward = -0.031350	array([[-1.9286358, -2.7698712]], dtype=float32)

time = 23595	action = 0	current_phase = 0	next_phase = 1	reward = -0.238748	array([[-2.2999628, -2.9106627]], dtype=float32)

time = 23600	action = 1	current_phase = 0	next_phase = 1	reward = -0.925463	array([[-4.0692697, -2.9021964]], dtype=float32)

time = 23608	action = 1	current_phase = 1	next_phase = 0	reward = -0.698475	array([[-3.4434676, -1.9910331]], dtype=float32)

time = 23616	action = 0	current_phase = 0	next_phase = 1	reward = -0.090151	array([[-1.5343328, -2.7950737]], dtype=float32)

time = 23621	action = 0	current_phase = 0	next_phase = 1	reward = -0.015922	array([[-2.1433985, -2.8632605]], dtype=float32)

time = 23626	action = 0	current_phase = 0	next_phase = 1	reward = 0.048775	array([[-2.3695922, -2.9509602]], dtype=float32)

time = 23631	action = 1	current_phase = 0	next_phase = 1	reward = -1.505823	array([[-4.732558 , -3.3316457]], dtype=float32)

time = 23639	action = 1	current_phase = 1	next_phase = 0	reward = -0.779651	array([[-3.6798735, -2.0951297]], dtype=float32)

time = 23647	action = 0	current_phase = 0	next_phase = 1	reward = -0.079989	array([[-1.498909 , -2.7734492]], dtype=float32)

time = 23652	action = 0	current_phase = 0	next_phase = 1	reward = 0.004586	array([[-2.1345863, -2.9266944]], dtype=float32)

time = 23657	action = 0	current_phase = 0	next_phase = 1	reward = 0.065201	array([[-2.580595, -2.879095]], dtype=float32)

time = 23662	action = 1	current_phase = 0	next_phase = 1	reward = -1.599777	array([[-4.8170466, -3.4310012]], dtype=float32)

time = 23670	action = 1	current_phase = 1	next_phase = 0	reward = -1.232257	array([[-3.704349 , -2.1942937]], dtype=float32)

time = 23678	action = 0	current_phase = 0	next_phase = 1	reward = 0.246792	array([[-1.551877, -2.744635]], dtype=float32)

time = 23683	action = 0	current_phase = 0	next_phase = 1	reward = 0.020886	array([[-2.1998842, -2.9799972]], dtype=float32)

time = 23688	action = 0	current_phase = 0	next_phase = 1	reward = 0.076183	array([[-2.478651 , -2.8549955]], dtype=float32)

time = 23693	action = 1	current_phase = 0	next_phase = 1	reward = -1.275531	array([[-4.712188 , -3.4767244]], dtype=float32)

time = 23701	action = 1	current_phase = 1	next_phase = 0	reward = -1.438040	array([[-3.4815989, -2.2757356]], dtype=float32)

time = 23709	action = 0	current_phase = 0	next_phase = 1	reward = 0.248811	array([[-1.3613181, -2.7325444]], dtype=float32)

time = 23714	action = 0	current_phase = 0	next_phase = 1	reward = 0.030858	array([[-1.7391319, -2.8090432]], dtype=float32)

time = 23719	action = 0	current_phase = 0	next_phase = 1	reward = 0.049156	array([[-2.4725616, -2.5433497]], dtype=float32)

time = 23724	action = 1	current_phase = 0	next_phase = 1	reward = -1.126537	array([[-4.352921, -2.970087]], dtype=float32)

time = 23732	action = 1	current_phase = 1	next_phase = 0	reward = -0.976376	array([[-3.3622327, -2.215464 ]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0779 - val_loss: 0.0334

Epoch 2/50

 - 3s - loss: 0.0697 - val_loss: 0.0269

Epoch 3/50

 - 3s - loss: 0.0562 - val_loss: 0.0268

Epoch 4/50

 - 3s - loss: 0.0905 - val_loss: 0.0312

Epoch 5/50

 - 3s - loss: 0.0594 - val_loss: 0.0268

Epoch 6/50

 - 3s - loss: 0.0627 - val_loss: 0.0272

Epoch 7/50

 - 3s - loss: 0.0573 - val_loss: 0.0264

Epoch 8/50

 - 3s - loss: 0.0629 - val_loss: 0.0261

Epoch 9/50

 - 3s - loss: 0.0498 - val_loss: 0.0239

Epoch 10/50

 - 3s - loss: 0.0658 - val_loss: 0.0252

Epoch 11/50

 - 3s - loss: 0.0555 - val_loss: 0.0275

Epoch 12/50

 - 3s - loss: 0.0512 - val_loss: 0.0283

Epoch 13/50

 - 3s - loss: 0.0738 - val_loss: 0.0253

Epoch 14/50

 - 3s - loss: 0.0578 - val_loss: 0.0309

Epoch 15/50

 - 3s - loss: 0.0516 - val_loss: 0.0302

Epoch 16/50

 - 3s - loss: 0.0677 - val_loss: 0.0265

Epoch 17/50

 - 3s - loss: 0.0500 - val_loss: 0.0271

Epoch 18/50

 - 3s - loss: 0.0538 - val_loss: 0.0251

Epoch 19/50

 - 3s - loss: 0.0533 - val_loss: 0.0248

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 23740	action = 0	current_phase = 0	next_phase = 1	reward = -0.021416	array([[-1.6967145, -2.71134  ]], dtype=float32)

time = 23745	action = 0	current_phase = 0	next_phase = 1	reward = 0.331368	array([[-2.1626005, -2.9124775]], dtype=float32)

time = 23750	action = 1	current_phase = 0	next_phase = 1	reward = -1.300197	array([[-3.4899492, -2.8912694]], dtype=float32)

time = 23758	action = 1	current_phase = 1	next_phase = 0	reward = -0.704585	array([[-3.642193 , -1.9983745]], dtype=float32)

time = 23766	action = 0	current_phase = 0	next_phase = 1	reward = -0.081665	array([[-1.4138906, -2.7706728]], dtype=float32)

time = 23771	action = 0	current_phase = 0	next_phase = 1	reward = -0.008540	array([[-2.018942 , -2.8436925]], dtype=float32)

time = 23776	action = 0	current_phase = 0	next_phase = 1	reward = 0.057006	array([[-2.4162319, -2.9505994]], dtype=float32)

time = 23781	action = 1	current_phase = 0	next_phase = 1	reward = -1.391312	array([[-4.731064 , -3.1355524]], dtype=float32)

time = 23789	action = 1	current_phase = 1	next_phase = 0	reward = -0.808122	array([[-3.5504868, -1.9234083]], dtype=float32)

time = 23797	action = 0	current_phase = 0	next_phase = 1	reward = -0.069280	array([[-1.4668281, -2.7904088]], dtype=float32)

time = 23802	action = 0	current_phase = 0	next_phase = 1	reward = -0.002011	array([[-1.8143221, -2.8241477]], dtype=float32)

time = 23807	action = 0	current_phase = 0	next_phase = 1	reward = 0.061233	array([[-2.3721132, -2.960114 ]], dtype=float32)

time = 23812	action = 1	current_phase = 0	next_phase = 1	reward = -1.665474	array([[-4.89074  , -3.2467673]], dtype=float32)

time = 23820	action = 1	current_phase = 1	next_phase = 0	reward = -1.241476	array([[-3.6456099, -2.128365 ]], dtype=float32)

time = 23828	action = 0	current_phase = 0	next_phase = 1	reward = 0.238490	array([[-1.4729006, -2.7342017]], dtype=float32)

time = 23833	action = 0	current_phase = 0	next_phase = 1	reward = 0.030051	array([[-2.0039506, -2.9424834]], dtype=float32)

time = 23838	action = 0	current_phase = 0	next_phase = 1	reward = 0.071946	array([[-2.4733906, -3.0497358]], dtype=float32)

time = 23843	action = 1	current_phase = 0	next_phase = 1	reward = -1.935565	array([[-4.715838 , -3.4744616]], dtype=float32)

time = 23851	action = 1	current_phase = 1	next_phase = 0	reward = -1.337819	array([[-3.7373898, -2.155151 ]], dtype=float32)

time = 23859	action = 0	current_phase = 0	next_phase = 1	reward = 0.255830	array([[-1.3100919, -2.7067325]], dtype=float32)

time = 23864	action = 0	current_phase = 0	next_phase = 1	reward = 0.037716	array([[-1.736295 , -2.8029609]], dtype=float32)

time = 23869	action = 0	current_phase = 0	next_phase = 1	reward = 0.049024	array([[-2.364687, -2.518945]], dtype=float32)

time = 23874	action = 1	current_phase = 0	next_phase = 1	reward = -1.105707	array([[-4.698144, -2.959555]], dtype=float32)

time = 23882	action = 1	current_phase = 1	next_phase = 0	reward = -1.323999	array([[-3.6436374, -2.0839922]], dtype=float32)

time = 23890	action = 0	current_phase = 0	next_phase = 1	reward = 0.538166	array([[-1.6229962, -2.6956468]], dtype=float32)

time = 23895	action = 0	current_phase = 0	next_phase = 1	reward = -0.238533	array([[-2.2149916, -2.8928368]], dtype=float32)

time = 23900	action = 1	current_phase = 0	next_phase = 1	reward = -0.922359	array([[-3.3239756, -2.748546 ]], dtype=float32)

time = 23908	action = 1	current_phase = 1	next_phase = 0	reward = -0.697528	array([[-3.1823738, -1.857582 ]], dtype=float32)

time = 23916	action = 0	current_phase = 0	next_phase = 1	reward = -0.094026	array([[-1.412885 , -2.7624629]], dtype=float32)

time = 23921	action = 0	current_phase = 0	next_phase = 1	reward = -0.021746	array([[-1.9852737, -2.802985 ]], dtype=float32)

time = 23926	action = 0	current_phase = 0	next_phase = 1	reward = 0.033714	array([[-2.2965062, -2.936219 ]], dtype=float32)

time = 23931	action = 1	current_phase = 0	next_phase = 1	reward = -1.395252	array([[-4.635468 , -2.9977186]], dtype=float32)

time = 23939	action = 1	current_phase = 1	next_phase = 0	reward = -0.830345	array([[-3.6122854, -2.0149436]], dtype=float32)

time = 23947	action = 0	current_phase = 0	next_phase = 1	reward = -0.073255	array([[-1.497457, -2.775398]], dtype=float32)

time = 23952	action = 0	current_phase = 0	next_phase = 1	reward = 0.000606	array([[-2.0237033, -2.8290226]], dtype=float32)

time = 23957	action = 0	current_phase = 0	next_phase = 1	reward = 0.057777	array([[-2.455219 , -2.9242694]], dtype=float32)

time = 23962	action = 1	current_phase = 0	next_phase = 1	reward = -1.632304	array([[-4.727286 , -3.2500737]], dtype=float32)

time = 23970	action = 1	current_phase = 1	next_phase = 0	reward = -0.935307	array([[-3.6490233, -2.0972452]], dtype=float32)

time = 23978	action = 0	current_phase = 0	next_phase = 1	reward = -0.060987	array([[-1.4829304, -2.7359803]], dtype=float32)

time = 23983	action = 0	current_phase = 0	next_phase = 1	reward = 0.005297	array([[-2.051909 , -2.9309535]], dtype=float32)

time = 23988	action = 0	current_phase = 0	next_phase = 1	reward = 0.077847	array([[-2.5838985, -2.9050448]], dtype=float32)

time = 23993	action = 1	current_phase = 0	next_phase = 1	reward = -1.272235	array([[-4.741699 , -3.4284372]], dtype=float32)

time = 24001	action = 1	current_phase = 1	next_phase = 0	reward = -1.489275	array([[-3.4574847, -2.1783943]], dtype=float32)

time = 24009	action = 0	current_phase = 0	next_phase = 1	reward = 0.251768	array([[-1.2927753, -2.6693897]], dtype=float32)

time = 24014	action = 0	current_phase = 0	next_phase = 1	reward = 0.038723	array([[-1.6306034, -2.78243  ]], dtype=float32)

time = 24019	action = 0	current_phase = 0	next_phase = 1	reward = 0.061041	array([[-2.1534333, -2.381944 ]], dtype=float32)

time = 24024	action = 1	current_phase = 0	next_phase = 1	reward = -1.025569	array([[-4.4014835, -2.9831862]], dtype=float32)

time = 24032	action = 1	current_phase = 1	next_phase = 0	reward = -1.021533	array([[-3.3951194, -2.1017637]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0969 - val_loss: 0.0275

Epoch 2/50

 - 3s - loss: 0.0696 - val_loss: 0.0232

Epoch 3/50

 - 3s - loss: 0.0662 - val_loss: 0.0269

Epoch 4/50

 - 3s - loss: 0.0641 - val_loss: 0.0272

Epoch 5/50

 - 3s - loss: 0.0648 - val_loss: 0.0229

Epoch 6/50

 - 3s - loss: 0.0696 - val_loss: 0.0317

Epoch 7/50

 - 3s - loss: 0.0767 - val_loss: 0.0263

Epoch 8/50

 - 3s - loss: 0.0591 - val_loss: 0.0270

Epoch 9/50

 - 3s - loss: 0.0600 - val_loss: 0.0326

Epoch 10/50

 - 3s - loss: 0.0504 - val_loss: 0.0333

Epoch 11/50

 - 3s - loss: 0.0558 - val_loss: 0.0278

Epoch 12/50

 - 3s - loss: 0.0604 - val_loss: 0.0342

Epoch 13/50

 - 3s - loss: 0.0525 - val_loss: 0.0335

Epoch 14/50

 - 3s - loss: 0.0608 - val_loss: 0.0300

Epoch 15/50

 - 3s - loss: 0.0475 - val_loss: 0.0325

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 24040	action = 0	current_phase = 0	next_phase = 1	reward = 0.258454	array([[-1.7323489, -2.72161  ]], dtype=float32)

time = 24045	action = 0	current_phase = 0	next_phase = 1	reward = -0.233159	array([[-2.179042 , -2.8871899]], dtype=float32)

time = 24050	action = 1	current_phase = 0	next_phase = 1	reward = -1.098304	array([[-3.5392232, -2.8635442]], dtype=float32)

time = 24058	action = 1	current_phase = 1	next_phase = 0	reward = -0.699106	array([[-3.5463352, -2.053103 ]], dtype=float32)

time = 24066	action = 0	current_phase = 0	next_phase = 1	reward = -0.090572	array([[-1.5670549, -2.7607467]], dtype=float32)

time = 24071	action = 0	current_phase = 0	next_phase = 1	reward = -0.003922	array([[-2.048045 , -2.8463185]], dtype=float32)

time = 24076	action = 0	current_phase = 0	next_phase = 1	reward = 0.059274	array([[-2.349388 , -2.9722905]], dtype=float32)

time = 24081	action = 1	current_phase = 0	next_phase = 1	reward = -1.498335	array([[-4.66733  , -3.1014292]], dtype=float32)

time = 24089	action = 1	current_phase = 1	next_phase = 0	reward = -0.794532	array([[-3.5504608, -2.048985 ]], dtype=float32)

time = 24097	action = 0	current_phase = 0	next_phase = 1	reward = -0.070032	array([[-1.5691416, -2.8121889]], dtype=float32)

time = 24102	action = 0	current_phase = 0	next_phase = 1	reward = 0.003920	array([[-2.0039856, -2.8279645]], dtype=float32)

time = 24107	action = 0	current_phase = 0	next_phase = 1	reward = 0.077291	array([[-2.4262648, -2.9718182]], dtype=float32)

time = 24112	action = 1	current_phase = 0	next_phase = 1	reward = -1.608973	array([[-4.916929, -3.39671 ]], dtype=float32)

time = 24120	action = 1	current_phase = 1	next_phase = 0	reward = -0.947439	array([[-3.57975 , -2.070084]], dtype=float32)

time = 24128	action = 0	current_phase = 0	next_phase = 1	reward = -0.047897	array([[-1.5438092, -2.7530158]], dtype=float32)

time = 24133	action = 0	current_phase = 0	next_phase = 1	reward = 0.032420	array([[-2.0552683, -2.9469361]], dtype=float32)

time = 24138	action = 0	current_phase = 0	next_phase = 1	reward = 0.076615	array([[-2.489963 , -2.9140332]], dtype=float32)

time = 24143	action = 1	current_phase = 0	next_phase = 1	reward = -1.284630	array([[-4.7644525, -3.3203137]], dtype=float32)

time = 24151	action = 1	current_phase = 1	next_phase = 0	reward = -1.492788	array([[-3.3315883, -2.248091 ]], dtype=float32)

time = 24159	action = 0	current_phase = 0	next_phase = 1	reward = 0.266542	array([[-1.4113156, -2.6670911]], dtype=float32)

time = 24164	action = 0	current_phase = 0	next_phase = 1	reward = 0.050250	array([[-1.9257648, -2.781958 ]], dtype=float32)

time = 24169	action = 1	current_phase = 0	next_phase = 1	reward = -0.743378	array([[-2.4948022, -2.484223 ]], dtype=float32)

time = 24177	action = 1	current_phase = 1	next_phase = 0	reward = -0.924137	array([[-3.4028168, -2.123685 ]], dtype=float32)

time = 24185	action = 0	current_phase = 0	next_phase = 1	reward = 0.179363	array([[-1.651447 , -2.8538463]], dtype=float32)

time = 24190	action = 0	current_phase = 0	next_phase = 1	reward = -0.015703	array([[-1.771388, -2.740599]], dtype=float32)

time = 24195	action = 0	current_phase = 0	next_phase = 1	reward = -0.222832	array([[-2.312006 , -2.9320242]], dtype=float32)

time = 24200	action = 1	current_phase = 0	next_phase = 1	reward = -1.115319	array([[-4.0160084, -2.6715248]], dtype=float32)

time = 24208	action = 1	current_phase = 1	next_phase = 0	reward = -0.705106	array([[-3.6576428, -2.0707884]], dtype=float32)

time = 24216	action = 0	current_phase = 0	next_phase = 1	reward = -0.075618	array([[-1.4940523, -2.771083 ]], dtype=float32)

time = 24221	action = 0	current_phase = 0	next_phase = 1	reward = 0.004387	array([[-2.0122502, -2.8542764]], dtype=float32)

time = 24226	action = 0	current_phase = 0	next_phase = 1	reward = 0.065879	array([[-2.339184 , -2.9562023]], dtype=float32)

time = 24231	action = 1	current_phase = 0	next_phase = 1	reward = -1.477625	array([[-4.816403, -3.05585 ]], dtype=float32)

time = 24239	action = 1	current_phase = 1	next_phase = 0	reward = -0.787374	array([[-3.6058693, -2.0661874]], dtype=float32)

time = 24247	action = 0	current_phase = 0	next_phase = 1	reward = -0.072816	array([[-1.5414002, -2.731738 ]], dtype=float32)

time = 24252	action = 0	current_phase = 0	next_phase = 1	reward = 0.011975	array([[-1.975786 , -2.8549838]], dtype=float32)

time = 24257	action = 0	current_phase = 0	next_phase = 1	reward = 0.059192	array([[-2.3930502, -2.9959302]], dtype=float32)

time = 24262	action = 1	current_phase = 0	next_phase = 1	reward = -1.646043	array([[-4.8953137, -3.298729 ]], dtype=float32)

time = 24270	action = 1	current_phase = 1	next_phase = 0	reward = -1.257218	array([[-3.6924133, -2.1764076]], dtype=float32)

time = 24278	action = 0	current_phase = 0	next_phase = 1	reward = 0.223567	array([[-1.5722454, -2.7519717]], dtype=float32)

time = 24283	action = 0	current_phase = 0	next_phase = 1	reward = 0.008490	array([[-1.9992633, -2.918389 ]], dtype=float32)

time = 24288	action = 0	current_phase = 0	next_phase = 1	reward = 0.083078	array([[-2.5311272, -2.943972 ]], dtype=float32)

time = 24293	action = 1	current_phase = 0	next_phase = 1	reward = -1.886717	array([[-4.7925515, -3.3491337]], dtype=float32)

time = 24301	action = 1	current_phase = 1	next_phase = 0	reward = -1.259653	array([[-3.5668602, -2.2678285]], dtype=float32)

time = 24309	action = 0	current_phase = 0	next_phase = 1	reward = 0.241430	array([[-1.4148704, -2.6753354]], dtype=float32)

time = 24314	action = 0	current_phase = 0	next_phase = 1	reward = 0.017287	array([[-1.9058163, -2.8414729]], dtype=float32)

time = 24319	action = 0	current_phase = 0	next_phase = 1	reward = 0.074967	array([[-2.584586 , -2.5923755]], dtype=float32)

time = 24324	action = 1	current_phase = 0	next_phase = 1	reward = -0.950640	array([[-4.8700438, -3.1145258]], dtype=float32)

time = 24332	action = 1	current_phase = 1	next_phase = 0	reward = -0.762155	array([[-3.4675593, -2.1947913]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0734 - val_loss: 0.0369

Epoch 2/50

 - 3s - loss: 0.0613 - val_loss: 0.0391

Epoch 3/50

 - 3s - loss: 0.0814 - val_loss: 0.0451

Epoch 4/50

 - 3s - loss: 0.0592 - val_loss: 0.0383

Epoch 5/50

 - 3s - loss: 0.0610 - val_loss: 0.0377

Epoch 6/50

 - 3s - loss: 0.0628 - val_loss: 0.0393

Epoch 7/50

 - 3s - loss: 0.0647 - val_loss: 0.0512

Epoch 8/50

 - 3s - loss: 0.0519 - val_loss: 0.0402

Epoch 9/50

 - 3s - loss: 0.0685 - val_loss: 0.0438

Epoch 10/50

 - 3s - loss: 0.0651 - val_loss: 0.0470

Epoch 11/50

 - 3s - loss: 0.0516 - val_loss: 0.0398

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 24340	action = 0	current_phase = 0	next_phase = 1	reward = -0.024935	array([[-1.6895471, -2.6954007]], dtype=float32)

time = 24345	action = 0	current_phase = 0	next_phase = 1	reward = -0.233265	array([[-2.1369982, -2.8555176]], dtype=float32)

time = 24350	action = 1	current_phase = 0	next_phase = 1	reward = -1.027726	array([[-3.5582714, -2.6849446]], dtype=float32)

time = 24358	action = 1	current_phase = 1	next_phase = 0	reward = -0.705700	array([[-3.433415 , -1.8780165]], dtype=float32)

time = 24366	action = 0	current_phase = 0	next_phase = 1	reward = -0.084476	array([[-1.5841957, -2.810679 ]], dtype=float32)

time = 24371	action = 0	current_phase = 0	next_phase = 1	reward = 0.000752	array([[-1.9786298, -2.8206816]], dtype=float32)

time = 24376	action = 0	current_phase = 0	next_phase = 1	reward = 0.056193	array([[-2.345308 , -2.9521425]], dtype=float32)

time = 24381	action = 1	current_phase = 0	next_phase = 1	reward = -1.454222	array([[-4.627333 , -3.0351753]], dtype=float32)

time = 24389	action = 1	current_phase = 1	next_phase = 0	reward = -0.771938	array([[-3.593279, -2.010288]], dtype=float32)

time = 24397	action = 0	current_phase = 0	next_phase = 1	reward = -0.065994	array([[-1.6051695, -2.7845635]], dtype=float32)

time = 24402	action = 0	current_phase = 0	next_phase = 1	reward = 0.021126	array([[-1.9192249, -2.8723426]], dtype=float32)

time = 24407	action = 0	current_phase = 0	next_phase = 1	reward = 0.091407	array([[-2.425025 , -2.9302382]], dtype=float32)

time = 24412	action = 1	current_phase = 0	next_phase = 1	reward = -1.520615	array([[-4.846406 , -3.4077466]], dtype=float32)

time = 24420	action = 1	current_phase = 1	next_phase = 0	reward = -1.308312	array([[-3.7353358, -2.2088811]], dtype=float32)

time = 24428	action = 0	current_phase = 0	next_phase = 1	reward = 0.250488	array([[-1.603952 , -2.7330725]], dtype=float32)

time = 24433	action = 0	current_phase = 0	next_phase = 1	reward = 0.028146	array([[-2.0024636, -2.9168599]], dtype=float32)

time = 24438	action = 0	current_phase = 0	next_phase = 1	reward = 0.082774	array([[-2.6329007, -2.9241874]], dtype=float32)

time = 24443	action = 1	current_phase = 0	next_phase = 1	reward = -1.929987	array([[-4.8351407, -3.4848976]], dtype=float32)

time = 24451	action = 1	current_phase = 1	next_phase = 0	reward = -1.406509	array([[-3.702499, -2.468832]], dtype=float32)

time = 24459	action = 0	current_phase = 0	next_phase = 1	reward = 0.257141	array([[-1.4208019, -2.6555517]], dtype=float32)

time = 24464	action = 0	current_phase = 0	next_phase = 1	reward = 0.044717	array([[-1.847976 , -2.8320122]], dtype=float32)

time = 24469	action = 1	current_phase = 0	next_phase = 1	reward = -0.719805	array([[-2.3471365, -2.3107808]], dtype=float32)

time = 24477	action = 1	current_phase = 1	next_phase = 0	reward = -0.913860	array([[-3.13523  , -1.8106925]], dtype=float32)

time = 24485	action = 0	current_phase = 0	next_phase = 1	reward = 0.171378	array([[-1.4128755, -2.8330784]], dtype=float32)

time = 24490	action = 0	current_phase = 0	next_phase = 1	reward = -0.294325	array([[-1.825239 , -2.7720537]], dtype=float32)

time = 24495	action = 0	current_phase = 0	next_phase = 1	reward = 0.345330	array([[-2.223463 , -2.9219332]], dtype=float32)

time = 24500	action = 1	current_phase = 0	next_phase = 1	reward = -1.306591	array([[-4.2235208, -2.8090048]], dtype=float32)

time = 24508	action = 1	current_phase = 1	next_phase = 0	reward = -0.707351	array([[-3.5691266, -1.9994011]], dtype=float32)

time = 24516	action = 0	current_phase = 0	next_phase = 1	reward = -0.088442	array([[-1.5678258, -2.7881675]], dtype=float32)

time = 24521	action = 0	current_phase = 0	next_phase = 1	reward = -0.010233	array([[-1.9979392, -2.8336158]], dtype=float32)

time = 24526	action = 0	current_phase = 0	next_phase = 1	reward = 0.056979	array([[-2.3284001, -2.9595308]], dtype=float32)

time = 24531	action = 1	current_phase = 0	next_phase = 1	reward = -1.552493	array([[-4.893263 , -3.1756954]], dtype=float32)

time = 24539	action = 1	current_phase = 1	next_phase = 0	reward = -0.784766	array([[-3.578279 , -2.0435765]], dtype=float32)

time = 24547	action = 0	current_phase = 0	next_phase = 1	reward = -0.080823	array([[-1.5747496, -2.7556126]], dtype=float32)

time = 24552	action = 0	current_phase = 0	next_phase = 1	reward = 0.004451	array([[-1.9348667, -2.8727863]], dtype=float32)

time = 24557	action = 0	current_phase = 0	next_phase = 1	reward = 0.071188	array([[-2.428505 , -2.9668612]], dtype=float32)

time = 24562	action = 1	current_phase = 0	next_phase = 1	reward = -1.466286	array([[-4.926035 , -3.2956777]], dtype=float32)

time = 24570	action = 1	current_phase = 1	next_phase = 0	reward = -1.038196	array([[-3.7004266, -2.1612096]], dtype=float32)

time = 24578	action = 0	current_phase = 0	next_phase = 1	reward = -0.040716	array([[-1.6622441, -2.784609 ]], dtype=float32)

time = 24583	action = 0	current_phase = 0	next_phase = 1	reward = 0.040484	array([[-2.0150287, -2.9208508]], dtype=float32)

time = 24588	action = 0	current_phase = 0	next_phase = 1	reward = 0.059427	array([[-2.7125247, -2.8097932]], dtype=float32)

time = 24593	action = 1	current_phase = 0	next_phase = 1	reward = -1.428226	array([[-4.878298 , -3.4056468]], dtype=float32)

time = 24601	action = 1	current_phase = 1	next_phase = 0	reward = -1.394307	array([[-3.3607297, -2.3202686]], dtype=float32)

time = 24609	action = 0	current_phase = 0	next_phase = 1	reward = 0.250802	array([[-1.6036363, -2.6900692]], dtype=float32)

time = 24614	action = 0	current_phase = 0	next_phase = 1	reward = 0.034841	array([[-1.9380374, -2.8014367]], dtype=float32)

time = 24619	action = 1	current_phase = 0	next_phase = 1	reward = -1.230179	array([[-2.8273532, -2.281024 ]], dtype=float32)

time = 24627	action = 1	current_phase = 1	next_phase = 0	reward = -0.820318	array([[-3.5964675, -2.1412573]], dtype=float32)

time = 24635	action = 0	current_phase = 0	next_phase = 1	reward = 0.183464	array([[-1.7708782, -2.9605603]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0734 - val_loss: 0.0273

Epoch 2/50

 - 3s - loss: 0.0605 - val_loss: 0.0294

Epoch 3/50

 - 3s - loss: 0.0664 - val_loss: 0.0264

Epoch 4/50

 - 3s - loss: 0.0700 - val_loss: 0.0319

Epoch 5/50

 - 3s - loss: 0.0634 - val_loss: 0.0263

Epoch 6/50

 - 3s - loss: 0.0680 - val_loss: 0.0397

Epoch 7/50

 - 3s - loss: 0.0703 - val_loss: 0.0391

Epoch 8/50

 - 3s - loss: 0.0527 - val_loss: 0.0291

Epoch 9/50

 - 3s - loss: 0.0607 - val_loss: 0.0311

Epoch 10/50

 - 3s - loss: 0.0499 - val_loss: 0.0350

Epoch 11/50

 - 3s - loss: 0.0599 - val_loss: 0.0316

Epoch 12/50

 - 3s - loss: 0.0638 - val_loss: 0.0346

Epoch 13/50

 - 3s - loss: 0.0596 - val_loss: 0.0309

Epoch 14/50

 - 3s - loss: 0.0537 - val_loss: 0.0282

Epoch 15/50

 - 3s - loss: 0.0578 - val_loss: 0.0318

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 24640	action = 0	current_phase = 0	next_phase = 1	reward = -0.015198	array([[-1.8812971, -2.813101 ]], dtype=float32)

time = 24645	action = 0	current_phase = 0	next_phase = 1	reward = 0.052724	array([[-2.2455065, -2.9931912]], dtype=float32)

time = 24650	action = 1	current_phase = 0	next_phase = 1	reward = -1.414104	array([[-4.40288 , -2.881243]], dtype=float32)

time = 24658	action = 1	current_phase = 1	next_phase = 0	reward = -0.705437	array([[-3.6951582, -2.0525546]], dtype=float32)

time = 24666	action = 0	current_phase = 0	next_phase = 1	reward = -0.084556	array([[-1.6197832, -2.841347 ]], dtype=float32)

time = 24671	action = 0	current_phase = 0	next_phase = 1	reward = -0.007930	array([[-1.9481962, -2.8808732]], dtype=float32)

time = 24676	action = 0	current_phase = 0	next_phase = 1	reward = 0.056064	array([[-2.3551733, -2.9917018]], dtype=float32)

time = 24681	action = 1	current_phase = 0	next_phase = 1	reward = -1.393865	array([[-4.9332876, -3.2704964]], dtype=float32)

time = 24689	action = 1	current_phase = 1	next_phase = 0	reward = -0.774531	array([[-3.6586158, -2.0934782]], dtype=float32)

time = 24697	action = 0	current_phase = 0	next_phase = 1	reward = -0.079064	array([[-1.6011133, -2.823922 ]], dtype=float32)

time = 24702	action = 0	current_phase = 0	next_phase = 1	reward = 0.001936	array([[-2.097928 , -2.9626856]], dtype=float32)

time = 24707	action = 0	current_phase = 0	next_phase = 1	reward = 0.075009	array([[-2.7013924, -2.8711915]], dtype=float32)

time = 24712	action = 1	current_phase = 0	next_phase = 1	reward = -1.537826	array([[-4.923178 , -3.4735434]], dtype=float32)

time = 24720	action = 1	current_phase = 1	next_phase = 0	reward = -1.247763	array([[-3.711861 , -2.2118585]], dtype=float32)

time = 24728	action = 0	current_phase = 0	next_phase = 1	reward = 0.242028	array([[-1.4828687, -2.7404172]], dtype=float32)

time = 24733	action = 0	current_phase = 0	next_phase = 1	reward = 0.018487	array([[-2.009826 , -2.9679596]], dtype=float32)

time = 24738	action = 0	current_phase = 0	next_phase = 1	reward = 0.072926	array([[-2.5660717, -2.8955772]], dtype=float32)

time = 24743	action = 1	current_phase = 0	next_phase = 1	reward = -0.772525	array([[-4.747955 , -3.2063594]], dtype=float32)

time = 24751	action = 1	current_phase = 1	next_phase = 0	reward = -1.294448	array([[-3.1637025, -2.480851 ]], dtype=float32)

time = 24759	action = 0	current_phase = 0	next_phase = 1	reward = -0.034730	array([[-1.4657407, -2.7344036]], dtype=float32)

time = 24764	action = 0	current_phase = 0	next_phase = 1	reward = 0.035646	array([[-1.8573667, -2.8358848]], dtype=float32)

time = 24769	action = 1	current_phase = 0	next_phase = 1	reward = -0.704228	array([[-2.7813652, -2.6075447]], dtype=float32)

time = 24777	action = 1	current_phase = 1	next_phase = 0	reward = -0.912931	array([[-3.4735868, -2.112132 ]], dtype=float32)

time = 24785	action = 0	current_phase = 0	next_phase = 1	reward = -0.081081	array([[-1.5252547, -3.034585 ]], dtype=float32)

time = 24790	action = 0	current_phase = 0	next_phase = 1	reward = 0.259633	array([[-1.801276, -2.73342 ]], dtype=float32)

time = 24795	action = 0	current_phase = 0	next_phase = 1	reward = -0.233209	array([[-2.298354, -2.996205]], dtype=float32)

time = 24800	action = 1	current_phase = 0	next_phase = 1	reward = -1.083901	array([[-4.3042855, -2.7511098]], dtype=float32)

time = 24808	action = 1	current_phase = 1	next_phase = 0	reward = -0.706271	array([[-3.6106234, -2.037924 ]], dtype=float32)

time = 24816	action = 0	current_phase = 0	next_phase = 1	reward = -0.089976	array([[-1.6553271, -2.915568 ]], dtype=float32)

time = 24821	action = 0	current_phase = 0	next_phase = 1	reward = -0.000866	array([[-1.912121 , -2.8543754]], dtype=float32)

time = 24826	action = 0	current_phase = 0	next_phase = 1	reward = 0.060367	array([[-2.3229737, -2.9976857]], dtype=float32)

time = 24831	action = 1	current_phase = 0	next_phase = 1	reward = -1.501611	array([[-5.0327315, -3.3009312]], dtype=float32)

time = 24839	action = 1	current_phase = 1	next_phase = 0	reward = -0.779167	array([[-3.5792234, -2.0220246]], dtype=float32)

time = 24847	action = 0	current_phase = 0	next_phase = 1	reward = -0.071045	array([[-1.6174873, -2.8158383]], dtype=float32)

time = 24852	action = 0	current_phase = 0	next_phase = 1	reward = 0.008506	array([[-2.0655746, -2.9579198]], dtype=float32)

time = 24857	action = 0	current_phase = 0	next_phase = 1	reward = 0.080139	array([[-2.7551904, -3.095638 ]], dtype=float32)

time = 24862	action = 1	current_phase = 0	next_phase = 1	reward = -1.674207	array([[-4.94792  , -3.3882449]], dtype=float32)

time = 24870	action = 1	current_phase = 1	next_phase = 0	reward = -0.953727	array([[-3.7907972, -2.211755 ]], dtype=float32)

time = 24878	action = 0	current_phase = 0	next_phase = 1	reward = -0.051790	array([[-1.4930394, -2.7676864]], dtype=float32)

time = 24883	action = 0	current_phase = 0	next_phase = 1	reward = 0.018338	array([[-2.0495374, -2.9565876]], dtype=float32)

time = 24888	action = 0	current_phase = 0	next_phase = 1	reward = 0.074581	array([[-2.645056, -2.91836 ]], dtype=float32)

time = 24893	action = 1	current_phase = 0	next_phase = 1	reward = -1.258654	array([[-4.8509383, -3.5792353]], dtype=float32)

time = 24901	action = 1	current_phase = 1	next_phase = 0	reward = -1.436051	array([[-3.3171341, -2.4422965]], dtype=float32)

time = 24909	action = 0	current_phase = 0	next_phase = 1	reward = 0.270621	array([[-1.3983049, -2.6693301]], dtype=float32)

time = 24914	action = 0	current_phase = 0	next_phase = 1	reward = 0.044126	array([[-2.0045862, -2.8700907]], dtype=float32)

time = 24919	action = 1	current_phase = 0	next_phase = 1	reward = -0.618757	array([[-2.4615104, -2.2449663]], dtype=float32)

time = 24927	action = 1	current_phase = 1	next_phase = 0	reward = -0.868077	array([[-3.1905804, -2.145484 ]], dtype=float32)

time = 24935	action = 0	current_phase = 0	next_phase = 1	reward = 0.189766	array([[-1.6030486, -2.9965107]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0591 - val_loss: 0.0351

Epoch 2/50

 - 3s - loss: 0.0610 - val_loss: 0.0368

Epoch 3/50

 - 3s - loss: 0.0667 - val_loss: 0.0390

Epoch 4/50

 - 3s - loss: 0.0515 - val_loss: 0.0406

Epoch 5/50

 - 3s - loss: 0.0528 - val_loss: 0.0418

Epoch 6/50

 - 3s - loss: 0.0565 - val_loss: 0.0371

Epoch 7/50

 - 3s - loss: 0.0530 - val_loss: 0.0380

Epoch 8/50

 - 3s - loss: 0.0489 - val_loss: 0.0441

Epoch 9/50

 - 3s - loss: 0.0494 - val_loss: 0.0472

Epoch 10/50

 - 3s - loss: 0.0483 - val_loss: 0.0494

Epoch 11/50

 - 3s - loss: 0.0524 - val_loss: 0.0493

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 24940	action = 0	current_phase = 0	next_phase = 1	reward = -0.020908	array([[-1.8006301, -2.8083625]], dtype=float32)

time = 24945	action = 0	current_phase = 0	next_phase = 1	reward = 0.044993	array([[-2.3375852, -2.987589 ]], dtype=float32)

time = 24950	action = 1	current_phase = 0	next_phase = 1	reward = -1.262787	array([[-4.014542 , -2.7867599]], dtype=float32)

time = 24958	action = 1	current_phase = 1	next_phase = 0	reward = -0.720575	array([[-3.6061914, -2.1156392]], dtype=float32)

time = 24966	action = 0	current_phase = 0	next_phase = 1	reward = -0.079322	array([[-1.5906754, -2.8596478]], dtype=float32)

time = 24971	action = 0	current_phase = 0	next_phase = 1	reward = -0.026202	array([[-1.8738546, -2.855694 ]], dtype=float32)

time = 24976	action = 0	current_phase = 0	next_phase = 1	reward = 0.049251	array([[-2.3132281, -3.0082016]], dtype=float32)

time = 24981	action = 1	current_phase = 0	next_phase = 1	reward = -1.442936	array([[-4.8245397, -3.195684 ]], dtype=float32)

time = 24989	action = 1	current_phase = 1	next_phase = 0	reward = -0.759246	array([[-3.7204037, -2.1310763]], dtype=float32)

time = 24997	action = 0	current_phase = 0	next_phase = 1	reward = -0.081542	array([[-1.5474012, -2.7983425]], dtype=float32)

time = 25002	action = 0	current_phase = 0	next_phase = 1	reward = 0.005667	array([[-1.9877241, -2.9319348]], dtype=float32)

time = 25007	action = 0	current_phase = 0	next_phase = 1	reward = 0.072296	array([[-2.372823 , -3.0135465]], dtype=float32)

time = 25012	action = 1	current_phase = 0	next_phase = 1	reward = -1.534254	array([[-4.846491 , -3.4818401]], dtype=float32)

time = 25020	action = 1	current_phase = 1	next_phase = 0	reward = -0.951834	array([[-3.8893473, -2.3461905]], dtype=float32)

time = 25028	action = 0	current_phase = 0	next_phase = 1	reward = -0.035278	array([[-1.5544503, -2.7928176]], dtype=float32)

time = 25033	action = 0	current_phase = 0	next_phase = 1	reward = 0.035310	array([[-2.0225997, -3.0055149]], dtype=float32)

time = 25038	action = 0	current_phase = 0	next_phase = 1	reward = 0.072432	array([[-2.689995 , -2.8633788]], dtype=float32)

time = 25043	action = 1	current_phase = 0	next_phase = 1	reward = -1.890892	array([[-4.7981124, -3.4742467]], dtype=float32)

time = 25051	action = 1	current_phase = 1	next_phase = 0	reward = -1.650736	array([[-3.7837844, -2.4366407]], dtype=float32)

time = 25059	action = 0	current_phase = 0	next_phase = 1	reward = 0.560087	array([[-1.3598092, -2.716648 ]], dtype=float32)

time = 25064	action = 0	current_phase = 0	next_phase = 1	reward = 0.038455	array([[-1.8516219, -2.914021 ]], dtype=float32)

time = 25069	action = 1	current_phase = 0	next_phase = 1	reward = -0.718113	array([[-2.390781 , -2.2666004]], dtype=float32)

time = 25077	action = 1	current_phase = 1	next_phase = 0	reward = -0.930489	array([[-3.1327696, -2.0743783]], dtype=float32)

time = 25085	action = 0	current_phase = 0	next_phase = 1	reward = 0.182809	array([[-1.3252594, -3.003348 ]], dtype=float32)

time = 25090	action = 0	current_phase = 0	next_phase = 1	reward = -0.026626	array([[-1.8264451, -2.788134 ]], dtype=float32)

time = 25095	action = 0	current_phase = 0	next_phase = 1	reward = 0.042900	array([[-2.2464175, -3.026442 ]], dtype=float32)

time = 25100	action = 1	current_phase = 0	next_phase = 1	reward = -1.254455	array([[-3.5955167, -2.7025476]], dtype=float32)

time = 25108	action = 1	current_phase = 1	next_phase = 0	reward = -0.709046	array([[-3.609555, -2.076519]], dtype=float32)

time = 25116	action = 0	current_phase = 0	next_phase = 1	reward = -0.086458	array([[-1.6373491, -2.8660796]], dtype=float32)

time = 25121	action = 0	current_phase = 0	next_phase = 1	reward = -0.004226	array([[-1.8601763, -2.8603647]], dtype=float32)

time = 25126	action = 0	current_phase = 0	next_phase = 1	reward = 0.064828	array([[-2.338711 , -3.0085258]], dtype=float32)

time = 25131	action = 1	current_phase = 0	next_phase = 1	reward = -1.384504	array([[-5.0653257, -3.1533995]], dtype=float32)

time = 25139	action = 1	current_phase = 1	next_phase = 0	reward = -0.831074	array([[-3.6924598, -2.1574817]], dtype=float32)

time = 25147	action = 0	current_phase = 0	next_phase = 1	reward = -0.074482	array([[-1.5397799, -2.8097732]], dtype=float32)

time = 25152	action = 0	current_phase = 0	next_phase = 1	reward = -0.002284	array([[-2.0205548, -3.0160124]], dtype=float32)

time = 25157	action = 0	current_phase = 0	next_phase = 1	reward = 0.065679	array([[-2.462297 , -2.9804084]], dtype=float32)

time = 25162	action = 1	current_phase = 0	next_phase = 1	reward = -1.651465	array([[-4.9761295, -3.4192052]], dtype=float32)

time = 25170	action = 1	current_phase = 1	next_phase = 0	reward = -1.066237	array([[-3.821216 , -2.2955196]], dtype=float32)

time = 25178	action = 0	current_phase = 0	next_phase = 1	reward = -0.060053	array([[-1.5939642, -2.7745574]], dtype=float32)

time = 25183	action = 0	current_phase = 0	next_phase = 1	reward = 0.035932	array([[-2.001825 , -3.0308838]], dtype=float32)

time = 25188	action = 0	current_phase = 0	next_phase = 1	reward = 0.083175	array([[-2.4429224, -3.0299916]], dtype=float32)

time = 25193	action = 1	current_phase = 0	next_phase = 1	reward = -1.861350	array([[-4.7229185, -3.372135 ]], dtype=float32)

time = 25201	action = 1	current_phase = 1	next_phase = 0	reward = -1.291065	array([[-3.755165 , -2.4411225]], dtype=float32)

time = 25209	action = 0	current_phase = 0	next_phase = 1	reward = 0.263886	array([[-1.3798392, -2.7652724]], dtype=float32)

time = 25214	action = 0	current_phase = 0	next_phase = 1	reward = 0.061338	array([[-1.9134622, -2.9218838]], dtype=float32)

time = 25219	action = 0	current_phase = 0	next_phase = 1	reward = 0.060301	array([[-2.3514054, -2.4512644]], dtype=float32)

time = 25224	action = 1	current_phase = 0	next_phase = 1	reward = -1.134363	array([[-4.8660107, -3.013335 ]], dtype=float32)

time = 25232	action = 1	current_phase = 1	next_phase = 0	reward = -0.963684	array([[-3.5303931, -2.3502111]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0741 - val_loss: 0.0308

Epoch 2/50

 - 3s - loss: 0.0733 - val_loss: 0.0304

Epoch 3/50

 - 3s - loss: 0.0663 - val_loss: 0.0285

Epoch 4/50

 - 3s - loss: 0.0589 - val_loss: 0.0285

Epoch 5/50

 - 3s - loss: 0.0562 - val_loss: 0.0306

Epoch 6/50

 - 3s - loss: 0.0589 - val_loss: 0.0375

Epoch 7/50

 - 3s - loss: 0.0627 - val_loss: 0.0298

Epoch 8/50

 - 3s - loss: 0.0617 - val_loss: 0.0372

Epoch 9/50

 - 3s - loss: 0.0529 - val_loss: 0.0348

Epoch 10/50

 - 3s - loss: 0.0630 - val_loss: 0.0538

Epoch 11/50

 - 3s - loss: 0.0565 - val_loss: 0.0353

Epoch 12/50

 - 3s - loss: 0.0608 - val_loss: 0.0409

Epoch 13/50

 - 3s - loss: 0.0596 - val_loss: 0.0532

Epoch 14/50

 - 3s - loss: 0.0521 - val_loss: 0.0350

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 25240	action = 0	current_phase = 0	next_phase = 1	reward = 0.264980	array([[-1.6379757, -2.6990664]], dtype=float32)

time = 25245	action = 0	current_phase = 0	next_phase = 1	reward = 0.049600	array([[-2.295278, -2.981341]], dtype=float32)

time = 25250	action = 1	current_phase = 0	next_phase = 1	reward = -1.350607	array([[-3.7246861, -2.979083 ]], dtype=float32)

time = 25258	action = 1	current_phase = 1	next_phase = 0	reward = -0.717601	array([[-3.6886964, -2.0130913]], dtype=float32)

time = 25266	action = 0	current_phase = 0	next_phase = 1	reward = -0.089123	array([[-1.6503427, -2.8584547]], dtype=float32)

time = 25271	action = 0	current_phase = 0	next_phase = 1	reward = -0.019735	array([[-1.8515971, -2.8308883]], dtype=float32)

time = 25276	action = 0	current_phase = 0	next_phase = 1	reward = 0.055800	array([[-2.3848505, -3.0210552]], dtype=float32)

time = 25281	action = 1	current_phase = 0	next_phase = 1	reward = -1.484882	array([[-5.0865645, -3.1242776]], dtype=float32)

time = 25289	action = 1	current_phase = 1	next_phase = 0	reward = -0.769319	array([[-3.7344894, -2.0723038]], dtype=float32)

time = 25297	action = 0	current_phase = 0	next_phase = 1	reward = -0.064649	array([[-1.558526 , -2.7833796]], dtype=float32)

time = 25302	action = 0	current_phase = 0	next_phase = 1	reward = 0.011623	array([[-1.9883493, -2.9348469]], dtype=float32)

time = 25307	action = 0	current_phase = 0	next_phase = 1	reward = 0.066093	array([[-2.416212 , -3.0279984]], dtype=float32)

time = 25312	action = 1	current_phase = 0	next_phase = 1	reward = -1.596866	array([[-4.943247, -3.431274]], dtype=float32)

time = 25320	action = 1	current_phase = 1	next_phase = 0	reward = -0.970745	array([[-3.8414063, -2.2659974]], dtype=float32)

time = 25328	action = 0	current_phase = 0	next_phase = 1	reward = -0.062398	array([[-1.6234515, -2.7777095]], dtype=float32)

time = 25333	action = 0	current_phase = 0	next_phase = 1	reward = 0.011277	array([[-2.076304 , -2.9966116]], dtype=float32)

time = 25338	action = 1	current_phase = 0	next_phase = 1	reward = -1.615232	array([[-2.8094337, -2.7952843]], dtype=float32)

time = 25346	action = 1	current_phase = 1	next_phase = 0	reward = -0.823615	array([[-3.7742925, -2.109383 ]], dtype=float32)

time = 25354	action = 0	current_phase = 0	next_phase = 1	reward = -0.113967	array([[-1.3980674, -2.8817492]], dtype=float32)

time = 25359	action = 0	current_phase = 0	next_phase = 1	reward = -0.048154	array([[-1.4116805, -2.5934563]], dtype=float32)

time = 25364	action = 0	current_phase = 0	next_phase = 1	reward = 0.012968	array([[-2.0547144, -2.9567876]], dtype=float32)

time = 25369	action = 1	current_phase = 0	next_phase = 1	reward = -0.700446	array([[-2.7884169, -2.6178317]], dtype=float32)

time = 25377	action = 1	current_phase = 1	next_phase = 0	reward = -0.907546	array([[-3.7229438, -2.1612203]], dtype=float32)

time = 25385	action = 0	current_phase = 0	next_phase = 1	reward = -0.082665	array([[-1.4759133, -2.835198 ]], dtype=float32)

time = 25390	action = 0	current_phase = 0	next_phase = 1	reward = 0.267314	array([[-1.7947257, -2.8173423]], dtype=float32)

time = 25395	action = 0	current_phase = 0	next_phase = 1	reward = 0.054149	array([[-2.3229063, -2.9988298]], dtype=float32)

time = 25400	action = 1	current_phase = 0	next_phase = 1	reward = -1.317591	array([[-4.091163 , -2.9185715]], dtype=float32)

time = 25408	action = 1	current_phase = 1	next_phase = 0	reward = -0.717893	array([[-3.7270665, -2.0363626]], dtype=float32)

time = 25416	action = 0	current_phase = 0	next_phase = 1	reward = -0.077306	array([[-1.6998779, -2.8394432]], dtype=float32)

time = 25421	action = 0	current_phase = 0	next_phase = 1	reward = -0.010196	array([[-1.9319783, -2.841597 ]], dtype=float32)

time = 25426	action = 0	current_phase = 0	next_phase = 1	reward = 0.059639	array([[-2.359564 , -3.0131764]], dtype=float32)

time = 25431	action = 1	current_phase = 0	next_phase = 1	reward = -1.452869	array([[-5.0232024, -3.131435 ]], dtype=float32)

time = 25439	action = 1	current_phase = 1	next_phase = 0	reward = -0.756326	array([[-3.7616196, -2.0872302]], dtype=float32)

time = 25447	action = 0	current_phase = 0	next_phase = 1	reward = -0.062011	array([[-1.6975837, -2.8411632]], dtype=float32)

time = 25452	action = 0	current_phase = 0	next_phase = 1	reward = 0.020038	array([[-2.0557327, -2.9150233]], dtype=float32)

time = 25457	action = 0	current_phase = 0	next_phase = 1	reward = 0.086935	array([[-2.5587833, -3.1266546]], dtype=float32)

time = 25462	action = 1	current_phase = 0	next_phase = 1	reward = -1.734716	array([[-4.9243083, -3.4182434]], dtype=float32)

time = 25470	action = 1	current_phase = 1	next_phase = 0	reward = -1.368285	array([[-3.8164368, -2.2385468]], dtype=float32)

time = 25478	action = 0	current_phase = 0	next_phase = 1	reward = 0.245720	array([[-1.6216109, -2.801712 ]], dtype=float32)

time = 25483	action = 0	current_phase = 0	next_phase = 1	reward = 0.034418	array([[-2.072865, -2.996368]], dtype=float32)

time = 25488	action = 0	current_phase = 0	next_phase = 1	reward = 0.080290	array([[-2.463319 , -3.0174694]], dtype=float32)

time = 25493	action = 1	current_phase = 0	next_phase = 1	reward = -1.334765	array([[-4.7773585, -3.3252635]], dtype=float32)

time = 25501	action = 1	current_phase = 1	next_phase = 0	reward = -1.113023	array([[-3.5174394, -2.2891562]], dtype=float32)

time = 25509	action = 0	current_phase = 0	next_phase = 1	reward = -0.023775	array([[-1.4468824, -2.7070599]], dtype=float32)

time = 25514	action = 0	current_phase = 0	next_phase = 1	reward = 0.049986	array([[-1.844307 , -2.8358264]], dtype=float32)

time = 25519	action = 0	current_phase = 0	next_phase = 1	reward = 0.047882	array([[-2.14599  , -2.4357646]], dtype=float32)

time = 25524	action = 1	current_phase = 0	next_phase = 1	reward = -1.088838	array([[-4.5905395, -2.9781618]], dtype=float32)

time = 25532	action = 1	current_phase = 1	next_phase = 0	reward = -0.771013	array([[-3.32617  , -2.3420951]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0528 - val_loss: 0.0556

Epoch 2/50

 - 3s - loss: 0.0510 - val_loss: 0.0499

Epoch 3/50

 - 3s - loss: 0.0388 - val_loss: 0.0335

Epoch 4/50

 - 3s - loss: 0.0637 - val_loss: 0.0298

Epoch 5/50

 - 3s - loss: 0.0609 - val_loss: 0.0402

Epoch 6/50

 - 3s - loss: 0.0449 - val_loss: 0.0357

Epoch 7/50

 - 3s - loss: 0.0475 - val_loss: 0.0314

Epoch 8/50

 - 3s - loss: 0.0437 - val_loss: 0.0306

Epoch 9/50

 - 3s - loss: 0.0503 - val_loss: 0.0304

Epoch 10/50

 - 3s - loss: 0.0482 - val_loss: 0.0342

Epoch 11/50

 - 3s - loss: 0.0522 - val_loss: 0.0299

Epoch 12/50

 - 3s - loss: 0.0414 - val_loss: 0.0282

Epoch 13/50

 - 3s - loss: 0.0405 - val_loss: 0.0302

Epoch 14/50

 - 3s - loss: 0.0518 - val_loss: 0.0355

Epoch 15/50

 - 3s - loss: 0.0497 - val_loss: 0.0312

Epoch 16/50

 - 3s - loss: 0.0349 - val_loss: 0.0332

Epoch 17/50

 - 3s - loss: 0.0455 - val_loss: 0.0306

Epoch 18/50

 - 3s - loss: 0.0422 - val_loss: 0.0324

Epoch 19/50

 - 3s - loss: 0.0475 - val_loss: 0.0315

Epoch 20/50

 - 3s - loss: 0.0431 - val_loss: 0.0337

Epoch 21/50

 - 3s - loss: 0.0361 - val_loss: 0.0370

Epoch 22/50

 - 3s - loss: 0.0520 - val_loss: 0.0364

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 25540	action = 0	current_phase = 0	next_phase = 1	reward = -0.041847	array([[-1.6529876, -2.7392414]], dtype=float32)

time = 25545	action = 0	current_phase = 0	next_phase = 1	reward = -0.515049	array([[-2.1518002, -2.9239082]], dtype=float32)

time = 25550	action = 1	current_phase = 0	next_phase = 1	reward = -0.683354	array([[-3.5902815, -2.769    ]], dtype=float32)

time = 25558	action = 1	current_phase = 1	next_phase = 0	reward = -0.709511	array([[-3.5822918, -1.9508985]], dtype=float32)

time = 25566	action = 0	current_phase = 0	next_phase = 1	reward = -0.094598	array([[-1.547725 , -2.7633634]], dtype=float32)

time = 25571	action = 0	current_phase = 0	next_phase = 1	reward = -0.025030	array([[-1.8146482, -2.7974482]], dtype=float32)

time = 25576	action = 0	current_phase = 0	next_phase = 1	reward = 0.052524	array([[-2.32473  , -2.9700222]], dtype=float32)

time = 25581	action = 1	current_phase = 0	next_phase = 1	reward = -1.376619	array([[-5.0703397, -3.003376 ]], dtype=float32)

time = 25589	action = 1	current_phase = 1	next_phase = 0	reward = -0.768444	array([[-3.7451925, -2.0309265]], dtype=float32)

time = 25597	action = 0	current_phase = 0	next_phase = 1	reward = -0.050612	array([[-1.4797721, -2.7743678]], dtype=float32)

time = 25602	action = 0	current_phase = 0	next_phase = 1	reward = 0.027839	array([[-1.9336501, -2.8799305]], dtype=float32)

time = 25607	action = 0	current_phase = 0	next_phase = 1	reward = 0.077752	array([[-2.4416075, -2.9802432]], dtype=float32)

time = 25612	action = 1	current_phase = 0	next_phase = 1	reward = -1.679872	array([[-4.9310575, -3.3404741]], dtype=float32)

time = 25620	action = 1	current_phase = 1	next_phase = 0	reward = -1.014400	array([[-3.8861008, -2.1894894]], dtype=float32)

time = 25628	action = 0	current_phase = 0	next_phase = 1	reward = -0.060676	array([[-1.4553335, -2.718544 ]], dtype=float32)

time = 25633	action = 0	current_phase = 0	next_phase = 1	reward = -0.002289	array([[-2.038365 , -2.9532032]], dtype=float32)

time = 25638	action = 1	current_phase = 0	next_phase = 1	reward = -1.730963	array([[-2.8589737, -2.7551775]], dtype=float32)

time = 25646	action = 1	current_phase = 1	next_phase = 0	reward = -0.724888	array([[-3.788212 , -1.9891927]], dtype=float32)

time = 25654	action = 0	current_phase = 0	next_phase = 1	reward = -0.107784	array([[-1.2620063, -2.8889027]], dtype=float32)

time = 25659	action = 0	current_phase = 0	next_phase = 1	reward = -0.032337	array([[-1.5002749, -2.579719 ]], dtype=float32)

time = 25664	action = 0	current_phase = 0	next_phase = 1	reward = 0.035758	array([[-1.9617783, -2.883016 ]], dtype=float32)

time = 25669	action = 1	current_phase = 0	next_phase = 1	reward = -0.639333	array([[-3.485591 , -2.7029982]], dtype=float32)

time = 25677	action = 1	current_phase = 1	next_phase = 0	reward = -0.643612	array([[-3.4501548, -1.9969736]], dtype=float32)

time = 25685	action = 0	current_phase = 0	next_phase = 1	reward = -0.383395	array([[-1.3724686, -2.912057 ]], dtype=float32)

time = 25690	action = 0	current_phase = 0	next_phase = 1	reward = 0.251154	array([[-1.777346 , -2.7740183]], dtype=float32)

time = 25695	action = 0	current_phase = 0	next_phase = 1	reward = -0.239675	array([[-2.249793, -2.957665]], dtype=float32)

time = 25700	action = 1	current_phase = 0	next_phase = 1	reward = -0.977422	array([[-3.9547148, -2.7359114]], dtype=float32)

time = 25708	action = 1	current_phase = 1	next_phase = 0	reward = -0.708656	array([[-3.541319 , -1.9158251]], dtype=float32)

time = 25716	action = 0	current_phase = 0	next_phase = 1	reward = -0.096863	array([[-1.5454332, -2.763414 ]], dtype=float32)

time = 25721	action = 0	current_phase = 0	next_phase = 1	reward = -0.006348	array([[-1.7890044, -2.8008127]], dtype=float32)

time = 25726	action = 0	current_phase = 0	next_phase = 1	reward = 0.067009	array([[-2.3119457, -2.9722338]], dtype=float32)

time = 25731	action = 1	current_phase = 0	next_phase = 1	reward = -1.451454	array([[-5.036499 , -3.0535846]], dtype=float32)

time = 25739	action = 1	current_phase = 1	next_phase = 0	reward = -0.792269	array([[-3.7457163, -2.0193048]], dtype=float32)

time = 25747	action = 0	current_phase = 0	next_phase = 1	reward = -0.063408	array([[-1.5210407, -2.764731 ]], dtype=float32)

time = 25752	action = 0	current_phase = 0	next_phase = 1	reward = 0.007777	array([[-1.8885252, -2.8228474]], dtype=float32)

time = 25757	action = 0	current_phase = 0	next_phase = 1	reward = 0.059173	array([[-2.4713733, -3.006486 ]], dtype=float32)

time = 25762	action = 1	current_phase = 0	next_phase = 1	reward = -1.657822	array([[-4.9797263, -3.3387766]], dtype=float32)

time = 25770	action = 1	current_phase = 1	next_phase = 0	reward = -1.347371	array([[-3.8310905, -2.0630462]], dtype=float32)

time = 25778	action = 0	current_phase = 0	next_phase = 1	reward = 0.239928	array([[-1.5257418, -2.7214918]], dtype=float32)

time = 25783	action = 0	current_phase = 0	next_phase = 1	reward = 0.034633	array([[-2.0245423, -2.939331 ]], dtype=float32)

time = 25788	action = 1	current_phase = 0	next_phase = 1	reward = -1.671693	array([[-2.7603297, -2.7168999]], dtype=float32)

time = 25796	action = 1	current_phase = 1	next_phase = 0	reward = -1.130926	array([[-3.839312, -2.055254]], dtype=float32)

time = 25804	action = 0	current_phase = 0	next_phase = 1	reward = 0.186316	array([[-1.3018731, -2.8616538]], dtype=float32)

time = 25809	action = 0	current_phase = 0	next_phase = 1	reward = -0.030566	array([[-1.425907 , -2.4907632]], dtype=float32)

time = 25814	action = 0	current_phase = 0	next_phase = 1	reward = 0.044041	array([[-1.8675907, -2.8979526]], dtype=float32)

time = 25819	action = 1	current_phase = 0	next_phase = 1	reward = -0.693606	array([[-2.729773 , -2.4012785]], dtype=float32)

time = 25827	action = 1	current_phase = 1	next_phase = 0	reward = -0.930111	array([[-3.5524328, -1.8879393]], dtype=float32)

time = 25835	action = 0	current_phase = 0	next_phase = 1	reward = -0.095681	array([[-1.395293 , -2.8282151]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0665 - val_loss: 0.0457

Epoch 2/50

 - 3s - loss: 0.0711 - val_loss: 0.0453

Epoch 3/50

 - 3s - loss: 0.0635 - val_loss: 0.0388

Epoch 4/50

 - 3s - loss: 0.0509 - val_loss: 0.0325

Epoch 5/50

 - 3s - loss: 0.0561 - val_loss: 0.0349

Epoch 6/50

 - 3s - loss: 0.0501 - val_loss: 0.0404

Epoch 7/50

 - 3s - loss: 0.0528 - val_loss: 0.0353

Epoch 8/50

 - 3s - loss: 0.0593 - val_loss: 0.0406

Epoch 9/50

 - 3s - loss: 0.0591 - val_loss: 0.0345

Epoch 10/50

 - 3s - loss: 0.0512 - val_loss: 0.0468

Epoch 11/50

 - 3s - loss: 0.0490 - val_loss: 0.0384

Epoch 12/50

 - 3s - loss: 0.0525 - val_loss: 0.0368

Epoch 13/50

 - 3s - loss: 0.0579 - val_loss: 0.0342

Epoch 14/50

 - 3s - loss: 0.0495 - val_loss: 0.0455

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 25840	action = 0	current_phase = 0	next_phase = 1	reward = 0.252145	array([[-1.7986677, -2.727727 ]], dtype=float32)

time = 25845	action = 0	current_phase = 0	next_phase = 1	reward = -0.236246	array([[-2.2542565, -2.9129744]], dtype=float32)

time = 25850	action = 1	current_phase = 0	next_phase = 1	reward = -1.013405	array([[-3.8311992, -2.6797915]], dtype=float32)

time = 25858	action = 1	current_phase = 1	next_phase = 0	reward = -0.724259	array([[-3.8165503, -2.0377417]], dtype=float32)

time = 25866	action = 0	current_phase = 0	next_phase = 1	reward = -0.096334	array([[-1.5359828, -2.71844  ]], dtype=float32)

time = 25871	action = 0	current_phase = 0	next_phase = 1	reward = -0.012993	array([[-1.8016987, -2.726521 ]], dtype=float32)

time = 25876	action = 0	current_phase = 0	next_phase = 1	reward = 0.049046	array([[-2.263356 , -2.9180603]], dtype=float32)

time = 25881	action = 1	current_phase = 0	next_phase = 1	reward = -1.435831	array([[-4.699401 , -3.0874405]], dtype=float32)

time = 25889	action = 1	current_phase = 1	next_phase = 0	reward = -0.767639	array([[-3.8478389, -2.0753708]], dtype=float32)

time = 25897	action = 0	current_phase = 0	next_phase = 1	reward = -0.078074	array([[-1.5149944, -2.7500753]], dtype=float32)

time = 25902	action = 0	current_phase = 0	next_phase = 1	reward = -0.014521	array([[-1.9660373, -2.8335037]], dtype=float32)

time = 25907	action = 0	current_phase = 0	next_phase = 1	reward = 0.054071	array([[-2.3317373, -2.9459062]], dtype=float32)

time = 25912	action = 1	current_phase = 0	next_phase = 1	reward = -1.532068	array([[-4.881821, -3.172069]], dtype=float32)

time = 25920	action = 1	current_phase = 1	next_phase = 0	reward = -0.825935	array([[-3.9125404, -2.1676552]], dtype=float32)

time = 25928	action = 0	current_phase = 0	next_phase = 1	reward = -0.054466	array([[-1.4926493, -2.6718688]], dtype=float32)

time = 25933	action = 0	current_phase = 0	next_phase = 1	reward = 0.030335	array([[-1.9423356, -2.9031873]], dtype=float32)

time = 25938	action = 0	current_phase = 0	next_phase = 1	reward = 0.079280	array([[-2.5622725, -2.8089256]], dtype=float32)

time = 25943	action = 1	current_phase = 0	next_phase = 1	reward = -1.994829	array([[-4.859003 , -3.3332992]], dtype=float32)

time = 25951	action = 1	current_phase = 1	next_phase = 0	reward = -1.435567	array([[-3.9206462, -2.2461348]], dtype=float32)

time = 25959	action = 0	current_phase = 0	next_phase = 1	reward = 0.260065	array([[-1.3786707, -2.6581292]], dtype=float32)

time = 25964	action = 0	current_phase = 0	next_phase = 1	reward = 0.015796	array([[-1.8346717, -2.805122 ]], dtype=float32)

time = 25969	action = 0	current_phase = 0	next_phase = 1	reward = 0.050684	array([[-2.1341615, -2.4012098]], dtype=float32)

time = 25974	action = 1	current_phase = 0	next_phase = 1	reward = -1.123837	array([[-4.646057 , -2.8256745]], dtype=float32)

time = 25982	action = 1	current_phase = 1	next_phase = 0	reward = -0.987394	array([[-3.5611525, -2.277835 ]], dtype=float32)

time = 25990	action = 0	current_phase = 0	next_phase = 1	reward = -0.299120	array([[-1.6511183, -2.6802738]], dtype=float32)

time = 25995	action = 0	current_phase = 0	next_phase = 1	reward = 0.619486	array([[-1.9434314, -2.7885575]], dtype=float32)

time = 26000	action = 1	current_phase = 0	next_phase = 1	reward = -1.239529	array([[-3.7130089, -2.9644842]], dtype=float32)

time = 26008	action = 1	current_phase = 1	next_phase = 0	reward = -0.719521	array([[-3.7140245, -2.094709 ]], dtype=float32)

time = 26016	action = 0	current_phase = 0	next_phase = 1	reward = -0.084011	array([[-1.4727731, -2.7412548]], dtype=float32)

time = 26021	action = 0	current_phase = 0	next_phase = 1	reward = 0.001145	array([[-1.7906356, -2.7587836]], dtype=float32)

time = 26026	action = 0	current_phase = 0	next_phase = 1	reward = 0.070079	array([[-2.3331945, -2.93356  ]], dtype=float32)

time = 26031	action = 1	current_phase = 0	next_phase = 1	reward = -1.496720	array([[-5.0035405, -3.0090675]], dtype=float32)

time = 26039	action = 1	current_phase = 1	next_phase = 0	reward = -0.781028	array([[-3.8497853, -2.0465057]], dtype=float32)

time = 26047	action = 0	current_phase = 0	next_phase = 1	reward = -0.073090	array([[-1.5084519, -2.726523 ]], dtype=float32)

time = 26052	action = 0	current_phase = 0	next_phase = 1	reward = 0.008625	array([[-1.9154093, -2.8311553]], dtype=float32)

time = 26057	action = 0	current_phase = 0	next_phase = 1	reward = 0.067629	array([[-2.3338773, -2.9625149]], dtype=float32)

time = 26062	action = 1	current_phase = 0	next_phase = 1	reward = -1.602800	array([[-4.878907, -3.29848 ]], dtype=float32)

time = 26070	action = 1	current_phase = 1	next_phase = 0	reward = -0.991240	array([[-3.995543 , -2.1637518]], dtype=float32)

time = 26078	action = 0	current_phase = 0	next_phase = 1	reward = -0.049860	array([[-1.4682904, -2.6821766]], dtype=float32)

time = 26083	action = 0	current_phase = 0	next_phase = 1	reward = 0.024160	array([[-2.003018 , -2.9332743]], dtype=float32)

time = 26088	action = 0	current_phase = 0	next_phase = 1	reward = 0.067819	array([[-2.599657, -2.791726]], dtype=float32)

time = 26093	action = 1	current_phase = 0	next_phase = 1	reward = -1.346899	array([[-4.7845006, -3.3075833]], dtype=float32)

time = 26101	action = 1	current_phase = 1	next_phase = 0	reward = -1.425629	array([[-3.6564317, -2.2982368]], dtype=float32)

time = 26109	action = 0	current_phase = 0	next_phase = 1	reward = 0.249566	array([[-1.3756363, -2.6514978]], dtype=float32)

time = 26114	action = 0	current_phase = 0	next_phase = 1	reward = 0.030811	array([[-1.7374377, -2.7504807]], dtype=float32)

time = 26119	action = 1	current_phase = 0	next_phase = 1	reward = -0.780667	array([[-2.321279, -2.293068]], dtype=float32)

time = 26127	action = 1	current_phase = 1	next_phase = 0	reward = -0.919095	array([[-3.8366008, -2.0365903]], dtype=float32)

time = 26135	action = 0	current_phase = 0	next_phase = 1	reward = 0.179613	array([[-1.3546426, -2.8053145]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0613 - val_loss: 0.0285

Epoch 2/50

 - 3s - loss: 0.0674 - val_loss: 0.0291

Epoch 3/50

 - 3s - loss: 0.0578 - val_loss: 0.0279

Epoch 4/50

 - 3s - loss: 0.0649 - val_loss: 0.0274

Epoch 5/50

 - 3s - loss: 0.0561 - val_loss: 0.0389

Epoch 6/50

 - 3s - loss: 0.0639 - val_loss: 0.0352

Epoch 7/50

 - 3s - loss: 0.0640 - val_loss: 0.0344

Epoch 8/50

 - 3s - loss: 0.0557 - val_loss: 0.0293

Epoch 9/50

 - 3s - loss: 0.0542 - val_loss: 0.0316

Epoch 10/50

 - 3s - loss: 0.0577 - val_loss: 0.0345

Epoch 11/50

 - 3s - loss: 0.0511 - val_loss: 0.0336

Epoch 12/50

 - 3s - loss: 0.0585 - val_loss: 0.0304

Epoch 13/50

 - 3s - loss: 0.0510 - val_loss: 0.0335

Epoch 14/50

 - 3s - loss: 0.0515 - val_loss: 0.0348

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 26140	action = 0	current_phase = 0	next_phase = 1	reward = -0.022915	array([[-1.7622793, -2.6990426]], dtype=float32)

time = 26145	action = 0	current_phase = 0	next_phase = 1	reward = 0.042940	array([[-2.1639507, -2.9232564]], dtype=float32)

time = 26150	action = 1	current_phase = 0	next_phase = 1	reward = -1.355517	array([[-3.671793 , -2.6636596]], dtype=float32)

time = 26158	action = 1	current_phase = 1	next_phase = 0	reward = -0.718310	array([[-3.768558 , -2.0077162]], dtype=float32)

time = 26166	action = 0	current_phase = 0	next_phase = 1	reward = -0.089530	array([[-1.4703467, -2.7479799]], dtype=float32)

time = 26171	action = 0	current_phase = 0	next_phase = 1	reward = -0.009522	array([[-1.870029 , -2.7470489]], dtype=float32)

time = 26176	action = 0	current_phase = 0	next_phase = 1	reward = 0.062214	array([[-2.347786 , -2.9465868]], dtype=float32)

time = 26181	action = 1	current_phase = 0	next_phase = 1	reward = -1.447697	array([[-5.008896, -3.050366]], dtype=float32)

time = 26189	action = 1	current_phase = 1	next_phase = 0	reward = -0.888678	array([[-3.8913312, -2.1200523]], dtype=float32)

time = 26197	action = 0	current_phase = 0	next_phase = 1	reward = -0.061621	array([[-1.5421181, -2.7179086]], dtype=float32)

time = 26202	action = 0	current_phase = 0	next_phase = 1	reward = 0.000864	array([[-1.9119649, -2.859829 ]], dtype=float32)

time = 26207	action = 0	current_phase = 0	next_phase = 1	reward = 0.089296	array([[-2.5332403, -2.907034 ]], dtype=float32)

time = 26212	action = 1	current_phase = 0	next_phase = 1	reward = -1.661854	array([[-4.9281945, -3.3853319]], dtype=float32)

time = 26220	action = 1	current_phase = 1	next_phase = 0	reward = -1.002389	array([[-3.9681735, -2.3552904]], dtype=float32)

time = 26228	action = 0	current_phase = 0	next_phase = 1	reward = -0.046656	array([[-1.5472274, -2.6663735]], dtype=float32)

time = 26233	action = 0	current_phase = 0	next_phase = 1	reward = 0.042000	array([[-2.016529 , -2.9367409]], dtype=float32)

time = 26238	action = 0	current_phase = 0	next_phase = 1	reward = 0.080161	array([[-2.5004153, -2.8895261]], dtype=float32)

time = 26243	action = 1	current_phase = 0	next_phase = 1	reward = -1.940946	array([[-4.832956 , -3.5315795]], dtype=float32)

time = 26251	action = 1	current_phase = 1	next_phase = 0	reward = -1.099384	array([[-3.892223 , -2.3570318]], dtype=float32)

time = 26259	action = 0	current_phase = 0	next_phase = 1	reward = -0.046399	array([[-1.3257847, -2.5580628]], dtype=float32)

time = 26264	action = 0	current_phase = 0	next_phase = 1	reward = 0.022335	array([[-1.7628785, -2.7615087]], dtype=float32)

time = 26269	action = 0	current_phase = 0	next_phase = 1	reward = 0.069352	array([[-2.1660206, -2.385723 ]], dtype=float32)

time = 26274	action = 1	current_phase = 0	next_phase = 1	reward = -1.058220	array([[-4.810955 , -2.9938266]], dtype=float32)

time = 26282	action = 1	current_phase = 1	next_phase = 0	reward = -0.755394	array([[-3.7539568, -2.2818205]], dtype=float32)

time = 26290	action = 0	current_phase = 0	next_phase = 1	reward = -0.303317	array([[-1.7068034, -2.6349924]], dtype=float32)

time = 26295	action = 0	current_phase = 0	next_phase = 1	reward = 0.320457	array([[-2.0277514, -2.8162374]], dtype=float32)

time = 26300	action = 1	current_phase = 0	next_phase = 1	reward = -1.320213	array([[-3.3463328, -2.694094 ]], dtype=float32)

time = 26308	action = 1	current_phase = 1	next_phase = 0	reward = -0.689838	array([[-3.7655082, -2.0470388]], dtype=float32)

time = 26316	action = 0	current_phase = 0	next_phase = 1	reward = -0.081313	array([[-1.5205295, -2.7161102]], dtype=float32)

time = 26321	action = 0	current_phase = 0	next_phase = 1	reward = -0.000331	array([[-1.909056 , -2.7915819]], dtype=float32)

time = 26326	action = 0	current_phase = 0	next_phase = 1	reward = 0.067836	array([[-2.4375436, -2.9227076]], dtype=float32)

time = 26331	action = 1	current_phase = 0	next_phase = 1	reward = -1.575530	array([[-4.913616 , -3.0822556]], dtype=float32)

time = 26339	action = 1	current_phase = 1	next_phase = 0	reward = -0.785078	array([[-3.8746986, -2.1086574]], dtype=float32)

time = 26347	action = 0	current_phase = 0	next_phase = 1	reward = -0.055749	array([[-1.4994984, -2.7271452]], dtype=float32)

time = 26352	action = 0	current_phase = 0	next_phase = 1	reward = 0.024245	array([[-2.0480635, -2.8767138]], dtype=float32)

time = 26357	action = 0	current_phase = 0	next_phase = 1	reward = 0.081495	array([[-2.4321175, -2.9420218]], dtype=float32)

time = 26362	action = 1	current_phase = 0	next_phase = 1	reward = -1.724365	array([[-4.946824 , -3.3245988]], dtype=float32)

time = 26370	action = 1	current_phase = 1	next_phase = 0	reward = -0.958928	array([[-3.9849124, -2.3177776]], dtype=float32)

time = 26378	action = 0	current_phase = 0	next_phase = 1	reward = -0.041460	array([[-1.5473127, -2.7035124]], dtype=float32)

time = 26383	action = 0	current_phase = 0	next_phase = 1	reward = 0.038400	array([[-2.0518556, -2.9078574]], dtype=float32)

time = 26388	action = 0	current_phase = 0	next_phase = 1	reward = 0.081041	array([[-2.6654747, -2.726959 ]], dtype=float32)

time = 26393	action = 1	current_phase = 0	next_phase = 1	reward = -0.818024	array([[-4.759308, -3.526864]], dtype=float32)

time = 26401	action = 1	current_phase = 1	next_phase = 0	reward = -1.307226	array([[-3.3430629, -2.4247794]], dtype=float32)

time = 26409	action = 0	current_phase = 0	next_phase = 1	reward = -0.044498	array([[-1.4410491, -2.6293569]], dtype=float32)

time = 26414	action = 0	current_phase = 0	next_phase = 1	reward = 0.035424	array([[-1.7452478, -2.7339928]], dtype=float32)

time = 26419	action = 0	current_phase = 0	next_phase = 1	reward = 0.076466	array([[-2.2428198, -2.4486248]], dtype=float32)

time = 26424	action = 1	current_phase = 0	next_phase = 1	reward = -1.004235	array([[-4.494408 , -3.0155182]], dtype=float32)

time = 26432	action = 1	current_phase = 1	next_phase = 0	reward = -0.986179	array([[-3.4188943, -2.3336601]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0721 - val_loss: 0.0543

Epoch 2/50

 - 3s - loss: 0.0779 - val_loss: 0.0382

Epoch 3/50

 - 3s - loss: 0.0802 - val_loss: 0.0516

Epoch 4/50

 - 3s - loss: 0.0620 - val_loss: 0.0454

Epoch 5/50

 - 3s - loss: 0.0564 - val_loss: 0.0449

Epoch 6/50

 - 3s - loss: 0.0730 - val_loss: 0.0542

Epoch 7/50

 - 3s - loss: 0.0512 - val_loss: 0.0507

Epoch 8/50

 - 3s - loss: 0.0546 - val_loss: 0.0564

Epoch 9/50

 - 3s - loss: 0.0560 - val_loss: 0.0561

Epoch 10/50

 - 3s - loss: 0.0432 - val_loss: 0.0661

Epoch 11/50

 - 3s - loss: 0.0542 - val_loss: 0.0526

Epoch 12/50

 - 3s - loss: 0.0580 - val_loss: 0.0512

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 26440	action = 0	current_phase = 0	next_phase = 1	reward = 0.238895	array([[-1.6824808, -2.6952677]], dtype=float32)

time = 26445	action = 0	current_phase = 0	next_phase = 1	reward = 0.026218	array([[-2.1117232, -2.8349857]], dtype=float32)

time = 26450	action = 1	current_phase = 0	next_phase = 1	reward = -1.200118	array([[-3.587363 , -2.7778172]], dtype=float32)

time = 26458	action = 1	current_phase = 1	next_phase = 0	reward = -0.696119	array([[-3.501169 , -2.0672946]], dtype=float32)

time = 26466	action = 0	current_phase = 0	next_phase = 1	reward = -0.089623	array([[-1.57947  , -2.7550132]], dtype=float32)

time = 26471	action = 0	current_phase = 0	next_phase = 1	reward = -0.022193	array([[-1.9165606, -2.8020337]], dtype=float32)

time = 26476	action = 0	current_phase = 0	next_phase = 1	reward = 0.041078	array([[-2.3414578, -2.9223623]], dtype=float32)

time = 26481	action = 1	current_phase = 0	next_phase = 1	reward = -1.496073	array([[-5.024615, -3.125791]], dtype=float32)

time = 26489	action = 1	current_phase = 1	next_phase = 0	reward = -1.198820	array([[-3.8524525, -2.1210413]], dtype=float32)

time = 26497	action = 0	current_phase = 0	next_phase = 1	reward = 0.210366	array([[-1.5821308, -2.7422962]], dtype=float32)

time = 26502	action = 0	current_phase = 0	next_phase = 1	reward = -0.007547	array([[-1.9340065, -2.7994165]], dtype=float32)

time = 26507	action = 0	current_phase = 0	next_phase = 1	reward = 0.050075	array([[-2.4353263, -2.9480896]], dtype=float32)

time = 26512	action = 1	current_phase = 0	next_phase = 1	reward = -1.637903	array([[-5.0280066, -3.22499  ]], dtype=float32)

time = 26520	action = 1	current_phase = 1	next_phase = 0	reward = -0.901092	array([[-3.9775863, -2.206997 ]], dtype=float32)

time = 26528	action = 0	current_phase = 0	next_phase = 1	reward = -0.039209	array([[-1.6153898, -2.6966178]], dtype=float32)

time = 26533	action = 0	current_phase = 0	next_phase = 1	reward = 0.013061	array([[-2.0036178, -2.8932939]], dtype=float32)

time = 26538	action = 0	current_phase = 0	next_phase = 1	reward = 0.072741	array([[-2.7438457, -2.7890673]], dtype=float32)

time = 26543	action = 1	current_phase = 0	next_phase = 1	reward = -1.981145	array([[-4.850043, -3.340945]], dtype=float32)

time = 26551	action = 1	current_phase = 1	next_phase = 0	reward = -1.094644	array([[-4.1403027, -2.4235606]], dtype=float32)

time = 26559	action = 0	current_phase = 0	next_phase = 1	reward = -0.032113	array([[-1.3884637, -2.6021347]], dtype=float32)

time = 26564	action = 0	current_phase = 0	next_phase = 1	reward = 0.034706	array([[-1.8153546, -2.7844596]], dtype=float32)

time = 26569	action = 1	current_phase = 0	next_phase = 1	reward = -0.642863	array([[-2.4146652, -2.2718024]], dtype=float32)

time = 26577	action = 1	current_phase = 1	next_phase = 0	reward = -0.861254	array([[-3.126511, -2.029581]], dtype=float32)

time = 26585	action = 0	current_phase = 0	next_phase = 1	reward = 0.189776	array([[-1.4089192, -2.810885 ]], dtype=float32)

time = 26590	action = 0	current_phase = 0	next_phase = 1	reward = -0.021483	array([[-1.7536876, -2.6563053]], dtype=float32)

time = 26595	action = 0	current_phase = 0	next_phase = 1	reward = -0.231065	array([[-2.344397 , -2.9419525]], dtype=float32)

time = 26600	action = 1	current_phase = 0	next_phase = 1	reward = -1.097750	array([[-3.8998966, -2.7551203]], dtype=float32)

time = 26608	action = 1	current_phase = 1	next_phase = 0	reward = -0.709504	array([[-3.8573308, -2.0612407]], dtype=float32)

time = 26616	action = 0	current_phase = 0	next_phase = 1	reward = -0.089383	array([[-1.6685057, -2.7631867]], dtype=float32)

time = 26621	action = 0	current_phase = 0	next_phase = 1	reward = -0.011595	array([[-2.0889728, -2.8402386]], dtype=float32)

time = 26626	action = 0	current_phase = 0	next_phase = 1	reward = 0.068295	array([[-2.3648252, -2.9491935]], dtype=float32)

time = 26631	action = 1	current_phase = 0	next_phase = 1	reward = -1.512597	array([[-4.740171, -3.130218]], dtype=float32)

time = 26639	action = 1	current_phase = 1	next_phase = 0	reward = -0.819333	array([[-3.8102112, -2.0440638]], dtype=float32)

time = 26647	action = 0	current_phase = 0	next_phase = 1	reward = -0.083409	array([[-1.5424243, -2.721658 ]], dtype=float32)

time = 26652	action = 0	current_phase = 0	next_phase = 1	reward = -0.002483	array([[-1.9607358, -2.83603  ]], dtype=float32)

time = 26657	action = 0	current_phase = 0	next_phase = 1	reward = 0.074882	array([[-2.4871516, -2.9123456]], dtype=float32)

time = 26662	action = 1	current_phase = 0	next_phase = 1	reward = -1.504647	array([[-4.985318 , -3.3201156]], dtype=float32)

time = 26670	action = 1	current_phase = 1	next_phase = 0	reward = -1.356433	array([[-3.8613558, -2.2199156]], dtype=float32)

time = 26678	action = 0	current_phase = 0	next_phase = 1	reward = 0.258765	array([[-1.6087482, -2.686399 ]], dtype=float32)

time = 26683	action = 0	current_phase = 0	next_phase = 1	reward = 0.042842	array([[-2.1017754, -2.9171505]], dtype=float32)

time = 26688	action = 0	current_phase = 0	next_phase = 1	reward = 0.089067	array([[-2.592877, -2.848897]], dtype=float32)

time = 26693	action = 1	current_phase = 0	next_phase = 1	reward = -1.941682	array([[-4.8540635, -3.4003334]], dtype=float32)

time = 26701	action = 1	current_phase = 1	next_phase = 0	reward = -1.436330	array([[-3.9268105, -2.2556424]], dtype=float32)

time = 26709	action = 0	current_phase = 0	next_phase = 1	reward = 0.267864	array([[-1.4174271, -2.5826502]], dtype=float32)

time = 26714	action = 0	current_phase = 0	next_phase = 1	reward = 0.036054	array([[-1.7807353, -2.7628374]], dtype=float32)

time = 26719	action = 0	current_phase = 0	next_phase = 1	reward = 0.062132	array([[-2.2045643, -2.3805242]], dtype=float32)

time = 26724	action = 1	current_phase = 0	next_phase = 1	reward = -1.140485	array([[-4.8025446, -3.0017502]], dtype=float32)

time = 26732	action = 1	current_phase = 1	next_phase = 0	reward = -1.071610	array([[-3.8643055, -2.1841023]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0654 - val_loss: 0.0241

Epoch 2/50

 - 3s - loss: 0.0549 - val_loss: 0.0198

Epoch 3/50

 - 3s - loss: 0.0626 - val_loss: 0.0248

Epoch 4/50

 - 3s - loss: 0.0603 - val_loss: 0.0218

Epoch 5/50

 - 3s - loss: 0.0485 - val_loss: 0.0234

Epoch 6/50

 - 3s - loss: 0.0493 - val_loss: 0.0227

Epoch 7/50

 - 3s - loss: 0.0618 - val_loss: 0.0254

Epoch 8/50

 - 3s - loss: 0.0493 - val_loss: 0.0324

Epoch 9/50

 - 3s - loss: 0.0632 - val_loss: 0.0293

Epoch 10/50

 - 3s - loss: 0.0637 - val_loss: 0.0207

Epoch 11/50

 - 3s - loss: 0.0450 - val_loss: 0.0230

Epoch 12/50

 - 3s - loss: 0.0495 - val_loss: 0.0264

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 26740	action = 0	current_phase = 0	next_phase = 1	reward = -0.031862	array([[-1.6615506, -2.6370115]], dtype=float32)

time = 26745	action = 0	current_phase = 0	next_phase = 1	reward = 0.033871	array([[-2.1473682, -2.8561635]], dtype=float32)

time = 26750	action = 1	current_phase = 0	next_phase = 1	reward = -1.040974	array([[-3.5218441, -2.8376622]], dtype=float32)

time = 26758	action = 1	current_phase = 1	next_phase = 0	reward = -0.699762	array([[-3.63708  , -2.1035843]], dtype=float32)

time = 26766	action = 0	current_phase = 0	next_phase = 1	reward = -0.075997	array([[-1.6231699, -2.7442932]], dtype=float32)

time = 26771	action = 0	current_phase = 0	next_phase = 1	reward = 0.001431	array([[-1.9545674, -2.8032923]], dtype=float32)

time = 26776	action = 0	current_phase = 0	next_phase = 1	reward = 0.054536	array([[-2.265467 , -2.9664922]], dtype=float32)

time = 26781	action = 1	current_phase = 0	next_phase = 1	reward = -1.541743	array([[-5.1260805, -3.115233 ]], dtype=float32)

time = 26789	action = 1	current_phase = 1	next_phase = 0	reward = -0.755867	array([[-3.9101717, -2.197792 ]], dtype=float32)

time = 26797	action = 0	current_phase = 0	next_phase = 1	reward = -0.076292	array([[-1.6344318, -2.7874098]], dtype=float32)

time = 26802	action = 0	current_phase = 0	next_phase = 1	reward = 0.001045	array([[-2.0422313, -2.8242707]], dtype=float32)

time = 26807	action = 0	current_phase = 0	next_phase = 1	reward = 0.054017	array([[-2.566875 , -3.0001025]], dtype=float32)

time = 26812	action = 1	current_phase = 0	next_phase = 1	reward = -1.541949	array([[-5.097413 , -3.2458558]], dtype=float32)

time = 26820	action = 1	current_phase = 1	next_phase = 0	reward = -1.000398	array([[-3.8849998, -2.2197976]], dtype=float32)

time = 26828	action = 0	current_phase = 0	next_phase = 1	reward = -0.046154	array([[-1.6093743, -2.6994429]], dtype=float32)

time = 26833	action = 0	current_phase = 0	next_phase = 1	reward = 0.024782	array([[-2.1238909, -2.9374151]], dtype=float32)

time = 26838	action = 0	current_phase = 0	next_phase = 1	reward = 0.078611	array([[-2.6431396, -2.783669 ]], dtype=float32)

time = 26843	action = 1	current_phase = 0	next_phase = 1	reward = -1.273207	array([[-4.9650593, -3.3906136]], dtype=float32)

time = 26851	action = 1	current_phase = 1	next_phase = 0	reward = -1.197024	array([[-3.5184166, -2.3898244]], dtype=float32)

time = 26859	action = 0	current_phase = 0	next_phase = 1	reward = -0.030995	array([[-1.4823742, -2.5662022]], dtype=float32)

time = 26864	action = 0	current_phase = 0	next_phase = 1	reward = 0.034695	array([[-1.8342927, -2.7523222]], dtype=float32)

time = 26869	action = 1	current_phase = 0	next_phase = 1	reward = -0.631581	array([[-2.4587893, -2.4316301]], dtype=float32)

time = 26877	action = 1	current_phase = 1	next_phase = 0	reward = -1.175464	array([[-3.3212576, -2.231091 ]], dtype=float32)

time = 26885	action = 0	current_phase = 0	next_phase = 1	reward = -0.104381	array([[-1.4689837, -2.7852373]], dtype=float32)

time = 26890	action = 0	current_phase = 0	next_phase = 1	reward = 0.536074	array([[-1.6470373, -2.6796622]], dtype=float32)

time = 26895	action = 0	current_phase = 0	next_phase = 1	reward = 0.039942	array([[-2.3198807, -2.939324 ]], dtype=float32)

time = 26900	action = 1	current_phase = 0	next_phase = 1	reward = -1.266190	array([[-4.090144 , -2.6645532]], dtype=float32)

time = 26908	action = 1	current_phase = 1	next_phase = 0	reward = -0.711435	array([[-3.655808 , -2.0591674]], dtype=float32)

time = 26916	action = 0	current_phase = 0	next_phase = 1	reward = -0.084572	array([[-1.649539 , -2.7083554]], dtype=float32)

time = 26921	action = 0	current_phase = 0	next_phase = 1	reward = 0.000299	array([[-1.9887118, -2.7982216]], dtype=float32)

time = 26926	action = 0	current_phase = 0	next_phase = 1	reward = 0.065159	array([[-2.3926811, -2.9568858]], dtype=float32)

time = 26931	action = 1	current_phase = 0	next_phase = 1	reward = -1.553499	array([[-5.0526843, -3.11976  ]], dtype=float32)

time = 26939	action = 1	current_phase = 1	next_phase = 0	reward = -0.836098	array([[-3.8229115, -2.1180925]], dtype=float32)

time = 26947	action = 0	current_phase = 0	next_phase = 1	reward = -0.066916	array([[-1.5857534, -2.7685075]], dtype=float32)

time = 26952	action = 0	current_phase = 0	next_phase = 1	reward = 0.019444	array([[-2.0376809, -2.8976731]], dtype=float32)

time = 26957	action = 0	current_phase = 0	next_phase = 1	reward = 0.071600	array([[-2.5403116, -2.9847999]], dtype=float32)

time = 26962	action = 1	current_phase = 0	next_phase = 1	reward = -1.617120	array([[-5.0261555, -3.3646574]], dtype=float32)

time = 26970	action = 1	current_phase = 1	next_phase = 0	reward = -1.131899	array([[-3.8710597, -2.3203301]], dtype=float32)

time = 26978	action = 0	current_phase = 0	next_phase = 1	reward = 0.228749	array([[-1.5406711, -2.686687 ]], dtype=float32)

time = 26983	action = 0	current_phase = 0	next_phase = 1	reward = 0.009973	array([[-2.071958 , -2.8961186]], dtype=float32)

time = 26988	action = 0	current_phase = 0	next_phase = 1	reward = 0.074540	array([[-2.6396239, -2.7924876]], dtype=float32)

time = 26993	action = 1	current_phase = 0	next_phase = 1	reward = -1.918864	array([[-4.9743266, -3.397049 ]], dtype=float32)

time = 27001	action = 1	current_phase = 1	next_phase = 0	reward = -1.329942	array([[-4.098288 , -2.3866751]], dtype=float32)

time = 27009	action = 0	current_phase = 0	next_phase = 1	reward = 0.263414	array([[-1.3321127, -2.62642  ]], dtype=float32)

time = 27014	action = 0	current_phase = 0	next_phase = 1	reward = 0.029603	array([[-1.8890462, -2.770257 ]], dtype=float32)

time = 27019	action = 0	current_phase = 0	next_phase = 1	reward = 0.064929	array([[-2.2536657, -2.3764195]], dtype=float32)

time = 27024	action = 1	current_phase = 0	next_phase = 1	reward = -1.116417	array([[-4.8844304, -2.9728222]], dtype=float32)

time = 27032	action = 1	current_phase = 1	next_phase = 0	reward = -1.041070	array([[-3.6784928, -2.273716 ]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0702 - val_loss: 0.0315

Epoch 2/50

 - 3s - loss: 0.0569 - val_loss: 0.0269

Epoch 3/50

 - 3s - loss: 0.0544 - val_loss: 0.0227

Epoch 4/50

 - 3s - loss: 0.0530 - val_loss: 0.0244

Epoch 5/50

 - 3s - loss: 0.0525 - val_loss: 0.0285

Epoch 6/50

 - 3s - loss: 0.0566 - val_loss: 0.0247

Epoch 7/50

 - 3s - loss: 0.0448 - val_loss: 0.0344

Epoch 8/50

 - 3s - loss: 0.0479 - val_loss: 0.0280

Epoch 9/50

 - 3s - loss: 0.0539 - val_loss: 0.0368

Epoch 10/50

 - 3s - loss: 0.0442 - val_loss: 0.0538

Epoch 11/50

 - 3s - loss: 0.0537 - val_loss: 0.0405

Epoch 12/50

 - 3s - loss: 0.0461 - val_loss: 0.0433

Epoch 13/50

 - 3s - loss: 0.0492 - val_loss: 0.0459

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 27040	action = 0	current_phase = 0	next_phase = 1	reward = 0.247489	array([[-1.6011748, -2.6554565]], dtype=float32)

time = 27045	action = 0	current_phase = 0	next_phase = 1	reward = 0.031160	array([[-2.0135474, -2.8765714]], dtype=float32)

time = 27050	action = 1	current_phase = 0	next_phase = 1	reward = -1.422652	array([[-3.5802903, -2.871763 ]], dtype=float32)

time = 27058	action = 1	current_phase = 1	next_phase = 0	reward = -0.724777	array([[-3.8680668, -2.058338 ]], dtype=float32)

time = 27066	action = 0	current_phase = 0	next_phase = 1	reward = -0.075448	array([[-1.4339991, -2.7660227]], dtype=float32)

time = 27071	action = 0	current_phase = 0	next_phase = 1	reward = -0.012531	array([[-1.7760249, -2.761537 ]], dtype=float32)

time = 27076	action = 0	current_phase = 0	next_phase = 1	reward = 0.051283	array([[-2.2185414, -2.9515069]], dtype=float32)

time = 27081	action = 1	current_phase = 0	next_phase = 1	reward = -1.411676	array([[-4.8557763, -3.0450268]], dtype=float32)

time = 27089	action = 1	current_phase = 1	next_phase = 0	reward = -0.756774	array([[-3.8104503, -2.0070648]], dtype=float32)

time = 27097	action = 0	current_phase = 0	next_phase = 1	reward = -0.063934	array([[-1.5445662, -2.7731   ]], dtype=float32)

time = 27102	action = 0	current_phase = 0	next_phase = 1	reward = 0.019728	array([[-1.9034967, -2.8431287]], dtype=float32)

time = 27107	action = 0	current_phase = 0	next_phase = 1	reward = 0.071498	array([[-2.407164 , -2.9552395]], dtype=float32)

time = 27112	action = 1	current_phase = 0	next_phase = 1	reward = -1.604496	array([[-5.066838 , -3.3394656]], dtype=float32)

time = 27120	action = 1	current_phase = 1	next_phase = 0	reward = -0.963618	array([[-3.8713412, -2.298428 ]], dtype=float32)

time = 27128	action = 0	current_phase = 0	next_phase = 1	reward = -0.053021	array([[-1.5531778, -2.726971 ]], dtype=float32)

time = 27133	action = 0	current_phase = 0	next_phase = 1	reward = 0.023801	array([[-1.9791317, -2.989105 ]], dtype=float32)

time = 27138	action = 0	current_phase = 0	next_phase = 1	reward = 0.080192	array([[-2.6037185, -2.7024498]], dtype=float32)

time = 27143	action = 1	current_phase = 0	next_phase = 1	reward = -1.890459	array([[-4.8905573, -3.8170362]], dtype=float32)

time = 27151	action = 1	current_phase = 1	next_phase = 0	reward = -1.078648	array([[-3.9468637, -2.2557716]], dtype=float32)

time = 27159	action = 0	current_phase = 0	next_phase = 1	reward = -0.035894	array([[-1.3897966, -2.4812837]], dtype=float32)

time = 27164	action = 0	current_phase = 0	next_phase = 1	reward = 0.033026	array([[-1.5544207, -2.7330213]], dtype=float32)

time = 27169	action = 0	current_phase = 0	next_phase = 1	reward = 0.059545	array([[-2.1556046, -2.3263977]], dtype=float32)

time = 27174	action = 1	current_phase = 0	next_phase = 1	reward = -1.116987	array([[-4.8406186, -3.05089  ]], dtype=float32)

time = 27182	action = 1	current_phase = 1	next_phase = 0	reward = -0.740908	array([[-3.834269 , -2.2018871]], dtype=float32)

time = 27190	action = 0	current_phase = 0	next_phase = 1	reward = -0.021683	array([[-1.6317756, -2.6689546]], dtype=float32)

time = 27195	action = 0	current_phase = 0	next_phase = 1	reward = 0.055095	array([[-2.0633347, -2.8842196]], dtype=float32)

time = 27200	action = 1	current_phase = 0	next_phase = 1	reward = -1.378125	array([[-3.6172285, -2.9552104]], dtype=float32)

time = 27208	action = 1	current_phase = 1	next_phase = 0	reward = -0.710156	array([[-3.8209677, -2.0141027]], dtype=float32)

time = 27216	action = 0	current_phase = 0	next_phase = 1	reward = -0.071139	array([[-1.4833801, -2.7983792]], dtype=float32)

time = 27221	action = 0	current_phase = 0	next_phase = 1	reward = -0.012343	array([[-1.8310735, -2.7832563]], dtype=float32)

time = 27226	action = 0	current_phase = 0	next_phase = 1	reward = 0.067903	array([[-2.286686, -2.94901 ]], dtype=float32)

time = 27231	action = 1	current_phase = 0	next_phase = 1	reward = -1.438850	array([[-5.1380506, -3.1326368]], dtype=float32)

time = 27239	action = 1	current_phase = 1	next_phase = 0	reward = -0.816701	array([[-3.8300407, -2.0835257]], dtype=float32)

time = 27247	action = 0	current_phase = 0	next_phase = 1	reward = -0.063961	array([[-1.4870579, -2.849695 ]], dtype=float32)

time = 27252	action = 0	current_phase = 0	next_phase = 1	reward = 0.018249	array([[-1.8566234, -2.8692162]], dtype=float32)

time = 27257	action = 0	current_phase = 0	next_phase = 1	reward = 0.069987	array([[-2.5110953, -3.0161574]], dtype=float32)

time = 27262	action = 1	current_phase = 0	next_phase = 1	reward = -1.682218	array([[-4.9866953, -3.4452012]], dtype=float32)

time = 27270	action = 1	current_phase = 1	next_phase = 0	reward = -1.361677	array([[-3.9187949, -2.2613482]], dtype=float32)

time = 27278	action = 0	current_phase = 0	next_phase = 1	reward = 0.229180	array([[-1.4204681, -2.6897943]], dtype=float32)

time = 27283	action = 0	current_phase = 0	next_phase = 1	reward = 0.001133	array([[-1.9423442, -2.9393744]], dtype=float32)

time = 27288	action = 0	current_phase = 0	next_phase = 1	reward = 0.068659	array([[-2.5788567, -2.8192315]], dtype=float32)

time = 27293	action = 1	current_phase = 0	next_phase = 1	reward = -1.262183	array([[-4.898676 , -3.4705021]], dtype=float32)

time = 27301	action = 1	current_phase = 1	next_phase = 0	reward = -1.368822	array([[-3.5411897, -2.3146708]], dtype=float32)

time = 27309	action = 0	current_phase = 0	next_phase = 1	reward = 0.248718	array([[-1.3864132, -2.5560627]], dtype=float32)

time = 27314	action = 0	current_phase = 0	next_phase = 1	reward = 0.031649	array([[-1.6402972, -2.7526748]], dtype=float32)

time = 27319	action = 1	current_phase = 0	next_phase = 1	reward = -0.651541	array([[-2.2900245, -2.2734816]], dtype=float32)

time = 27327	action = 1	current_phase = 1	next_phase = 0	reward = -1.142199	array([[-3.4814591, -2.0966775]], dtype=float32)

time = 27335	action = 0	current_phase = 0	next_phase = 1	reward = 0.191520	array([[-1.371804, -2.872061]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0643 - val_loss: 0.0411

Epoch 2/50

 - 3s - loss: 0.0708 - val_loss: 0.0444

Epoch 3/50

 - 3s - loss: 0.0670 - val_loss: 0.0361

Epoch 4/50

 - 3s - loss: 0.0687 - val_loss: 0.0353

Epoch 5/50

 - 3s - loss: 0.0606 - val_loss: 0.0408

Epoch 6/50

 - 3s - loss: 0.0619 - val_loss: 0.0351

Epoch 7/50

 - 3s - loss: 0.0593 - val_loss: 0.0383

Epoch 8/50

 - 3s - loss: 0.0491 - val_loss: 0.0377

Epoch 9/50

 - 3s - loss: 0.0669 - val_loss: 0.0401

Epoch 10/50

 - 3s - loss: 0.0535 - val_loss: 0.0377

Epoch 11/50

 - 3s - loss: 0.0505 - val_loss: 0.0412

Epoch 12/50

 - 3s - loss: 0.0539 - val_loss: 0.0418

Epoch 13/50

 - 3s - loss: 0.0642 - val_loss: 0.0353

Epoch 14/50

 - 3s - loss: 0.0548 - val_loss: 0.0390

Epoch 15/50

 - 3s - loss: 0.0475 - val_loss: 0.0331

Epoch 16/50

 - 3s - loss: 0.0503 - val_loss: 0.0410

Epoch 17/50

 - 3s - loss: 0.0458 - val_loss: 0.0319

Epoch 18/50

 - 3s - loss: 0.0578 - val_loss: 0.0358

Epoch 19/50

 - 3s - loss: 0.0463 - val_loss: 0.0393

Epoch 20/50

 - 3s - loss: 0.0510 - val_loss: 0.0346

Epoch 21/50

 - 3s - loss: 0.0741 - val_loss: 0.0364

Epoch 22/50

 - 3s - loss: 0.0543 - val_loss: 0.0339

Epoch 23/50

 - 3s - loss: 0.0410 - val_loss: 0.0340

Epoch 24/50

 - 3s - loss: 0.0488 - val_loss: 0.0297

Epoch 25/50

 - 3s - loss: 0.0561 - val_loss: 0.0376

Epoch 26/50

 - 3s - loss: 0.0476 - val_loss: 0.0401

Epoch 27/50

 - 3s - loss: 0.0646 - val_loss: 0.0373

Epoch 28/50

 - 3s - loss: 0.0497 - val_loss: 0.0302

Epoch 29/50

 - 3s - loss: 0.0530 - val_loss: 0.0334

Epoch 30/50

 - 3s - loss: 0.0518 - val_loss: 0.0347

Epoch 31/50

 - 3s - loss: 0.0515 - val_loss: 0.0359

Epoch 32/50

 - 3s - loss: 0.0499 - val_loss: 0.0392

Epoch 33/50

 - 3s - loss: 0.0466 - val_loss: 0.0336

Epoch 34/50

 - 3s - loss: 0.0433 - val_loss: 0.0384

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 27340	action = 0	current_phase = 0	next_phase = 1	reward = 0.254023	array([[-1.6524446, -2.7163424]], dtype=float32)

time = 27345	action = 0	current_phase = 0	next_phase = 1	reward = -0.246462	array([[-2.2934916, -2.999694 ]], dtype=float32)

time = 27350	action = 1	current_phase = 0	next_phase = 1	reward = -1.027909	array([[-4.0331616, -2.7612169]], dtype=float32)

time = 27358	action = 1	current_phase = 1	next_phase = 0	reward = -0.710785	array([[-3.794977 , -1.8994372]], dtype=float32)

time = 27366	action = 0	current_phase = 0	next_phase = 1	reward = -0.091013	array([[-1.5064921, -2.7734857]], dtype=float32)

time = 27371	action = 0	current_phase = 0	next_phase = 1	reward = -0.005203	array([[-1.7794285, -2.7872672]], dtype=float32)

time = 27376	action = 0	current_phase = 0	next_phase = 1	reward = 0.052207	array([[-2.3324955, -2.9777489]], dtype=float32)

time = 27381	action = 1	current_phase = 0	next_phase = 1	reward = -1.396020	array([[-5.275367, -3.125274]], dtype=float32)

time = 27389	action = 1	current_phase = 1	next_phase = 0	reward = -0.823226	array([[-3.8325863, -1.9572682]], dtype=float32)

time = 27397	action = 0	current_phase = 0	next_phase = 1	reward = -0.081840	array([[-1.4530821, -2.808848 ]], dtype=float32)

time = 27402	action = 0	current_phase = 0	next_phase = 1	reward = 0.010172	array([[-1.8075898, -2.8351219]], dtype=float32)

time = 27407	action = 0	current_phase = 0	next_phase = 1	reward = 0.069531	array([[-2.536858 , -2.9342744]], dtype=float32)

time = 27412	action = 1	current_phase = 0	next_phase = 1	reward = -1.587604	array([[-5.1986184, -3.4998233]], dtype=float32)

time = 27420	action = 1	current_phase = 1	next_phase = 0	reward = -1.604357	array([[-4.0180364, -2.126361 ]], dtype=float32)

time = 27428	action = 0	current_phase = 0	next_phase = 1	reward = 0.519047	array([[-1.4348335, -2.6836107]], dtype=float32)

time = 27433	action = 0	current_phase = 0	next_phase = 1	reward = 0.015004	array([[-2.0378559, -2.9532735]], dtype=float32)

time = 27438	action = 1	current_phase = 0	next_phase = 1	reward = -1.715236	array([[-2.8973186, -2.7574258]], dtype=float32)

time = 27446	action = 1	current_phase = 1	next_phase = 0	reward = -0.827703	array([[-3.9174469, -2.025229 ]], dtype=float32)

time = 27454	action = 0	current_phase = 0	next_phase = 1	reward = -0.107594	array([[-1.2270391, -2.8596914]], dtype=float32)

time = 27459	action = 0	current_phase = 0	next_phase = 1	reward = -0.037064	array([[-1.4285038, -2.52983  ]], dtype=float32)

time = 27464	action = 0	current_phase = 0	next_phase = 1	reward = 0.030296	array([[-2.110126 , -2.9385564]], dtype=float32)

time = 27469	action = 1	current_phase = 0	next_phase = 1	reward = -0.697476	array([[-3.2908795, -2.4924808]], dtype=float32)

time = 27477	action = 1	current_phase = 1	next_phase = 0	reward = -0.861921	array([[-3.7600403, -1.9703507]], dtype=float32)

time = 27485	action = 0	current_phase = 0	next_phase = 1	reward = 0.185227	array([[-1.3608748, -2.861302 ]], dtype=float32)

time = 27490	action = 0	current_phase = 0	next_phase = 1	reward = -0.043065	array([[-1.8364708, -2.7585359]], dtype=float32)

time = 27495	action = 0	current_phase = 0	next_phase = 1	reward = -0.532141	array([[-2.2836719, -2.9717052]], dtype=float32)

time = 27500	action = 1	current_phase = 0	next_phase = 1	reward = -0.753083	array([[-4.1566057, -2.6819634]], dtype=float32)

time = 27508	action = 1	current_phase = 1	next_phase = 0	reward = -0.694400	array([[-3.8293016, -1.8595306]], dtype=float32)

time = 27516	action = 0	current_phase = 0	next_phase = 1	reward = -0.082056	array([[-1.5481026, -2.7814834]], dtype=float32)

time = 27521	action = 0	current_phase = 0	next_phase = 1	reward = 0.004034	array([[-1.793951 , -2.7835789]], dtype=float32)

time = 27526	action = 0	current_phase = 0	next_phase = 1	reward = 0.069592	array([[-2.3324556, -2.9969046]], dtype=float32)

time = 27531	action = 1	current_phase = 0	next_phase = 1	reward = -1.464813	array([[-5.070494 , -3.1916418]], dtype=float32)

time = 27539	action = 1	current_phase = 1	next_phase = 0	reward = -1.144872	array([[-3.876049 , -1.9902334]], dtype=float32)

time = 27547	action = 0	current_phase = 0	next_phase = 1	reward = 0.218426	array([[-1.5121119, -2.7676265]], dtype=float32)

time = 27552	action = 0	current_phase = 0	next_phase = 1	reward = 0.032605	array([[-1.8840387, -2.8454814]], dtype=float32)

time = 27557	action = 0	current_phase = 0	next_phase = 1	reward = 0.091712	array([[-2.604192 , -2.9362864]], dtype=float32)

time = 27562	action = 1	current_phase = 0	next_phase = 1	reward = -1.546835	array([[-5.1344724, -3.4723842]], dtype=float32)

time = 27570	action = 1	current_phase = 1	next_phase = 0	reward = -1.304991	array([[-3.8676724, -2.0881405]], dtype=float32)

time = 27578	action = 0	current_phase = 0	next_phase = 1	reward = 0.237424	array([[-1.4412019, -2.7211046]], dtype=float32)

time = 27583	action = 0	current_phase = 0	next_phase = 1	reward = 0.037746	array([[-1.9488478, -2.903174 ]], dtype=float32)

time = 27588	action = 0	current_phase = 0	next_phase = 1	reward = 0.083640	array([[-2.8243225, -2.8604474]], dtype=float32)

time = 27593	action = 1	current_phase = 0	next_phase = 1	reward = -1.910348	array([[-4.9856944, -3.546877 ]], dtype=float32)

time = 27601	action = 1	current_phase = 1	next_phase = 0	reward = -1.147080	array([[-4.163241 , -2.3125718]], dtype=float32)

time = 27609	action = 0	current_phase = 0	next_phase = 1	reward = -0.036704	array([[-1.2470534, -2.6004746]], dtype=float32)

time = 27614	action = 0	current_phase = 0	next_phase = 1	reward = 0.034953	array([[-1.829551 , -2.8449574]], dtype=float32)

time = 27619	action = 1	current_phase = 0	next_phase = 1	reward = -0.698209	array([[-2.3158648, -2.2892265]], dtype=float32)

time = 27627	action = 1	current_phase = 1	next_phase = 0	reward = -0.932549	array([[-3.341723 , -1.8096325]], dtype=float32)

time = 27635	action = 0	current_phase = 0	next_phase = 1	reward = 0.169566	array([[-1.2336315, -2.8902748]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0627 - val_loss: 0.0386

Epoch 2/50

 - 3s - loss: 0.0598 - val_loss: 0.0373

Epoch 3/50

 - 3s - loss: 0.0537 - val_loss: 0.0353

Epoch 4/50

 - 3s - loss: 0.0585 - val_loss: 0.0442

Epoch 5/50

 - 3s - loss: 0.0536 - val_loss: 0.0378

Epoch 6/50

 - 3s - loss: 0.0661 - val_loss: 0.0387

Epoch 7/50

 - 3s - loss: 0.0504 - val_loss: 0.0427

Epoch 8/50

 - 3s - loss: 0.0502 - val_loss: 0.0466

Epoch 9/50

 - 3s - loss: 0.0593 - val_loss: 0.0426

Epoch 10/50

 - 3s - loss: 0.0476 - val_loss: 0.0350

Epoch 11/50

 - 3s - loss: 0.0469 - val_loss: 0.0462

Epoch 12/50

 - 3s - loss: 0.0424 - val_loss: 0.0380

Epoch 13/50

 - 3s - loss: 0.0503 - val_loss: 0.0444

Epoch 14/50

 - 3s - loss: 0.0465 - val_loss: 0.0380

Epoch 15/50

 - 3s - loss: 0.0508 - val_loss: 0.0523

Epoch 16/50

 - 3s - loss: 0.0500 - val_loss: 0.0401

Epoch 17/50

 - 3s - loss: 0.0461 - val_loss: 0.0366

Epoch 18/50

 - 3s - loss: 0.0490 - val_loss: 0.0457

Epoch 19/50

 - 3s - loss: 0.0527 - val_loss: 0.0373

Epoch 20/50

 - 3s - loss: 0.0428 - val_loss: 0.0443

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 27640	action = 0	current_phase = 0	next_phase = 1	reward = -0.036491	array([[-1.6496702, -2.6504102]], dtype=float32)

time = 27645	action = 0	current_phase = 0	next_phase = 1	reward = -0.241119	array([[-2.1971254, -2.9955826]], dtype=float32)

time = 27650	action = 1	current_phase = 0	next_phase = 1	reward = -0.959014	array([[-4.058433 , -2.6351204]], dtype=float32)

time = 27658	action = 1	current_phase = 1	next_phase = 0	reward = -0.699534	array([[-3.7672348, -1.9851876]], dtype=float32)

time = 27666	action = 0	current_phase = 0	next_phase = 1	reward = -0.094579	array([[-1.5662555, -2.7810812]], dtype=float32)

time = 27671	action = 0	current_phase = 0	next_phase = 1	reward = 0.017041	array([[-1.9131261, -2.8581107]], dtype=float32)

time = 27676	action = 0	current_phase = 0	next_phase = 1	reward = 0.064111	array([[-2.423954, -3.012649]], dtype=float32)

time = 27681	action = 1	current_phase = 0	next_phase = 1	reward = -1.449593	array([[-4.940268, -3.100475]], dtype=float32)

time = 27689	action = 1	current_phase = 1	next_phase = 0	reward = -0.832084	array([[-3.7883024, -1.9792529]], dtype=float32)

time = 27697	action = 0	current_phase = 0	next_phase = 1	reward = -0.065455	array([[-1.5374644, -2.842397 ]], dtype=float32)

time = 27702	action = 0	current_phase = 0	next_phase = 1	reward = 0.027036	array([[-1.8787975, -2.8717334]], dtype=float32)

time = 27707	action = 0	current_phase = 0	next_phase = 1	reward = 0.083072	array([[-2.747908 , -2.8685167]], dtype=float32)

time = 27712	action = 1	current_phase = 0	next_phase = 1	reward = -1.615975	array([[-5.0399027, -3.5848336]], dtype=float32)

time = 27720	action = 1	current_phase = 1	next_phase = 0	reward = -1.015982	array([[-3.972907, -2.236767]], dtype=float32)

time = 27728	action = 0	current_phase = 0	next_phase = 1	reward = -0.053668	array([[-1.3921299, -2.7636418]], dtype=float32)

time = 27733	action = 0	current_phase = 0	next_phase = 1	reward = 0.012336	array([[-2.0927267, -2.9820755]], dtype=float32)

time = 27738	action = 0	current_phase = 0	next_phase = 1	reward = 0.075635	array([[-2.853641 , -2.8858817]], dtype=float32)

time = 27743	action = 1	current_phase = 0	next_phase = 1	reward = -1.377636	array([[-4.987339, -3.501692]], dtype=float32)

time = 27751	action = 1	current_phase = 1	next_phase = 0	reward = -1.399978	array([[-3.8474212, -2.3379774]], dtype=float32)

time = 27759	action = 0	current_phase = 0	next_phase = 1	reward = 0.223560	array([[-1.3502648, -2.5356536]], dtype=float32)

time = 27764	action = 0	current_phase = 0	next_phase = 1	reward = 0.007224	array([[-1.7889521, -2.8390331]], dtype=float32)

time = 27769	action = 1	current_phase = 0	next_phase = 1	reward = -0.714257	array([[-2.6241221, -2.4105363]], dtype=float32)

time = 27777	action = 1	current_phase = 1	next_phase = 0	reward = -0.920141	array([[-3.538198 , -1.8481101]], dtype=float32)

time = 27785	action = 0	current_phase = 0	next_phase = 1	reward = 0.186817	array([[-1.2407758, -2.9290116]], dtype=float32)

time = 27790	action = 0	current_phase = 0	next_phase = 1	reward = -0.024039	array([[-1.6499238, -2.5539258]], dtype=float32)

time = 27795	action = 0	current_phase = 0	next_phase = 1	reward = -0.233280	array([[-2.2798786, -3.0729747]], dtype=float32)

time = 27800	action = 1	current_phase = 0	next_phase = 1	reward = -1.084705	array([[-4.0936804, -2.6797614]], dtype=float32)

time = 27808	action = 1	current_phase = 1	next_phase = 0	reward = -0.720805	array([[-3.8813858, -2.0104976]], dtype=float32)

time = 27816	action = 0	current_phase = 0	next_phase = 1	reward = -0.076825	array([[-1.5098146, -2.759276 ]], dtype=float32)

time = 27821	action = 0	current_phase = 0	next_phase = 1	reward = -0.011643	array([[-1.7728246, -2.78955  ]], dtype=float32)

time = 27826	action = 0	current_phase = 0	next_phase = 1	reward = 0.069416	array([[-2.4021664, -3.0158873]], dtype=float32)

time = 27831	action = 1	current_phase = 0	next_phase = 1	reward = -1.439370	array([[-5.24817 , -3.029721]], dtype=float32)

time = 27839	action = 1	current_phase = 1	next_phase = 0	reward = -0.854323	array([[-3.8463101, -2.050507 ]], dtype=float32)

time = 27847	action = 0	current_phase = 0	next_phase = 1	reward = -0.049519	array([[-1.5187346, -2.7868829]], dtype=float32)

time = 27852	action = 0	current_phase = 0	next_phase = 1	reward = 0.017103	array([[-1.909559, -2.881896]], dtype=float32)

time = 27857	action = 0	current_phase = 0	next_phase = 1	reward = 0.073875	array([[-2.6026268, -2.9757302]], dtype=float32)

time = 27862	action = 1	current_phase = 0	next_phase = 1	reward = -1.565863	array([[-5.103268, -3.575619]], dtype=float32)

time = 27870	action = 1	current_phase = 1	next_phase = 0	reward = -0.943645	array([[-3.8618913, -2.1862645]], dtype=float32)

time = 27878	action = 0	current_phase = 0	next_phase = 1	reward = -0.047858	array([[-1.4256929, -2.7327828]], dtype=float32)

time = 27883	action = 0	current_phase = 0	next_phase = 1	reward = 0.039858	array([[-1.9755608, -2.9550426]], dtype=float32)

time = 27888	action = 0	current_phase = 0	next_phase = 1	reward = 0.077835	array([[-2.8539188, -2.8833141]], dtype=float32)

time = 27893	action = 1	current_phase = 0	next_phase = 1	reward = -1.947497	array([[-4.9622817, -3.682507 ]], dtype=float32)

time = 27901	action = 1	current_phase = 1	next_phase = 0	reward = -1.090277	array([[-4.101317 , -2.3116374]], dtype=float32)

time = 27909	action = 0	current_phase = 0	next_phase = 1	reward = -0.036998	array([[-1.368599 , -2.5413525]], dtype=float32)

time = 27914	action = 0	current_phase = 0	next_phase = 1	reward = 0.033103	array([[-1.7268268, -2.7856803]], dtype=float32)

time = 27919	action = 1	current_phase = 0	next_phase = 1	reward = -0.643115	array([[-2.4246175, -2.344163 ]], dtype=float32)

time = 27927	action = 1	current_phase = 1	next_phase = 0	reward = -0.648497	array([[-2.9966576, -1.807366 ]], dtype=float32)

time = 27935	action = 0	current_phase = 0	next_phase = 1	reward = -0.384432	array([[-1.270718 , -2.8633473]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0663 - val_loss: 0.0206

Epoch 2/50

 - 3s - loss: 0.0643 - val_loss: 0.0240

Epoch 3/50

 - 3s - loss: 0.0670 - val_loss: 0.0213

Epoch 4/50

 - 3s - loss: 0.0569 - val_loss: 0.0205

Epoch 5/50

 - 3s - loss: 0.0526 - val_loss: 0.0206

Epoch 6/50

 - 3s - loss: 0.0557 - val_loss: 0.0227

Epoch 7/50

 - 3s - loss: 0.0647 - val_loss: 0.0222

Epoch 8/50

 - 3s - loss: 0.0478 - val_loss: 0.0346

Epoch 9/50

 - 3s - loss: 0.0516 - val_loss: 0.0296

Epoch 10/50

 - 3s - loss: 0.0581 - val_loss: 0.0210

Epoch 11/50

 - 3s - loss: 0.0499 - val_loss: 0.0206

Epoch 12/50

 - 3s - loss: 0.0504 - val_loss: 0.0241

Epoch 13/50

 - 3s - loss: 0.0559 - val_loss: 0.0325

Epoch 14/50

 - 3s - loss: 0.0504 - val_loss: 0.0219

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 27940	action = 0	current_phase = 0	next_phase = 1	reward = 0.255101	array([[-1.5872831, -2.5858192]], dtype=float32)

time = 27945	action = 0	current_phase = 0	next_phase = 1	reward = 0.035507	array([[-2.1951017, -2.980844 ]], dtype=float32)

time = 27950	action = 1	current_phase = 0	next_phase = 1	reward = -1.214374	array([[-4.108276 , -2.7019436]], dtype=float32)

time = 27958	action = 1	current_phase = 1	next_phase = 0	reward = -0.701241	array([[-3.6673982, -1.9068366]], dtype=float32)

time = 27966	action = 0	current_phase = 0	next_phase = 1	reward = -0.084613	array([[-1.4148319, -2.8216715]], dtype=float32)

time = 27971	action = 0	current_phase = 0	next_phase = 1	reward = -0.029279	array([[-1.8167037, -2.7940564]], dtype=float32)

time = 27976	action = 0	current_phase = 0	next_phase = 1	reward = 0.049697	array([[-2.347117 , -2.9828396]], dtype=float32)

time = 27981	action = 1	current_phase = 0	next_phase = 1	reward = -1.440839	array([[-5.0173364, -3.1195965]], dtype=float32)

time = 27989	action = 1	current_phase = 1	next_phase = 0	reward = -0.756801	array([[-3.90679  , -2.0238886]], dtype=float32)

time = 27997	action = 0	current_phase = 0	next_phase = 1	reward = -0.055203	array([[-1.5322418, -2.8041365]], dtype=float32)

time = 28002	action = 0	current_phase = 0	next_phase = 1	reward = 0.018547	array([[-1.8548968, -2.8547316]], dtype=float32)

time = 28007	action = 0	current_phase = 0	next_phase = 1	reward = 0.074920	array([[-2.71708 , -2.847166]], dtype=float32)

time = 28012	action = 1	current_phase = 0	next_phase = 1	reward = -1.623169	array([[-5.149002, -3.42108 ]], dtype=float32)

time = 28020	action = 1	current_phase = 1	next_phase = 0	reward = -1.265245	array([[-3.9695795, -2.1774938]], dtype=float32)

time = 28028	action = 0	current_phase = 0	next_phase = 1	reward = 0.219513	array([[-1.3546828, -2.7196393]], dtype=float32)

time = 28033	action = 0	current_phase = 0	next_phase = 1	reward = 0.011797	array([[-1.8798585, -2.9292836]], dtype=float32)

time = 28038	action = 0	current_phase = 0	next_phase = 1	reward = 0.080786	array([[-2.5893316, -2.7224264]], dtype=float32)

time = 28043	action = 1	current_phase = 0	next_phase = 1	reward = -1.283579	array([[-4.9779987, -3.2717042]], dtype=float32)

time = 28051	action = 1	current_phase = 1	next_phase = 0	reward = -1.123848	array([[-3.884978 , -2.1961353]], dtype=float32)

time = 28059	action = 0	current_phase = 0	next_phase = 1	reward = -0.032293	array([[-1.3326937, -2.5833035]], dtype=float32)

time = 28064	action = 0	current_phase = 0	next_phase = 1	reward = 0.037060	array([[-1.6738307, -2.790701 ]], dtype=float32)

time = 28069	action = 1	current_phase = 0	next_phase = 1	reward = -0.632116	array([[-2.5682688, -2.5057695]], dtype=float32)

time = 28077	action = 1	current_phase = 1	next_phase = 0	reward = -0.907930	array([[-3.464353 , -1.9563056]], dtype=float32)

time = 28085	action = 0	current_phase = 0	next_phase = 1	reward = 0.193152	array([[-1.3197973, -2.876998 ]], dtype=float32)

time = 28090	action = 0	current_phase = 0	next_phase = 1	reward = -0.020060	array([[-1.6258147, -2.684723 ]], dtype=float32)

time = 28095	action = 0	current_phase = 0	next_phase = 1	reward = 0.053020	array([[-2.2449973, -2.9746518]], dtype=float32)

time = 28100	action = 1	current_phase = 0	next_phase = 1	reward = -1.427222	array([[-3.9783468, -2.8511853]], dtype=float32)

time = 28108	action = 1	current_phase = 1	next_phase = 0	reward = -0.713807	array([[-3.9664853, -1.9952627]], dtype=float32)

time = 28116	action = 0	current_phase = 0	next_phase = 1	reward = -0.086510	array([[-1.5157974, -2.7641392]], dtype=float32)

time = 28121	action = 0	current_phase = 0	next_phase = 1	reward = 0.002492	array([[-1.6884661, -2.754242 ]], dtype=float32)

time = 28126	action = 0	current_phase = 0	next_phase = 1	reward = 0.055050	array([[-2.3661032, -2.9899964]], dtype=float32)

time = 28131	action = 1	current_phase = 0	next_phase = 1	reward = -1.490720	array([[-5.1798663, -3.1071038]], dtype=float32)

time = 28139	action = 1	current_phase = 1	next_phase = 0	reward = -0.764499	array([[-3.8328288, -1.966383 ]], dtype=float32)

time = 28147	action = 0	current_phase = 0	next_phase = 1	reward = -0.055943	array([[-1.4900923, -2.8751373]], dtype=float32)

time = 28152	action = 0	current_phase = 0	next_phase = 1	reward = 0.027656	array([[-1.7998084, -2.8116674]], dtype=float32)

time = 28157	action = 0	current_phase = 0	next_phase = 1	reward = 0.074079	array([[-2.3656945, -3.0065436]], dtype=float32)

time = 28162	action = 1	current_phase = 0	next_phase = 1	reward = -1.689479	array([[-5.090983 , -3.4411397]], dtype=float32)

time = 28170	action = 1	current_phase = 1	next_phase = 0	reward = -1.011071	array([[-3.994579 , -2.2255158]], dtype=float32)

time = 28178	action = 0	current_phase = 0	next_phase = 1	reward = -0.048928	array([[-1.3572391, -2.7291555]], dtype=float32)

time = 28183	action = 0	current_phase = 0	next_phase = 1	reward = 0.008733	array([[-1.9134204, -2.9273176]], dtype=float32)

time = 28188	action = 0	current_phase = 0	next_phase = 1	reward = 0.073148	array([[-2.6828837, -2.7132096]], dtype=float32)

time = 28193	action = 1	current_phase = 0	next_phase = 1	reward = -1.910849	array([[-5.004074 , -3.4464188]], dtype=float32)

time = 28201	action = 1	current_phase = 1	next_phase = 0	reward = -1.040606	array([[-4.2119646, -2.219181 ]], dtype=float32)

time = 28209	action = 0	current_phase = 0	next_phase = 1	reward = -0.039659	array([[-1.22417  , -2.5989633]], dtype=float32)

time = 28214	action = 0	current_phase = 0	next_phase = 1	reward = 0.020721	array([[-1.715922 , -2.8306112]], dtype=float32)

time = 28219	action = 1	current_phase = 0	next_phase = 1	reward = -0.683447	array([[-2.4170208, -2.3277698]], dtype=float32)

time = 28227	action = 1	current_phase = 1	next_phase = 0	reward = -0.918261	array([[-3.3314712, -1.9938422]], dtype=float32)

time = 28235	action = 0	current_phase = 0	next_phase = 1	reward = 0.183871	array([[-1.1760852, -2.9840913]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0599 - val_loss: 0.0328

Epoch 2/50

 - 3s - loss: 0.0654 - val_loss: 0.0275

Epoch 3/50

 - 3s - loss: 0.0594 - val_loss: 0.0265

Epoch 4/50

 - 3s - loss: 0.0539 - val_loss: 0.0280

Epoch 5/50

 - 3s - loss: 0.0541 - val_loss: 0.0293

Epoch 6/50

 - 3s - loss: 0.0576 - val_loss: 0.0300

Epoch 7/50

 - 3s - loss: 0.0564 - val_loss: 0.0317

Epoch 8/50

 - 3s - loss: 0.0507 - val_loss: 0.0349

Epoch 9/50

 - 3s - loss: 0.0543 - val_loss: 0.0332

Epoch 10/50

 - 3s - loss: 0.0456 - val_loss: 0.0292

Epoch 11/50

 - 3s - loss: 0.0568 - val_loss: 0.0287

Epoch 12/50

 - 3s - loss: 0.0545 - val_loss: 0.0319

Epoch 13/50

 - 3s - loss: 0.0480 - val_loss: 0.0418

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 28240	action = 0	current_phase = 0	next_phase = 1	reward = -0.025437	array([[-1.8059489, -2.5332444]], dtype=float32)

time = 28245	action = 0	current_phase = 0	next_phase = 1	reward = 0.037639	array([[-2.2717507, -2.9733186]], dtype=float32)

time = 28250	action = 1	current_phase = 0	next_phase = 1	reward = -1.327720	array([[-4.01213 , -2.774997]], dtype=float32)

time = 28258	action = 1	current_phase = 1	next_phase = 0	reward = -0.705080	array([[-3.913269 , -1.9748396]], dtype=float32)

time = 28266	action = 0	current_phase = 0	next_phase = 1	reward = -0.104500	array([[-1.6145557, -2.716895 ]], dtype=float32)

time = 28271	action = 0	current_phase = 0	next_phase = 1	reward = -0.029146	array([[-1.7993714, -2.7353146]], dtype=float32)

time = 28276	action = 0	current_phase = 0	next_phase = 1	reward = 0.060919	array([[-2.3888748, -2.9683602]], dtype=float32)

time = 28281	action = 1	current_phase = 0	next_phase = 1	reward = -1.370481	array([[-5.0332813, -3.1950562]], dtype=float32)

time = 28289	action = 1	current_phase = 1	next_phase = 0	reward = -0.783593	array([[-3.8920374, -2.045442 ]], dtype=float32)

time = 28297	action = 0	current_phase = 0	next_phase = 1	reward = -0.075490	array([[-1.5040379, -2.739389 ]], dtype=float32)

time = 28302	action = 0	current_phase = 0	next_phase = 1	reward = 0.007604	array([[-1.9505234, -2.8265529]], dtype=float32)

time = 28307	action = 0	current_phase = 0	next_phase = 1	reward = 0.073865	array([[-2.4309087, -2.9525542]], dtype=float32)

time = 28312	action = 1	current_phase = 0	next_phase = 1	reward = -1.602306	array([[-5.2081823, -3.253547 ]], dtype=float32)

time = 28320	action = 1	current_phase = 1	next_phase = 0	reward = -1.054480	array([[-4.02032  , -2.3079972]], dtype=float32)

time = 28328	action = 0	current_phase = 0	next_phase = 1	reward = -0.048326	array([[-1.458352, -2.686331]], dtype=float32)

time = 28333	action = 0	current_phase = 0	next_phase = 1	reward = 0.035685	array([[-1.9275506, -2.8777113]], dtype=float32)

time = 28338	action = 0	current_phase = 0	next_phase = 1	reward = 0.078192	array([[-2.749456, -2.752681]], dtype=float32)

time = 28343	action = 1	current_phase = 0	next_phase = 1	reward = -1.349725	array([[-4.970126 , -3.4185987]], dtype=float32)

time = 28351	action = 1	current_phase = 1	next_phase = 0	reward = -1.452964	array([[-3.9390464, -2.2914107]], dtype=float32)

time = 28359	action = 0	current_phase = 0	next_phase = 1	reward = 0.268039	array([[-1.3644994, -2.5996957]], dtype=float32)

time = 28364	action = 0	current_phase = 0	next_phase = 1	reward = 0.039895	array([[-1.7972429, -2.8125885]], dtype=float32)

time = 28369	action = 0	current_phase = 0	next_phase = 1	reward = 0.079464	array([[-2.325814, -2.45198 ]], dtype=float32)

time = 28374	action = 1	current_phase = 0	next_phase = 1	reward = -1.164424	array([[-4.853072, -2.966015]], dtype=float32)

time = 28382	action = 1	current_phase = 1	next_phase = 0	reward = -0.771598	array([[-4.1001267, -2.128036 ]], dtype=float32)

time = 28390	action = 0	current_phase = 0	next_phase = 1	reward = -0.290482	array([[-1.6227498, -2.651987 ]], dtype=float32)

time = 28395	action = 0	current_phase = 0	next_phase = 1	reward = 0.336524	array([[-2.150671 , -2.8786762]], dtype=float32)

time = 28400	action = 1	current_phase = 0	next_phase = 1	reward = -1.409194	array([[-3.8608048, -2.7692256]], dtype=float32)

time = 28408	action = 1	current_phase = 1	next_phase = 0	reward = -0.720891	array([[-3.817586 , -1.9162731]], dtype=float32)

time = 28416	action = 0	current_phase = 0	next_phase = 1	reward = -0.099427	array([[-1.5991445, -2.7443223]], dtype=float32)

time = 28421	action = 0	current_phase = 0	next_phase = 1	reward = -0.015490	array([[-1.7725776, -2.704186 ]], dtype=float32)

time = 28426	action = 0	current_phase = 0	next_phase = 1	reward = 0.041852	array([[-2.3143635, -2.9192386]], dtype=float32)

time = 28431	action = 1	current_phase = 0	next_phase = 1	reward = -1.376277	array([[-5.050699, -3.074128]], dtype=float32)

time = 28439	action = 1	current_phase = 1	next_phase = 0	reward = -1.121369	array([[-3.8926568, -2.0391068]], dtype=float32)

time = 28447	action = 0	current_phase = 0	next_phase = 1	reward = 0.236652	array([[-1.4972106, -2.7292442]], dtype=float32)

time = 28452	action = 0	current_phase = 0	next_phase = 1	reward = 0.017033	array([[-1.9748259, -2.8418915]], dtype=float32)

time = 28457	action = 0	current_phase = 0	next_phase = 1	reward = 0.064446	array([[-2.6849217, -2.896447 ]], dtype=float32)

time = 28462	action = 1	current_phase = 0	next_phase = 1	reward = -1.629346	array([[-5.181575 , -3.4283826]], dtype=float32)

time = 28470	action = 1	current_phase = 1	next_phase = 0	reward = -0.941210	array([[-3.9595547, -2.1889358]], dtype=float32)

time = 28478	action = 0	current_phase = 0	next_phase = 1	reward = -0.043017	array([[-1.5070238, -2.6795852]], dtype=float32)

time = 28483	action = 0	current_phase = 0	next_phase = 1	reward = 0.037130	array([[-2.0041437, -2.9589722]], dtype=float32)

time = 28488	action = 0	current_phase = 0	next_phase = 1	reward = 0.072842	array([[-2.7184606, -2.8173175]], dtype=float32)

time = 28493	action = 1	current_phase = 0	next_phase = 1	reward = -1.411888	array([[-4.9685173, -3.3198593]], dtype=float32)

time = 28501	action = 1	current_phase = 1	next_phase = 0	reward = -1.393823	array([[-3.8817868, -2.359456 ]], dtype=float32)

time = 28509	action = 0	current_phase = 0	next_phase = 1	reward = 0.256554	array([[-1.4250668, -2.5356   ]], dtype=float32)

time = 28514	action = 0	current_phase = 0	next_phase = 1	reward = 0.022456	array([[-1.870135 , -2.7650576]], dtype=float32)

time = 28519	action = 0	current_phase = 0	next_phase = 1	reward = 0.056890	array([[-2.3747306, -2.4145408]], dtype=float32)

time = 28524	action = 1	current_phase = 0	next_phase = 1	reward = -1.008492	array([[-4.6129055, -2.9976175]], dtype=float32)

time = 28532	action = 1	current_phase = 1	next_phase = 0	reward = -1.052023	array([[-3.77348 , -2.189982]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0535 - val_loss: 0.0325

Epoch 2/50

 - 3s - loss: 0.0609 - val_loss: 0.0290

Epoch 3/50

 - 3s - loss: 0.0591 - val_loss: 0.0244

Epoch 4/50

 - 3s - loss: 0.0545 - val_loss: 0.0285

Epoch 5/50

 - 3s - loss: 0.0575 - val_loss: 0.0269

Epoch 6/50

 - 3s - loss: 0.0580 - val_loss: 0.0309

Epoch 7/50

 - 3s - loss: 0.0476 - val_loss: 0.0342

Epoch 8/50

 - 3s - loss: 0.0496 - val_loss: 0.0308

Epoch 9/50

 - 3s - loss: 0.0626 - val_loss: 0.0304

Epoch 10/50

 - 3s - loss: 0.0583 - val_loss: 0.0260

Epoch 11/50

 - 3s - loss: 0.0452 - val_loss: 0.0308

Epoch 12/50

 - 3s - loss: 0.0536 - val_loss: 0.0360

Epoch 13/50

 - 3s - loss: 0.0384 - val_loss: 0.0264

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 28540	action = 0	current_phase = 0	next_phase = 1	reward = 0.246186	array([[-1.6725371, -2.6633058]], dtype=float32)

time = 28545	action = 0	current_phase = 0	next_phase = 1	reward = 0.034053	array([[-2.0389783, -2.8820777]], dtype=float32)

time = 28550	action = 1	current_phase = 0	next_phase = 1	reward = -1.265555	array([[-3.7869818, -2.789176 ]], dtype=float32)

time = 28558	action = 1	current_phase = 1	next_phase = 0	reward = -0.700996	array([[-3.7012882, -1.8995209]], dtype=float32)

time = 28566	action = 0	current_phase = 0	next_phase = 1	reward = -0.079295	array([[-1.4893851, -2.7536619]], dtype=float32)

time = 28571	action = 0	current_phase = 0	next_phase = 1	reward = 0.002132	array([[-1.7497191, -2.7297153]], dtype=float32)

time = 28576	action = 0	current_phase = 0	next_phase = 1	reward = 0.062098	array([[-2.343258, -2.978156]], dtype=float32)

time = 28581	action = 1	current_phase = 0	next_phase = 1	reward = -1.546405	array([[-5.076719 , -3.1692636]], dtype=float32)

time = 28589	action = 1	current_phase = 1	next_phase = 0	reward = -0.788639	array([[-3.929754 , -2.0357423]], dtype=float32)

time = 28597	action = 0	current_phase = 0	next_phase = 1	reward = -0.052592	array([[-1.513845 , -2.7411215]], dtype=float32)

time = 28602	action = 0	current_phase = 0	next_phase = 1	reward = 0.030538	array([[-1.8401706, -2.8068676]], dtype=float32)

time = 28607	action = 0	current_phase = 0	next_phase = 1	reward = 0.082084	array([[-2.6009855, -2.9388008]], dtype=float32)

time = 28612	action = 1	current_phase = 0	next_phase = 1	reward = -1.586729	array([[-5.1533666, -3.4226785]], dtype=float32)

time = 28620	action = 1	current_phase = 1	next_phase = 0	reward = -0.937331	array([[-3.9181132, -2.137952 ]], dtype=float32)

time = 28628	action = 0	current_phase = 0	next_phase = 1	reward = -0.048352	array([[-1.3145139, -2.7547538]], dtype=float32)

time = 28633	action = 0	current_phase = 0	next_phase = 1	reward = 0.023450	array([[-1.9271038, -2.9721751]], dtype=float32)

time = 28638	action = 0	current_phase = 0	next_phase = 1	reward = 0.071540	array([[-2.753021, -2.786942]], dtype=float32)

time = 28643	action = 1	current_phase = 0	next_phase = 1	reward = -1.939919	array([[-4.924549 , -3.7646136]], dtype=float32)

time = 28651	action = 1	current_phase = 1	next_phase = 0	reward = -1.047540	array([[-4.292159 , -2.3416126]], dtype=float32)

time = 28659	action = 0	current_phase = 0	next_phase = 1	reward = -0.046956	array([[-1.3071122, -2.5430534]], dtype=float32)

time = 28664	action = 0	current_phase = 0	next_phase = 1	reward = 0.016217	array([[-1.7080951, -2.7699904]], dtype=float32)

time = 28669	action = 0	current_phase = 0	next_phase = 1	reward = 0.068833	array([[-2.3384092, -2.4826334]], dtype=float32)

time = 28674	action = 1	current_phase = 0	next_phase = 1	reward = -0.993208	array([[-4.9720516, -2.9264362]], dtype=float32)

time = 28682	action = 1	current_phase = 1	next_phase = 0	reward = -0.732689	array([[-3.8944728, -2.1597261]], dtype=float32)

time = 28690	action = 0	current_phase = 0	next_phase = 1	reward = -0.013833	array([[-1.7654109, -2.5891871]], dtype=float32)

time = 28695	action = 0	current_phase = 0	next_phase = 1	reward = 0.053929	array([[-2.2196882, -2.920772 ]], dtype=float32)

time = 28700	action = 1	current_phase = 0	next_phase = 1	reward = -1.311903	array([[-3.9353442, -2.8753679]], dtype=float32)

time = 28708	action = 1	current_phase = 1	next_phase = 0	reward = -0.709215	array([[-3.8387136, -1.9256908]], dtype=float32)

time = 28716	action = 0	current_phase = 0	next_phase = 1	reward = -0.075423	array([[-1.5810754, -2.7356763]], dtype=float32)

time = 28721	action = 0	current_phase = 0	next_phase = 1	reward = 0.002978	array([[-1.8637381, -2.7444758]], dtype=float32)

time = 28726	action = 0	current_phase = 0	next_phase = 1	reward = 0.071310	array([[-2.4479966, -2.9545147]], dtype=float32)

time = 28731	action = 1	current_phase = 0	next_phase = 1	reward = -1.445630	array([[-5.084746 , -3.1080904]], dtype=float32)

time = 28739	action = 1	current_phase = 1	next_phase = 0	reward = -0.826131	array([[-3.8698792, -1.9352332]], dtype=float32)

time = 28747	action = 0	current_phase = 0	next_phase = 1	reward = -0.065233	array([[-1.5012319, -2.7116034]], dtype=float32)

time = 28752	action = 0	current_phase = 0	next_phase = 1	reward = 0.019766	array([[-1.9219847, -2.846606 ]], dtype=float32)

time = 28757	action = 0	current_phase = 0	next_phase = 1	reward = 0.076064	array([[-2.567945 , -2.9965534]], dtype=float32)

time = 28762	action = 1	current_phase = 0	next_phase = 1	reward = -1.676742	array([[-5.1297927, -3.3673942]], dtype=float32)

time = 28770	action = 1	current_phase = 1	next_phase = 0	reward = -0.996052	array([[-3.9004452, -2.1822784]], dtype=float32)

time = 28778	action = 0	current_phase = 0	next_phase = 1	reward = -0.056569	array([[-1.5712191, -2.7128532]], dtype=float32)

time = 28783	action = 0	current_phase = 0	next_phase = 1	reward = 0.019429	array([[-1.9493732, -2.93283  ]], dtype=float32)

time = 28788	action = 0	current_phase = 0	next_phase = 1	reward = 0.078870	array([[-2.5050256, -2.9513333]], dtype=float32)

time = 28793	action = 1	current_phase = 0	next_phase = 1	reward = -1.270642	array([[-4.939115, -3.147606]], dtype=float32)

time = 28801	action = 1	current_phase = 1	next_phase = 0	reward = -1.136623	array([[-3.9109201, -2.363868 ]], dtype=float32)

time = 28809	action = 0	current_phase = 0	next_phase = 1	reward = -0.050946	array([[-1.3527168, -2.6169322]], dtype=float32)

time = 28814	action = 0	current_phase = 0	next_phase = 1	reward = 0.017531	array([[-1.7845507, -2.8165073]], dtype=float32)

time = 28819	action = 0	current_phase = 0	next_phase = 1	reward = 0.077031	array([[-2.2969642, -2.3705747]], dtype=float32)

time = 28824	action = 1	current_phase = 0	next_phase = 1	reward = -0.957044	array([[-4.9205112, -3.0293243]], dtype=float32)

time = 28832	action = 1	current_phase = 1	next_phase = 0	reward = -0.707548	array([[-3.7372088, -2.0851064]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0724 - val_loss: 0.0371

Epoch 2/50

 - 3s - loss: 0.0786 - val_loss: 0.0397

Epoch 3/50

 - 3s - loss: 0.0634 - val_loss: 0.0347

Epoch 4/50

 - 3s - loss: 0.0672 - val_loss: 0.0393

Epoch 5/50

 - 3s - loss: 0.0532 - val_loss: 0.0370

Epoch 6/50

 - 3s - loss: 0.0590 - val_loss: 0.0492

Epoch 7/50

 - 3s - loss: 0.0571 - val_loss: 0.0405

Epoch 8/50

 - 3s - loss: 0.0707 - val_loss: 0.0446

Epoch 9/50

 - 3s - loss: 0.0567 - val_loss: 0.0516

Epoch 10/50

 - 3s - loss: 0.0612 - val_loss: 0.0321

Epoch 11/50

 - 3s - loss: 0.0483 - val_loss: 0.0429

Epoch 12/50

 - 3s - loss: 0.0532 - val_loss: 0.0424

Epoch 13/50

 - 3s - loss: 0.0502 - val_loss: 0.0402

Epoch 14/50

 - 3s - loss: 0.0461 - val_loss: 0.0438

Epoch 15/50

 - 3s - loss: 0.0457 - val_loss: 0.0412

Epoch 16/50

 - 3s - loss: 0.0563 - val_loss: 0.0322

Epoch 17/50

 - 3s - loss: 0.0562 - val_loss: 0.0345

Epoch 18/50

 - 3s - loss: 0.0555 - val_loss: 0.0364

Epoch 19/50

 - 3s - loss: 0.0467 - val_loss: 0.0376

Epoch 20/50

 - 3s - loss: 0.0518 - val_loss: 0.0419

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 28840	action = 0	current_phase = 0	next_phase = 1	reward = -0.304637	array([[-1.6881521, -2.5783424]], dtype=float32)

time = 28845	action = 0	current_phase = 0	next_phase = 1	reward = 0.322847	array([[-2.2305682, -2.934811 ]], dtype=float32)

time = 28850	action = 1	current_phase = 0	next_phase = 1	reward = -1.255439	array([[-4.030196 , -2.7498064]], dtype=float32)

time = 28858	action = 1	current_phase = 1	next_phase = 0	reward = -0.703720	array([[-3.6859248, -1.9607769]], dtype=float32)

time = 28866	action = 0	current_phase = 0	next_phase = 1	reward = -0.096231	array([[-1.5463874, -2.7594986]], dtype=float32)

time = 28871	action = 0	current_phase = 0	next_phase = 1	reward = -0.008432	array([[-1.8412038, -2.7335377]], dtype=float32)

time = 28876	action = 0	current_phase = 0	next_phase = 1	reward = 0.059325	array([[-2.361907 , -2.9688673]], dtype=float32)

time = 28881	action = 1	current_phase = 0	next_phase = 1	reward = -1.491959	array([[-5.28539 , -3.044806]], dtype=float32)

time = 28889	action = 1	current_phase = 1	next_phase = 0	reward = -0.839322	array([[-3.8597708, -2.0195014]], dtype=float32)

time = 28897	action = 0	current_phase = 0	next_phase = 1	reward = -0.076416	array([[-1.5606246, -2.8136663]], dtype=float32)

time = 28902	action = 0	current_phase = 0	next_phase = 1	reward = -0.034556	array([[-2.073808, -2.871183]], dtype=float32)

time = 28907	action = 0	current_phase = 0	next_phase = 1	reward = 0.042338	array([[-2.4149754, -2.942819 ]], dtype=float32)

time = 28912	action = 1	current_phase = 0	next_phase = 1	reward = -1.496952	array([[-5.173842, -3.350431]], dtype=float32)

time = 28920	action = 1	current_phase = 1	next_phase = 0	reward = -1.003206	array([[-3.9441047, -2.1396956]], dtype=float32)

time = 28928	action = 0	current_phase = 0	next_phase = 1	reward = -0.068378	array([[-1.6104105, -2.741969 ]], dtype=float32)

time = 28933	action = 0	current_phase = 0	next_phase = 1	reward = 0.020695	array([[-2.0141532, -2.9696474]], dtype=float32)

time = 28938	action = 0	current_phase = 0	next_phase = 1	reward = 0.066282	array([[-2.686238, -2.824037]], dtype=float32)

time = 28943	action = 1	current_phase = 0	next_phase = 1	reward = -1.760162	array([[-5.0095024, -3.5358582]], dtype=float32)

time = 28951	action = 1	current_phase = 1	next_phase = 0	reward = -0.960096	array([[-4.178057, -2.252832]], dtype=float32)

time = 28959	action = 0	current_phase = 0	next_phase = 1	reward = -0.033842	array([[-1.2780036, -2.5197906]], dtype=float32)

time = 28964	action = 0	current_phase = 0	next_phase = 1	reward = 0.030884	array([[-1.729717 , -2.7894158]], dtype=float32)

time = 28969	action = 0	current_phase = 0	next_phase = 1	reward = 0.053308	array([[-2.4220796, -2.470334 ]], dtype=float32)

time = 28974	action = 1	current_phase = 0	next_phase = 1	reward = -1.151539	array([[-4.97528  , -2.9662619]], dtype=float32)

time = 28982	action = 1	current_phase = 1	next_phase = 0	reward = -1.028366	array([[-3.8213608, -2.1952562]], dtype=float32)

time = 28990	action = 0	current_phase = 0	next_phase = 1	reward = 0.260488	array([[-1.6744525, -2.6102953]], dtype=float32)

time = 28995	action = 0	current_phase = 0	next_phase = 1	reward = -0.222243	array([[-2.2069342, -2.8862505]], dtype=float32)

time = 29000	action = 1	current_phase = 0	next_phase = 1	reward = -1.023475	array([[-3.7581363, -2.692288 ]], dtype=float32)

time = 29008	action = 1	current_phase = 1	next_phase = 0	reward = -0.727797	array([[-3.8446703, -2.006836 ]], dtype=float32)

time = 29016	action = 0	current_phase = 0	next_phase = 1	reward = -0.087082	array([[-1.5669099, -2.736073 ]], dtype=float32)

time = 29021	action = 0	current_phase = 0	next_phase = 1	reward = 0.005165	array([[-1.8226362, -2.7208424]], dtype=float32)

time = 29026	action = 0	current_phase = 0	next_phase = 1	reward = 0.065046	array([[-2.3992436, -2.9792871]], dtype=float32)

time = 29031	action = 1	current_phase = 0	next_phase = 1	reward = -1.461059	array([[-5.313146, -3.05088 ]], dtype=float32)

time = 29039	action = 1	current_phase = 1	next_phase = 0	reward = -0.768684	array([[-3.9477808, -2.068359 ]], dtype=float32)

time = 29047	action = 0	current_phase = 0	next_phase = 1	reward = -0.056272	array([[-1.5970521, -2.792242 ]], dtype=float32)

time = 29052	action = 0	current_phase = 0	next_phase = 1	reward = 0.015251	array([[-2.0208287, -2.8244748]], dtype=float32)

time = 29057	action = 0	current_phase = 0	next_phase = 1	reward = 0.072930	array([[-2.5609055, -2.90943  ]], dtype=float32)

time = 29062	action = 1	current_phase = 0	next_phase = 1	reward = -1.658289	array([[-5.172951 , -3.4109378]], dtype=float32)

time = 29070	action = 1	current_phase = 1	next_phase = 0	reward = -0.954374	array([[-4.061127 , -2.2129729]], dtype=float32)

time = 29078	action = 0	current_phase = 0	next_phase = 1	reward = -0.069137	array([[-1.517194, -2.723021]], dtype=float32)

time = 29083	action = 0	current_phase = 0	next_phase = 1	reward = 0.014325	array([[-2.072052 , -2.9677806]], dtype=float32)

time = 29088	action = 0	current_phase = 0	next_phase = 1	reward = 0.078041	array([[-2.7367852, -2.7992365]], dtype=float32)

time = 29093	action = 1	current_phase = 0	next_phase = 1	reward = -1.853945	array([[-4.9982777, -3.5622811]], dtype=float32)

time = 29101	action = 1	current_phase = 1	next_phase = 0	reward = -1.338317	array([[-4.2095037, -2.2953434]], dtype=float32)

time = 29109	action = 0	current_phase = 0	next_phase = 1	reward = 0.260921	array([[-1.2544533, -2.5498185]], dtype=float32)

time = 29114	action = 0	current_phase = 0	next_phase = 1	reward = 0.311565	array([[-1.8300915, -2.841484 ]], dtype=float32)

time = 29119	action = 0	current_phase = 0	next_phase = 1	reward = -0.210147	array([[-2.3552427, -2.3772373]], dtype=float32)

time = 29124	action = 1	current_phase = 0	next_phase = 1	reward = -1.057735	array([[-4.9712467, -3.1002564]], dtype=float32)

time = 29132	action = 1	current_phase = 1	next_phase = 0	reward = -0.712971	array([[-3.9349506, -2.1396508]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0602 - val_loss: 0.0450

Epoch 2/50

 - 3s - loss: 0.0628 - val_loss: 0.0427

Epoch 3/50

 - 3s - loss: 0.0476 - val_loss: 0.0427

Epoch 4/50

 - 3s - loss: 0.0580 - val_loss: 0.0464

Epoch 5/50

 - 3s - loss: 0.0492 - val_loss: 0.0468

Epoch 6/50

 - 3s - loss: 0.0531 - val_loss: 0.0455

Epoch 7/50

 - 3s - loss: 0.0506 - val_loss: 0.0461

Epoch 8/50

 - 3s - loss: 0.0542 - val_loss: 0.0453

Epoch 9/50

 - 3s - loss: 0.0463 - val_loss: 0.0485

Epoch 10/50

 - 3s - loss: 0.0481 - val_loss: 0.0462

Epoch 11/50

 - 3s - loss: 0.0495 - val_loss: 0.0469

Epoch 12/50

 - 3s - loss: 0.0494 - val_loss: 0.0457

Epoch 13/50

 - 3s - loss: 0.0432 - val_loss: 0.0482

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 29140	action = 0	current_phase = 0	next_phase = 1	reward = -0.016814	array([[-1.6487839, -2.6363268]], dtype=float32)

time = 29145	action = 0	current_phase = 0	next_phase = 1	reward = 0.054970	array([[-2.1999745, -2.9695077]], dtype=float32)

time = 29150	action = 1	current_phase = 0	next_phase = 1	reward = -1.206957	array([[-4.033319 , -2.7125154]], dtype=float32)

time = 29158	action = 1	current_phase = 1	next_phase = 0	reward = -0.714694	array([[-3.593947 , -1.8616734]], dtype=float32)

time = 29166	action = 0	current_phase = 0	next_phase = 1	reward = -0.081540	array([[-1.5071096, -2.8433309]], dtype=float32)

time = 29171	action = 0	current_phase = 0	next_phase = 1	reward = -0.008004	array([[-1.8016224, -2.7723732]], dtype=float32)

time = 29176	action = 0	current_phase = 0	next_phase = 1	reward = 0.066773	array([[-2.3353815, -3.0032277]], dtype=float32)

time = 29181	action = 1	current_phase = 0	next_phase = 1	reward = -1.441410	array([[-5.3198156, -3.1152472]], dtype=float32)

time = 29189	action = 1	current_phase = 1	next_phase = 0	reward = -1.074837	array([[-3.9200797, -2.0710316]], dtype=float32)

time = 29197	action = 0	current_phase = 0	next_phase = 1	reward = 0.213921	array([[-1.3439754, -2.780538 ]], dtype=float32)

time = 29202	action = 0	current_phase = 0	next_phase = 1	reward = -0.005273	array([[-2.1128798, -2.940793 ]], dtype=float32)

time = 29207	action = 0	current_phase = 0	next_phase = 1	reward = 0.071692	array([[-2.747798 , -2.9022088]], dtype=float32)

time = 29212	action = 1	current_phase = 0	next_phase = 1	reward = -1.644612	array([[-5.2491364, -3.3709683]], dtype=float32)

time = 29220	action = 1	current_phase = 1	next_phase = 0	reward = -0.898811	array([[-3.9697247, -2.173738 ]], dtype=float32)

time = 29228	action = 0	current_phase = 0	next_phase = 1	reward = -0.053059	array([[-1.480212 , -2.7225752]], dtype=float32)

time = 29233	action = 0	current_phase = 0	next_phase = 1	reward = 0.016044	array([[-1.9522865, -2.992508 ]], dtype=float32)

time = 29238	action = 0	current_phase = 0	next_phase = 1	reward = 0.080167	array([[-2.5207775, -2.9765806]], dtype=float32)

time = 29243	action = 1	current_phase = 0	next_phase = 1	reward = -0.758854	array([[-5.027445, -3.184579]], dtype=float32)

time = 29251	action = 1	current_phase = 1	next_phase = 0	reward = -1.245238	array([[-3.569684 , -2.3118854]], dtype=float32)

time = 29259	action = 0	current_phase = 0	next_phase = 1	reward = -0.054808	array([[-1.3602177, -2.682898 ]], dtype=float32)

time = 29264	action = 0	current_phase = 0	next_phase = 1	reward = 0.025398	array([[-1.7379217, -2.859387 ]], dtype=float32)

time = 29269	action = 1	current_phase = 0	next_phase = 1	reward = -0.670032	array([[-2.9027474, -2.6228783]], dtype=float32)

time = 29277	action = 1	current_phase = 1	next_phase = 0	reward = -0.912853	array([[-3.185253 , -1.9438803]], dtype=float32)

time = 29285	action = 0	current_phase = 0	next_phase = 1	reward = 0.178486	array([[-1.3668307, -2.888298 ]], dtype=float32)

time = 29290	action = 0	current_phase = 0	next_phase = 1	reward = -0.027531	array([[-1.7226596, -2.631794 ]], dtype=float32)

time = 29295	action = 0	current_phase = 0	next_phase = 1	reward = 0.033126	array([[-2.2625802, -3.000102 ]], dtype=float32)

time = 29300	action = 1	current_phase = 0	next_phase = 1	reward = -1.366388	array([[-4.277352 , -2.7918515]], dtype=float32)

time = 29308	action = 1	current_phase = 1	next_phase = 0	reward = -0.694461	array([[-3.8702626, -1.9303614]], dtype=float32)

time = 29316	action = 0	current_phase = 0	next_phase = 1	reward = -0.089671	array([[-1.5294192, -2.7636647]], dtype=float32)

time = 29321	action = 0	current_phase = 0	next_phase = 1	reward = -0.013243	array([[-1.8794749, -2.7931247]], dtype=float32)

time = 29326	action = 0	current_phase = 0	next_phase = 1	reward = 0.059102	array([[-2.320628 , -3.0237327]], dtype=float32)

time = 29331	action = 1	current_phase = 0	next_phase = 1	reward = -1.363545	array([[-5.283861 , -3.0667233]], dtype=float32)

time = 29339	action = 1	current_phase = 1	next_phase = 0	reward = -0.841186	array([[-3.9418697, -2.106205 ]], dtype=float32)

time = 29347	action = 0	current_phase = 0	next_phase = 1	reward = -0.065913	array([[-1.4749105, -2.7971163]], dtype=float32)

time = 29352	action = 0	current_phase = 0	next_phase = 1	reward = -0.000999	array([[-2.0034792, -2.9591393]], dtype=float32)

time = 29357	action = 0	current_phase = 0	next_phase = 1	reward = 0.074574	array([[-2.5422113, -2.9432392]], dtype=float32)

time = 29362	action = 1	current_phase = 0	next_phase = 1	reward = -1.660661	array([[-5.2257447, -3.3952556]], dtype=float32)

time = 29370	action = 1	current_phase = 1	next_phase = 0	reward = -1.247580	array([[-3.9483829, -2.1884944]], dtype=float32)

time = 29378	action = 0	current_phase = 0	next_phase = 1	reward = 0.248347	array([[-1.5391295, -2.7384624]], dtype=float32)

time = 29383	action = 0	current_phase = 0	next_phase = 1	reward = 0.036072	array([[-1.9894507, -3.0079536]], dtype=float32)

time = 29388	action = 0	current_phase = 0	next_phase = 1	reward = 0.071484	array([[-2.6333349, -2.9317012]], dtype=float32)

time = 29393	action = 1	current_phase = 0	next_phase = 1	reward = -1.393091	array([[-5.08696 , -3.342123]], dtype=float32)

time = 29401	action = 1	current_phase = 1	next_phase = 0	reward = -1.179256	array([[-3.860662, -2.176192]], dtype=float32)

time = 29409	action = 0	current_phase = 0	next_phase = 1	reward = -0.030504	array([[-1.370312 , -2.5858545]], dtype=float32)

time = 29414	action = 0	current_phase = 0	next_phase = 1	reward = 0.046247	array([[-1.8001127, -2.8248749]], dtype=float32)

time = 29419	action = 1	current_phase = 0	next_phase = 1	reward = -0.696509	array([[-2.71098  , -2.4409976]], dtype=float32)

time = 29427	action = 1	current_phase = 1	next_phase = 0	reward = -0.658441	array([[-3.5275679, -1.8080883]], dtype=float32)

time = 29435	action = 0	current_phase = 0	next_phase = 1	reward = -0.104122	array([[-1.2847542, -2.990045 ]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0554 - val_loss: 0.0195

Epoch 2/50

 - 3s - loss: 0.0622 - val_loss: 0.0230

Epoch 3/50

 - 3s - loss: 0.0709 - val_loss: 0.0215

Epoch 4/50

 - 3s - loss: 0.0776 - val_loss: 0.0246

Epoch 5/50

 - 3s - loss: 0.0509 - val_loss: 0.0239

Epoch 6/50

 - 3s - loss: 0.0437 - val_loss: 0.0254

Epoch 7/50

 - 3s - loss: 0.0488 - val_loss: 0.0217

Epoch 8/50

 - 3s - loss: 0.0417 - val_loss: 0.0297

Epoch 9/50

 - 3s - loss: 0.0556 - val_loss: 0.0322

Epoch 10/50

 - 3s - loss: 0.0491 - val_loss: 0.0275

Epoch 11/50

 - 3s - loss: 0.0564 - val_loss: 0.0234

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 29440	action = 0	current_phase = 0	next_phase = 1	reward = -0.303705	array([[-1.762766 , -2.6402519]], dtype=float32)

time = 29445	action = 0	current_phase = 0	next_phase = 1	reward = 0.060064	array([[-2.2821615, -2.9932325]], dtype=float32)

time = 29450	action = 1	current_phase = 0	next_phase = 1	reward = -0.986481	array([[-4.0222173, -2.6383512]], dtype=float32)

time = 29458	action = 1	current_phase = 1	next_phase = 0	reward = -0.709029	array([[-3.7525702, -1.9661739]], dtype=float32)

time = 29466	action = 0	current_phase = 0	next_phase = 1	reward = -0.085373	array([[-1.5711175, -2.7886357]], dtype=float32)

time = 29471	action = 0	current_phase = 0	next_phase = 1	reward = -0.019560	array([[-1.9920812, -2.8439138]], dtype=float32)

time = 29476	action = 0	current_phase = 0	next_phase = 1	reward = 0.048650	array([[-2.3256326, -2.9920146]], dtype=float32)

time = 29481	action = 1	current_phase = 0	next_phase = 1	reward = -1.498882	array([[-5.377846 , -3.0436795]], dtype=float32)

time = 29489	action = 1	current_phase = 1	next_phase = 0	reward = -0.776192	array([[-3.9121988, -2.1292338]], dtype=float32)

time = 29497	action = 0	current_phase = 0	next_phase = 1	reward = -0.053736	array([[-1.6066754, -2.808252 ]], dtype=float32)

time = 29502	action = 0	current_phase = 0	next_phase = 1	reward = 0.010981	array([[-2.096886 , -2.9790955]], dtype=float32)

time = 29507	action = 1	current_phase = 0	next_phase = 1	reward = -1.661869	array([[-2.898539, -2.862096]], dtype=float32)

time = 29515	action = 1	current_phase = 1	next_phase = 0	reward = -0.665695	array([[-3.944143, -2.02104 ]], dtype=float32)

time = 29523	action = 0	current_phase = 0	next_phase = 1	reward = -0.127657	array([[-1.4622179, -3.1806335]], dtype=float32)

time = 29528	action = 0	current_phase = 0	next_phase = 1	reward = -0.030104	array([[-1.5755825, -2.725649 ]], dtype=float32)

time = 29533	action = 0	current_phase = 0	next_phase = 1	reward = 0.041667	array([[-2.0120535, -3.0865932]], dtype=float32)

time = 29538	action = 0	current_phase = 0	next_phase = 1	reward = 0.074302	array([[-2.7227182, -2.8957846]], dtype=float32)

time = 29543	action = 1	current_phase = 0	next_phase = 1	reward = -2.019919	array([[-5.001739 , -3.5918944]], dtype=float32)

time = 29551	action = 1	current_phase = 1	next_phase = 0	reward = -1.131625	array([[-4.0390186, -2.157677 ]], dtype=float32)

time = 29559	action = 0	current_phase = 0	next_phase = 1	reward = -0.013560	array([[-1.3902373, -2.5347805]], dtype=float32)

time = 29564	action = 0	current_phase = 0	next_phase = 1	reward = 0.045958	array([[-1.8341794, -2.9303277]], dtype=float32)

time = 29569	action = 0	current_phase = 0	next_phase = 1	reward = 0.059897	array([[-2.0663943, -2.370876 ]], dtype=float32)

time = 29574	action = 1	current_phase = 0	next_phase = 1	reward = -1.153725	array([[-4.8666205, -3.0018857]], dtype=float32)

time = 29582	action = 1	current_phase = 1	next_phase = 0	reward = -1.253757	array([[-3.923706 , -2.2129123]], dtype=float32)

time = 29590	action = 0	current_phase = 0	next_phase = 1	reward = 0.268824	array([[-1.5674199, -2.6491928]], dtype=float32)

time = 29595	action = 0	current_phase = 0	next_phase = 1	reward = 0.324467	array([[-2.1194444, -2.9328659]], dtype=float32)

time = 29600	action = 1	current_phase = 0	next_phase = 1	reward = -1.261051	array([[-4.0805516, -2.673266 ]], dtype=float32)

time = 29608	action = 1	current_phase = 1	next_phase = 0	reward = -0.660263	array([[-3.6859317, -1.9146314]], dtype=float32)

time = 29616	action = 0	current_phase = 0	next_phase = 1	reward = -0.084168	array([[-1.4093845, -2.8390102]], dtype=float32)

time = 29621	action = 0	current_phase = 0	next_phase = 1	reward = -0.007220	array([[-1.9034866, -2.8384364]], dtype=float32)

time = 29626	action = 0	current_phase = 0	next_phase = 1	reward = 0.063751	array([[-2.3494911, -3.0226774]], dtype=float32)

time = 29631	action = 1	current_phase = 0	next_phase = 1	reward = -1.566962	array([[-5.2135797, -3.0878627]], dtype=float32)

time = 29639	action = 1	current_phase = 1	next_phase = 0	reward = -0.838765	array([[-3.9329374, -2.1060314]], dtype=float32)

time = 29647	action = 0	current_phase = 0	next_phase = 1	reward = -0.071985	array([[-1.6301271, -2.7415693]], dtype=float32)

time = 29652	action = 0	current_phase = 0	next_phase = 1	reward = 0.017548	array([[-2.1009185, -2.9664924]], dtype=float32)

time = 29657	action = 0	current_phase = 0	next_phase = 1	reward = 0.074816	array([[-2.478673 , -3.0142992]], dtype=float32)

time = 29662	action = 1	current_phase = 0	next_phase = 1	reward = -1.666398	array([[-5.2271986, -3.4267552]], dtype=float32)

time = 29670	action = 1	current_phase = 1	next_phase = 0	reward = -1.304999	array([[-3.966989 , -2.2388666]], dtype=float32)

time = 29678	action = 0	current_phase = 0	next_phase = 1	reward = 0.232176	array([[-1.5626421, -2.7413118]], dtype=float32)

time = 29683	action = 0	current_phase = 0	next_phase = 1	reward = 0.013018	array([[-2.041623, -3.085255]], dtype=float32)

time = 29688	action = 0	current_phase = 0	next_phase = 1	reward = 0.083959	array([[-2.8076944, -2.8905563]], dtype=float32)

time = 29693	action = 1	current_phase = 0	next_phase = 1	reward = -1.851511	array([[-5.1068454, -3.562836 ]], dtype=float32)

time = 29701	action = 1	current_phase = 1	next_phase = 0	reward = -0.987163	array([[-4.1924105, -2.271021 ]], dtype=float32)

time = 29709	action = 0	current_phase = 0	next_phase = 1	reward = -0.027933	array([[-1.2051208, -2.5653918]], dtype=float32)

time = 29714	action = 0	current_phase = 0	next_phase = 1	reward = 0.029866	array([[-1.8018699, -2.8624554]], dtype=float32)

time = 29719	action = 1	current_phase = 0	next_phase = 1	reward = -0.649050	array([[-2.315038 , -2.2469745]], dtype=float32)

time = 29727	action = 1	current_phase = 1	next_phase = 0	reward = -0.811110	array([[-3.0765958, -1.9050446]], dtype=float32)

time = 29735	action = 0	current_phase = 0	next_phase = 1	reward = 0.194255	array([[-1.3883195, -2.8968465]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0582 - val_loss: 0.0247

Epoch 2/50

 - 3s - loss: 0.0563 - val_loss: 0.0267

Epoch 3/50

 - 3s - loss: 0.0717 - val_loss: 0.0255

Epoch 4/50

 - 3s - loss: 0.0526 - val_loss: 0.0247

Epoch 5/50

 - 3s - loss: 0.0521 - val_loss: 0.0238

Epoch 6/50

 - 3s - loss: 0.0490 - val_loss: 0.0277

Epoch 7/50

 - 3s - loss: 0.0567 - val_loss: 0.0246

Epoch 8/50

 - 3s - loss: 0.0522 - val_loss: 0.0238

Epoch 9/50

 - 3s - loss: 0.0586 - val_loss: 0.0254

Epoch 10/50

 - 3s - loss: 0.0509 - val_loss: 0.0322

Epoch 11/50

 - 3s - loss: 0.0578 - val_loss: 0.0269

Epoch 12/50

 - 3s - loss: 0.0504 - val_loss: 0.0253

Epoch 13/50

 - 3s - loss: 0.0442 - val_loss: 0.0261

Epoch 14/50

 - 3s - loss: 0.0528 - val_loss: 0.0278

Epoch 15/50

 - 3s - loss: 0.0479 - val_loss: 0.0249

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 29740	action = 0	current_phase = 0	next_phase = 1	reward = -0.017688	array([[-1.8151767, -2.6650352]], dtype=float32)

time = 29745	action = 0	current_phase = 0	next_phase = 1	reward = 0.049364	array([[-2.3060873, -3.0127149]], dtype=float32)

time = 29750	action = 1	current_phase = 0	next_phase = 1	reward = -1.367692	array([[-4.367662, -2.724833]], dtype=float32)

time = 29758	action = 1	current_phase = 1	next_phase = 0	reward = -0.712316	array([[-3.8080888, -1.9698236]], dtype=float32)

time = 29766	action = 0	current_phase = 0	next_phase = 1	reward = -0.088404	array([[-1.6358554, -2.7753544]], dtype=float32)

time = 29771	action = 0	current_phase = 0	next_phase = 1	reward = -0.030498	array([[-1.8573003, -2.8343258]], dtype=float32)

time = 29776	action = 0	current_phase = 0	next_phase = 1	reward = 0.036588	array([[-2.404583 , -3.0454392]], dtype=float32)

time = 29781	action = 1	current_phase = 0	next_phase = 1	reward = -1.496621	array([[-5.431834 , -3.1642952]], dtype=float32)

time = 29789	action = 1	current_phase = 1	next_phase = 0	reward = -0.775497	array([[-3.9232016, -2.0592203]], dtype=float32)

time = 29797	action = 0	current_phase = 0	next_phase = 1	reward = -0.072044	array([[-1.515173 , -2.8135552]], dtype=float32)

time = 29802	action = 0	current_phase = 0	next_phase = 1	reward = 0.023453	array([[-2.1818259, -2.9657607]], dtype=float32)

time = 29807	action = 0	current_phase = 0	next_phase = 1	reward = 0.081732	array([[-2.588028 , -2.9993534]], dtype=float32)

time = 29812	action = 1	current_phase = 0	next_phase = 1	reward = -1.611372	array([[-5.2870455, -3.4719214]], dtype=float32)

time = 29820	action = 1	current_phase = 1	next_phase = 0	reward = -1.005334	array([[-3.8878293, -2.1620169]], dtype=float32)

time = 29828	action = 0	current_phase = 0	next_phase = 1	reward = -0.050805	array([[-1.6436675, -2.7559128]], dtype=float32)

time = 29833	action = 0	current_phase = 0	next_phase = 1	reward = 0.022771	array([[-2.1762452, -3.048767 ]], dtype=float32)

time = 29838	action = 0	current_phase = 0	next_phase = 1	reward = 0.078043	array([[-2.575476 , -3.0540705]], dtype=float32)

time = 29843	action = 1	current_phase = 0	next_phase = 1	reward = -1.823730	array([[-5.113963, -3.674262]], dtype=float32)

time = 29851	action = 1	current_phase = 1	next_phase = 0	reward = -1.382390	array([[-4.179544, -2.233278]], dtype=float32)

time = 29859	action = 0	current_phase = 0	next_phase = 1	reward = 0.251400	array([[-1.1971165, -2.6133265]], dtype=float32)

time = 29864	action = 0	current_phase = 0	next_phase = 1	reward = 0.019303	array([[-1.8556712, -2.8804727]], dtype=float32)

time = 29869	action = 0	current_phase = 0	next_phase = 1	reward = 0.073588	array([[-2.3790889, -2.444878 ]], dtype=float32)

time = 29874	action = 1	current_phase = 0	next_phase = 1	reward = -1.027489	array([[-4.9407034, -2.9415226]], dtype=float32)

time = 29882	action = 1	current_phase = 1	next_phase = 0	reward = -1.026122	array([[-3.7276373, -2.2144148]], dtype=float32)

time = 29890	action = 0	current_phase = 0	next_phase = 1	reward = -0.015958	array([[-1.6679484, -2.6573496]], dtype=float32)

time = 29895	action = 0	current_phase = 0	next_phase = 1	reward = 0.342230	array([[-2.1800902, -2.9644098]], dtype=float32)

time = 29900	action = 1	current_phase = 0	next_phase = 1	reward = -1.368164	array([[-4.1449027, -2.7708921]], dtype=float32)

time = 29908	action = 1	current_phase = 1	next_phase = 0	reward = -0.709189	array([[-3.846128 , -1.9783392]], dtype=float32)

time = 29916	action = 0	current_phase = 0	next_phase = 1	reward = -0.081475	array([[-1.5727913, -2.7911415]], dtype=float32)

time = 29921	action = 0	current_phase = 0	next_phase = 1	reward = -0.010254	array([[-1.8698454, -2.8029628]], dtype=float32)

time = 29926	action = 0	current_phase = 0	next_phase = 1	reward = 0.069408	array([[-2.4127371, -3.0723448]], dtype=float32)

time = 29931	action = 1	current_phase = 0	next_phase = 1	reward = -1.541341	array([[-5.210677 , -3.1221676]], dtype=float32)

time = 29939	action = 1	current_phase = 1	next_phase = 0	reward = -0.766127	array([[-3.911168, -2.070438]], dtype=float32)

time = 29947	action = 0	current_phase = 0	next_phase = 1	reward = -0.067211	array([[-1.5667276, -2.821002 ]], dtype=float32)

time = 29952	action = 0	current_phase = 0	next_phase = 1	reward = 0.008039	array([[-2.0737922, -2.9346862]], dtype=float32)

time = 29957	action = 0	current_phase = 0	next_phase = 1	reward = 0.069941	array([[-2.5745294, -2.9824467]], dtype=float32)

time = 29962	action = 1	current_phase = 0	next_phase = 1	reward = -1.489232	array([[-5.2231274, -3.4523354]], dtype=float32)

time = 29970	action = 1	current_phase = 1	next_phase = 0	reward = -0.995780	array([[-3.9033303, -2.1380558]], dtype=float32)

time = 29978	action = 0	current_phase = 0	next_phase = 1	reward = -0.043770	array([[-1.5357554, -2.7000585]], dtype=float32)

time = 29983	action = 0	current_phase = 0	next_phase = 1	reward = 0.033527	array([[-2.1556048, -3.047902 ]], dtype=float32)

time = 29988	action = 0	current_phase = 0	next_phase = 1	reward = 0.076478	array([[-2.6613357, -2.9252272]], dtype=float32)

time = 29993	action = 1	current_phase = 0	next_phase = 1	reward = -1.400322	array([[-5.158671 , -3.6857095]], dtype=float32)

time = 30001	action = 1	current_phase = 1	next_phase = 0	reward = -1.426709	array([[-3.7594209, -2.2124362]], dtype=float32)

time = 30009	action = 0	current_phase = 0	next_phase = 1	reward = 0.261363	array([[-1.3379267, -2.5242186]], dtype=float32)

time = 30014	action = 0	current_phase = 0	next_phase = 1	reward = 0.026140	array([[-1.9051135, -2.8830643]], dtype=float32)

time = 30019	action = 0	current_phase = 0	next_phase = 1	reward = 0.060495	array([[-2.230832 , -2.3424358]], dtype=float32)

time = 30024	action = 1	current_phase = 0	next_phase = 1	reward = -1.021625	array([[-4.954002 , -2.9991088]], dtype=float32)

time = 30032	action = 1	current_phase = 1	next_phase = 0	reward = -0.693819	array([[-3.7079692, -2.1750402]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0826 - val_loss: 0.0262

Epoch 2/50

 - 3s - loss: 0.0667 - val_loss: 0.0293

Epoch 3/50

 - 3s - loss: 0.0570 - val_loss: 0.0338

Epoch 4/50

 - 3s - loss: 0.0743 - val_loss: 0.0430

Epoch 5/50

 - 3s - loss: 0.0581 - val_loss: 0.0357

Epoch 6/50

 - 3s - loss: 0.0537 - val_loss: 0.0376

Epoch 7/50

 - 3s - loss: 0.0457 - val_loss: 0.0369

Epoch 8/50

 - 3s - loss: 0.0512 - val_loss: 0.0343

Epoch 9/50

 - 3s - loss: 0.0528 - val_loss: 0.0266

Epoch 10/50

 - 3s - loss: 0.0532 - val_loss: 0.0335

Epoch 11/50

 - 3s - loss: 0.0535 - val_loss: 0.0393

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 30040	action = 0	current_phase = 0	next_phase = 1	reward = -0.015990	array([[-1.6389284, -2.5942738]], dtype=float32)

time = 30045	action = 0	current_phase = 0	next_phase = 1	reward = 0.042350	array([[-2.0799868, -2.9161391]], dtype=float32)

time = 30050	action = 1	current_phase = 0	next_phase = 1	reward = -1.372957	array([[-4.182616, -2.696715]], dtype=float32)

time = 30058	action = 1	current_phase = 1	next_phase = 0	reward = -0.701403	array([[-3.872261, -2.10206 ]], dtype=float32)

time = 30066	action = 0	current_phase = 0	next_phase = 1	reward = -0.061636	array([[-1.5128598, -2.8068018]], dtype=float32)

time = 30071	action = 0	current_phase = 0	next_phase = 1	reward = 0.016872	array([[-1.8879914, -2.8355508]], dtype=float32)

time = 30076	action = 0	current_phase = 0	next_phase = 1	reward = 0.072886	array([[-2.480932, -3.01894 ]], dtype=float32)

time = 30081	action = 1	current_phase = 0	next_phase = 1	reward = -1.474649	array([[-5.4049416, -3.055898 ]], dtype=float32)

time = 30089	action = 1	current_phase = 1	next_phase = 0	reward = -0.777382	array([[-3.9031062, -2.1686025]], dtype=float32)

time = 30097	action = 0	current_phase = 0	next_phase = 1	reward = -0.079928	array([[-1.4396157, -2.7736611]], dtype=float32)

time = 30102	action = 0	current_phase = 0	next_phase = 1	reward = 0.007913	array([[-2.0421515, -2.9196763]], dtype=float32)

time = 30107	action = 0	current_phase = 0	next_phase = 1	reward = 0.064211	array([[-2.527598, -2.967504]], dtype=float32)

time = 30112	action = 1	current_phase = 0	next_phase = 1	reward = -1.639608	array([[-5.3643556, -3.2559714]], dtype=float32)

time = 30120	action = 1	current_phase = 1	next_phase = 0	reward = -0.952056	array([[-3.9245696, -2.297399 ]], dtype=float32)

time = 30128	action = 0	current_phase = 0	next_phase = 1	reward = -0.063203	array([[-1.4708579, -2.756429 ]], dtype=float32)

time = 30133	action = 0	current_phase = 0	next_phase = 1	reward = 0.005631	array([[-1.9800963, -3.000012 ]], dtype=float32)

time = 30138	action = 0	current_phase = 0	next_phase = 1	reward = 0.071526	array([[-2.6655912, -2.9424584]], dtype=float32)

time = 30143	action = 1	current_phase = 0	next_phase = 1	reward = -1.340414	array([[-5.154557, -3.223567]], dtype=float32)

time = 30151	action = 1	current_phase = 1	next_phase = 0	reward = -1.127041	array([[-3.952374 , -2.4147131]], dtype=float32)

time = 30159	action = 0	current_phase = 0	next_phase = 1	reward = -0.030059	array([[-1.2698575, -2.5833902]], dtype=float32)

time = 30164	action = 0	current_phase = 0	next_phase = 1	reward = 0.034951	array([[-1.8829193, -2.8901865]], dtype=float32)

time = 30169	action = 1	current_phase = 0	next_phase = 1	reward = -0.715308	array([[-2.856789 , -2.2909226]], dtype=float32)

time = 30177	action = 1	current_phase = 1	next_phase = 0	reward = -0.916564	array([[-3.5504017, -2.0154321]], dtype=float32)

time = 30185	action = 0	current_phase = 0	next_phase = 1	reward = 0.169804	array([[-1.3139886, -2.957936 ]], dtype=float32)

time = 30190	action = 0	current_phase = 0	next_phase = 1	reward = -0.036535	array([[-1.5605595, -2.538293 ]], dtype=float32)

time = 30195	action = 0	current_phase = 0	next_phase = 1	reward = 0.029480	array([[-2.2552757, -2.9946003]], dtype=float32)

time = 30200	action = 1	current_phase = 0	next_phase = 1	reward = -1.370081	array([[-4.2381325, -2.6365604]], dtype=float32)

time = 30208	action = 1	current_phase = 1	next_phase = 0	reward = -0.712580	array([[-3.9036517, -2.0886867]], dtype=float32)

time = 30216	action = 0	current_phase = 0	next_phase = 1	reward = -0.082799	array([[-1.5290465, -2.7833695]], dtype=float32)

time = 30221	action = 0	current_phase = 0	next_phase = 1	reward = -0.016102	array([[-1.8789542, -2.8312795]], dtype=float32)

time = 30226	action = 0	current_phase = 0	next_phase = 1	reward = 0.058418	array([[-2.3046298, -3.026677 ]], dtype=float32)

time = 30231	action = 1	current_phase = 0	next_phase = 1	reward = -1.545483	array([[-5.2986894, -3.045337 ]], dtype=float32)

time = 30239	action = 1	current_phase = 1	next_phase = 0	reward = -0.761465	array([[-3.92654  , -2.1552086]], dtype=float32)

time = 30247	action = 0	current_phase = 0	next_phase = 1	reward = -0.080866	array([[-1.5420163, -2.8053062]], dtype=float32)

time = 30252	action = 0	current_phase = 0	next_phase = 1	reward = -0.001653	array([[-2.06954 , -2.927737]], dtype=float32)

time = 30257	action = 0	current_phase = 0	next_phase = 1	reward = 0.065347	array([[-2.5849888, -3.0330155]], dtype=float32)

time = 30262	action = 1	current_phase = 0	next_phase = 1	reward = -1.479451	array([[-5.325058 , -3.2406273]], dtype=float32)

time = 30270	action = 1	current_phase = 1	next_phase = 0	reward = -1.053322	array([[-3.8747993, -2.208725 ]], dtype=float32)

time = 30278	action = 0	current_phase = 0	next_phase = 1	reward = -0.074753	array([[-1.4471855, -2.630704 ]], dtype=float32)

time = 30283	action = 0	current_phase = 0	next_phase = 1	reward = 0.016602	array([[-2.019384 , -3.0086274]], dtype=float32)

time = 30288	action = 0	current_phase = 0	next_phase = 1	reward = 0.073638	array([[-2.6202216, -2.9118357]], dtype=float32)

time = 30293	action = 1	current_phase = 0	next_phase = 1	reward = -1.804788	array([[-5.2241287, -3.2242708]], dtype=float32)

time = 30301	action = 1	current_phase = 1	next_phase = 0	reward = -1.020995	array([[-4.1195745, -2.3634753]], dtype=float32)

time = 30309	action = 0	current_phase = 0	next_phase = 1	reward = -0.018620	array([[-1.2709601, -2.5245655]], dtype=float32)

time = 30314	action = 0	current_phase = 0	next_phase = 1	reward = 0.035416	array([[-1.7781732, -2.8493938]], dtype=float32)

time = 30319	action = 1	current_phase = 0	next_phase = 1	reward = -0.639834	array([[-2.3873541, -2.3260922]], dtype=float32)

time = 30327	action = 1	current_phase = 1	next_phase = 0	reward = -1.199078	array([[-3.391685 , -2.0409153]], dtype=float32)

time = 30335	action = 0	current_phase = 0	next_phase = 1	reward = 0.192979	array([[-1.2023513, -3.0491114]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0503 - val_loss: 0.0394

Epoch 2/50

 - 3s - loss: 0.0602 - val_loss: 0.0437

Epoch 3/50

 - 3s - loss: 0.0513 - val_loss: 0.0405

Epoch 4/50

 - 3s - loss: 0.0514 - val_loss: 0.0447

Epoch 5/50

 - 3s - loss: 0.0481 - val_loss: 0.0420

Epoch 6/50

 - 3s - loss: 0.0449 - val_loss: 0.0413

Epoch 7/50

 - 3s - loss: 0.0440 - val_loss: 0.0416

Epoch 8/50

 - 3s - loss: 0.0455 - val_loss: 0.0423

Epoch 9/50

 - 3s - loss: 0.0392 - val_loss: 0.0402

Epoch 10/50

 - 3s - loss: 0.0382 - val_loss: 0.0463

Epoch 11/50

 - 3s - loss: 0.0428 - val_loss: 0.0539

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 30340	action = 0	current_phase = 0	next_phase = 1	reward = 0.260546	array([[-1.640453 , -2.5225942]], dtype=float32)

time = 30345	action = 0	current_phase = 0	next_phase = 1	reward = 0.046141	array([[-2.2333927, -2.9611552]], dtype=float32)

time = 30350	action = 1	current_phase = 0	next_phase = 1	reward = -1.311884	array([[-4.369162 , -2.7060995]], dtype=float32)

time = 30358	action = 1	current_phase = 1	next_phase = 0	reward = -0.709990	array([[-3.8846798, -1.9831054]], dtype=float32)

time = 30366	action = 0	current_phase = 0	next_phase = 1	reward = -0.068528	array([[-1.6210217, -2.7466419]], dtype=float32)

time = 30371	action = 0	current_phase = 0	next_phase = 1	reward = 0.000496	array([[-1.9588106, -2.8018298]], dtype=float32)

time = 30376	action = 0	current_phase = 0	next_phase = 1	reward = 0.070453	array([[-2.2629488, -2.9778388]], dtype=float32)

time = 30381	action = 1	current_phase = 0	next_phase = 1	reward = -1.496290	array([[-5.4363713, -3.1774297]], dtype=float32)

time = 30389	action = 1	current_phase = 1	next_phase = 0	reward = -0.808452	array([[-3.9603395, -2.065133 ]], dtype=float32)

time = 30397	action = 0	current_phase = 0	next_phase = 1	reward = -0.058662	array([[-1.6528556, -2.8040385]], dtype=float32)

time = 30402	action = 0	current_phase = 0	next_phase = 1	reward = 0.017392	array([[-2.0750434, -2.9320889]], dtype=float32)

time = 30407	action = 0	current_phase = 0	next_phase = 1	reward = 0.079878	array([[-2.490718 , -2.9902384]], dtype=float32)

time = 30412	action = 1	current_phase = 0	next_phase = 1	reward = -1.501082	array([[-5.364361, -3.436759]], dtype=float32)

time = 30420	action = 1	current_phase = 1	next_phase = 0	reward = -1.005022	array([[-4.0717607, -2.2689023]], dtype=float32)

time = 30428	action = 0	current_phase = 0	next_phase = 1	reward = -0.067481	array([[-1.4677329, -2.6872988]], dtype=float32)

time = 30433	action = 0	current_phase = 0	next_phase = 1	reward = -0.005907	array([[-2.0841472, -3.0072112]], dtype=float32)

time = 30438	action = 0	current_phase = 0	next_phase = 1	reward = 0.081875	array([[-2.7207253, -2.7724075]], dtype=float32)

time = 30443	action = 1	current_phase = 0	next_phase = 1	reward = -1.298476	array([[-5.2423444, -3.383125 ]], dtype=float32)

time = 30451	action = 1	current_phase = 1	next_phase = 0	reward = -1.549486	array([[-3.8950582, -2.3992789]], dtype=float32)

time = 30459	action = 0	current_phase = 0	next_phase = 1	reward = 0.246445	array([[-1.470465, -2.521299]], dtype=float32)

time = 30464	action = 0	current_phase = 0	next_phase = 1	reward = 0.302752	array([[-1.7131052, -2.7890983]], dtype=float32)

time = 30469	action = 1	current_phase = 0	next_phase = 1	reward = -0.919927	array([[-2.4062047, -2.2933173]], dtype=float32)

time = 30477	action = 1	current_phase = 1	next_phase = 0	reward = -0.858503	array([[-3.5286965, -1.9637558]], dtype=float32)

time = 30485	action = 0	current_phase = 0	next_phase = 1	reward = 0.183783	array([[-1.2833885, -3.0403128]], dtype=float32)

time = 30490	action = 0	current_phase = 0	next_phase = 1	reward = -0.300764	array([[-1.7479393, -2.4702845]], dtype=float32)

time = 30495	action = 0	current_phase = 0	next_phase = 1	reward = 0.342147	array([[-2.1512399, -2.915258 ]], dtype=float32)

time = 30500	action = 1	current_phase = 0	next_phase = 1	reward = -1.323774	array([[-4.4028897, -2.8231936]], dtype=float32)

time = 30508	action = 1	current_phase = 1	next_phase = 0	reward = -0.696779	array([[-3.8914065, -1.9907262]], dtype=float32)

time = 30516	action = 0	current_phase = 0	next_phase = 1	reward = -0.088265	array([[-1.5752478, -2.8111224]], dtype=float32)

time = 30521	action = 0	current_phase = 0	next_phase = 1	reward = -0.007462	array([[-1.9887915, -2.8412557]], dtype=float32)

time = 30526	action = 0	current_phase = 0	next_phase = 1	reward = 0.058020	array([[-2.2884598, -2.9913654]], dtype=float32)

time = 30531	action = 1	current_phase = 0	next_phase = 1	reward = -1.454632	array([[-5.3643084, -3.1323261]], dtype=float32)

time = 30539	action = 1	current_phase = 1	next_phase = 0	reward = -0.780787	array([[-3.946268 , -2.0551114]], dtype=float32)

time = 30547	action = 0	current_phase = 0	next_phase = 1	reward = -0.057925	array([[-1.5906582, -2.789626 ]], dtype=float32)

time = 30552	action = 0	current_phase = 0	next_phase = 1	reward = 0.005348	array([[-2.0602968, -2.9614322]], dtype=float32)

time = 30557	action = 0	current_phase = 0	next_phase = 1	reward = 0.062019	array([[-2.3677762, -3.0078292]], dtype=float32)

time = 30562	action = 1	current_phase = 0	next_phase = 1	reward = -1.649161	array([[-5.425268 , -3.3663716]], dtype=float32)

time = 30570	action = 1	current_phase = 1	next_phase = 0	reward = -0.894053	array([[-4.067088, -2.343883]], dtype=float32)

time = 30578	action = 0	current_phase = 0	next_phase = 1	reward = -0.056000	array([[-1.5290319, -2.685371 ]], dtype=float32)

time = 30583	action = 0	current_phase = 0	next_phase = 1	reward = 0.017278	array([[-2.0278134, -2.9839787]], dtype=float32)

time = 30588	action = 0	current_phase = 0	next_phase = 1	reward = 0.079717	array([[-2.578727 , -2.9317358]], dtype=float32)

time = 30593	action = 1	current_phase = 0	next_phase = 1	reward = -1.345349	array([[-5.3059874, -3.4203384]], dtype=float32)

time = 30601	action = 1	current_phase = 1	next_phase = 0	reward = -1.492146	array([[-3.9831667, -2.2934036]], dtype=float32)

time = 30609	action = 0	current_phase = 0	next_phase = 1	reward = 0.265434	array([[-1.391289 , -2.5062342]], dtype=float32)

time = 30614	action = 0	current_phase = 0	next_phase = 1	reward = 0.036510	array([[-1.7491364, -2.8742557]], dtype=float32)

time = 30619	action = 1	current_phase = 0	next_phase = 1	reward = -0.703695	array([[-2.3179543, -2.2759087]], dtype=float32)

time = 30627	action = 1	current_phase = 1	next_phase = 0	reward = -0.873409	array([[-3.6696486, -1.8863258]], dtype=float32)

time = 30635	action = 0	current_phase = 0	next_phase = 1	reward = 0.178390	array([[-1.4007604, -2.9729505]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0592 - val_loss: 0.0262

Epoch 2/50

 - 3s - loss: 0.0559 - val_loss: 0.0254

Epoch 3/50

 - 3s - loss: 0.0633 - val_loss: 0.0257

Epoch 4/50

 - 3s - loss: 0.0484 - val_loss: 0.0368

Epoch 5/50

 - 3s - loss: 0.0570 - val_loss: 0.0229

Epoch 6/50

 - 3s - loss: 0.0562 - val_loss: 0.0291

Epoch 7/50

 - 3s - loss: 0.0463 - val_loss: 0.0268

Epoch 8/50

 - 3s - loss: 0.0528 - val_loss: 0.0361

Epoch 9/50

 - 3s - loss: 0.0569 - val_loss: 0.0232

Epoch 10/50

 - 3s - loss: 0.0504 - val_loss: 0.0282

Epoch 11/50

 - 3s - loss: 0.0712 - val_loss: 0.0338

Epoch 12/50

 - 3s - loss: 0.0424 - val_loss: 0.0280

Epoch 13/50

 - 3s - loss: 0.0427 - val_loss: 0.0302

Epoch 14/50

 - 3s - loss: 0.0538 - val_loss: 0.0261

Epoch 15/50

 - 3s - loss: 0.0481 - val_loss: 0.0306

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 30640	action = 0	current_phase = 0	next_phase = 1	reward = -0.019797	array([[-1.7458869, -2.531715 ]], dtype=float32)

time = 30645	action = 0	current_phase = 0	next_phase = 1	reward = -0.230168	array([[-2.1353548, -2.9656618]], dtype=float32)

time = 30650	action = 1	current_phase = 0	next_phase = 1	reward = -0.982583	array([[-4.404185 , -2.7385309]], dtype=float32)

time = 30658	action = 1	current_phase = 1	next_phase = 0	reward = -0.691189	array([[-3.7627714, -1.922262 ]], dtype=float32)

time = 30666	action = 0	current_phase = 0	next_phase = 1	reward = -0.087060	array([[-1.6897964, -2.7870562]], dtype=float32)

time = 30671	action = 0	current_phase = 0	next_phase = 1	reward = -0.014304	array([[-1.9283011, -2.743602 ]], dtype=float32)

time = 30676	action = 0	current_phase = 0	next_phase = 1	reward = 0.050834	array([[-2.283814 , -3.0056498]], dtype=float32)

time = 30681	action = 1	current_phase = 0	next_phase = 1	reward = -1.377662	array([[-5.293105 , -3.2213519]], dtype=float32)

time = 30689	action = 1	current_phase = 1	next_phase = 0	reward = -0.768691	array([[-3.8621752, -1.9684561]], dtype=float32)

time = 30697	action = 0	current_phase = 0	next_phase = 1	reward = -0.069528	array([[-1.5712895, -2.714824 ]], dtype=float32)

time = 30702	action = 0	current_phase = 0	next_phase = 1	reward = 0.027202	array([[-2.0519905, -2.9175553]], dtype=float32)

time = 30707	action = 0	current_phase = 0	next_phase = 1	reward = 0.080358	array([[-2.5387728, -2.9237323]], dtype=float32)

time = 30712	action = 1	current_phase = 0	next_phase = 1	reward = -1.670756	array([[-5.4236994, -3.4319475]], dtype=float32)

time = 30720	action = 1	current_phase = 1	next_phase = 0	reward = -0.948414	array([[-3.9614303, -2.2351968]], dtype=float32)

time = 30728	action = 0	current_phase = 0	next_phase = 1	reward = -0.049731	array([[-1.5831063, -2.6582391]], dtype=float32)

time = 30733	action = 0	current_phase = 0	next_phase = 1	reward = 0.016381	array([[-1.9911222, -2.9580543]], dtype=float32)

time = 30738	action = 0	current_phase = 0	next_phase = 1	reward = 0.075206	array([[-2.6207366, -2.8417091]], dtype=float32)

time = 30743	action = 1	current_phase = 0	next_phase = 1	reward = -1.832931	array([[-5.193482 , -3.5631108]], dtype=float32)

time = 30751	action = 1	current_phase = 1	next_phase = 0	reward = -1.329391	array([[-4.1628227, -2.2543974]], dtype=float32)

time = 30759	action = 0	current_phase = 0	next_phase = 1	reward = 0.255155	array([[-1.4369664, -2.4801395]], dtype=float32)

time = 30764	action = 0	current_phase = 0	next_phase = 1	reward = 0.039774	array([[-1.8688697, -2.819464 ]], dtype=float32)

time = 30769	action = 0	current_phase = 0	next_phase = 1	reward = 0.069126	array([[-2.3177087, -2.3640828]], dtype=float32)

time = 30774	action = 1	current_phase = 0	next_phase = 1	reward = -1.186380	array([[-5.1794157, -3.0290215]], dtype=float32)

time = 30782	action = 1	current_phase = 1	next_phase = 0	reward = -1.044269	array([[-4.0045137, -2.1812568]], dtype=float32)

time = 30790	action = 0	current_phase = 0	next_phase = 1	reward = -0.297428	array([[-1.703991 , -2.5521247]], dtype=float32)

time = 30795	action = 0	current_phase = 0	next_phase = 1	reward = 0.618916	array([[-1.9769683, -2.838651 ]], dtype=float32)

time = 30800	action = 1	current_phase = 0	next_phase = 1	reward = -1.321663	array([[-4.2965813, -2.8312805]], dtype=float32)

time = 30808	action = 1	current_phase = 1	next_phase = 0	reward = -0.645495	array([[-3.7651536, -1.9072299]], dtype=float32)

time = 30816	action = 0	current_phase = 0	next_phase = 1	reward = -0.091074	array([[-1.5548763, -2.7787347]], dtype=float32)

time = 30821	action = 0	current_phase = 0	next_phase = 1	reward = -0.023495	array([[-1.8306863, -2.7330487]], dtype=float32)

time = 30826	action = 0	current_phase = 0	next_phase = 1	reward = 0.035609	array([[-2.2347393, -2.9667208]], dtype=float32)

time = 30831	action = 1	current_phase = 0	next_phase = 1	reward = -1.372783	array([[-5.4646783, -3.1637187]], dtype=float32)

time = 30839	action = 1	current_phase = 1	next_phase = 0	reward = -0.835300	array([[-3.9498224, -2.038327 ]], dtype=float32)

time = 30847	action = 0	current_phase = 0	next_phase = 1	reward = -0.068779	array([[-1.6341331, -2.7334275]], dtype=float32)

time = 30852	action = 0	current_phase = 0	next_phase = 1	reward = 0.027188	array([[-2.053244, -2.944298]], dtype=float32)

time = 30857	action = 0	current_phase = 0	next_phase = 1	reward = 0.082799	array([[-2.5310645, -2.9731483]], dtype=float32)

time = 30862	action = 1	current_phase = 0	next_phase = 1	reward = -1.672751	array([[-5.404919 , -3.3979206]], dtype=float32)

time = 30870	action = 1	current_phase = 1	next_phase = 0	reward = -0.999355	array([[-3.9723446, -2.2273464]], dtype=float32)

time = 30878	action = 0	current_phase = 0	next_phase = 1	reward = -0.041555	array([[-1.6251836, -2.701016 ]], dtype=float32)

time = 30883	action = 0	current_phase = 0	next_phase = 1	reward = 0.029123	array([[-2.136283 , -2.9608598]], dtype=float32)

time = 30888	action = 0	current_phase = 0	next_phase = 1	reward = 0.074077	array([[-2.7453408, -2.7990406]], dtype=float32)

time = 30893	action = 1	current_phase = 0	next_phase = 1	reward = -0.800180	array([[-5.2455306, -3.297941 ]], dtype=float32)

time = 30901	action = 1	current_phase = 1	next_phase = 0	reward = -1.235842	array([[-3.625941 , -2.3375945]], dtype=float32)

time = 30909	action = 0	current_phase = 0	next_phase = 1	reward = -0.031445	array([[-1.4557552, -2.5499914]], dtype=float32)

time = 30914	action = 0	current_phase = 0	next_phase = 1	reward = 0.042312	array([[-1.6866658, -2.7027388]], dtype=float32)

time = 30919	action = 1	current_phase = 0	next_phase = 1	reward = -0.645960	array([[-2.3781474, -2.3009002]], dtype=float32)

time = 30927	action = 1	current_phase = 1	next_phase = 0	reward = -0.644630	array([[-3.278602 , -2.0269654]], dtype=float32)

time = 30935	action = 0	current_phase = 0	next_phase = 1	reward = -0.091940	array([[-1.5725994, -2.940999 ]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0558 - val_loss: 0.0290

Epoch 2/50

 - 3s - loss: 0.0646 - val_loss: 0.0399

Epoch 3/50

 - 3s - loss: 0.0608 - val_loss: 0.0393

Epoch 4/50

 - 3s - loss: 0.0638 - val_loss: 0.0326

Epoch 5/50

 - 3s - loss: 0.0537 - val_loss: 0.0303

Epoch 6/50

 - 3s - loss: 0.0587 - val_loss: 0.0338

Epoch 7/50

 - 3s - loss: 0.0542 - val_loss: 0.0308

Epoch 8/50

 - 3s - loss: 0.0531 - val_loss: 0.0482

Epoch 9/50

 - 3s - loss: 0.0516 - val_loss: 0.0417

Epoch 10/50

 - 3s - loss: 0.0485 - val_loss: 0.0341

Epoch 11/50

 - 3s - loss: 0.0548 - val_loss: 0.0370

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 30940	action = 0	current_phase = 0	next_phase = 1	reward = -0.014085	array([[-1.6088891, -2.4806664]], dtype=float32)

time = 30945	action = 0	current_phase = 0	next_phase = 1	reward = 0.055689	array([[-2.1675909, -3.0156908]], dtype=float32)

time = 30950	action = 1	current_phase = 0	next_phase = 1	reward = -1.377645	array([[-4.4465647, -2.7674234]], dtype=float32)

time = 30958	action = 1	current_phase = 1	next_phase = 0	reward = -0.708762	array([[-3.9318936, -1.9407883]], dtype=float32)

time = 30966	action = 0	current_phase = 0	next_phase = 1	reward = -0.086650	array([[-1.5540378, -2.6922786]], dtype=float32)

time = 30971	action = 0	current_phase = 0	next_phase = 1	reward = -0.016887	array([[-1.8646332, -2.7751422]], dtype=float32)

time = 30976	action = 0	current_phase = 0	next_phase = 1	reward = 0.049719	array([[-2.199924 , -2.9767597]], dtype=float32)

time = 30981	action = 1	current_phase = 0	next_phase = 1	reward = -1.445558	array([[-5.464047 , -3.2342958]], dtype=float32)

time = 30989	action = 1	current_phase = 1	next_phase = 0	reward = -0.765825	array([[-3.9534166, -2.0493708]], dtype=float32)

time = 30997	action = 0	current_phase = 0	next_phase = 1	reward = -0.064753	array([[-1.6248517, -2.833417 ]], dtype=float32)

time = 31002	action = 0	current_phase = 0	next_phase = 1	reward = 0.011306	array([[-2.0823112, -2.9651756]], dtype=float32)

time = 31007	action = 0	current_phase = 0	next_phase = 1	reward = 0.064192	array([[-2.3427515, -3.0839539]], dtype=float32)

time = 31012	action = 1	current_phase = 0	next_phase = 1	reward = -1.658849	array([[-5.4535356, -3.4400558]], dtype=float32)

time = 31020	action = 1	current_phase = 1	next_phase = 0	reward = -0.958741	array([[-4.0214767, -2.1468716]], dtype=float32)

time = 31028	action = 0	current_phase = 0	next_phase = 1	reward = -0.036031	array([[-1.5504384, -2.735424 ]], dtype=float32)

time = 31033	action = 0	current_phase = 0	next_phase = 1	reward = 0.019789	array([[-1.9723053, -2.984886 ]], dtype=float32)

time = 31038	action = 0	current_phase = 0	next_phase = 1	reward = 0.074300	array([[-2.4624333, -2.9181228]], dtype=float32)

time = 31043	action = 1	current_phase = 0	next_phase = 1	reward = -1.330061	array([[-5.2610264, -3.3374822]], dtype=float32)

time = 31051	action = 1	current_phase = 1	next_phase = 0	reward = -1.399047	array([[-3.8411064, -2.3250835]], dtype=float32)

time = 31059	action = 0	current_phase = 0	next_phase = 1	reward = 0.229495	array([[-1.3212205, -2.4979722]], dtype=float32)

time = 31064	action = 0	current_phase = 0	next_phase = 1	reward = 0.020130	array([[-1.7177383, -2.8179219]], dtype=float32)

time = 31069	action = 0	current_phase = 0	next_phase = 1	reward = 0.066184	array([[-2.1830237, -2.4197884]], dtype=float32)

time = 31074	action = 1	current_phase = 0	next_phase = 1	reward = -1.003212	array([[-5.140024 , -3.0394294]], dtype=float32)

time = 31082	action = 1	current_phase = 1	next_phase = 0	reward = -0.686577	array([[-3.959536 , -2.1509655]], dtype=float32)

time = 31090	action = 0	current_phase = 0	next_phase = 1	reward = -0.028717	array([[-1.6917223, -2.562059 ]], dtype=float32)

time = 31095	action = 0	current_phase = 0	next_phase = 1	reward = -0.236232	array([[-2.049927, -2.900917]], dtype=float32)

time = 31100	action = 1	current_phase = 0	next_phase = 1	reward = -1.090842	array([[-4.336116 , -2.7434218]], dtype=float32)

time = 31108	action = 1	current_phase = 1	next_phase = 0	reward = -0.711965	array([[-3.962996 , -2.0216866]], dtype=float32)

time = 31116	action = 0	current_phase = 0	next_phase = 1	reward = -0.081370	array([[-1.5191687, -2.6800592]], dtype=float32)

time = 31121	action = 0	current_phase = 0	next_phase = 1	reward = 0.016241	array([[-1.9320604, -2.8077056]], dtype=float32)

time = 31126	action = 0	current_phase = 0	next_phase = 1	reward = 0.074864	array([[-2.1897447, -3.0791266]], dtype=float32)

time = 31131	action = 1	current_phase = 0	next_phase = 1	reward = -1.459693	array([[-5.2944555, -3.274085 ]], dtype=float32)

time = 31139	action = 1	current_phase = 1	next_phase = 0	reward = -1.121768	array([[-3.8726273, -1.9520137]], dtype=float32)

time = 31147	action = 0	current_phase = 0	next_phase = 1	reward = 0.219004	array([[-1.5356169, -2.7260485]], dtype=float32)

time = 31152	action = 0	current_phase = 0	next_phase = 1	reward = 0.007501	array([[-2.0723438, -2.9805894]], dtype=float32)

time = 31157	action = 0	current_phase = 0	next_phase = 1	reward = 0.055894	array([[-2.457888 , -3.0258393]], dtype=float32)

time = 31162	action = 1	current_phase = 0	next_phase = 1	reward = -1.550875	array([[-5.4184017, -3.3327687]], dtype=float32)

time = 31170	action = 1	current_phase = 1	next_phase = 0	reward = -0.943905	array([[-3.966999 , -2.1492605]], dtype=float32)

time = 31178	action = 0	current_phase = 0	next_phase = 1	reward = -0.050635	array([[-1.4501745, -2.7142613]], dtype=float32)

time = 31183	action = 0	current_phase = 0	next_phase = 1	reward = 0.032334	array([[-1.9348204, -3.0606008]], dtype=float32)

time = 31188	action = 0	current_phase = 0	next_phase = 1	reward = 0.073133	array([[-2.3953345, -2.9318967]], dtype=float32)

time = 31193	action = 1	current_phase = 0	next_phase = 1	reward = -1.335068	array([[-5.299418 , -3.4531968]], dtype=float32)

time = 31201	action = 1	current_phase = 1	next_phase = 0	reward = -1.471757	array([[-3.7951174, -2.3113508]], dtype=float32)

time = 31209	action = 0	current_phase = 0	next_phase = 1	reward = 0.257813	array([[-1.3125206, -2.4591062]], dtype=float32)

time = 31214	action = 0	current_phase = 0	next_phase = 1	reward = 0.050955	array([[-1.8609846, -2.8206627]], dtype=float32)

time = 31219	action = 0	current_phase = 0	next_phase = 1	reward = 0.058778	array([[-2.1325455, -2.3560572]], dtype=float32)

time = 31224	action = 1	current_phase = 0	next_phase = 1	reward = -1.083365	array([[-4.899589 , -2.9063568]], dtype=float32)

time = 31232	action = 1	current_phase = 1	next_phase = 0	reward = -0.751927	array([[-3.7374282, -2.229414 ]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0623 - val_loss: 0.0277

Epoch 2/50

 - 3s - loss: 0.0645 - val_loss: 0.0253

Epoch 3/50

 - 3s - loss: 0.0575 - val_loss: 0.0264

Epoch 4/50

 - 3s - loss: 0.0563 - val_loss: 0.0278

Epoch 5/50

 - 3s - loss: 0.0519 - val_loss: 0.0276

Epoch 6/50

 - 3s - loss: 0.0565 - val_loss: 0.0275

Epoch 7/50

 - 3s - loss: 0.0462 - val_loss: 0.0272

Epoch 8/50

 - 3s - loss: 0.0539 - val_loss: 0.0289

Epoch 9/50

 - 3s - loss: 0.0538 - val_loss: 0.0267

Epoch 10/50

 - 3s - loss: 0.0539 - val_loss: 0.0299

Epoch 11/50

 - 3s - loss: 0.0496 - val_loss: 0.0319

Epoch 12/50

 - 3s - loss: 0.0502 - val_loss: 0.0283

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 31240	action = 0	current_phase = 0	next_phase = 1	reward = -0.318502	array([[-1.612361 , -2.5148833]], dtype=float32)

time = 31245	action = 0	current_phase = 0	next_phase = 1	reward = 0.036777	array([[-1.8994658, -2.8030539]], dtype=float32)

time = 31250	action = 1	current_phase = 0	next_phase = 1	reward = -0.956383	array([[-4.196116 , -2.7500522]], dtype=float32)

time = 31258	action = 1	current_phase = 1	next_phase = 0	reward = -0.650432	array([[-3.6323957, -1.7822924]], dtype=float32)

time = 31266	action = 0	current_phase = 0	next_phase = 1	reward = -0.088571	array([[-1.500881 , -2.6802492]], dtype=float32)

time = 31271	action = 0	current_phase = 0	next_phase = 1	reward = 0.005821	array([[-1.9586053, -2.83906  ]], dtype=float32)

time = 31276	action = 0	current_phase = 0	next_phase = 1	reward = 0.085361	array([[-2.4040163, -2.9554484]], dtype=float32)

time = 31281	action = 1	current_phase = 0	next_phase = 1	reward = -1.495011	array([[-5.5049496, -3.147824 ]], dtype=float32)

time = 31289	action = 1	current_phase = 1	next_phase = 0	reward = -0.776566	array([[-3.8032494, -1.8917489]], dtype=float32)

time = 31297	action = 0	current_phase = 0	next_phase = 1	reward = -0.062331	array([[-1.5565846, -2.7313576]], dtype=float32)

time = 31302	action = 0	current_phase = 0	next_phase = 1	reward = 0.013987	array([[-2.0364408, -2.918399 ]], dtype=float32)

time = 31307	action = 0	current_phase = 0	next_phase = 1	reward = 0.074111	array([[-2.5463252, -2.9187315]], dtype=float32)

time = 31312	action = 1	current_phase = 0	next_phase = 1	reward = -1.671564	array([[-5.4909797, -3.313798 ]], dtype=float32)

time = 31320	action = 1	current_phase = 1	next_phase = 0	reward = -1.002455	array([[-3.9259   , -2.1133342]], dtype=float32)

time = 31328	action = 0	current_phase = 0	next_phase = 1	reward = -0.052397	array([[-1.5098662, -2.6718953]], dtype=float32)

time = 31333	action = 0	current_phase = 0	next_phase = 1	reward = 0.011864	array([[-1.8901944, -3.0022807]], dtype=float32)

time = 31338	action = 0	current_phase = 0	next_phase = 1	reward = 0.070971	array([[-2.4209566, -2.8320932]], dtype=float32)

time = 31343	action = 1	current_phase = 0	next_phase = 1	reward = -1.858249	array([[-5.247346 , -3.5151336]], dtype=float32)

time = 31351	action = 1	current_phase = 1	next_phase = 0	reward = -1.022245	array([[-4.0429544, -2.2218893]], dtype=float32)

time = 31359	action = 0	current_phase = 0	next_phase = 1	reward = -0.037055	array([[-1.287698 , -2.4131713]], dtype=float32)

time = 31364	action = 0	current_phase = 0	next_phase = 1	reward = 0.042922	array([[-1.8129308, -2.8213878]], dtype=float32)

time = 31369	action = 0	current_phase = 0	next_phase = 1	reward = 0.046888	array([[-2.220375 , -2.4126325]], dtype=float32)

time = 31374	action = 1	current_phase = 0	next_phase = 1	reward = -1.123802	array([[-5.1712275, -2.942075 ]], dtype=float32)

time = 31382	action = 1	current_phase = 1	next_phase = 0	reward = -1.306707	array([[-3.7529337, -2.182033 ]], dtype=float32)

time = 31390	action = 0	current_phase = 0	next_phase = 1	reward = 0.530341	array([[-1.5981596, -2.5173905]], dtype=float32)

time = 31395	action = 0	current_phase = 0	next_phase = 1	reward = -0.521372	array([[-1.8920202, -2.7989757]], dtype=float32)

time = 31400	action = 1	current_phase = 0	next_phase = 1	reward = -0.796631	array([[-4.1552596, -2.680667 ]], dtype=float32)

time = 31408	action = 1	current_phase = 1	next_phase = 0	reward = -0.692242	array([[-3.8501325, -1.9557179]], dtype=float32)

time = 31416	action = 0	current_phase = 0	next_phase = 1	reward = -0.077047	array([[-1.5801136, -2.7015405]], dtype=float32)

time = 31421	action = 0	current_phase = 0	next_phase = 1	reward = -0.013129	array([[-1.8470135, -2.803302 ]], dtype=float32)

time = 31426	action = 0	current_phase = 0	next_phase = 1	reward = 0.048644	array([[-2.2319188, -2.996901 ]], dtype=float32)

time = 31431	action = 1	current_phase = 0	next_phase = 1	reward = -1.507758	array([[-5.4329996, -3.1794415]], dtype=float32)

time = 31439	action = 1	current_phase = 1	next_phase = 0	reward = -0.775845	array([[-3.9055512, -2.0117548]], dtype=float32)

time = 31447	action = 0	current_phase = 0	next_phase = 1	reward = -0.057427	array([[-1.5443387, -2.7455842]], dtype=float32)

time = 31452	action = 0	current_phase = 0	next_phase = 1	reward = 0.000808	array([[-2.058996 , -2.9156482]], dtype=float32)

time = 31457	action = 0	current_phase = 0	next_phase = 1	reward = 0.064854	array([[-2.5077298, -3.0139973]], dtype=float32)

time = 31462	action = 1	current_phase = 0	next_phase = 1	reward = -1.605611	array([[-5.4619823, -3.3257558]], dtype=float32)

time = 31470	action = 1	current_phase = 1	next_phase = 0	reward = -1.280310	array([[-3.8784392, -2.039653 ]], dtype=float32)

time = 31478	action = 0	current_phase = 0	next_phase = 1	reward = 0.246611	array([[-1.4951458, -2.659479 ]], dtype=float32)

time = 31483	action = 0	current_phase = 0	next_phase = 1	reward = 0.021882	array([[-1.9391074, -2.941212 ]], dtype=float32)

time = 31488	action = 0	current_phase = 0	next_phase = 1	reward = 0.078689	array([[-2.603478 , -2.8173358]], dtype=float32)

time = 31493	action = 1	current_phase = 0	next_phase = 1	reward = -1.400210	array([[-5.341979, -3.284131]], dtype=float32)

time = 31501	action = 1	current_phase = 1	next_phase = 0	reward = -1.191104	array([[-3.656816 , -2.3980584]], dtype=float32)

time = 31509	action = 0	current_phase = 0	next_phase = 1	reward = -0.044991	array([[-1.4215944, -2.486819 ]], dtype=float32)

time = 31514	action = 0	current_phase = 0	next_phase = 1	reward = 0.021406	array([[-1.7931743, -2.7811947]], dtype=float32)

time = 31519	action = 0	current_phase = 0	next_phase = 1	reward = 0.067704	array([[-2.1901598, -2.2927892]], dtype=float32)

time = 31524	action = 1	current_phase = 0	next_phase = 1	reward = -0.959372	array([[-5.101808 , -2.9487019]], dtype=float32)

time = 31532	action = 1	current_phase = 1	next_phase = 0	reward = -0.684928	array([[-3.867686 , -2.1342592]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0397 - val_loss: 0.0338

Epoch 2/50

 - 3s - loss: 0.0548 - val_loss: 0.0440

Epoch 3/50

 - 3s - loss: 0.0524 - val_loss: 0.0353

Epoch 4/50

 - 3s - loss: 0.0408 - val_loss: 0.0400

Epoch 5/50

 - 3s - loss: 0.0409 - val_loss: 0.0332

Epoch 6/50

 - 3s - loss: 0.0381 - val_loss: 0.0365

Epoch 7/50

 - 3s - loss: 0.0393 - val_loss: 0.0351

Epoch 8/50

 - 3s - loss: 0.0368 - val_loss: 0.0316

Epoch 9/50

 - 3s - loss: 0.0433 - val_loss: 0.0320

Epoch 10/50

 - 3s - loss: 0.0433 - val_loss: 0.0371

Epoch 11/50

 - 3s - loss: 0.0314 - val_loss: 0.0432

Epoch 12/50

 - 3s - loss: 0.0408 - val_loss: 0.0425

Epoch 13/50

 - 3s - loss: 0.0374 - val_loss: 0.0316

Epoch 14/50

 - 3s - loss: 0.0310 - val_loss: 0.0373

Epoch 15/50

 - 3s - loss: 0.0365 - val_loss: 0.0367

Epoch 16/50

 - 3s - loss: 0.0347 - val_loss: 0.0394

Epoch 17/50

 - 3s - loss: 0.0340 - val_loss: 0.0347

Epoch 18/50

 - 3s - loss: 0.0350 - val_loss: 0.0365

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 31540	action = 0	current_phase = 0	next_phase = 1	reward = -0.279791	array([[-1.6955471, -2.5456562]], dtype=float32)

time = 31545	action = 0	current_phase = 0	next_phase = 1	reward = 0.353933	array([[-2.063271 , -2.8772602]], dtype=float32)

time = 31550	action = 1	current_phase = 0	next_phase = 1	reward = -1.446854	array([[-4.344346 , -2.8262107]], dtype=float32)

time = 31558	action = 1	current_phase = 1	next_phase = 0	reward = -0.719720	array([[-3.8083198, -1.8998357]], dtype=float32)

time = 31566	action = 0	current_phase = 0	next_phase = 1	reward = -0.091582	array([[-1.6767929, -2.7251139]], dtype=float32)

time = 31571	action = 0	current_phase = 0	next_phase = 1	reward = -0.006463	array([[-1.9332135, -2.7775102]], dtype=float32)

time = 31576	action = 0	current_phase = 0	next_phase = 1	reward = 0.064539	array([[-2.2913692, -2.9922378]], dtype=float32)

time = 31581	action = 1	current_phase = 0	next_phase = 1	reward = -1.441017	array([[-5.4388304, -3.2078052]], dtype=float32)

time = 31589	action = 1	current_phase = 1	next_phase = 0	reward = -1.122463	array([[-3.876726 , -1.9828684]], dtype=float32)

time = 31597	action = 0	current_phase = 0	next_phase = 1	reward = 0.220987	array([[-1.70534 , -2.749199]], dtype=float32)

time = 31602	action = 0	current_phase = 0	next_phase = 1	reward = 0.001319	array([[-2.104603 , -2.9198782]], dtype=float32)

time = 31607	action = 0	current_phase = 0	next_phase = 1	reward = 0.064539	array([[-2.5799787, -2.9794528]], dtype=float32)

time = 31612	action = 1	current_phase = 0	next_phase = 1	reward = -1.587253	array([[-5.534539, -3.463895]], dtype=float32)

time = 31620	action = 1	current_phase = 1	next_phase = 0	reward = -1.008291	array([[-3.9170747, -2.1320992]], dtype=float32)

time = 31628	action = 0	current_phase = 0	next_phase = 1	reward = -0.056359	array([[-1.5933194, -2.667334 ]], dtype=float32)

time = 31633	action = 0	current_phase = 0	next_phase = 1	reward = 0.021011	array([[-2.0616324, -2.9440773]], dtype=float32)

time = 31638	action = 0	current_phase = 0	next_phase = 1	reward = 0.090771	array([[-2.6779332, -2.8450406]], dtype=float32)

time = 31643	action = 1	current_phase = 0	next_phase = 1	reward = -1.820770	array([[-5.276038, -3.617695]], dtype=float32)

time = 31651	action = 1	current_phase = 1	next_phase = 0	reward = -1.123053	array([[-4.0971203, -2.2312717]], dtype=float32)

time = 31659	action = 0	current_phase = 0	next_phase = 1	reward = -0.015670	array([[-1.4863551, -2.4814463]], dtype=float32)

time = 31664	action = 0	current_phase = 0	next_phase = 1	reward = 0.062882	array([[-1.8950155, -2.8120096]], dtype=float32)

time = 31669	action = 0	current_phase = 0	next_phase = 1	reward = 0.041162	array([[-2.276284 , -2.4403436]], dtype=float32)

time = 31674	action = 1	current_phase = 0	next_phase = 1	reward = -1.195582	array([[-5.213484 , -3.0366924]], dtype=float32)

time = 31682	action = 1	current_phase = 1	next_phase = 0	reward = -1.038681	array([[-3.7400486, -2.2079015]], dtype=float32)

time = 31690	action = 0	current_phase = 0	next_phase = 1	reward = -0.010240	array([[-1.6281905, -2.537498 ]], dtype=float32)

time = 31695	action = 0	current_phase = 0	next_phase = 1	reward = 0.338471	array([[-1.9614944, -2.8064673]], dtype=float32)

time = 31700	action = 1	current_phase = 0	next_phase = 1	reward = -1.367343	array([[-4.3804393, -2.9072258]], dtype=float32)

time = 31708	action = 1	current_phase = 1	next_phase = 0	reward = -0.712579	array([[-3.8298821, -1.9569421]], dtype=float32)

time = 31716	action = 0	current_phase = 0	next_phase = 1	reward = -0.100271	array([[-1.5912459, -2.717777 ]], dtype=float32)

time = 31721	action = 0	current_phase = 0	next_phase = 1	reward = -0.005811	array([[-2.0077262, -2.8025117]], dtype=float32)

time = 31726	action = 0	current_phase = 0	next_phase = 1	reward = 0.061661	array([[-2.2763126, -2.9811912]], dtype=float32)

time = 31731	action = 1	current_phase = 0	next_phase = 1	reward = -1.453572	array([[-5.3778195, -3.2629128]], dtype=float32)

time = 31739	action = 1	current_phase = 1	next_phase = 0	reward = -0.754940	array([[-3.7950895, -1.9255525]], dtype=float32)

time = 31747	action = 0	current_phase = 0	next_phase = 1	reward = -0.065914	array([[-1.6529223, -2.7484596]], dtype=float32)

time = 31752	action = 0	current_phase = 0	next_phase = 1	reward = 0.022937	array([[-2.104021, -2.923403]], dtype=float32)

time = 31757	action = 0	current_phase = 0	next_phase = 1	reward = 0.074199	array([[-2.63029  , -2.9191668]], dtype=float32)

time = 31762	action = 1	current_phase = 0	next_phase = 1	reward = -1.566461	array([[-5.492665 , -3.3987908]], dtype=float32)

time = 31770	action = 1	current_phase = 1	next_phase = 0	reward = -1.306824	array([[-3.9852185, -2.147332 ]], dtype=float32)

time = 31778	action = 0	current_phase = 0	next_phase = 1	reward = 0.227657	array([[-1.5395869, -2.6453514]], dtype=float32)

time = 31783	action = 0	current_phase = 0	next_phase = 1	reward = -0.012835	array([[-2.051126, -2.981459]], dtype=float32)

time = 31788	action = 0	current_phase = 0	next_phase = 1	reward = 0.069549	array([[-2.4760556, -2.83243  ]], dtype=float32)

time = 31793	action = 1	current_phase = 0	next_phase = 1	reward = -1.782014	array([[-5.397138 , -3.4890323]], dtype=float32)

time = 31801	action = 1	current_phase = 1	next_phase = 0	reward = -1.015827	array([[-4.201602, -2.28259 ]], dtype=float32)

time = 31809	action = 0	current_phase = 0	next_phase = 1	reward = -0.032422	array([[-1.4366391, -2.4274347]], dtype=float32)

time = 31814	action = 0	current_phase = 0	next_phase = 1	reward = 0.295688	array([[-1.7919939, -2.7915323]], dtype=float32)

time = 31819	action = 1	current_phase = 0	next_phase = 1	reward = -1.035099	array([[-2.451611, -2.346132]], dtype=float32)

time = 31827	action = 1	current_phase = 1	next_phase = 0	reward = -0.922584	array([[-3.6842463, -1.8471467]], dtype=float32)

time = 31835	action = 0	current_phase = 0	next_phase = 1	reward = 0.177347	array([[-1.4598637, -2.8403258]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0521 - val_loss: 0.0233

Epoch 2/50

 - 3s - loss: 0.0487 - val_loss: 0.0271

Epoch 3/50

 - 3s - loss: 0.0480 - val_loss: 0.0217

Epoch 4/50

 - 3s - loss: 0.0582 - val_loss: 0.0246

Epoch 5/50

 - 3s - loss: 0.0526 - val_loss: 0.0221

Epoch 6/50

 - 3s - loss: 0.0465 - val_loss: 0.0233

Epoch 7/50

 - 3s - loss: 0.0430 - val_loss: 0.0281

Epoch 8/50

 - 3s - loss: 0.0364 - val_loss: 0.0330

Epoch 9/50

 - 3s - loss: 0.0419 - val_loss: 0.0221

Epoch 10/50

 - 3s - loss: 0.0391 - val_loss: 0.0207

Epoch 11/50

 - 3s - loss: 0.0398 - val_loss: 0.0253

Epoch 12/50

 - 3s - loss: 0.0385 - val_loss: 0.0250

Epoch 13/50

 - 3s - loss: 0.0490 - val_loss: 0.0316

Epoch 14/50

 - 3s - loss: 0.0538 - val_loss: 0.0335

Epoch 15/50

 - 3s - loss: 0.0401 - val_loss: 0.0290

Epoch 16/50

 - 3s - loss: 0.0437 - val_loss: 0.0283

Epoch 17/50

 - 3s - loss: 0.0444 - val_loss: 0.0275

Epoch 18/50

 - 3s - loss: 0.0326 - val_loss: 0.0308

Epoch 19/50

 - 3s - loss: 0.0440 - val_loss: 0.0251

Epoch 20/50

 - 3s - loss: 0.0425 - val_loss: 0.0414

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 31840	action = 0	current_phase = 0	next_phase = 1	reward = -0.298811	array([[-1.7136619, -2.4650786]], dtype=float32)

time = 31845	action = 0	current_phase = 0	next_phase = 1	reward = 0.329121	array([[-2.2230482, -2.927756 ]], dtype=float32)

time = 31850	action = 1	current_phase = 0	next_phase = 1	reward = -1.326032	array([[-4.5511675, -2.8580608]], dtype=float32)

time = 31858	action = 1	current_phase = 1	next_phase = 0	reward = -0.713198	array([[-3.8372471, -1.9950835]], dtype=float32)

time = 31866	action = 0	current_phase = 0	next_phase = 1	reward = -0.074912	array([[-1.6512423, -2.742304 ]], dtype=float32)

time = 31871	action = 0	current_phase = 0	next_phase = 1	reward = 0.008217	array([[-1.8598596, -2.7622561]], dtype=float32)

time = 31876	action = 0	current_phase = 0	next_phase = 1	reward = 0.064650	array([[-2.4165378, -3.0435863]], dtype=float32)

time = 31881	action = 1	current_phase = 0	next_phase = 1	reward = -1.435064	array([[-5.673366 , -3.0996761]], dtype=float32)

time = 31889	action = 1	current_phase = 1	next_phase = 0	reward = -0.758962	array([[-3.9583046, -2.2179995]], dtype=float32)

time = 31897	action = 0	current_phase = 0	next_phase = 1	reward = -0.060901	array([[-1.7243558, -2.776601 ]], dtype=float32)

time = 31902	action = 0	current_phase = 0	next_phase = 1	reward = -0.000871	array([[-2.1191745, -2.916512 ]], dtype=float32)

time = 31907	action = 0	current_phase = 0	next_phase = 1	reward = 0.054845	array([[-2.5925298, -2.9737031]], dtype=float32)

time = 31912	action = 1	current_phase = 0	next_phase = 1	reward = -1.526475	array([[-5.5721183, -3.2416449]], dtype=float32)

time = 31920	action = 1	current_phase = 1	next_phase = 0	reward = -0.957185	array([[-4.0478535, -2.3765543]], dtype=float32)

time = 31928	action = 0	current_phase = 0	next_phase = 1	reward = -0.055009	array([[-1.6303769, -2.662376 ]], dtype=float32)

time = 31933	action = 0	current_phase = 0	next_phase = 1	reward = 0.009351	array([[-2.0819263, -2.9796252]], dtype=float32)

time = 31938	action = 0	current_phase = 0	next_phase = 1	reward = 0.076686	array([[-2.7743654, -2.8343742]], dtype=float32)

time = 31943	action = 1	current_phase = 0	next_phase = 1	reward = -1.319431	array([[-5.458868 , -3.3303833]], dtype=float32)

time = 31951	action = 1	current_phase = 1	next_phase = 0	reward = -1.585684	array([[-3.889972 , -2.4640179]], dtype=float32)

time = 31959	action = 0	current_phase = 0	next_phase = 1	reward = 0.258686	array([[-1.4032427, -2.423875 ]], dtype=float32)

time = 31964	action = 0	current_phase = 0	next_phase = 1	reward = 0.041409	array([[-1.763591 , -2.7860258]], dtype=float32)

time = 31969	action = 1	current_phase = 0	next_phase = 1	reward = -0.697086	array([[-2.3921144, -2.345231 ]], dtype=float32)

time = 31977	action = 1	current_phase = 1	next_phase = 0	reward = -1.215593	array([[-3.669793 , -1.9465294]], dtype=float32)

time = 31985	action = 0	current_phase = 0	next_phase = 1	reward = 0.453710	array([[-1.473347, -2.824757]], dtype=float32)

time = 31990	action = 0	current_phase = 0	next_phase = 1	reward = -0.309358	array([[-1.7353523, -2.4800773]], dtype=float32)

time = 31995	action = 0	current_phase = 0	next_phase = 1	reward = 0.039234	array([[-2.320929 , -2.9868934]], dtype=float32)

time = 32000	action = 1	current_phase = 0	next_phase = 1	reward = -1.143388	array([[-4.54777  , -2.7178416]], dtype=float32)

time = 32008	action = 1	current_phase = 1	next_phase = 0	reward = -0.695195	array([[-3.885185 , -2.0571237]], dtype=float32)

time = 32016	action = 0	current_phase = 0	next_phase = 1	reward = -0.075012	array([[-1.6570072, -2.710011 ]], dtype=float32)

time = 32021	action = 0	current_phase = 0	next_phase = 1	reward = -0.002239	array([[-1.8061978, -2.7731884]], dtype=float32)

time = 32026	action = 0	current_phase = 0	next_phase = 1	reward = 0.056684	array([[-2.3600748, -3.0268018]], dtype=float32)

time = 32031	action = 1	current_phase = 0	next_phase = 1	reward = -1.467684	array([[-5.5150228, -3.1147733]], dtype=float32)

time = 32039	action = 1	current_phase = 1	next_phase = 0	reward = -0.843378	array([[-3.8999043, -2.126765 ]], dtype=float32)

time = 32047	action = 0	current_phase = 0	next_phase = 1	reward = -0.065745	array([[-1.6582325, -2.7605035]], dtype=float32)

time = 32052	action = 0	current_phase = 0	next_phase = 1	reward = 0.007184	array([[-2.092462 , -2.9182353]], dtype=float32)

time = 32057	action = 0	current_phase = 0	next_phase = 1	reward = 0.054927	array([[-2.4935713, -3.014905 ]], dtype=float32)

time = 32062	action = 1	current_phase = 0	next_phase = 1	reward = -1.581390	array([[-5.6380954, -3.2717865]], dtype=float32)

time = 32070	action = 1	current_phase = 1	next_phase = 0	reward = -0.989388	array([[-4.066993 , -2.4744744]], dtype=float32)

time = 32078	action = 0	current_phase = 0	next_phase = 1	reward = -0.059423	array([[-1.5599668, -2.6511068]], dtype=float32)

time = 32083	action = 0	current_phase = 0	next_phase = 1	reward = 0.012810	array([[-2.0481343, -2.9885929]], dtype=float32)

time = 32088	action = 0	current_phase = 0	next_phase = 1	reward = 0.075563	array([[-2.799996 , -2.9205885]], dtype=float32)

time = 32093	action = 1	current_phase = 0	next_phase = 1	reward = -1.872700	array([[-5.4580626, -3.4650924]], dtype=float32)

time = 32101	action = 1	current_phase = 1	next_phase = 0	reward = -1.078814	array([[-4.2010975, -2.4078026]], dtype=float32)

time = 32109	action = 0	current_phase = 0	next_phase = 1	reward = -0.019713	array([[-1.384881, -2.404476]], dtype=float32)

time = 32114	action = 0	current_phase = 0	next_phase = 1	reward = 0.063688	array([[-1.9210162, -2.926198 ]], dtype=float32)

time = 32119	action = 1	current_phase = 0	next_phase = 1	reward = -0.696712	array([[-2.3708806, -2.3146818]], dtype=float32)

time = 32127	action = 1	current_phase = 1	next_phase = 0	reward = -0.655577	array([[-3.548936, -2.004283]], dtype=float32)

time = 32135	action = 0	current_phase = 0	next_phase = 1	reward = -0.108089	array([[-1.3908186, -2.8943586]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0514 - val_loss: 0.0404

Epoch 2/50

 - 3s - loss: 0.0418 - val_loss: 0.0363

Epoch 3/50

 - 3s - loss: 0.0503 - val_loss: 0.0285

Epoch 4/50

 - 3s - loss: 0.0477 - val_loss: 0.0253

Epoch 5/50

 - 3s - loss: 0.0531 - val_loss: 0.0264

Epoch 6/50

 - 3s - loss: 0.0374 - val_loss: 0.0282

Epoch 7/50

 - 3s - loss: 0.0418 - val_loss: 0.0369

Epoch 8/50

 - 3s - loss: 0.0451 - val_loss: 0.0292

Epoch 9/50

 - 3s - loss: 0.0365 - val_loss: 0.0268

Epoch 10/50

 - 3s - loss: 0.0457 - val_loss: 0.0384

Epoch 11/50

 - 3s - loss: 0.0389 - val_loss: 0.0285

Epoch 12/50

 - 3s - loss: 0.0384 - val_loss: 0.0375

Epoch 13/50

 - 3s - loss: 0.0468 - val_loss: 0.0314

Epoch 14/50

 - 3s - loss: 0.0420 - val_loss: 0.0240

Epoch 15/50

 - 3s - loss: 0.0476 - val_loss: 0.0263

Epoch 16/50

 - 3s - loss: 0.0421 - val_loss: 0.0239

Epoch 17/50

 - 3s - loss: 0.0400 - val_loss: 0.0265

Epoch 18/50

 - 3s - loss: 0.0414 - val_loss: 0.0269

Epoch 19/50

 - 3s - loss: 0.0359 - val_loss: 0.0367

Epoch 20/50

 - 3s - loss: 0.0479 - val_loss: 0.0377

Epoch 21/50

 - 3s - loss: 0.0411 - val_loss: 0.0462

Epoch 22/50

 - 3s - loss: 0.0382 - val_loss: 0.0348

Epoch 23/50

 - 3s - loss: 0.0412 - val_loss: 0.0334

Epoch 24/50

 - 3s - loss: 0.0429 - val_loss: 0.0339

Epoch 25/50

 - 3s - loss: 0.0377 - val_loss: 0.0414

Epoch 26/50

 - 3s - loss: 0.0395 - val_loss: 0.0326

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 32140	action = 0	current_phase = 0	next_phase = 1	reward = -0.567609	array([[-1.6093782, -2.416875 ]], dtype=float32)

time = 32145	action = 0	current_phase = 0	next_phase = 1	reward = 0.612180	array([[-2.1776974, -2.8777795]], dtype=float32)

time = 32150	action = 1	current_phase = 0	next_phase = 1	reward = -1.265249	array([[-4.594392, -2.699298]], dtype=float32)

time = 32158	action = 1	current_phase = 1	next_phase = 0	reward = -0.711951	array([[-3.8243957, -2.0821133]], dtype=float32)

time = 32166	action = 0	current_phase = 0	next_phase = 1	reward = -0.082677	array([[-1.6099868, -2.7154355]], dtype=float32)

time = 32171	action = 0	current_phase = 0	next_phase = 1	reward = 0.001751	array([[-1.7963934, -2.7228265]], dtype=float32)

time = 32176	action = 0	current_phase = 0	next_phase = 1	reward = 0.074032	array([[-2.434799 , -3.0184398]], dtype=float32)

time = 32181	action = 1	current_phase = 0	next_phase = 1	reward = -1.437811	array([[-5.603809 , -3.0071077]], dtype=float32)

time = 32189	action = 1	current_phase = 1	next_phase = 0	reward = -0.826373	array([[-3.929546 , -2.1918793]], dtype=float32)

time = 32197	action = 0	current_phase = 0	next_phase = 1	reward = -0.071809	array([[-1.6400073, -2.7868369]], dtype=float32)

time = 32202	action = 0	current_phase = 0	next_phase = 1	reward = -0.006623	array([[-2.02298 , -2.900597]], dtype=float32)

time = 32207	action = 0	current_phase = 0	next_phase = 1	reward = 0.072966	array([[-2.6624084, -2.9649105]], dtype=float32)

time = 32212	action = 1	current_phase = 0	next_phase = 1	reward = -1.646981	array([[-5.531849 , -3.3158803]], dtype=float32)

time = 32220	action = 1	current_phase = 1	next_phase = 0	reward = -0.894424	array([[-3.959698 , -2.2438354]], dtype=float32)

time = 32228	action = 0	current_phase = 0	next_phase = 1	reward = -0.062671	array([[-1.5790987, -2.7135077]], dtype=float32)

time = 32233	action = 0	current_phase = 0	next_phase = 1	reward = 0.017108	array([[-1.9929304, -2.967362 ]], dtype=float32)

time = 32238	action = 0	current_phase = 0	next_phase = 1	reward = 0.084821	array([[-2.7153504, -2.8941865]], dtype=float32)

time = 32243	action = 1	current_phase = 0	next_phase = 1	reward = -1.330943	array([[-5.4529486, -3.2331324]], dtype=float32)

time = 32251	action = 1	current_phase = 1	next_phase = 0	reward = -1.451203	array([[-3.8674533, -2.352675 ]], dtype=float32)

time = 32259	action = 0	current_phase = 0	next_phase = 1	reward = 0.250195	array([[-1.3719442, -2.4660373]], dtype=float32)

time = 32264	action = 0	current_phase = 0	next_phase = 1	reward = 0.068528	array([[-1.8287659, -2.774057 ]], dtype=float32)

time = 32269	action = 0	current_phase = 0	next_phase = 1	reward = 0.061633	array([[-2.2360404, -2.25563  ]], dtype=float32)

time = 32274	action = 1	current_phase = 0	next_phase = 1	reward = -1.150270	array([[-5.136655, -2.814598]], dtype=float32)

time = 32282	action = 1	current_phase = 1	next_phase = 0	reward = -0.758887	array([[-3.8250592, -2.2499452]], dtype=float32)

time = 32290	action = 0	current_phase = 0	next_phase = 1	reward = -0.027475	array([[-1.7082369, -2.5221462]], dtype=float32)

time = 32295	action = 0	current_phase = 0	next_phase = 1	reward = -0.235026	array([[-2.1553183, -2.8825502]], dtype=float32)

time = 32300	action = 1	current_phase = 0	next_phase = 1	reward = -1.136042	array([[-4.3400426, -2.6181295]], dtype=float32)

time = 32308	action = 1	current_phase = 1	next_phase = 0	reward = -0.714109	array([[-3.8203437, -2.066846 ]], dtype=float32)

time = 32316	action = 0	current_phase = 0	next_phase = 1	reward = -0.090881	array([[-1.6460518, -2.736432 ]], dtype=float32)

time = 32321	action = 0	current_phase = 0	next_phase = 1	reward = -0.012048	array([[-1.7700069, -2.755218 ]], dtype=float32)

time = 32326	action = 0	current_phase = 0	next_phase = 1	reward = 0.067773	array([[-2.3765855, -3.0060592]], dtype=float32)

time = 32331	action = 1	current_phase = 0	next_phase = 1	reward = -1.485067	array([[-5.6471467, -2.999776 ]], dtype=float32)

time = 32339	action = 1	current_phase = 1	next_phase = 0	reward = -0.853782	array([[-3.9189365, -2.1633797]], dtype=float32)

time = 32347	action = 0	current_phase = 0	next_phase = 1	reward = -0.080886	array([[-1.630502 , -2.7726874]], dtype=float32)

time = 32352	action = 0	current_phase = 0	next_phase = 1	reward = -0.002899	array([[-1.9245856, -2.8195815]], dtype=float32)

time = 32357	action = 0	current_phase = 0	next_phase = 1	reward = 0.078807	array([[-2.4864795, -2.9743156]], dtype=float32)

time = 32362	action = 1	current_phase = 0	next_phase = 1	reward = -1.568905	array([[-5.585404, -3.209167]], dtype=float32)

time = 32370	action = 1	current_phase = 1	next_phase = 0	reward = -0.958155	array([[-3.9984024, -2.2915888]], dtype=float32)

time = 32378	action = 0	current_phase = 0	next_phase = 1	reward = -0.044961	array([[-1.5742368, -2.700904 ]], dtype=float32)

time = 32383	action = 0	current_phase = 0	next_phase = 1	reward = 0.029458	array([[-2.079987 , -2.9821296]], dtype=float32)

time = 32388	action = 0	current_phase = 0	next_phase = 1	reward = 0.074388	array([[-2.755813 , -2.8215532]], dtype=float32)

time = 32393	action = 1	current_phase = 0	next_phase = 1	reward = -1.396161	array([[-5.4887347, -3.33181  ]], dtype=float32)

time = 32401	action = 1	current_phase = 1	next_phase = 0	reward = -1.131651	array([[-3.8308814, -2.3075564]], dtype=float32)

time = 32409	action = 0	current_phase = 0	next_phase = 1	reward = -0.023355	array([[-1.4420183, -2.382649 ]], dtype=float32)

time = 32414	action = 0	current_phase = 0	next_phase = 1	reward = 0.047920	array([[-1.8365703, -2.7818518]], dtype=float32)

time = 32419	action = 1	current_phase = 0	next_phase = 1	reward = -0.720910	array([[-2.7254794, -2.3856392]], dtype=float32)

time = 32427	action = 1	current_phase = 1	next_phase = 0	reward = -0.642720	array([[-3.6442401, -2.1254776]], dtype=float32)

time = 32435	action = 0	current_phase = 0	next_phase = 1	reward = -0.375052	array([[-1.4745399, -2.7780714]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0591 - val_loss: 0.0208

Epoch 2/50

 - 3s - loss: 0.0517 - val_loss: 0.0195

Epoch 3/50

 - 3s - loss: 0.0503 - val_loss: 0.0169

Epoch 4/50

 - 3s - loss: 0.0491 - val_loss: 0.0173

Epoch 5/50

 - 3s - loss: 0.0462 - val_loss: 0.0201

Epoch 6/50

 - 3s - loss: 0.0546 - val_loss: 0.0204

Epoch 7/50

 - 3s - loss: 0.0445 - val_loss: 0.0180

Epoch 8/50

 - 3s - loss: 0.0478 - val_loss: 0.0186

Epoch 9/50

 - 3s - loss: 0.0420 - val_loss: 0.0205

Epoch 10/50

 - 3s - loss: 0.0458 - val_loss: 0.0221

Epoch 11/50

 - 3s - loss: 0.0387 - val_loss: 0.0197

Epoch 12/50

 - 3s - loss: 0.0457 - val_loss: 0.0224

Epoch 13/50

 - 3s - loss: 0.0353 - val_loss: 0.0176

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 32440	action = 0	current_phase = 0	next_phase = 1	reward = 0.255673	array([[-1.6200798, -2.4400682]], dtype=float32)

time = 32445	action = 0	current_phase = 0	next_phase = 1	reward = 0.047965	array([[-2.3593376, -3.0211325]], dtype=float32)

time = 32450	action = 1	current_phase = 0	next_phase = 1	reward = -1.299666	array([[-4.466507, -2.651857]], dtype=float32)

time = 32458	action = 1	current_phase = 1	next_phase = 0	reward = -0.713510	array([[-3.7322412, -2.073687 ]], dtype=float32)

time = 32466	action = 0	current_phase = 0	next_phase = 1	reward = -0.085977	array([[-1.6352336, -2.7922084]], dtype=float32)

time = 32471	action = 0	current_phase = 0	next_phase = 1	reward = -0.011247	array([[-1.8830965, -2.804277 ]], dtype=float32)

time = 32476	action = 0	current_phase = 0	next_phase = 1	reward = 0.053429	array([[-2.437809 , -3.0359006]], dtype=float32)

time = 32481	action = 1	current_phase = 0	next_phase = 1	reward = -1.439789	array([[-5.6243706, -3.0411358]], dtype=float32)

time = 32489	action = 1	current_phase = 1	next_phase = 0	reward = -0.778849	array([[-3.9043608, -2.1327126]], dtype=float32)

time = 32497	action = 0	current_phase = 0	next_phase = 1	reward = -0.065968	array([[-1.6581744, -2.797451 ]], dtype=float32)

time = 32502	action = 0	current_phase = 0	next_phase = 1	reward = 0.010037	array([[-1.9581058, -2.867322 ]], dtype=float32)

time = 32507	action = 0	current_phase = 0	next_phase = 1	reward = 0.062166	array([[-2.6285105, -2.9953175]], dtype=float32)

time = 32512	action = 1	current_phase = 0	next_phase = 1	reward = -1.592365	array([[-5.605463 , -3.3008437]], dtype=float32)

time = 32520	action = 1	current_phase = 1	next_phase = 0	reward = -0.846466	array([[-3.9560986, -2.2257605]], dtype=float32)

time = 32528	action = 0	current_phase = 0	next_phase = 1	reward = -0.052942	array([[-1.6317563, -2.7249844]], dtype=float32)

time = 32533	action = 0	current_phase = 0	next_phase = 1	reward = 0.000846	array([[-2.0198033, -2.9413195]], dtype=float32)

time = 32538	action = 0	current_phase = 0	next_phase = 1	reward = 0.075762	array([[-2.800984 , -2.8535314]], dtype=float32)

time = 32543	action = 1	current_phase = 0	next_phase = 1	reward = -1.802902	array([[-5.484246 , -3.3866658]], dtype=float32)

time = 32551	action = 1	current_phase = 1	next_phase = 0	reward = -1.032680	array([[-4.128949 , -2.2983432]], dtype=float32)

time = 32559	action = 0	current_phase = 0	next_phase = 1	reward = -0.029211	array([[-1.3314519, -2.386723 ]], dtype=float32)

time = 32564	action = 0	current_phase = 0	next_phase = 1	reward = 0.038313	array([[-1.9179456, -2.8624945]], dtype=float32)

time = 32569	action = 1	current_phase = 0	next_phase = 1	reward = -0.700208	array([[-2.3917232, -2.385787 ]], dtype=float32)

time = 32577	action = 1	current_phase = 1	next_phase = 0	reward = -0.880161	array([[-3.3554287, -1.8927703]], dtype=float32)

time = 32585	action = 0	current_phase = 0	next_phase = 1	reward = -0.089325	array([[-1.3688419, -2.9157333]], dtype=float32)

time = 32590	action = 0	current_phase = 0	next_phase = 1	reward = 0.263381	array([[-1.6226532, -2.4350133]], dtype=float32)

time = 32595	action = 0	current_phase = 0	next_phase = 1	reward = 0.045035	array([[-2.3423858, -3.026744 ]], dtype=float32)

time = 32600	action = 1	current_phase = 0	next_phase = 1	reward = -1.429881	array([[-4.5242987, -2.6956272]], dtype=float32)

time = 32608	action = 1	current_phase = 1	next_phase = 0	reward = -0.729715	array([[-3.8949227, -2.1010733]], dtype=float32)

time = 32616	action = 0	current_phase = 0	next_phase = 1	reward = -0.087915	array([[-1.7043707, -2.8069835]], dtype=float32)

time = 32621	action = 0	current_phase = 0	next_phase = 1	reward = 0.010789	array([[-1.8330731, -2.7965684]], dtype=float32)

time = 32626	action = 0	current_phase = 0	next_phase = 1	reward = 0.067063	array([[-2.5778427, -2.9893608]], dtype=float32)

time = 32631	action = 1	current_phase = 0	next_phase = 1	reward = -1.433711	array([[-5.644662 , -3.0944967]], dtype=float32)

time = 32639	action = 1	current_phase = 1	next_phase = 0	reward = -0.835632	array([[-3.869256 , -2.1144617]], dtype=float32)

time = 32647	action = 0	current_phase = 0	next_phase = 1	reward = -0.086236	array([[-1.6309836, -2.7774687]], dtype=float32)

time = 32652	action = 0	current_phase = 0	next_phase = 1	reward = -0.013598	array([[-2.008979 , -2.8521242]], dtype=float32)

time = 32657	action = 0	current_phase = 0	next_phase = 1	reward = 0.065849	array([[-2.5867732, -3.0181942]], dtype=float32)

time = 32662	action = 1	current_phase = 0	next_phase = 1	reward = -1.488039	array([[-5.5920544, -3.389615 ]], dtype=float32)

time = 32670	action = 1	current_phase = 1	next_phase = 0	reward = -0.890384	array([[-3.9002194, -2.21884  ]], dtype=float32)

time = 32678	action = 0	current_phase = 0	next_phase = 1	reward = -0.043631	array([[-1.5750735, -2.6929011]], dtype=float32)

time = 32683	action = 0	current_phase = 0	next_phase = 1	reward = 0.024637	array([[-2.0435193, -2.9677591]], dtype=float32)

time = 32688	action = 0	current_phase = 0	next_phase = 1	reward = 0.080003	array([[-2.6601233, -2.892612 ]], dtype=float32)

time = 32693	action = 1	current_phase = 0	next_phase = 1	reward = -1.338436	array([[-5.469605 , -3.4112473]], dtype=float32)

time = 32701	action = 1	current_phase = 1	next_phase = 0	reward = -1.186583	array([[-3.7566233, -2.3184426]], dtype=float32)

time = 32709	action = 0	current_phase = 0	next_phase = 1	reward = -0.022736	array([[-1.3358002, -2.4024243]], dtype=float32)

time = 32714	action = 0	current_phase = 0	next_phase = 1	reward = 0.038086	array([[-1.9036851, -2.8556285]], dtype=float32)

time = 32719	action = 1	current_phase = 0	next_phase = 1	reward = -0.690492	array([[-2.3752992, -2.2979913]], dtype=float32)

time = 32727	action = 1	current_phase = 1	next_phase = 0	reward = -0.660993	array([[-3.5874538, -1.9338939]], dtype=float32)

time = 32735	action = 0	current_phase = 0	next_phase = 1	reward = -0.117350	array([[-1.402828 , -2.8218908]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0396 - val_loss: 0.0344

Epoch 2/50

 - 3s - loss: 0.0453 - val_loss: 0.0277

Epoch 3/50

 - 3s - loss: 0.0404 - val_loss: 0.0249

Epoch 4/50

 - 3s - loss: 0.0391 - val_loss: 0.0269

Epoch 5/50

 - 3s - loss: 0.0373 - val_loss: 0.0204

Epoch 6/50

 - 3s - loss: 0.0371 - val_loss: 0.0223

Epoch 7/50

 - 3s - loss: 0.0312 - val_loss: 0.0246

Epoch 8/50

 - 3s - loss: 0.0300 - val_loss: 0.0269

Epoch 9/50

 - 3s - loss: 0.0403 - val_loss: 0.0233

Epoch 10/50

 - 3s - loss: 0.0316 - val_loss: 0.0251

Epoch 11/50

 - 3s - loss: 0.0351 - val_loss: 0.0215

Epoch 12/50

 - 3s - loss: 0.0316 - val_loss: 0.0263

Epoch 13/50

 - 3s - loss: 0.0330 - val_loss: 0.0260

Epoch 14/50

 - 3s - loss: 0.0305 - val_loss: 0.0313

Epoch 15/50

 - 3s - loss: 0.0343 - val_loss: 0.0359

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 32740	action = 0	current_phase = 0	next_phase = 1	reward = -0.029133	array([[-1.5839918, -2.3784218]], dtype=float32)

time = 32745	action = 0	current_phase = 0	next_phase = 1	reward = -0.237436	array([[-2.3674073, -3.0538533]], dtype=float32)

time = 32750	action = 1	current_phase = 0	next_phase = 1	reward = -1.027968	array([[-4.533203, -2.580533]], dtype=float32)

time = 32758	action = 1	current_phase = 1	next_phase = 0	reward = -0.712480	array([[-3.7760148, -2.110194 ]], dtype=float32)

time = 32766	action = 0	current_phase = 0	next_phase = 1	reward = -0.102498	array([[-1.7134618, -2.7922273]], dtype=float32)

time = 32771	action = 0	current_phase = 0	next_phase = 1	reward = -0.014854	array([[-1.7755597, -2.750668 ]], dtype=float32)

time = 32776	action = 0	current_phase = 0	next_phase = 1	reward = 0.045498	array([[-2.4828718, -3.0240545]], dtype=float32)

time = 32781	action = 1	current_phase = 0	next_phase = 1	reward = -1.329211	array([[-5.6258593, -3.0250385]], dtype=float32)

time = 32789	action = 1	current_phase = 1	next_phase = 0	reward = -0.777560	array([[-3.8042655, -2.0995502]], dtype=float32)

time = 32797	action = 0	current_phase = 0	next_phase = 1	reward = -0.083859	array([[-1.6766634, -2.7989755]], dtype=float32)

time = 32802	action = 0	current_phase = 0	next_phase = 1	reward = -0.007122	array([[-2.013383 , -2.8794496]], dtype=float32)

time = 32807	action = 0	current_phase = 0	next_phase = 1	reward = 0.056667	array([[-2.5441194, -3.0331728]], dtype=float32)

time = 32812	action = 1	current_phase = 0	next_phase = 1	reward = -1.523311	array([[-5.665058 , -3.3040328]], dtype=float32)

time = 32820	action = 1	current_phase = 1	next_phase = 0	reward = -0.881254	array([[-3.901229 , -2.2632823]], dtype=float32)

time = 32828	action = 0	current_phase = 0	next_phase = 1	reward = -0.051992	array([[-1.640003 , -2.7396412]], dtype=float32)

time = 32833	action = 0	current_phase = 0	next_phase = 1	reward = 0.003754	array([[-2.0979447, -2.9595442]], dtype=float32)

time = 32838	action = 0	current_phase = 0	next_phase = 1	reward = 0.067936	array([[-2.7449236, -2.8391724]], dtype=float32)

time = 32843	action = 1	current_phase = 0	next_phase = 1	reward = -1.372118	array([[-5.5184145, -3.3718717]], dtype=float32)

time = 32851	action = 1	current_phase = 1	next_phase = 0	reward = -1.441527	array([[-3.8199444, -2.3800125]], dtype=float32)

time = 32859	action = 0	current_phase = 0	next_phase = 1	reward = 0.250974	array([[-1.4014095, -2.37844  ]], dtype=float32)

time = 32864	action = 0	current_phase = 0	next_phase = 1	reward = 0.022709	array([[-2.0320578, -2.873163 ]], dtype=float32)

time = 32869	action = 1	current_phase = 0	next_phase = 1	reward = -0.699514	array([[-2.5075436, -2.3704271]], dtype=float32)

time = 32877	action = 1	current_phase = 1	next_phase = 0	reward = -0.923764	array([[-3.643177, -2.026991]], dtype=float32)

time = 32885	action = 0	current_phase = 0	next_phase = 1	reward = 0.182171	array([[-1.3911476, -2.8495717]], dtype=float32)

time = 32890	action = 0	current_phase = 0	next_phase = 1	reward = -0.298218	array([[-1.6666026, -2.3798895]], dtype=float32)

time = 32895	action = 0	current_phase = 0	next_phase = 1	reward = 0.332075	array([[-2.3726096, -3.0158253]], dtype=float32)

time = 32900	action = 1	current_phase = 0	next_phase = 1	reward = -1.279284	array([[-4.5790033, -2.631298 ]], dtype=float32)

time = 32908	action = 1	current_phase = 1	next_phase = 0	reward = -0.710449	array([[-3.6240034, -1.9652901]], dtype=float32)

time = 32916	action = 0	current_phase = 0	next_phase = 1	reward = -0.082576	array([[-1.7033055, -2.7586288]], dtype=float32)

time = 32921	action = 0	current_phase = 0	next_phase = 1	reward = -0.017630	array([[-1.9257178, -2.8144326]], dtype=float32)

time = 32926	action = 0	current_phase = 0	next_phase = 1	reward = 0.059871	array([[-2.4286473, -3.029985 ]], dtype=float32)

time = 32931	action = 1	current_phase = 0	next_phase = 1	reward = -1.434770	array([[-5.6488676, -3.0281172]], dtype=float32)

time = 32939	action = 1	current_phase = 1	next_phase = 0	reward = -0.774421	array([[-3.8551016, -2.1415226]], dtype=float32)

time = 32947	action = 0	current_phase = 0	next_phase = 1	reward = -0.063611	array([[-1.6516423, -2.8249385]], dtype=float32)

time = 32952	action = 0	current_phase = 0	next_phase = 1	reward = -0.001226	array([[-2.0398726, -2.9057236]], dtype=float32)

time = 32957	action = 0	current_phase = 0	next_phase = 1	reward = 0.071152	array([[-2.8317144, -2.897401 ]], dtype=float32)

time = 32962	action = 1	current_phase = 0	next_phase = 1	reward = -1.534560	array([[-5.6682625, -3.2501626]], dtype=float32)

time = 32970	action = 1	current_phase = 1	next_phase = 0	reward = -0.960644	array([[-3.87352  , -2.2135143]], dtype=float32)

time = 32978	action = 0	current_phase = 0	next_phase = 1	reward = -0.064106	array([[-1.4932871, -2.7126603]], dtype=float32)

time = 32983	action = 0	current_phase = 0	next_phase = 1	reward = 0.028405	array([[-2.1018758, -2.980774 ]], dtype=float32)

time = 32988	action = 0	current_phase = 0	next_phase = 1	reward = 0.087562	array([[-2.7079773, -2.968798 ]], dtype=float32)

time = 32993	action = 1	current_phase = 0	next_phase = 1	reward = -1.805047	array([[-5.483961 , -3.4667358]], dtype=float32)

time = 33001	action = 1	current_phase = 1	next_phase = 0	reward = -1.337824	array([[-4.0599413, -2.3403318]], dtype=float32)

time = 33009	action = 0	current_phase = 0	next_phase = 1	reward = 0.265297	array([[-1.3475684, -2.357966 ]], dtype=float32)

time = 33014	action = 0	current_phase = 0	next_phase = 1	reward = 0.028657	array([[-1.9314547, -2.8807344]], dtype=float32)

time = 33019	action = 1	current_phase = 0	next_phase = 1	reward = -0.662276	array([[-2.3672824, -2.3076053]], dtype=float32)

time = 33027	action = 1	current_phase = 1	next_phase = 0	reward = -0.868176	array([[-3.118627 , -2.0111222]], dtype=float32)

time = 33035	action = 0	current_phase = 0	next_phase = 1	reward = -0.092434	array([[-1.3614082, -2.8715026]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0447 - val_loss: 0.0232

Epoch 2/50

 - 3s - loss: 0.0381 - val_loss: 0.0196

Epoch 3/50

 - 3s - loss: 0.0340 - val_loss: 0.0190

Epoch 4/50

 - 3s - loss: 0.0381 - val_loss: 0.0188

Epoch 5/50

 - 3s - loss: 0.0354 - val_loss: 0.0261

Epoch 6/50

 - 3s - loss: 0.0332 - val_loss: 0.0215

Epoch 7/50

 - 3s - loss: 0.0366 - val_loss: 0.0205

Epoch 8/50

 - 3s - loss: 0.0367 - val_loss: 0.0272

Epoch 9/50

 - 3s - loss: 0.0381 - val_loss: 0.0208

Epoch 10/50

 - 3s - loss: 0.0341 - val_loss: 0.0175

Epoch 11/50

 - 3s - loss: 0.0316 - val_loss: 0.0176

Epoch 12/50

 - 3s - loss: 0.0340 - val_loss: 0.0173

Epoch 13/50

 - 3s - loss: 0.0382 - val_loss: 0.0182

Epoch 14/50

 - 3s - loss: 0.0264 - val_loss: 0.0193

Epoch 15/50

 - 3s - loss: 0.0332 - val_loss: 0.0275

Epoch 16/50

 - 3s - loss: 0.0394 - val_loss: 0.0274

Epoch 17/50

 - 3s - loss: 0.0289 - val_loss: 0.0272

Epoch 18/50

 - 3s - loss: 0.0300 - val_loss: 0.0264

Epoch 19/50

 - 3s - loss: 0.0347 - val_loss: 0.0199

Epoch 20/50

 - 3s - loss: 0.0377 - val_loss: 0.0207

Epoch 21/50

 - 3s - loss: 0.0364 - val_loss: 0.0260

Epoch 22/50

 - 3s - loss: 0.0345 - val_loss: 0.0199

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 33040	action = 0	current_phase = 0	next_phase = 1	reward = -0.018927	array([[-1.6156764, -2.3531814]], dtype=float32)

time = 33045	action = 0	current_phase = 0	next_phase = 1	reward = 0.048177	array([[-2.267674 , -2.9848454]], dtype=float32)

time = 33050	action = 1	current_phase = 0	next_phase = 1	reward = -1.093538	array([[-4.6323986, -2.6392133]], dtype=float32)

time = 33058	action = 1	current_phase = 1	next_phase = 0	reward = -0.999136	array([[-3.7824345, -2.039418 ]], dtype=float32)

time = 33066	action = 0	current_phase = 0	next_phase = 1	reward = 0.227482	array([[-1.6347545, -2.7524831]], dtype=float32)

time = 33071	action = 0	current_phase = 0	next_phase = 1	reward = 0.015797	array([[-1.9000436, -2.827722 ]], dtype=float32)

time = 33076	action = 0	current_phase = 0	next_phase = 1	reward = 0.078699	array([[-2.4519925, -3.0332534]], dtype=float32)

time = 33081	action = 1	current_phase = 0	next_phase = 1	reward = -1.568735	array([[-5.7005496, -3.222518 ]], dtype=float32)

time = 33089	action = 1	current_phase = 1	next_phase = 0	reward = -0.797705	array([[-3.836372 , -2.1004045]], dtype=float32)

time = 33097	action = 0	current_phase = 0	next_phase = 1	reward = -0.055196	array([[-1.6758169, -2.7829688]], dtype=float32)

time = 33102	action = 0	current_phase = 0	next_phase = 1	reward = 0.020269	array([[-2.0115008, -2.9134376]], dtype=float32)

time = 33107	action = 0	current_phase = 0	next_phase = 1	reward = 0.073768	array([[-2.7002063, -2.9589524]], dtype=float32)

time = 33112	action = 1	current_phase = 0	next_phase = 1	reward = -1.667319	array([[-5.6619477, -3.3178275]], dtype=float32)

time = 33120	action = 1	current_phase = 1	next_phase = 0	reward = -0.993080	array([[-3.8878403, -2.1647043]], dtype=float32)

time = 33128	action = 0	current_phase = 0	next_phase = 1	reward = -0.046341	array([[-1.5670102, -2.6607692]], dtype=float32)

time = 33133	action = 0	current_phase = 0	next_phase = 1	reward = 0.012677	array([[-2.0037262, -2.9588811]], dtype=float32)

time = 33138	action = 0	current_phase = 0	next_phase = 1	reward = 0.068338	array([[-2.9601076, -2.9728158]], dtype=float32)

time = 33143	action = 1	current_phase = 0	next_phase = 1	reward = -1.924727	array([[-5.5246496, -3.4533389]], dtype=float32)

time = 33151	action = 1	current_phase = 1	next_phase = 0	reward = -1.280646	array([[-4.117291 , -2.2912326]], dtype=float32)

time = 33159	action = 0	current_phase = 0	next_phase = 1	reward = 0.259504	array([[-1.4090265, -2.3526447]], dtype=float32)

time = 33164	action = 0	current_phase = 0	next_phase = 1	reward = 0.016788	array([[-1.7820237, -2.833361 ]], dtype=float32)

time = 33169	action = 1	current_phase = 0	next_phase = 1	reward = -0.696290	array([[-2.2795942, -2.2483172]], dtype=float32)

time = 33177	action = 1	current_phase = 1	next_phase = 0	reward = -0.865025	array([[-3.382328, -1.832624]], dtype=float32)

time = 33185	action = 0	current_phase = 0	next_phase = 1	reward = 0.180083	array([[-1.405598, -2.931679]], dtype=float32)

time = 33190	action = 0	current_phase = 0	next_phase = 1	reward = -0.301345	array([[-1.709712 , -2.3716998]], dtype=float32)

time = 33195	action = 0	current_phase = 0	next_phase = 1	reward = 0.334587	array([[-2.2969012, -2.9866176]], dtype=float32)

time = 33200	action = 1	current_phase = 0	next_phase = 1	reward = -1.362421	array([[-4.5584354, -2.737932 ]], dtype=float32)

time = 33208	action = 1	current_phase = 1	next_phase = 0	reward = -0.701777	array([[-3.7595363, -2.0006824]], dtype=float32)

time = 33216	action = 0	current_phase = 0	next_phase = 1	reward = -0.077067	array([[-1.6211233, -2.7556424]], dtype=float32)

time = 33221	action = 0	current_phase = 0	next_phase = 1	reward = -0.002877	array([[-1.8472861, -2.8108275]], dtype=float32)

time = 33226	action = 0	current_phase = 0	next_phase = 1	reward = 0.058159	array([[-2.3688588, -3.0455155]], dtype=float32)

time = 33231	action = 1	current_phase = 0	next_phase = 1	reward = -1.468057	array([[-5.606815 , -3.0955408]], dtype=float32)

time = 33239	action = 1	current_phase = 1	next_phase = 0	reward = -0.788117	array([[-3.8152869, -2.0854182]], dtype=float32)

time = 33247	action = 0	current_phase = 0	next_phase = 1	reward = -0.066278	array([[-1.7247331, -2.8195302]], dtype=float32)

time = 33252	action = 0	current_phase = 0	next_phase = 1	reward = 0.003424	array([[-2.011222 , -2.9012663]], dtype=float32)

time = 33257	action = 0	current_phase = 0	next_phase = 1	reward = 0.061140	array([[-2.6084678, -3.0277758]], dtype=float32)

time = 33262	action = 1	current_phase = 0	next_phase = 1	reward = -1.548411	array([[-5.719186 , -3.3020356]], dtype=float32)

time = 33270	action = 1	current_phase = 1	next_phase = 0	reward = -1.291529	array([[-3.8381183, -2.1734836]], dtype=float32)

time = 33278	action = 0	current_phase = 0	next_phase = 1	reward = 0.249235	array([[-1.6624055, -2.6839552]], dtype=float32)

time = 33283	action = 0	current_phase = 0	next_phase = 1	reward = 0.029393	array([[-2.017511 , -2.9731963]], dtype=float32)

time = 33288	action = 0	current_phase = 0	next_phase = 1	reward = 0.076249	array([[-2.7081869, -2.848955 ]], dtype=float32)

time = 33293	action = 1	current_phase = 0	next_phase = 1	reward = -1.339099	array([[-5.514018, -3.450597]], dtype=float32)

time = 33301	action = 1	current_phase = 1	next_phase = 0	reward = -1.429474	array([[-3.7697213, -2.27503  ]], dtype=float32)

time = 33309	action = 0	current_phase = 0	next_phase = 1	reward = 0.246463	array([[-1.4417915, -2.4387279]], dtype=float32)

time = 33314	action = 0	current_phase = 0	next_phase = 1	reward = 0.032014	array([[-1.9220964, -2.8274162]], dtype=float32)

time = 33319	action = 0	current_phase = 0	next_phase = 1	reward = 0.041133	array([[-2.2506373, -2.3867502]], dtype=float32)

time = 33324	action = 1	current_phase = 0	next_phase = 1	reward = -1.087904	array([[-5.3976765, -2.9106672]], dtype=float32)

time = 33332	action = 1	current_phase = 1	next_phase = 0	reward = -1.018921	array([[-3.7642024, -2.2015762]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0384 - val_loss: 0.0255

Epoch 2/50

 - 3s - loss: 0.0378 - val_loss: 0.0238

Epoch 3/50

 - 3s - loss: 0.0378 - val_loss: 0.0227

Epoch 4/50

 - 3s - loss: 0.0353 - val_loss: 0.0239

Epoch 5/50

 - 3s - loss: 0.0366 - val_loss: 0.0267

Epoch 6/50

 - 3s - loss: 0.0412 - val_loss: 0.0223

Epoch 7/50

 - 3s - loss: 0.0370 - val_loss: 0.0273

Epoch 8/50

 - 3s - loss: 0.0317 - val_loss: 0.0298

Epoch 9/50

 - 3s - loss: 0.0437 - val_loss: 0.0244

Epoch 10/50

 - 3s - loss: 0.0318 - val_loss: 0.0220

Epoch 11/50

 - 3s - loss: 0.0337 - val_loss: 0.0206

Epoch 12/50

 - 3s - loss: 0.0285 - val_loss: 0.0246

Epoch 13/50

 - 3s - loss: 0.0293 - val_loss: 0.0215

Epoch 14/50

 - 3s - loss: 0.0312 - val_loss: 0.0239

Epoch 15/50

 - 3s - loss: 0.0314 - val_loss: 0.0252

Epoch 16/50

 - 3s - loss: 0.0347 - val_loss: 0.0251

Epoch 17/50

 - 3s - loss: 0.0390 - val_loss: 0.0243

Epoch 18/50

 - 3s - loss: 0.0370 - val_loss: 0.0224

Epoch 19/50

 - 3s - loss: 0.0334 - val_loss: 0.0260

Epoch 20/50

 - 3s - loss: 0.0273 - val_loss: 0.0289

Epoch 21/50

 - 3s - loss: 0.0300 - val_loss: 0.0236

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 33340	action = 0	current_phase = 0	next_phase = 1	reward = -0.012252	array([[-1.6602703, -2.3449306]], dtype=float32)

time = 33345	action = 0	current_phase = 0	next_phase = 1	reward = 0.341279	array([[-2.1858099, -2.8907428]], dtype=float32)

time = 33350	action = 1	current_phase = 0	next_phase = 1	reward = -1.376660	array([[-4.4091444, -2.7105522]], dtype=float32)

time = 33358	action = 1	current_phase = 1	next_phase = 0	reward = -0.695447	array([[-3.7191148, -2.0206192]], dtype=float32)

time = 33366	action = 0	current_phase = 0	next_phase = 1	reward = -0.080257	array([[-1.620441 , -2.7481484]], dtype=float32)

time = 33371	action = 0	current_phase = 0	next_phase = 1	reward = -0.007690	array([[-1.9422884, -2.8122237]], dtype=float32)

time = 33376	action = 0	current_phase = 0	next_phase = 1	reward = 0.053488	array([[-2.3893898, -3.0018213]], dtype=float32)

time = 33381	action = 1	current_phase = 0	next_phase = 1	reward = -1.552179	array([[-5.764001 , -3.1707685]], dtype=float32)

time = 33389	action = 1	current_phase = 1	next_phase = 0	reward = -0.786208	array([[-3.851976 , -2.2130783]], dtype=float32)

time = 33397	action = 0	current_phase = 0	next_phase = 1	reward = -0.055415	array([[-1.6928403, -2.790121 ]], dtype=float32)

time = 33402	action = 0	current_phase = 0	next_phase = 1	reward = 0.009047	array([[-2.1482697, -2.884893 ]], dtype=float32)

time = 33407	action = 0	current_phase = 0	next_phase = 1	reward = 0.078129	array([[-2.6139438, -2.964469 ]], dtype=float32)

time = 33412	action = 1	current_phase = 0	next_phase = 1	reward = -1.553347	array([[-5.601646 , -3.3173091]], dtype=float32)

time = 33420	action = 1	current_phase = 1	next_phase = 0	reward = -0.991410	array([[-3.883102 , -2.2733898]], dtype=float32)

time = 33428	action = 0	current_phase = 0	next_phase = 1	reward = -0.035075	array([[-1.5645108, -2.5889401]], dtype=float32)

time = 33433	action = 0	current_phase = 0	next_phase = 1	reward = 0.024795	array([[-2.048184 , -2.9002798]], dtype=float32)

time = 33438	action = 0	current_phase = 0	next_phase = 1	reward = 0.081931	array([[-2.7790575, -2.8490171]], dtype=float32)

time = 33443	action = 1	current_phase = 0	next_phase = 1	reward = -1.458314	array([[-5.4918685, -3.396097 ]], dtype=float32)

time = 33451	action = 1	current_phase = 1	next_phase = 0	reward = -1.513494	array([[-3.8483548, -2.2986906]], dtype=float32)

time = 33459	action = 0	current_phase = 0	next_phase = 1	reward = 0.241183	array([[-1.4498608, -2.3819003]], dtype=float32)

time = 33464	action = 0	current_phase = 0	next_phase = 1	reward = 0.040139	array([[-1.9343841, -2.8132405]], dtype=float32)

time = 33469	action = 1	current_phase = 0	next_phase = 1	reward = -0.669741	array([[-2.5456936, -2.4393969]], dtype=float32)

time = 33477	action = 1	current_phase = 1	next_phase = 0	reward = -0.649651	array([[-3.216444 , -1.9232237]], dtype=float32)

time = 33485	action = 0	current_phase = 0	next_phase = 1	reward = -0.397009	array([[-1.3535303, -2.8636065]], dtype=float32)

time = 33490	action = 0	current_phase = 0	next_phase = 1	reward = -0.024989	array([[-1.6964223, -2.3350842]], dtype=float32)

time = 33495	action = 0	current_phase = 0	next_phase = 1	reward = 0.323766	array([[-2.182446 , -2.9144094]], dtype=float32)

time = 33500	action = 1	current_phase = 0	next_phase = 1	reward = -1.416140	array([[-4.6129107, -2.6743622]], dtype=float32)

time = 33508	action = 1	current_phase = 1	next_phase = 0	reward = -0.721554	array([[-3.8137755, -2.080554 ]], dtype=float32)

time = 33516	action = 0	current_phase = 0	next_phase = 1	reward = -0.096039	array([[-1.5398448, -2.7306252]], dtype=float32)

time = 33521	action = 0	current_phase = 0	next_phase = 1	reward = -0.022041	array([[-1.8394326, -2.7409077]], dtype=float32)

time = 33526	action = 0	current_phase = 0	next_phase = 1	reward = 0.060332	array([[-2.3740158, -2.984837 ]], dtype=float32)

time = 33531	action = 1	current_phase = 0	next_phase = 1	reward = -1.439060	array([[-5.667848 , -3.0410576]], dtype=float32)

time = 33539	action = 1	current_phase = 1	next_phase = 0	reward = -1.123161	array([[-3.8114552, -2.1226907]], dtype=float32)

time = 33547	action = 0	current_phase = 0	next_phase = 1	reward = 0.230207	array([[-1.5947888, -2.7451391]], dtype=float32)

time = 33552	action = 0	current_phase = 0	next_phase = 1	reward = 0.010829	array([[-2.1292105, -2.8885522]], dtype=float32)

time = 33557	action = 0	current_phase = 0	next_phase = 1	reward = 0.065092	array([[-2.6433015, -2.9288726]], dtype=float32)

time = 33562	action = 1	current_phase = 0	next_phase = 1	reward = -1.604947	array([[-5.7167115, -3.275833 ]], dtype=float32)

time = 33570	action = 1	current_phase = 1	next_phase = 0	reward = -1.002816	array([[-3.8907824, -2.2593129]], dtype=float32)

time = 33578	action = 0	current_phase = 0	next_phase = 1	reward = -0.065930	array([[-1.6020169, -2.5508754]], dtype=float32)

time = 33583	action = 0	current_phase = 0	next_phase = 1	reward = 0.013216	array([[-2.0711796, -2.8997679]], dtype=float32)

time = 33588	action = 0	current_phase = 0	next_phase = 1	reward = 0.071021	array([[-2.6658258, -2.847857 ]], dtype=float32)

time = 33593	action = 1	current_phase = 0	next_phase = 1	reward = -1.370151	array([[-5.5773463, -3.3229272]], dtype=float32)

time = 33601	action = 1	current_phase = 1	next_phase = 0	reward = -1.438701	array([[-3.7600703, -2.3231943]], dtype=float32)

time = 33609	action = 0	current_phase = 0	next_phase = 1	reward = 0.248539	array([[-1.4321141, -2.3326614]], dtype=float32)

time = 33614	action = 0	current_phase = 0	next_phase = 1	reward = 0.061939	array([[-1.9180031, -2.8047519]], dtype=float32)

time = 33619	action = 0	current_phase = 0	next_phase = 1	reward = 0.053917	array([[-2.123362 , -2.4185152]], dtype=float32)

time = 33624	action = 1	current_phase = 0	next_phase = 1	reward = -1.136718	array([[-5.244393 , -2.8782587]], dtype=float32)

time = 33632	action = 1	current_phase = 1	next_phase = 0	reward = -0.703818	array([[-3.6762862, -2.214131 ]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0413 - val_loss: 0.0198

Epoch 2/50

 - 3s - loss: 0.0358 - val_loss: 0.0241

Epoch 3/50

 - 3s - loss: 0.0325 - val_loss: 0.0314

Epoch 4/50

 - 3s - loss: 0.0436 - val_loss: 0.0197

Epoch 5/50

 - 3s - loss: 0.0296 - val_loss: 0.0302

Epoch 6/50

 - 3s - loss: 0.0305 - val_loss: 0.0303

Epoch 7/50

 - 3s - loss: 0.0460 - val_loss: 0.0211

Epoch 8/50

 - 3s - loss: 0.0305 - val_loss: 0.0247

Epoch 9/50

 - 3s - loss: 0.0309 - val_loss: 0.0228

Epoch 10/50

 - 3s - loss: 0.0257 - val_loss: 0.0224

Epoch 11/50

 - 3s - loss: 0.0299 - val_loss: 0.0297

Epoch 12/50

 - 3s - loss: 0.0326 - val_loss: 0.0210

Epoch 13/50

 - 3s - loss: 0.0421 - val_loss: 0.0185

Epoch 14/50

 - 3s - loss: 0.0318 - val_loss: 0.0213

Epoch 15/50

 - 3s - loss: 0.0373 - val_loss: 0.0229

Epoch 16/50

 - 3s - loss: 0.0300 - val_loss: 0.0230

Epoch 17/50

 - 3s - loss: 0.0327 - val_loss: 0.0187

Epoch 18/50

 - 3s - loss: 0.0305 - val_loss: 0.0164

Epoch 19/50

 - 3s - loss: 0.0324 - val_loss: 0.0250

Epoch 20/50

 - 3s - loss: 0.0305 - val_loss: 0.0225

Epoch 21/50

 - 3s - loss: 0.0338 - val_loss: 0.0223

Epoch 22/50

 - 3s - loss: 0.0283 - val_loss: 0.0175

Epoch 23/50

 - 3s - loss: 0.0253 - val_loss: 0.0193

Epoch 24/50

 - 3s - loss: 0.0303 - val_loss: 0.0168

Epoch 25/50

 - 3s - loss: 0.0314 - val_loss: 0.0179

Epoch 26/50

 - 3s - loss: 0.0269 - val_loss: 0.0193

Epoch 27/50

 - 3s - loss: 0.0308 - val_loss: 0.0255

Epoch 28/50

 - 3s - loss: 0.0296 - val_loss: 0.0195

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 33640	action = 0	current_phase = 0	next_phase = 1	reward = -0.022811	array([[-1.6765928, -2.327474 ]], dtype=float32)

time = 33645	action = 0	current_phase = 0	next_phase = 1	reward = 0.044498	array([[-2.137382 , -2.9078016]], dtype=float32)

time = 33650	action = 1	current_phase = 0	next_phase = 1	reward = -1.365427	array([[-4.5104046, -2.8052983]], dtype=float32)

time = 33658	action = 1	current_phase = 1	next_phase = 0	reward = -0.706211	array([[-3.6939044, -2.0106494]], dtype=float32)

time = 33666	action = 0	current_phase = 0	next_phase = 1	reward = -0.083451	array([[-1.5236573, -2.73266  ]], dtype=float32)

time = 33671	action = 0	current_phase = 0	next_phase = 1	reward = -0.007568	array([[-1.7636993, -2.7750368]], dtype=float32)

time = 33676	action = 0	current_phase = 0	next_phase = 1	reward = 0.057034	array([[-2.284542, -2.979587]], dtype=float32)

time = 33681	action = 1	current_phase = 0	next_phase = 1	reward = -1.500812	array([[-5.7304726, -3.0894833]], dtype=float32)

time = 33689	action = 1	current_phase = 1	next_phase = 0	reward = -0.761114	array([[-3.8268895, -2.130764 ]], dtype=float32)

time = 33697	action = 0	current_phase = 0	next_phase = 1	reward = -0.065877	array([[-1.6492982, -2.792325 ]], dtype=float32)

time = 33702	action = 0	current_phase = 0	next_phase = 1	reward = 0.011712	array([[-2.1163032, -2.8993406]], dtype=float32)

time = 33707	action = 0	current_phase = 0	next_phase = 1	reward = 0.090254	array([[-2.7332637, -2.9532003]], dtype=float32)

time = 33712	action = 1	current_phase = 0	next_phase = 1	reward = -1.608387	array([[-5.689869 , -3.4119482]], dtype=float32)

time = 33720	action = 1	current_phase = 1	next_phase = 0	reward = -1.009656	array([[-3.936718 , -2.2636037]], dtype=float32)

time = 33728	action = 0	current_phase = 0	next_phase = 1	reward = -0.052251	array([[-1.6034797, -2.6097832]], dtype=float32)

time = 33733	action = 0	current_phase = 0	next_phase = 1	reward = 0.027960	array([[-2.011925 , -2.9164324]], dtype=float32)

time = 33738	action = 0	current_phase = 0	next_phase = 1	reward = 0.064136	array([[-2.712677 , -2.8274264]], dtype=float32)

time = 33743	action = 1	current_phase = 0	next_phase = 1	reward = -0.806348	array([[-5.6002007, -3.2669497]], dtype=float32)

time = 33751	action = 1	current_phase = 1	next_phase = 0	reward = -1.239529	array([[-3.5584803, -2.3957353]], dtype=float32)

time = 33759	action = 0	current_phase = 0	next_phase = 1	reward = -0.024927	array([[-1.4605954, -2.3472252]], dtype=float32)

time = 33764	action = 0	current_phase = 0	next_phase = 1	reward = 0.057987	array([[-1.7638278, -2.7762241]], dtype=float32)

time = 33769	action = 0	current_phase = 0	next_phase = 1	reward = 0.056147	array([[-2.2562015, -2.3239923]], dtype=float32)

time = 33774	action = 1	current_phase = 0	next_phase = 1	reward = -1.207683	array([[-5.4495993, -2.9990191]], dtype=float32)

time = 33782	action = 1	current_phase = 1	next_phase = 0	reward = -1.047333	array([[-3.9940944, -2.2200706]], dtype=float32)

time = 33790	action = 0	current_phase = 0	next_phase = 1	reward = -0.037069	array([[-1.6620148, -2.3288522]], dtype=float32)

time = 33795	action = 0	current_phase = 0	next_phase = 1	reward = -0.237722	array([[-1.8341593, -2.7861834]], dtype=float32)

time = 33800	action = 1	current_phase = 0	next_phase = 1	reward = -0.644215	array([[-4.361166 , -2.7240484]], dtype=float32)

time = 33808	action = 1	current_phase = 1	next_phase = 0	reward = -0.683889	array([[-3.5349245, -1.9914892]], dtype=float32)

time = 33816	action = 0	current_phase = 0	next_phase = 1	reward = -0.081124	array([[-1.5260834, -2.7429733]], dtype=float32)

time = 33821	action = 0	current_phase = 0	next_phase = 1	reward = 0.001998	array([[-1.7998257, -2.788392 ]], dtype=float32)

time = 33826	action = 0	current_phase = 0	next_phase = 1	reward = 0.062499	array([[-2.3116012, -2.9997296]], dtype=float32)

time = 33831	action = 1	current_phase = 0	next_phase = 1	reward = -1.544151	array([[-5.739707 , -3.1399689]], dtype=float32)

time = 33839	action = 1	current_phase = 1	next_phase = 0	reward = -0.894464	array([[-3.8465147, -2.156063 ]], dtype=float32)

time = 33847	action = 0	current_phase = 0	next_phase = 1	reward = -0.057299	array([[-1.619348 , -2.7847056]], dtype=float32)

time = 33852	action = 0	current_phase = 0	next_phase = 1	reward = 0.015203	array([[-2.0793967, -2.907446 ]], dtype=float32)

time = 33857	action = 0	current_phase = 0	next_phase = 1	reward = 0.062134	array([[-2.648818 , -2.9339428]], dtype=float32)

time = 33862	action = 1	current_phase = 0	next_phase = 1	reward = -1.627132	array([[-5.693323 , -3.3894992]], dtype=float32)

time = 33870	action = 1	current_phase = 1	next_phase = 0	reward = -0.939168	array([[-3.866726 , -2.2118022]], dtype=float32)

time = 33878	action = 0	current_phase = 0	next_phase = 1	reward = -0.052410	array([[-1.5005513, -2.5837388]], dtype=float32)

time = 33883	action = 0	current_phase = 0	next_phase = 1	reward = 0.015428	array([[-2.0142567, -2.9116   ]], dtype=float32)

time = 33888	action = 0	current_phase = 0	next_phase = 1	reward = 0.072784	array([[-2.7324073, -2.8377438]], dtype=float32)

time = 33893	action = 1	current_phase = 0	next_phase = 1	reward = -1.926363	array([[-5.544611 , -3.5228062]], dtype=float32)

time = 33901	action = 1	current_phase = 1	next_phase = 0	reward = -1.083311	array([[-4.081411, -2.370306]], dtype=float32)

time = 33909	action = 0	current_phase = 0	next_phase = 1	reward = -0.030850	array([[-1.428678 , -2.3137913]], dtype=float32)

time = 33914	action = 0	current_phase = 0	next_phase = 1	reward = 0.025085	array([[-1.7983521, -2.788162 ]], dtype=float32)

time = 33919	action = 0	current_phase = 0	next_phase = 1	reward = 0.056291	array([[-2.2156825, -2.2717779]], dtype=float32)

time = 33924	action = 1	current_phase = 0	next_phase = 1	reward = -1.020437	array([[-5.4707947, -2.9067674]], dtype=float32)

time = 33932	action = 1	current_phase = 1	next_phase = 0	reward = -1.027999	array([[-3.7830734, -2.2357774]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0431 - val_loss: 0.0145

Epoch 2/50

 - 3s - loss: 0.0404 - val_loss: 0.0173

Epoch 3/50

 - 3s - loss: 0.0297 - val_loss: 0.0180

Epoch 4/50

 - 3s - loss: 0.0337 - val_loss: 0.0166

Epoch 5/50

 - 3s - loss: 0.0308 - val_loss: 0.0151

Epoch 6/50

 - 3s - loss: 0.0333 - val_loss: 0.0183

Epoch 7/50

 - 3s - loss: 0.0261 - val_loss: 0.0163

Epoch 8/50

 - 3s - loss: 0.0311 - val_loss: 0.0222

Epoch 9/50

 - 3s - loss: 0.0344 - val_loss: 0.0169

Epoch 10/50

 - 3s - loss: 0.0250 - val_loss: 0.0170

Epoch 11/50

 - 3s - loss: 0.0283 - val_loss: 0.0228

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 33940	action = 0	current_phase = 0	next_phase = 1	reward = -0.018030	array([[-1.6756103, -2.3061457]], dtype=float32)

time = 33945	action = 0	current_phase = 0	next_phase = 1	reward = 0.334311	array([[-2.0109131, -2.7927985]], dtype=float32)

time = 33950	action = 1	current_phase = 0	next_phase = 1	reward = -1.327302	array([[-4.3587384, -2.7758586]], dtype=float32)

time = 33958	action = 1	current_phase = 1	next_phase = 0	reward = -0.705854	array([[-3.7239544, -2.0690563]], dtype=float32)

time = 33966	action = 0	current_phase = 0	next_phase = 1	reward = -0.091110	array([[-1.6420391, -2.720812 ]], dtype=float32)

time = 33971	action = 0	current_phase = 0	next_phase = 1	reward = -0.012639	array([[-1.8319151, -2.6570761]], dtype=float32)

time = 33976	action = 0	current_phase = 0	next_phase = 1	reward = 0.056740	array([[-2.263678 , -2.9108422]], dtype=float32)

time = 33981	action = 1	current_phase = 0	next_phase = 1	reward = -1.441670	array([[-5.744951 , -3.0799549]], dtype=float32)

time = 33989	action = 1	current_phase = 1	next_phase = 0	reward = -0.891283	array([[-3.8465002, -2.227116 ]], dtype=float32)

time = 33997	action = 0	current_phase = 0	next_phase = 1	reward = -0.063215	array([[-1.5634086, -2.6859384]], dtype=float32)

time = 34002	action = 0	current_phase = 0	next_phase = 1	reward = 0.026996	array([[-2.1131155, -2.8655207]], dtype=float32)

time = 34007	action = 0	current_phase = 0	next_phase = 1	reward = 0.072132	array([[-2.511144 , -2.9000914]], dtype=float32)

time = 34012	action = 1	current_phase = 0	next_phase = 1	reward = -1.621112	array([[-5.741973 , -3.3317745]], dtype=float32)

time = 34020	action = 1	current_phase = 1	next_phase = 0	reward = -0.953849	array([[-3.9319584, -2.3515584]], dtype=float32)

time = 34028	action = 0	current_phase = 0	next_phase = 1	reward = -0.056570	array([[-1.6224022, -2.6449275]], dtype=float32)

time = 34033	action = 0	current_phase = 0	next_phase = 1	reward = 0.025738	array([[-2.163607 , -2.9221604]], dtype=float32)

time = 34038	action = 0	current_phase = 0	next_phase = 1	reward = 0.079957	array([[-2.8150063, -2.820556 ]], dtype=float32)

time = 34043	action = 1	current_phase = 0	next_phase = 1	reward = -1.838604	array([[-5.6277657, -3.3589141]], dtype=float32)

time = 34051	action = 1	current_phase = 1	next_phase = 0	reward = -1.398055	array([[-4.1336064, -2.5446548]], dtype=float32)

time = 34059	action = 0	current_phase = 0	next_phase = 1	reward = 0.260145	array([[-1.401185 , -2.3227565]], dtype=float32)

time = 34064	action = 0	current_phase = 0	next_phase = 1	reward = 0.050358	array([[-1.891459 , -2.8083918]], dtype=float32)

time = 34069	action = 1	current_phase = 0	next_phase = 1	reward = -0.720176	array([[-2.2938194, -2.238811 ]], dtype=float32)

time = 34077	action = 1	current_phase = 1	next_phase = 0	reward = -0.869974	array([[-3.5170326, -2.0638976]], dtype=float32)

time = 34085	action = 0	current_phase = 0	next_phase = 1	reward = 0.171675	array([[-1.3057865, -2.7686164]], dtype=float32)

time = 34090	action = 0	current_phase = 0	next_phase = 1	reward = -0.311485	array([[-1.7532504, -2.3229508]], dtype=float32)

time = 34095	action = 0	current_phase = 0	next_phase = 1	reward = 0.328135	array([[-2.1017761, -2.8667316]], dtype=float32)

time = 34100	action = 1	current_phase = 0	next_phase = 1	reward = -1.369579	array([[-4.60117  , -2.7512145]], dtype=float32)

time = 34108	action = 1	current_phase = 1	next_phase = 0	reward = -0.713161	array([[-3.7825296, -2.0790703]], dtype=float32)

time = 34116	action = 0	current_phase = 0	next_phase = 1	reward = -0.078157	array([[-1.4754329, -2.646587 ]], dtype=float32)

time = 34121	action = 0	current_phase = 0	next_phase = 1	reward = -0.016153	array([[-1.8667604, -2.7399683]], dtype=float32)

time = 34126	action = 0	current_phase = 0	next_phase = 1	reward = 0.043950	array([[-2.251341 , -2.9248538]], dtype=float32)

time = 34131	action = 1	current_phase = 0	next_phase = 1	reward = -1.433914	array([[-5.720498 , -3.0877697]], dtype=float32)

time = 34139	action = 1	current_phase = 1	next_phase = 0	reward = -0.759247	array([[-3.824349, -2.19282 ]], dtype=float32)

time = 34147	action = 0	current_phase = 0	next_phase = 1	reward = -0.067550	array([[-1.6581063, -2.7271152]], dtype=float32)

time = 34152	action = 0	current_phase = 0	next_phase = 1	reward = 0.008470	array([[-2.1173136, -2.860515 ]], dtype=float32)

time = 34157	action = 0	current_phase = 0	next_phase = 1	reward = 0.075221	array([[-2.606704 , -2.9269135]], dtype=float32)

time = 34162	action = 1	current_phase = 0	next_phase = 1	reward = -1.546549	array([[-5.7053556, -3.2811284]], dtype=float32)

time = 34170	action = 1	current_phase = 1	next_phase = 0	reward = -1.243394	array([[-3.927403 , -2.3381515]], dtype=float32)

time = 34178	action = 0	current_phase = 0	next_phase = 1	reward = 0.238380	array([[-1.5356562, -2.6334636]], dtype=float32)

time = 34183	action = 0	current_phase = 0	next_phase = 1	reward = 0.012887	array([[-2.1023955, -2.9116805]], dtype=float32)

time = 34188	action = 0	current_phase = 0	next_phase = 1	reward = 0.078062	array([[-2.8015075, -2.8106189]], dtype=float32)

time = 34193	action = 1	current_phase = 0	next_phase = 1	reward = -1.872468	array([[-5.6683497, -3.3507788]], dtype=float32)

time = 34201	action = 1	current_phase = 1	next_phase = 0	reward = -1.025440	array([[-4.196432 , -2.6014924]], dtype=float32)

time = 34209	action = 0	current_phase = 0	next_phase = 1	reward = -0.030095	array([[-1.4739609, -2.31571  ]], dtype=float32)

time = 34214	action = 0	current_phase = 0	next_phase = 1	reward = 0.043068	array([[-1.8539319, -2.8023293]], dtype=float32)

time = 34219	action = 1	current_phase = 0	next_phase = 1	reward = -0.690814	array([[-2.3315926, -2.2661586]], dtype=float32)

time = 34227	action = 1	current_phase = 1	next_phase = 0	reward = -0.644562	array([[-3.4797792, -1.9026136]], dtype=float32)

time = 34235	action = 0	current_phase = 0	next_phase = 1	reward = -0.114048	array([[-1.3578166, -2.7837427]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0476 - val_loss: 0.0164

Epoch 2/50

 - 3s - loss: 0.0315 - val_loss: 0.0130

Epoch 3/50

 - 3s - loss: 0.0309 - val_loss: 0.0161

Epoch 4/50

 - 3s - loss: 0.0307 - val_loss: 0.0199

Epoch 5/50

 - 3s - loss: 0.0343 - val_loss: 0.0159

Epoch 6/50

 - 3s - loss: 0.0292 - val_loss: 0.0140

Epoch 7/50

 - 3s - loss: 0.0293 - val_loss: 0.0153

Epoch 8/50

 - 3s - loss: 0.0400 - val_loss: 0.0220

Epoch 9/50

 - 3s - loss: 0.0393 - val_loss: 0.0192

Epoch 10/50

 - 3s - loss: 0.0347 - val_loss: 0.0190

Epoch 11/50

 - 3s - loss: 0.0321 - val_loss: 0.0160

Epoch 12/50

 - 3s - loss: 0.0266 - val_loss: 0.0176

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 34240	action = 0	current_phase = 0	next_phase = 1	reward = -0.039836	array([[-1.759836 , -2.3261714]], dtype=float32)

time = 34245	action = 0	current_phase = 0	next_phase = 1	reward = -0.246733	array([[-2.128877 , -2.8722677]], dtype=float32)

time = 34250	action = 1	current_phase = 0	next_phase = 1	reward = -1.079406	array([[-4.6071796, -2.7954135]], dtype=float32)

time = 34258	action = 1	current_phase = 1	next_phase = 0	reward = -0.719986	array([[-3.8500547, -2.126457 ]], dtype=float32)

time = 34266	action = 0	current_phase = 0	next_phase = 1	reward = -0.096035	array([[-1.4767449, -2.6608267]], dtype=float32)

time = 34271	action = 0	current_phase = 0	next_phase = 1	reward = -0.028457	array([[-1.7844222, -2.6866658]], dtype=float32)

time = 34276	action = 0	current_phase = 0	next_phase = 1	reward = 0.054090	array([[-2.2878025, -2.908204 ]], dtype=float32)

time = 34281	action = 1	current_phase = 0	next_phase = 1	reward = -1.425162	array([[-5.7696195, -3.126411 ]], dtype=float32)

time = 34289	action = 1	current_phase = 1	next_phase = 0	reward = -0.766737	array([[-3.8670845, -2.1744556]], dtype=float32)

time = 34297	action = 0	current_phase = 0	next_phase = 1	reward = -0.066572	array([[-1.5940928, -2.7203345]], dtype=float32)

time = 34302	action = 0	current_phase = 0	next_phase = 1	reward = 0.014937	array([[-2.0455067, -2.81984  ]], dtype=float32)

time = 34307	action = 0	current_phase = 0	next_phase = 1	reward = 0.072769	array([[-2.495114 , -2.8923898]], dtype=float32)

time = 34312	action = 1	current_phase = 0	next_phase = 1	reward = -1.660089	array([[-5.699394, -3.362461]], dtype=float32)

time = 34320	action = 1	current_phase = 1	next_phase = 0	reward = -1.004476	array([[-3.9474149, -2.3069603]], dtype=float32)

time = 34328	action = 0	current_phase = 0	next_phase = 1	reward = -0.058036	array([[-1.5425913, -2.584601 ]], dtype=float32)

time = 34333	action = 0	current_phase = 0	next_phase = 1	reward = 0.021341	array([[-2.0196788, -2.887669 ]], dtype=float32)

time = 34338	action = 0	current_phase = 0	next_phase = 1	reward = 0.066663	array([[-2.4539466, -2.8934608]], dtype=float32)

time = 34343	action = 1	current_phase = 0	next_phase = 1	reward = -1.361701	array([[-5.6099563, -3.259375 ]], dtype=float32)

time = 34351	action = 1	current_phase = 1	next_phase = 0	reward = -1.445614	array([[-3.913076 , -2.3529897]], dtype=float32)

time = 34359	action = 0	current_phase = 0	next_phase = 1	reward = 0.253147	array([[-1.2073507, -2.2805324]], dtype=float32)

time = 34364	action = 0	current_phase = 0	next_phase = 1	reward = 0.041610	array([[-1.7932217, -2.802013 ]], dtype=float32)

time = 34369	action = 0	current_phase = 0	next_phase = 1	reward = 0.074153	array([[-2.1747901, -2.3270774]], dtype=float32)

time = 34374	action = 1	current_phase = 0	next_phase = 1	reward = -1.178794	array([[-5.181035, -2.883111]], dtype=float32)

time = 34382	action = 1	current_phase = 1	next_phase = 0	reward = -0.772657	array([[-3.9794807, -2.2048101]], dtype=float32)

time = 34390	action = 0	current_phase = 0	next_phase = 1	reward = -0.315896	array([[-1.6930692, -2.347034 ]], dtype=float32)

time = 34395	action = 0	current_phase = 0	next_phase = 1	reward = -0.234653	array([[-2.0973706, -2.8700323]], dtype=float32)

time = 34400	action = 1	current_phase = 0	next_phase = 1	reward = -0.697008	array([[-4.433287 , -2.7019162]], dtype=float32)

time = 34408	action = 1	current_phase = 1	next_phase = 0	reward = -0.692372	array([[-3.715219 , -2.0976956]], dtype=float32)

time = 34416	action = 0	current_phase = 0	next_phase = 1	reward = -0.088564	array([[-1.5117676, -2.6848621]], dtype=float32)

time = 34421	action = 0	current_phase = 0	next_phase = 1	reward = -0.019517	array([[-1.9191356, -2.7499795]], dtype=float32)

time = 34426	action = 0	current_phase = 0	next_phase = 1	reward = 0.056005	array([[-2.2858894, -2.9225993]], dtype=float32)

time = 34431	action = 1	current_phase = 0	next_phase = 1	reward = -1.436165	array([[-5.6897874, -3.1006184]], dtype=float32)

time = 34439	action = 1	current_phase = 1	next_phase = 0	reward = -0.830016	array([[-3.8349867, -2.1605191]], dtype=float32)

time = 34447	action = 0	current_phase = 0	next_phase = 1	reward = -0.076516	array([[-1.5653496, -2.6850238]], dtype=float32)

time = 34452	action = 0	current_phase = 0	next_phase = 1	reward = 0.017153	array([[-2.0388505, -2.8149977]], dtype=float32)

time = 34457	action = 0	current_phase = 0	next_phase = 1	reward = 0.073746	array([[-2.430173 , -2.8774495]], dtype=float32)

time = 34462	action = 1	current_phase = 0	next_phase = 1	reward = -1.611448	array([[-5.712942, -3.447041]], dtype=float32)

time = 34470	action = 1	current_phase = 1	next_phase = 0	reward = -0.959099	array([[-3.9514427, -2.3140912]], dtype=float32)

time = 34478	action = 0	current_phase = 0	next_phase = 1	reward = -0.067106	array([[-1.517349 , -2.5343494]], dtype=float32)

time = 34483	action = 0	current_phase = 0	next_phase = 1	reward = -0.002037	array([[-2.1443555, -2.891902 ]], dtype=float32)

time = 34488	action = 0	current_phase = 0	next_phase = 1	reward = 0.058201	array([[-2.5560865, -2.8541398]], dtype=float32)

time = 34493	action = 1	current_phase = 0	next_phase = 1	reward = -1.298130	array([[-5.687424 , -3.3854303]], dtype=float32)

time = 34501	action = 1	current_phase = 1	next_phase = 0	reward = -1.476234	array([[-3.8662882, -2.4069152]], dtype=float32)

time = 34509	action = 0	current_phase = 0	next_phase = 1	reward = 0.246920	array([[-1.3784013, -2.290432 ]], dtype=float32)

time = 34514	action = 0	current_phase = 0	next_phase = 1	reward = 0.024387	array([[-1.7779243, -2.7564769]], dtype=float32)

time = 34519	action = 1	current_phase = 0	next_phase = 1	reward = -0.731517	array([[-2.3152401, -2.2743163]], dtype=float32)

time = 34527	action = 1	current_phase = 1	next_phase = 0	reward = -1.215702	array([[-3.5672407, -1.9558272]], dtype=float32)

time = 34535	action = 0	current_phase = 0	next_phase = 1	reward = 0.185010	array([[-1.2303641, -2.7651844]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0368 - val_loss: 0.0244

Epoch 2/50

 - 3s - loss: 0.0314 - val_loss: 0.0202

Epoch 3/50

 - 3s - loss: 0.0334 - val_loss: 0.0168

Epoch 4/50

 - 3s - loss: 0.0265 - val_loss: 0.0212

Epoch 5/50

 - 3s - loss: 0.0243 - val_loss: 0.0206

Epoch 6/50

 - 3s - loss: 0.0279 - val_loss: 0.0168

Epoch 7/50

 - 3s - loss: 0.0252 - val_loss: 0.0224

Epoch 8/50

 - 3s - loss: 0.0287 - val_loss: 0.0175

Epoch 9/50

 - 3s - loss: 0.0310 - val_loss: 0.0184

Epoch 10/50

 - 3s - loss: 0.0358 - val_loss: 0.0184

Epoch 11/50

 - 3s - loss: 0.0243 - val_loss: 0.0176

Epoch 12/50

 - 3s - loss: 0.0288 - val_loss: 0.0203

Epoch 13/50

 - 3s - loss: 0.0321 - val_loss: 0.0209

Epoch 14/50

 - 3s - loss: 0.0278 - val_loss: 0.0178

Epoch 15/50

 - 3s - loss: 0.0294 - val_loss: 0.0174

Epoch 16/50

 - 3s - loss: 0.0294 - val_loss: 0.0188

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 34540	action = 0	current_phase = 0	next_phase = 1	reward = -0.283254	array([[-1.7546237, -2.2852507]], dtype=float32)

time = 34545	action = 0	current_phase = 0	next_phase = 1	reward = 0.623772	array([[-2.2128787, -2.813947 ]], dtype=float32)

time = 34550	action = 1	current_phase = 0	next_phase = 1	reward = -1.239945	array([[-4.587362 , -2.7616363]], dtype=float32)

time = 34558	action = 1	current_phase = 1	next_phase = 0	reward = -0.647418	array([[-3.5994444, -1.9786123]], dtype=float32)

time = 34566	action = 0	current_phase = 0	next_phase = 1	reward = -0.080198	array([[-1.7230957, -2.6969998]], dtype=float32)

time = 34571	action = 0	current_phase = 0	next_phase = 1	reward = -0.015280	array([[-2.0129457, -2.723941 ]], dtype=float32)

time = 34576	action = 0	current_phase = 0	next_phase = 1	reward = 0.045020	array([[-2.3757582, -2.9006183]], dtype=float32)

time = 34581	action = 1	current_phase = 0	next_phase = 1	reward = -1.496036	array([[-5.8541408, -3.1065702]], dtype=float32)

time = 34589	action = 1	current_phase = 1	next_phase = 0	reward = -0.816074	array([[-3.8201902, -2.1597657]], dtype=float32)

time = 34597	action = 0	current_phase = 0	next_phase = 1	reward = -0.060453	array([[-1.741988, -2.729865]], dtype=float32)

time = 34602	action = 0	current_phase = 0	next_phase = 1	reward = 0.008537	array([[-2.2300587, -2.8438523]], dtype=float32)

time = 34607	action = 0	current_phase = 0	next_phase = 1	reward = 0.068169	array([[-2.5721462, -2.8675144]], dtype=float32)

time = 34612	action = 1	current_phase = 0	next_phase = 1	reward = -1.650218	array([[-5.8266683, -3.4382432]], dtype=float32)

time = 34620	action = 1	current_phase = 1	next_phase = 0	reward = -1.057488	array([[-3.8972666, -2.280093 ]], dtype=float32)

time = 34628	action = 0	current_phase = 0	next_phase = 1	reward = -0.059071	array([[-1.6627872, -2.578252 ]], dtype=float32)

time = 34633	action = 0	current_phase = 0	next_phase = 1	reward = 0.014705	array([[-2.1993928, -2.8563678]], dtype=float32)

time = 34638	action = 0	current_phase = 0	next_phase = 1	reward = 0.071222	array([[-2.771876 , -2.8797004]], dtype=float32)

time = 34643	action = 1	current_phase = 0	next_phase = 1	reward = -1.824536	array([[-5.6727023, -3.5564759]], dtype=float32)

time = 34651	action = 1	current_phase = 1	next_phase = 0	reward = -1.083232	array([[-4.0427504, -2.3668997]], dtype=float32)

time = 34659	action = 0	current_phase = 0	next_phase = 1	reward = -0.027614	array([[-1.5382967, -2.3101683]], dtype=float32)

time = 34664	action = 0	current_phase = 0	next_phase = 1	reward = 0.047211	array([[-1.891407 , -2.7399883]], dtype=float32)

time = 34669	action = 0	current_phase = 0	next_phase = 1	reward = 0.054956	array([[-2.2083566, -2.227728 ]], dtype=float32)

time = 34674	action = 1	current_phase = 0	next_phase = 1	reward = -1.088345	array([[-5.5805073, -2.9314344]], dtype=float32)

time = 34682	action = 1	current_phase = 1	next_phase = 0	reward = -1.040188	array([[-3.7363672, -2.2599113]], dtype=float32)

time = 34690	action = 0	current_phase = 0	next_phase = 1	reward = 0.241756	array([[-1.7361042, -2.2905395]], dtype=float32)

time = 34695	action = 0	current_phase = 0	next_phase = 1	reward = 0.031741	array([[-2.2129033, -2.823081 ]], dtype=float32)

time = 34700	action = 1	current_phase = 0	next_phase = 1	reward = -1.254326	array([[-4.293978 , -2.6829884]], dtype=float32)

time = 34708	action = 1	current_phase = 1	next_phase = 0	reward = -0.688248	array([[-3.6646771, -2.0870605]], dtype=float32)

time = 34716	action = 0	current_phase = 0	next_phase = 1	reward = -0.084237	array([[-1.64733  , -2.6927543]], dtype=float32)

time = 34721	action = 0	current_phase = 0	next_phase = 1	reward = -0.010087	array([[-1.9399481, -2.7062259]], dtype=float32)

time = 34726	action = 0	current_phase = 0	next_phase = 1	reward = 0.044791	array([[-2.3763711, -2.8899407]], dtype=float32)

time = 34731	action = 1	current_phase = 0	next_phase = 1	reward = -1.397691	array([[-5.6916094, -3.0785599]], dtype=float32)

time = 34739	action = 1	current_phase = 1	next_phase = 0	reward = -0.759564	array([[-3.8226242, -2.1495461]], dtype=float32)

time = 34747	action = 0	current_phase = 0	next_phase = 1	reward = -0.057900	array([[-1.8136554, -2.7724707]], dtype=float32)

time = 34752	action = 0	current_phase = 0	next_phase = 1	reward = 0.001248	array([[-2.2342916, -2.846524 ]], dtype=float32)

time = 34757	action = 0	current_phase = 0	next_phase = 1	reward = 0.060871	array([[-2.7199593, -2.8641984]], dtype=float32)

time = 34762	action = 1	current_phase = 0	next_phase = 1	reward = -1.604923	array([[-5.811519 , -3.3685122]], dtype=float32)

time = 34770	action = 1	current_phase = 1	next_phase = 0	reward = -0.948691	array([[-3.9371648, -2.2799206]], dtype=float32)

time = 34778	action = 0	current_phase = 0	next_phase = 1	reward = -0.063230	array([[-1.631556 , -2.6230955]], dtype=float32)

time = 34783	action = 0	current_phase = 0	next_phase = 1	reward = 0.016252	array([[-2.1666126, -2.864508 ]], dtype=float32)

time = 34788	action = 0	current_phase = 0	next_phase = 1	reward = 0.073980	array([[-2.7359562, -2.8172035]], dtype=float32)

time = 34793	action = 1	current_phase = 0	next_phase = 1	reward = -1.740567	array([[-5.724575 , -3.3924606]], dtype=float32)

time = 34801	action = 1	current_phase = 1	next_phase = 0	reward = -1.540915	array([[-4.090803 , -2.4245093]], dtype=float32)

time = 34809	action = 0	current_phase = 0	next_phase = 1	reward = 0.546267	array([[-1.4454249, -2.3713381]], dtype=float32)

time = 34814	action = 0	current_phase = 0	next_phase = 1	reward = 0.032823	array([[-1.9948783, -2.7748156]], dtype=float32)

time = 34819	action = 1	current_phase = 0	next_phase = 1	reward = -0.653365	array([[-2.328853 , -2.2275913]], dtype=float32)

time = 34827	action = 1	current_phase = 1	next_phase = 0	reward = -0.649557	array([[-3.2856135, -1.9955213]], dtype=float32)

time = 34835	action = 0	current_phase = 0	next_phase = 1	reward = -0.105692	array([[-1.3926914, -2.7660766]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0296 - val_loss: 0.0176

Epoch 2/50

 - 3s - loss: 0.0386 - val_loss: 0.0165

Epoch 3/50

 - 3s - loss: 0.0308 - val_loss: 0.0189

Epoch 4/50

 - 3s - loss: 0.0295 - val_loss: 0.0186

Epoch 5/50

 - 3s - loss: 0.0269 - val_loss: 0.0204

Epoch 6/50

 - 3s - loss: 0.0341 - val_loss: 0.0165

Epoch 7/50

 - 3s - loss: 0.0256 - val_loss: 0.0204

Epoch 8/50

 - 3s - loss: 0.0264 - val_loss: 0.0170

Epoch 9/50

 - 3s - loss: 0.0291 - val_loss: 0.0174

Epoch 10/50

 - 3s - loss: 0.0291 - val_loss: 0.0155

Epoch 11/50

 - 3s - loss: 0.0358 - val_loss: 0.0148

Epoch 12/50

 - 3s - loss: 0.0278 - val_loss: 0.0197

Epoch 13/50

 - 3s - loss: 0.0322 - val_loss: 0.0191

Epoch 14/50

 - 3s - loss: 0.0301 - val_loss: 0.0157

Epoch 15/50

 - 3s - loss: 0.0289 - val_loss: 0.0151

Epoch 16/50

 - 3s - loss: 0.0271 - val_loss: 0.0149

Epoch 17/50

 - 3s - loss: 0.0263 - val_loss: 0.0158

Epoch 18/50

 - 3s - loss: 0.0253 - val_loss: 0.0183

Epoch 19/50

 - 3s - loss: 0.0286 - val_loss: 0.0147

Epoch 20/50

 - 3s - loss: 0.0249 - val_loss: 0.0162

Epoch 21/50

 - 3s - loss: 0.0275 - val_loss: 0.0137

Epoch 22/50

 - 3s - loss: 0.0279 - val_loss: 0.0134

Epoch 23/50

 - 3s - loss: 0.0266 - val_loss: 0.0154

Epoch 24/50

 - 3s - loss: 0.0255 - val_loss: 0.0190

Epoch 25/50

 - 3s - loss: 0.0259 - val_loss: 0.0177

Epoch 26/50

 - 3s - loss: 0.0260 - val_loss: 0.0192

Epoch 27/50

 - 3s - loss: 0.0258 - val_loss: 0.0197

Epoch 28/50

 - 3s - loss: 0.0225 - val_loss: 0.0170

Epoch 29/50

 - 3s - loss: 0.0233 - val_loss: 0.0151

Epoch 30/50

 - 3s - loss: 0.0263 - val_loss: 0.0152

Epoch 31/50

 - 3s - loss: 0.0250 - val_loss: 0.0134

Epoch 32/50

 - 3s - loss: 0.0266 - val_loss: 0.0157

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 34840	action = 0	current_phase = 0	next_phase = 1	reward = -0.302653	array([[-1.7215614, -2.2799838]], dtype=float32)

time = 34845	action = 0	current_phase = 0	next_phase = 1	reward = 0.061268	array([[-2.300994 , -2.8667831]], dtype=float32)

time = 34850	action = 1	current_phase = 0	next_phase = 1	reward = -1.090448	array([[-4.5827327, -2.9130538]], dtype=float32)

time = 34858	action = 1	current_phase = 1	next_phase = 0	reward = -0.707400	array([[-3.7722   , -2.0986636]], dtype=float32)

time = 34866	action = 0	current_phase = 0	next_phase = 1	reward = -0.093598	array([[-1.7319239, -2.6918564]], dtype=float32)

time = 34871	action = 0	current_phase = 0	next_phase = 1	reward = -0.018627	array([[-1.9811025, -2.7066765]], dtype=float32)

time = 34876	action = 0	current_phase = 0	next_phase = 1	reward = 0.043533	array([[-2.419287 , -2.9095094]], dtype=float32)

time = 34881	action = 1	current_phase = 0	next_phase = 1	reward = -1.483942	array([[-5.8039374, -3.1688623]], dtype=float32)

time = 34889	action = 1	current_phase = 1	next_phase = 0	reward = -0.773987	array([[-3.7783833, -2.1213436]], dtype=float32)

time = 34897	action = 0	current_phase = 0	next_phase = 1	reward = -0.059481	array([[-1.7413654, -2.7555323]], dtype=float32)

time = 34902	action = 0	current_phase = 0	next_phase = 1	reward = 0.019391	array([[-2.1594179, -2.8176007]], dtype=float32)

time = 34907	action = 0	current_phase = 0	next_phase = 1	reward = 0.075725	array([[-2.6408072, -2.9203014]], dtype=float32)

time = 34912	action = 1	current_phase = 0	next_phase = 1	reward = -1.735315	array([[-5.8994856, -3.4557123]], dtype=float32)

time = 34920	action = 1	current_phase = 1	next_phase = 0	reward = -0.941086	array([[-3.8815846, -2.2758155]], dtype=float32)

time = 34928	action = 0	current_phase = 0	next_phase = 1	reward = -0.052917	array([[-1.7158935, -2.585481 ]], dtype=float32)

time = 34933	action = 0	current_phase = 0	next_phase = 1	reward = 0.024411	array([[-2.1723142, -2.8798516]], dtype=float32)

time = 34938	action = 0	current_phase = 0	next_phase = 1	reward = 0.071443	array([[-2.7019079, -2.8602002]], dtype=float32)

time = 34943	action = 1	current_phase = 0	next_phase = 1	reward = -1.827944	array([[-5.8491907, -3.4019785]], dtype=float32)

time = 34951	action = 1	current_phase = 1	next_phase = 0	reward = -1.084837	array([[-4.024043 , -2.3935108]], dtype=float32)

time = 34959	action = 0	current_phase = 0	next_phase = 1	reward = -0.029708	array([[-1.4633472, -2.2763696]], dtype=float32)

time = 34964	action = 0	current_phase = 0	next_phase = 1	reward = 0.056175	array([[-1.7863967, -2.690261 ]], dtype=float32)

time = 34969	action = 0	current_phase = 0	next_phase = 1	reward = 0.338540	array([[-2.1814003, -2.2570372]], dtype=float32)

time = 34974	action = 1	current_phase = 0	next_phase = 1	reward = -1.455781	array([[-5.7664676, -3.1903865]], dtype=float32)

time = 34982	action = 1	current_phase = 1	next_phase = 0	reward = -0.767720	array([[-3.8207393, -2.3753304]], dtype=float32)

time = 34990	action = 0	current_phase = 0	next_phase = 1	reward = -0.298500	array([[-1.7173104, -2.2748814]], dtype=float32)

time = 34995	action = 0	current_phase = 0	next_phase = 1	reward = 0.330769	array([[-2.1126425, -2.7951365]], dtype=float32)

time = 35000	action = 1	current_phase = 0	next_phase = 1	reward = -1.255383	array([[-4.639613 , -2.8877537]], dtype=float32)

time = 35008	action = 1	current_phase = 1	next_phase = 0	reward = -0.698020	array([[-3.6335378, -1.9944062]], dtype=float32)

time = 35016	action = 0	current_phase = 0	next_phase = 1	reward = -0.085125	array([[-1.7276335, -2.7224767]], dtype=float32)

time = 35021	action = 0	current_phase = 0	next_phase = 1	reward = -0.001827	array([[-1.9932542, -2.723273 ]], dtype=float32)

time = 35026	action = 0	current_phase = 0	next_phase = 1	reward = 0.071635	array([[-2.4035497, -2.9019167]], dtype=float32)

time = 35031	action = 1	current_phase = 0	next_phase = 1	reward = -1.478768	array([[-5.9629097, -3.2341545]], dtype=float32)

time = 35039	action = 1	current_phase = 1	next_phase = 0	reward = -0.849958	array([[-3.8416548, -2.2024286]], dtype=float32)

time = 35047	action = 0	current_phase = 0	next_phase = 1	reward = -0.077674	array([[-1.7815542, -2.7360764]], dtype=float32)

time = 35052	action = 0	current_phase = 0	next_phase = 1	reward = 0.007100	array([[-2.1474826, -2.8001268]], dtype=float32)

time = 35057	action = 0	current_phase = 0	next_phase = 1	reward = 0.072992	array([[-2.5358388, -2.8512092]], dtype=float32)

time = 35062	action = 1	current_phase = 0	next_phase = 1	reward = -1.637789	array([[-5.9752   , -3.4368613]], dtype=float32)

time = 35070	action = 1	current_phase = 1	next_phase = 0	reward = -1.046410	array([[-3.879126 , -2.3224425]], dtype=float32)

time = 35078	action = 0	current_phase = 0	next_phase = 1	reward = -0.045643	array([[-1.7141042, -2.5681968]], dtype=float32)

time = 35083	action = 0	current_phase = 0	next_phase = 1	reward = 0.006025	array([[-2.1542814, -2.8484812]], dtype=float32)

time = 35088	action = 0	current_phase = 0	next_phase = 1	reward = 0.064235	array([[-2.76381  , -2.8837783]], dtype=float32)

time = 35093	action = 1	current_phase = 0	next_phase = 1	reward = -1.995144	array([[-5.8242817, -3.4883435]], dtype=float32)

time = 35101	action = 1	current_phase = 1	next_phase = 0	reward = -1.340517	array([[-4.106671 , -2.4341218]], dtype=float32)

time = 35109	action = 0	current_phase = 0	next_phase = 1	reward = 0.243892	array([[-1.4395657, -2.272171 ]], dtype=float32)

time = 35114	action = 0	current_phase = 0	next_phase = 1	reward = 0.011924	array([[-1.8724018, -2.718601 ]], dtype=float32)

time = 35119	action = 1	current_phase = 0	next_phase = 1	reward = -0.653445	array([[-2.395842 , -2.2528174]], dtype=float32)

time = 35127	action = 1	current_phase = 1	next_phase = 0	reward = -0.597535	array([[-3.3047223, -1.9004515]], dtype=float32)

time = 35135	action = 0	current_phase = 0	next_phase = 1	reward = -0.371886	array([[-1.358032, -2.860016]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0349 - val_loss: 0.0178

Epoch 2/50

 - 3s - loss: 0.0327 - val_loss: 0.0167

Epoch 3/50

 - 3s - loss: 0.0364 - val_loss: 0.0227

Epoch 4/50

 - 3s - loss: 0.0280 - val_loss: 0.0155

Epoch 5/50

 - 3s - loss: 0.0276 - val_loss: 0.0194

Epoch 6/50

 - 3s - loss: 0.0380 - val_loss: 0.0173

Epoch 7/50

 - 3s - loss: 0.0250 - val_loss: 0.0173

Epoch 8/50

 - 3s - loss: 0.0249 - val_loss: 0.0174

Epoch 9/50

 - 3s - loss: 0.0277 - val_loss: 0.0158

Epoch 10/50

 - 3s - loss: 0.0246 - val_loss: 0.0164

Epoch 11/50

 - 3s - loss: 0.0274 - val_loss: 0.0149

Epoch 12/50

 - 3s - loss: 0.0302 - val_loss: 0.0250

Epoch 13/50

 - 3s - loss: 0.0313 - val_loss: 0.0193

Epoch 14/50

 - 3s - loss: 0.0305 - val_loss: 0.0165

Epoch 15/50

 - 3s - loss: 0.0275 - val_loss: 0.0153

Epoch 16/50

 - 3s - loss: 0.0256 - val_loss: 0.0177

Epoch 17/50

 - 3s - loss: 0.0257 - val_loss: 0.0176

Epoch 18/50

 - 3s - loss: 0.0286 - val_loss: 0.0159

Epoch 19/50

 - 3s - loss: 0.0273 - val_loss: 0.0177

Epoch 20/50

 - 3s - loss: 0.0251 - val_loss: 0.0207

Epoch 21/50

 - 3s - loss: 0.0268 - val_loss: 0.0198

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 35140	action = 0	current_phase = 0	next_phase = 1	reward = 0.264429	array([[-1.799689 , -2.3207853]], dtype=float32)

time = 35145	action = 0	current_phase = 0	next_phase = 1	reward = 0.049666	array([[-2.295826 , -2.8688178]], dtype=float32)

time = 35150	action = 1	current_phase = 0	next_phase = 1	reward = -1.279408	array([[-4.843081 , -2.8693027]], dtype=float32)

time = 35158	action = 1	current_phase = 1	next_phase = 0	reward = -0.705060	array([[-3.6477408, -2.0493073]], dtype=float32)

time = 35166	action = 0	current_phase = 0	next_phase = 1	reward = -0.080289	array([[-1.884882 , -2.7566466]], dtype=float32)

time = 35171	action = 0	current_phase = 0	next_phase = 1	reward = -0.015167	array([[-2.1249874, -2.8163412]], dtype=float32)

time = 35176	action = 0	current_phase = 0	next_phase = 1	reward = 0.057403	array([[-2.4199283, -2.954358 ]], dtype=float32)

time = 35181	action = 1	current_phase = 0	next_phase = 1	reward = -1.490702	array([[-5.955601 , -3.2754962]], dtype=float32)

time = 35189	action = 1	current_phase = 1	next_phase = 0	reward = -0.796768	array([[-3.8480296, -2.2423894]], dtype=float32)

time = 35197	action = 0	current_phase = 0	next_phase = 1	reward = -0.071560	array([[-1.8225768, -2.7679627]], dtype=float32)

time = 35202	action = 0	current_phase = 0	next_phase = 1	reward = 0.015264	array([[-2.1284914, -2.8129396]], dtype=float32)

time = 35207	action = 0	current_phase = 0	next_phase = 1	reward = 0.066452	array([[-2.535171 , -2.9642396]], dtype=float32)

time = 35212	action = 1	current_phase = 0	next_phase = 1	reward = -1.553247	array([[-5.997442 , -3.4696739]], dtype=float32)

time = 35220	action = 1	current_phase = 1	next_phase = 0	reward = -0.954164	array([[-3.942823 , -2.4035997]], dtype=float32)

time = 35228	action = 0	current_phase = 0	next_phase = 1	reward = -0.040404	array([[-1.7632673, -2.5541847]], dtype=float32)

time = 35233	action = 0	current_phase = 0	next_phase = 1	reward = 0.025766	array([[-2.1944432, -2.8906088]], dtype=float32)

time = 35238	action = 0	current_phase = 0	next_phase = 1	reward = 0.076909	array([[-2.8210652, -2.8452573]], dtype=float32)

time = 35243	action = 1	current_phase = 0	next_phase = 1	reward = -1.334628	array([[-5.9146247, -3.5700772]], dtype=float32)

time = 35251	action = 1	current_phase = 1	next_phase = 0	reward = -1.444960	array([[-3.886994, -2.520743]], dtype=float32)

time = 35259	action = 0	current_phase = 0	next_phase = 1	reward = 0.246337	array([[-1.6096396, -2.2800581]], dtype=float32)

time = 35264	action = 0	current_phase = 0	next_phase = 1	reward = 0.027162	array([[-1.8578451, -2.7176497]], dtype=float32)

time = 35269	action = 1	current_phase = 0	next_phase = 1	reward = -0.639918	array([[-2.449767 , -2.2783315]], dtype=float32)

time = 35277	action = 1	current_phase = 1	next_phase = 0	reward = -0.641684	array([[-3.5073285, -2.0666142]], dtype=float32)

time = 35285	action = 0	current_phase = 0	next_phase = 1	reward = -0.104502	array([[-1.4999917, -2.7929587]], dtype=float32)

time = 35290	action = 0	current_phase = 0	next_phase = 1	reward = -0.026873	array([[-1.8581548, -2.3186972]], dtype=float32)

time = 35295	action = 0	current_phase = 0	next_phase = 1	reward = -0.240474	array([[-2.2668974, -2.857293 ]], dtype=float32)

time = 35300	action = 1	current_phase = 0	next_phase = 1	reward = -1.026189	array([[-4.856626 , -2.8173192]], dtype=float32)

time = 35308	action = 1	current_phase = 1	next_phase = 0	reward = -0.712659	array([[-3.7469182, -2.1499853]], dtype=float32)

time = 35316	action = 0	current_phase = 0	next_phase = 1	reward = -0.087401	array([[-1.761404 , -2.7232814]], dtype=float32)

time = 35321	action = 0	current_phase = 0	next_phase = 1	reward = -0.020522	array([[-1.983943 , -2.7507238]], dtype=float32)

time = 35326	action = 0	current_phase = 0	next_phase = 1	reward = 0.045174	array([[-2.4129164, -2.9435935]], dtype=float32)

time = 35331	action = 1	current_phase = 0	next_phase = 1	reward = -1.440564	array([[-5.928954 , -3.1951375]], dtype=float32)

time = 35339	action = 1	current_phase = 1	next_phase = 0	reward = -0.839969	array([[-3.8080606, -2.1932993]], dtype=float32)

time = 35347	action = 0	current_phase = 0	next_phase = 1	reward = -0.064514	array([[-1.8364284, -2.7558875]], dtype=float32)

time = 35352	action = 0	current_phase = 0	next_phase = 1	reward = 0.006101	array([[-2.1610434, -2.8204029]], dtype=float32)

time = 35357	action = 0	current_phase = 0	next_phase = 1	reward = 0.067251	array([[-2.6516976, -2.9576583]], dtype=float32)

time = 35362	action = 1	current_phase = 0	next_phase = 1	reward = -1.645044	array([[-6.003068, -3.509561]], dtype=float32)

time = 35370	action = 1	current_phase = 1	next_phase = 0	reward = -0.948813	array([[-3.880651 , -2.3271165]], dtype=float32)

time = 35378	action = 0	current_phase = 0	next_phase = 1	reward = -0.055024	array([[-1.808208, -2.608617]], dtype=float32)

time = 35383	action = 0	current_phase = 0	next_phase = 1	reward = 0.030967	array([[-2.175051 , -2.8971925]], dtype=float32)

time = 35388	action = 0	current_phase = 0	next_phase = 1	reward = 0.075979	array([[-2.7252798, -2.9141076]], dtype=float32)

time = 35393	action = 1	current_phase = 0	next_phase = 1	reward = -1.347976	array([[-5.8966136, -3.5269732]], dtype=float32)

time = 35401	action = 1	current_phase = 1	next_phase = 0	reward = -1.502427	array([[-4.083518 , -2.7032533]], dtype=float32)

time = 35409	action = 0	current_phase = 0	next_phase = 1	reward = 0.258923	array([[-1.583066 , -2.2960687]], dtype=float32)

time = 35414	action = 0	current_phase = 0	next_phase = 1	reward = 0.042555	array([[-1.8299766, -2.7065125]], dtype=float32)

time = 35419	action = 1	current_phase = 0	next_phase = 1	reward = -0.628400	array([[-2.6121676, -2.2904365]], dtype=float32)

time = 35427	action = 1	current_phase = 1	next_phase = 0	reward = -0.868904	array([[-3.448793 , -1.9960225]], dtype=float32)

time = 35435	action = 0	current_phase = 0	next_phase = 1	reward = -0.088712	array([[-1.5429829, -2.7737513]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0286 - val_loss: 0.0170

Epoch 2/50

 - 3s - loss: 0.0270 - val_loss: 0.0131

Epoch 3/50

 - 3s - loss: 0.0348 - val_loss: 0.0227

Epoch 4/50

 - 3s - loss: 0.0377 - val_loss: 0.0122

Epoch 5/50

 - 3s - loss: 0.0248 - val_loss: 0.0115

Epoch 6/50

 - 3s - loss: 0.0258 - val_loss: 0.0122

Epoch 7/50

 - 3s - loss: 0.0257 - val_loss: 0.0142

Epoch 8/50

 - 3s - loss: 0.0260 - val_loss: 0.0164

Epoch 9/50

 - 3s - loss: 0.0260 - val_loss: 0.0215

Epoch 10/50

 - 3s - loss: 0.0259 - val_loss: 0.0121

Epoch 11/50

 - 3s - loss: 0.0218 - val_loss: 0.0148

Epoch 12/50

 - 3s - loss: 0.0271 - val_loss: 0.0193

Epoch 13/50

 - 3s - loss: 0.0217 - val_loss: 0.0157

Epoch 14/50

 - 3s - loss: 0.0259 - val_loss: 0.0222

Epoch 15/50

 - 3s - loss: 0.0266 - val_loss: 0.0130

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 35440	action = 0	current_phase = 0	next_phase = 1	reward = 0.256323	array([[-1.6567138, -2.3338401]], dtype=float32)

time = 35445	action = 0	current_phase = 0	next_phase = 1	reward = -0.240844	array([[-2.1415575, -2.8852022]], dtype=float32)

time = 35450	action = 1	current_phase = 0	next_phase = 1	reward = -1.084006	array([[-4.8322787, -2.7085776]], dtype=float32)

time = 35458	action = 1	current_phase = 1	next_phase = 0	reward = -0.688372	array([[-3.7378101, -2.0619159]], dtype=float32)

time = 35466	action = 0	current_phase = 0	next_phase = 1	reward = -0.073004	array([[-1.6703928, -2.7349086]], dtype=float32)

time = 35471	action = 0	current_phase = 0	next_phase = 1	reward = 0.002871	array([[-1.8795156, -2.7854986]], dtype=float32)

time = 35476	action = 0	current_phase = 0	next_phase = 1	reward = 0.057781	array([[-2.4171786, -2.9734104]], dtype=float32)

time = 35481	action = 1	current_phase = 0	next_phase = 1	reward = -1.409858	array([[-5.948608 , -3.2367704]], dtype=float32)

time = 35489	action = 1	current_phase = 1	next_phase = 0	reward = -0.828037	array([[-3.743103 , -2.0964413]], dtype=float32)

time = 35497	action = 0	current_phase = 0	next_phase = 1	reward = -0.071378	array([[-1.6933115, -2.7910013]], dtype=float32)

time = 35502	action = 0	current_phase = 0	next_phase = 1	reward = -0.005178	array([[-2.0186605, -2.8458326]], dtype=float32)

time = 35507	action = 0	current_phase = 0	next_phase = 1	reward = 0.062040	array([[-2.51161  , -2.9420707]], dtype=float32)

time = 35512	action = 1	current_phase = 0	next_phase = 1	reward = -1.481326	array([[-6.029498, -3.399803]], dtype=float32)

time = 35520	action = 1	current_phase = 1	next_phase = 0	reward = -0.949633	array([[-3.9037614, -2.3572142]], dtype=float32)

time = 35528	action = 0	current_phase = 0	next_phase = 1	reward = -0.068606	array([[-1.7363834, -2.7241573]], dtype=float32)

time = 35533	action = 0	current_phase = 0	next_phase = 1	reward = -0.001553	array([[-2.0454507, -2.9053106]], dtype=float32)

time = 35538	action = 0	current_phase = 0	next_phase = 1	reward = 0.080102	array([[-2.7544434, -2.9111872]], dtype=float32)

time = 35543	action = 1	current_phase = 0	next_phase = 1	reward = -1.832051	array([[-5.8783917, -3.482255 ]], dtype=float32)

time = 35551	action = 1	current_phase = 1	next_phase = 0	reward = -1.047029	array([[-4.069791 , -2.4693103]], dtype=float32)

time = 35559	action = 0	current_phase = 0	next_phase = 1	reward = -0.051153	array([[-1.4218675, -2.2841284]], dtype=float32)

time = 35564	action = 0	current_phase = 0	next_phase = 1	reward = 0.015898	array([[-1.754829, -2.746552]], dtype=float32)

time = 35569	action = 0	current_phase = 0	next_phase = 1	reward = 0.064300	array([[-2.319174 , -2.4857306]], dtype=float32)

time = 35574	action = 1	current_phase = 0	next_phase = 1	reward = -1.556088	array([[-5.8663545, -3.1141708]], dtype=float32)

time = 35582	action = 1	current_phase = 1	next_phase = 0	reward = -0.870215	array([[-3.8652415, -2.2641447]], dtype=float32)

time = 35590	action = 0	current_phase = 0	next_phase = 1	reward = 0.263054	array([[-1.6130083, -2.301784 ]], dtype=float32)

time = 35595	action = 0	current_phase = 0	next_phase = 1	reward = 0.049904	array([[-2.065481, -2.854458]], dtype=float32)

time = 35600	action = 1	current_phase = 0	next_phase = 1	reward = -1.428452	array([[-4.6958885, -2.8803117]], dtype=float32)

time = 35608	action = 1	current_phase = 1	next_phase = 0	reward = -0.716586	array([[-3.6610303, -1.952328 ]], dtype=float32)

time = 35616	action = 0	current_phase = 0	next_phase = 1	reward = -0.088052	array([[-1.6199207, -2.7458196]], dtype=float32)

time = 35621	action = 0	current_phase = 0	next_phase = 1	reward = 0.000687	array([[-1.8990586, -2.7851079]], dtype=float32)

time = 35626	action = 0	current_phase = 0	next_phase = 1	reward = 0.062107	array([[-2.3487551, -2.9869478]], dtype=float32)

time = 35631	action = 1	current_phase = 0	next_phase = 1	reward = -1.456154	array([[-5.946533 , -3.1920369]], dtype=float32)

time = 35639	action = 1	current_phase = 1	next_phase = 0	reward = -0.903433	array([[-3.7593985, -2.1278982]], dtype=float32)

time = 35647	action = 0	current_phase = 0	next_phase = 1	reward = -0.068064	array([[-1.7220074, -2.7574103]], dtype=float32)

time = 35652	action = 0	current_phase = 0	next_phase = 1	reward = 0.023654	array([[-2.0342746, -2.8513305]], dtype=float32)

time = 35657	action = 0	current_phase = 0	next_phase = 1	reward = 0.075225	array([[-2.4655223, -2.9698117]], dtype=float32)

time = 35662	action = 1	current_phase = 0	next_phase = 1	reward = -1.659360	array([[-6.011903 , -3.4896777]], dtype=float32)

time = 35670	action = 1	current_phase = 1	next_phase = 0	reward = -0.946028	array([[-3.9004579, -2.3937373]], dtype=float32)

time = 35678	action = 0	current_phase = 0	next_phase = 1	reward = -0.041719	array([[-1.6657085, -2.7094562]], dtype=float32)

time = 35683	action = 0	current_phase = 0	next_phase = 1	reward = 0.029856	array([[-2.0795648, -2.8963392]], dtype=float32)

time = 35688	action = 0	current_phase = 0	next_phase = 1	reward = 0.075263	array([[-2.5908868, -2.9674325]], dtype=float32)

time = 35693	action = 1	current_phase = 0	next_phase = 1	reward = -1.939235	array([[-5.8500056, -3.7325408]], dtype=float32)

time = 35701	action = 1	current_phase = 1	next_phase = 0	reward = -1.092250	array([[-4.0983562, -2.443801 ]], dtype=float32)

time = 35709	action = 0	current_phase = 0	next_phase = 1	reward = -0.029328	array([[-1.4507754, -2.3108044]], dtype=float32)

time = 35714	action = 0	current_phase = 0	next_phase = 1	reward = 0.030280	array([[-1.8446047, -2.7772837]], dtype=float32)

time = 35719	action = 1	current_phase = 0	next_phase = 1	reward = -1.237744	array([[-2.3774455, -2.2616885]], dtype=float32)

time = 35727	action = 1	current_phase = 1	next_phase = 0	reward = -1.334621	array([[-3.6169248, -1.9420276]], dtype=float32)

time = 35735	action = 0	current_phase = 0	next_phase = 1	reward = 0.762291	array([[-1.4037619, -2.7474308]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0280 - val_loss: 0.0163

Epoch 2/50

 - 3s - loss: 0.0302 - val_loss: 0.0167

Epoch 3/50

 - 3s - loss: 0.0267 - val_loss: 0.0118

Epoch 4/50

 - 3s - loss: 0.0267 - val_loss: 0.0139

Epoch 5/50

 - 3s - loss: 0.0280 - val_loss: 0.0169

Epoch 6/50

 - 3s - loss: 0.0215 - val_loss: 0.0143

Epoch 7/50

 - 3s - loss: 0.0293 - val_loss: 0.0215

Epoch 8/50

 - 3s - loss: 0.0291 - val_loss: 0.0132

Epoch 9/50

 - 3s - loss: 0.0216 - val_loss: 0.0162

Epoch 10/50

 - 3s - loss: 0.0272 - val_loss: 0.0135

Epoch 11/50

 - 3s - loss: 0.0222 - val_loss: 0.0130

Epoch 12/50

 - 3s - loss: 0.0244 - val_loss: 0.0172

Epoch 13/50

 - 3s - loss: 0.0244 - val_loss: 0.0184

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 35740	action = 0	current_phase = 0	next_phase = 1	reward = -0.022998	array([[-1.7871027, -2.330452 ]], dtype=float32)

time = 35745	action = 0	current_phase = 0	next_phase = 1	reward = 0.047599	array([[-2.3020234, -2.9396124]], dtype=float32)

time = 35750	action = 1	current_phase = 0	next_phase = 1	reward = -1.310057	array([[-4.977555 , -2.8936367]], dtype=float32)

time = 35758	action = 1	current_phase = 1	next_phase = 0	reward = -0.699541	array([[-3.5592985, -1.9280035]], dtype=float32)

time = 35766	action = 0	current_phase = 0	next_phase = 1	reward = -0.093489	array([[-1.6898634, -2.7404392]], dtype=float32)

time = 35771	action = 0	current_phase = 0	next_phase = 1	reward = -0.014457	array([[-1.9169905, -2.7583835]], dtype=float32)

time = 35776	action = 0	current_phase = 0	next_phase = 1	reward = 0.061425	array([[-2.3678486, -2.9774776]], dtype=float32)

time = 35781	action = 1	current_phase = 0	next_phase = 1	reward = -1.418344	array([[-6.075656 , -3.1404672]], dtype=float32)

time = 35789	action = 1	current_phase = 1	next_phase = 0	reward = -0.789135	array([[-3.6922274, -2.0613706]], dtype=float32)

time = 35797	action = 0	current_phase = 0	next_phase = 1	reward = -0.069593	array([[-1.7800051, -2.768421 ]], dtype=float32)

time = 35802	action = 0	current_phase = 0	next_phase = 1	reward = 0.023040	array([[-2.0816255, -2.8629124]], dtype=float32)

time = 35807	action = 0	current_phase = 0	next_phase = 1	reward = 0.083187	array([[-2.7220702, -2.9172883]], dtype=float32)

time = 35812	action = 1	current_phase = 0	next_phase = 1	reward = -1.596195	array([[-6.1079173, -3.3779736]], dtype=float32)

time = 35820	action = 1	current_phase = 1	next_phase = 0	reward = -0.904076	array([[-3.7964683, -2.2041044]], dtype=float32)

time = 35828	action = 0	current_phase = 0	next_phase = 1	reward = -0.066178	array([[-1.7398744, -2.6875663]], dtype=float32)

time = 35833	action = 0	current_phase = 0	next_phase = 1	reward = 0.003799	array([[-1.9882734, -2.8626971]], dtype=float32)

time = 35838	action = 0	current_phase = 0	next_phase = 1	reward = 0.069981	array([[-2.7827504, -2.8231952]], dtype=float32)

time = 35843	action = 1	current_phase = 0	next_phase = 1	reward = -1.899127	array([[-6.006958 , -3.5361888]], dtype=float32)

time = 35851	action = 1	current_phase = 1	next_phase = 0	reward = -1.338598	array([[-3.998743, -2.360199]], dtype=float32)

time = 35859	action = 0	current_phase = 0	next_phase = 1	reward = 0.281020	array([[-1.469281 , -2.3185515]], dtype=float32)

time = 35864	action = 0	current_phase = 0	next_phase = 1	reward = 0.339902	array([[-1.7765888, -2.7472174]], dtype=float32)

time = 35869	action = 1	current_phase = 0	next_phase = 1	reward = -0.912408	array([[-2.6089451, -2.220786 ]], dtype=float32)

time = 35877	action = 1	current_phase = 1	next_phase = 0	reward = -0.642455	array([[-3.2662838, -1.7997553]], dtype=float32)

time = 35885	action = 0	current_phase = 0	next_phase = 1	reward = -0.103394	array([[-1.452246 , -2.8030372]], dtype=float32)

time = 35890	action = 0	current_phase = 0	next_phase = 1	reward = -0.025320	array([[-1.831696 , -2.3588119]], dtype=float32)

time = 35895	action = 0	current_phase = 0	next_phase = 1	reward = 0.048594	array([[-2.2931843, -2.9293566]], dtype=float32)

time = 35900	action = 1	current_phase = 0	next_phase = 1	reward = -1.270343	array([[-5.0118213, -2.8639207]], dtype=float32)

time = 35908	action = 1	current_phase = 1	next_phase = 0	reward = -0.694732	array([[-3.5844276, -2.0014853]], dtype=float32)

time = 35916	action = 0	current_phase = 0	next_phase = 1	reward = -0.078985	array([[-1.7114358, -2.757657 ]], dtype=float32)

time = 35921	action = 0	current_phase = 0	next_phase = 1	reward = -0.000423	array([[-1.9391465, -2.7774496]], dtype=float32)

time = 35926	action = 0	current_phase = 0	next_phase = 1	reward = 0.065168	array([[-2.391025 , -2.9702678]], dtype=float32)

time = 35931	action = 1	current_phase = 0	next_phase = 1	reward = -1.438483	array([[-6.036245 , -3.0967116]], dtype=float32)

time = 35939	action = 1	current_phase = 1	next_phase = 0	reward = -0.772984	array([[-3.7140853, -2.0897436]], dtype=float32)

time = 35947	action = 0	current_phase = 0	next_phase = 1	reward = -0.079622	array([[-1.8129287, -2.787585 ]], dtype=float32)

time = 35952	action = 0	current_phase = 0	next_phase = 1	reward = 0.001652	array([[-2.148313 , -2.8842087]], dtype=float32)

time = 35957	action = 0	current_phase = 0	next_phase = 1	reward = 0.075877	array([[-2.8162014, -2.9409628]], dtype=float32)

time = 35962	action = 1	current_phase = 0	next_phase = 1	reward = -1.664323	array([[-6.121066 , -3.3865266]], dtype=float32)

time = 35970	action = 1	current_phase = 1	next_phase = 0	reward = -0.990605	array([[-3.8333673, -2.3041167]], dtype=float32)

time = 35978	action = 0	current_phase = 0	next_phase = 1	reward = -0.050226	array([[-1.7611791, -2.691375 ]], dtype=float32)

time = 35983	action = 0	current_phase = 0	next_phase = 1	reward = 0.020325	array([[-2.1423752, -2.9255614]], dtype=float32)

time = 35988	action = 0	current_phase = 0	next_phase = 1	reward = 0.072472	array([[-2.8693352, -2.8849907]], dtype=float32)

time = 35993	action = 1	current_phase = 0	next_phase = 1	reward = -1.880340	array([[-5.9297957, -3.7008092]], dtype=float32)

time = 36001	action = 1	current_phase = 1	next_phase = 0	reward = -1.099935	array([[-4.0262394, -2.3994663]], dtype=float32)

time = 36009	action = 0	current_phase = 0	next_phase = 1	reward = -0.032737	array([[-1.5038275, -2.3073955]], dtype=float32)

time = 36014	action = 0	current_phase = 0	next_phase = 1	reward = 0.043340	array([[-1.7224853, -2.740488 ]], dtype=float32)

time = 36019	action = 1	current_phase = 0	next_phase = 1	reward = -0.642477	array([[-2.4965336, -2.2507408]], dtype=float32)

time = 36027	action = 1	current_phase = 1	next_phase = 0	reward = -0.593879	array([[-3.2076612, -1.7458165]], dtype=float32)

time = 36035	action = 0	current_phase = 0	next_phase = 1	reward = -0.101831	array([[-1.4768372, -2.795132 ]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0280 - val_loss: 0.0153

Epoch 2/50

 - 3s - loss: 0.0268 - val_loss: 0.0198

Epoch 3/50

 - 3s - loss: 0.0273 - val_loss: 0.0128

Epoch 4/50

 - 3s - loss: 0.0242 - val_loss: 0.0128

Epoch 5/50

 - 3s - loss: 0.0237 - val_loss: 0.0186

Epoch 6/50

 - 3s - loss: 0.0260 - val_loss: 0.0133

Epoch 7/50

 - 3s - loss: 0.0250 - val_loss: 0.0109

Epoch 8/50

 - 3s - loss: 0.0257 - val_loss: 0.0136

Epoch 9/50

 - 3s - loss: 0.0249 - val_loss: 0.0216

Epoch 10/50

 - 3s - loss: 0.0261 - val_loss: 0.0140

Epoch 11/50

 - 3s - loss: 0.0245 - val_loss: 0.0173

Epoch 12/50

 - 3s - loss: 0.0218 - val_loss: 0.0178

Epoch 13/50

 - 3s - loss: 0.0216 - val_loss: 0.0259

Epoch 14/50

 - 3s - loss: 0.0246 - val_loss: 0.0186

Epoch 15/50

 - 3s - loss: 0.0230 - val_loss: 0.0125

Epoch 16/50

 - 3s - loss: 0.0264 - val_loss: 0.0154

Epoch 17/50

 - 3s - loss: 0.0232 - val_loss: 0.0169

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 36040	action = 0	current_phase = 0	next_phase = 1	reward = -0.028748	array([[-1.8536823, -2.3765707]], dtype=float32)

time = 36045	action = 0	current_phase = 0	next_phase = 1	reward = 0.037913	array([[-2.209321 , -2.9620667]], dtype=float32)

time = 36050	action = 1	current_phase = 0	next_phase = 1	reward = -1.240834	array([[-4.9314313, -2.9180746]], dtype=float32)

time = 36058	action = 1	current_phase = 1	next_phase = 0	reward = -0.711412	array([[-3.5905695, -2.178385 ]], dtype=float32)

time = 36066	action = 0	current_phase = 0	next_phase = 1	reward = -0.087832	array([[-1.6815927, -2.7840803]], dtype=float32)

time = 36071	action = 0	current_phase = 0	next_phase = 1	reward = -0.015864	array([[-1.9987509, -2.8260684]], dtype=float32)

time = 36076	action = 0	current_phase = 0	next_phase = 1	reward = 0.049950	array([[-2.45632  , -3.0007725]], dtype=float32)

time = 36081	action = 1	current_phase = 0	next_phase = 1	reward = -1.385126	array([[-6.100564 , -3.1863518]], dtype=float32)

time = 36089	action = 1	current_phase = 1	next_phase = 0	reward = -1.058253	array([[-3.652494, -2.186679]], dtype=float32)

time = 36097	action = 0	current_phase = 0	next_phase = 1	reward = 0.229925	array([[-1.7787341, -2.8016071]], dtype=float32)

time = 36102	action = 0	current_phase = 0	next_phase = 1	reward = 0.038439	array([[-2.0782542, -2.88634  ]], dtype=float32)

time = 36107	action = 0	current_phase = 0	next_phase = 1	reward = 0.087422	array([[-2.6484702, -3.0397286]], dtype=float32)

time = 36112	action = 1	current_phase = 0	next_phase = 1	reward = -1.736857	array([[-6.1122785, -3.646185 ]], dtype=float32)

time = 36120	action = 1	current_phase = 1	next_phase = 0	reward = -1.032179	array([[-3.7863073, -2.3763683]], dtype=float32)

time = 36128	action = 0	current_phase = 0	next_phase = 1	reward = -0.073631	array([[-1.7510238, -2.7601495]], dtype=float32)

time = 36133	action = 0	current_phase = 0	next_phase = 1	reward = 0.016028	array([[-2.0967417, -2.9699569]], dtype=float32)

time = 36138	action = 0	current_phase = 0	next_phase = 1	reward = 0.067272	array([[-2.5454762, -3.00052  ]], dtype=float32)

time = 36143	action = 1	current_phase = 0	next_phase = 1	reward = -1.191705	array([[-6.0494833, -3.474326 ]], dtype=float32)

time = 36151	action = 1	current_phase = 1	next_phase = 0	reward = -1.453544	array([[-3.83539  , -2.6722975]], dtype=float32)

time = 36159	action = 0	current_phase = 0	next_phase = 1	reward = 0.240956	array([[-1.4930604, -2.3433452]], dtype=float32)

time = 36164	action = 0	current_phase = 0	next_phase = 1	reward = 0.036556	array([[-1.7663667, -2.7845433]], dtype=float32)

time = 36169	action = 0	current_phase = 0	next_phase = 1	reward = 0.062528	array([[-2.3260837, -2.491363 ]], dtype=float32)

time = 36174	action = 1	current_phase = 0	next_phase = 1	reward = -0.990482	array([[-5.7640443, -2.919867 ]], dtype=float32)

time = 36182	action = 1	current_phase = 1	next_phase = 0	reward = -0.701905	array([[-3.5154734, -2.28989  ]], dtype=float32)

time = 36190	action = 0	current_phase = 0	next_phase = 1	reward = -0.308978	array([[-1.7142513, -2.3583627]], dtype=float32)

time = 36195	action = 0	current_phase = 0	next_phase = 1	reward = 0.045919	array([[-2.1039996, -2.9184115]], dtype=float32)

time = 36200	action = 1	current_phase = 0	next_phase = 1	reward = -1.074479	array([[-4.8821073, -2.8366523]], dtype=float32)

time = 36208	action = 1	current_phase = 1	next_phase = 0	reward = -0.717697	array([[-3.5985737, -2.14485  ]], dtype=float32)

time = 36216	action = 0	current_phase = 0	next_phase = 1	reward = -0.077403	array([[-1.6777415, -2.7772245]], dtype=float32)

time = 36221	action = 0	current_phase = 0	next_phase = 1	reward = -0.013702	array([[-2.0110738, -2.8514698]], dtype=float32)

time = 36226	action = 0	current_phase = 0	next_phase = 1	reward = 0.055546	array([[-2.3655605, -3.039792 ]], dtype=float32)

time = 36231	action = 1	current_phase = 0	next_phase = 1	reward = -1.566467	array([[-6.1249847, -3.3186986]], dtype=float32)

time = 36239	action = 1	current_phase = 1	next_phase = 0	reward = -0.843404	array([[-3.709468 , -2.2283425]], dtype=float32)

time = 36247	action = 0	current_phase = 0	next_phase = 1	reward = -0.059767	array([[-1.8187567, -2.8589282]], dtype=float32)

time = 36252	action = 0	current_phase = 0	next_phase = 1	reward = 0.016704	array([[-2.136899, -2.93945 ]], dtype=float32)

time = 36257	action = 0	current_phase = 0	next_phase = 1	reward = 0.082862	array([[-2.6164806, -3.0112057]], dtype=float32)

time = 36262	action = 1	current_phase = 0	next_phase = 1	reward = -1.663735	array([[-6.1375704, -3.5838866]], dtype=float32)

time = 36270	action = 1	current_phase = 1	next_phase = 0	reward = -0.960928	array([[-3.825107 , -2.4277313]], dtype=float32)

time = 36278	action = 0	current_phase = 0	next_phase = 1	reward = -0.050375	array([[-1.7970884, -2.774516 ]], dtype=float32)

time = 36283	action = 0	current_phase = 0	next_phase = 1	reward = 0.024346	array([[-2.1644542, -2.9700127]], dtype=float32)

time = 36288	action = 0	current_phase = 0	next_phase = 1	reward = 0.060157	array([[-2.6037443, -2.9907436]], dtype=float32)

time = 36293	action = 1	current_phase = 0	next_phase = 1	reward = -1.936775	array([[-5.962409, -3.878407]], dtype=float32)

time = 36301	action = 1	current_phase = 1	next_phase = 0	reward = -1.039254	array([[-3.9931536, -2.6402535]], dtype=float32)

time = 36309	action = 0	current_phase = 0	next_phase = 1	reward = -0.023562	array([[-1.5690906, -2.3639696]], dtype=float32)

time = 36314	action = 0	current_phase = 0	next_phase = 1	reward = 0.031890	array([[-1.725297 , -2.7703443]], dtype=float32)

time = 36319	action = 1	current_phase = 0	next_phase = 1	reward = -0.704390	array([[-2.4663675, -2.4647458]], dtype=float32)

time = 36327	action = 1	current_phase = 1	next_phase = 0	reward = -0.922248	array([[-3.3472834, -1.9261789]], dtype=float32)

time = 36335	action = 0	current_phase = 0	next_phase = 1	reward = 0.180477	array([[-1.5041449, -2.8016212]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0419 - val_loss: 0.0121

Epoch 2/50

 - 3s - loss: 0.0355 - val_loss: 0.0112

Epoch 3/50

 - 3s - loss: 0.0371 - val_loss: 0.0141

Epoch 4/50

 - 3s - loss: 0.0317 - val_loss: 0.0129

Epoch 5/50

 - 3s - loss: 0.0325 - val_loss: 0.0107

Epoch 6/50

 - 3s - loss: 0.0271 - val_loss: 0.0100

Epoch 7/50

 - 3s - loss: 0.0343 - val_loss: 0.0109

Epoch 8/50

 - 3s - loss: 0.0301 - val_loss: 0.0108

Epoch 9/50

 - 3s - loss: 0.0274 - val_loss: 0.0111

Epoch 10/50

 - 3s - loss: 0.0256 - val_loss: 0.0123

Epoch 11/50

 - 3s - loss: 0.0268 - val_loss: 0.0123

Epoch 12/50

 - 3s - loss: 0.0241 - val_loss: 0.0115

Epoch 13/50

 - 3s - loss: 0.0234 - val_loss: 0.0105

Epoch 14/50

 - 3s - loss: 0.0294 - val_loss: 0.0118

Epoch 15/50

 - 3s - loss: 0.0238 - val_loss: 0.0218

Epoch 16/50

 - 3s - loss: 0.0287 - val_loss: 0.0111

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 36340	action = 0	current_phase = 0	next_phase = 1	reward = -0.032453	array([[-1.831959 , -2.3763297]], dtype=float32)

time = 36345	action = 0	current_phase = 0	next_phase = 1	reward = -0.248736	array([[-2.3942928, -3.0011408]], dtype=float32)

time = 36350	action = 1	current_phase = 0	next_phase = 1	reward = -1.029831	array([[-4.979164 , -2.8445756]], dtype=float32)

time = 36358	action = 1	current_phase = 1	next_phase = 0	reward = -0.684421	array([[-3.653982 , -2.1403759]], dtype=float32)

time = 36366	action = 0	current_phase = 0	next_phase = 1	reward = -0.091152	array([[-1.676662 , -2.8060913]], dtype=float32)

time = 36371	action = 0	current_phase = 0	next_phase = 1	reward = -0.024293	array([[-1.9410496, -2.8332398]], dtype=float32)

time = 36376	action = 0	current_phase = 0	next_phase = 1	reward = 0.043546	array([[-2.5497813, -3.0267909]], dtype=float32)

time = 36381	action = 1	current_phase = 0	next_phase = 1	reward = -1.562793	array([[-6.085678 , -3.2091782]], dtype=float32)

time = 36389	action = 1	current_phase = 1	next_phase = 0	reward = -0.774565	array([[-3.669777 , -2.1987689]], dtype=float32)

time = 36397	action = 0	current_phase = 0	next_phase = 1	reward = -0.072469	array([[-1.7615075, -2.8305233]], dtype=float32)

time = 36402	action = 0	current_phase = 0	next_phase = 1	reward = 0.010268	array([[-2.085042 , -2.8986375]], dtype=float32)

time = 36407	action = 0	current_phase = 0	next_phase = 1	reward = 0.082000	array([[-2.7115622, -3.0286288]], dtype=float32)

time = 36412	action = 1	current_phase = 0	next_phase = 1	reward = -1.670053	array([[-6.1348953, -3.454379 ]], dtype=float32)

time = 36420	action = 1	current_phase = 1	next_phase = 0	reward = -0.966923	array([[-3.7872183, -2.3414202]], dtype=float32)

time = 36428	action = 0	current_phase = 0	next_phase = 1	reward = -0.068859	array([[-1.7513998, -2.7758927]], dtype=float32)

time = 36433	action = 0	current_phase = 0	next_phase = 1	reward = -0.006004	array([[-2.1493592, -2.9728818]], dtype=float32)

time = 36438	action = 0	current_phase = 0	next_phase = 1	reward = 0.069473	array([[-2.7134466, -3.0179224]], dtype=float32)

time = 36443	action = 1	current_phase = 0	next_phase = 1	reward = -1.707251	array([[-6.07335  , -3.7026432]], dtype=float32)

time = 36451	action = 1	current_phase = 1	next_phase = 0	reward = -1.336896	array([[-4.000437 , -2.5102823]], dtype=float32)

time = 36459	action = 0	current_phase = 0	next_phase = 1	reward = 0.262566	array([[-1.3224804, -2.3577118]], dtype=float32)

time = 36464	action = 0	current_phase = 0	next_phase = 1	reward = 0.028731	array([[-1.8214848, -2.8198714]], dtype=float32)

time = 36469	action = 0	current_phase = 0	next_phase = 1	reward = 0.067766	array([[-2.4109678, -2.572403 ]], dtype=float32)

time = 36474	action = 1	current_phase = 0	next_phase = 1	reward = -0.921728	array([[-5.9045334, -3.0514512]], dtype=float32)

time = 36482	action = 1	current_phase = 1	next_phase = 0	reward = -0.746298	array([[-3.7236335, -2.349235 ]], dtype=float32)

time = 36490	action = 0	current_phase = 0	next_phase = 1	reward = -0.032552	array([[-1.7223861, -2.3705812]], dtype=float32)

time = 36495	action = 0	current_phase = 0	next_phase = 1	reward = -0.244441	array([[-2.2087927, -2.9387565]], dtype=float32)

time = 36500	action = 1	current_phase = 0	next_phase = 1	reward = -1.035514	array([[-4.9186783, -2.860239 ]], dtype=float32)

time = 36508	action = 1	current_phase = 1	next_phase = 0	reward = -0.683352	array([[-3.4846444, -2.0221307]], dtype=float32)

time = 36516	action = 0	current_phase = 0	next_phase = 1	reward = -0.078495	array([[-1.7001626, -2.8043032]], dtype=float32)

time = 36521	action = 0	current_phase = 0	next_phase = 1	reward = -0.004873	array([[-1.9040565, -2.8394446]], dtype=float32)

time = 36526	action = 0	current_phase = 0	next_phase = 1	reward = 0.061521	array([[-2.5241997, -3.0599275]], dtype=float32)

time = 36531	action = 1	current_phase = 0	next_phase = 1	reward = -1.505384	array([[-6.1137514, -3.3217535]], dtype=float32)

time = 36539	action = 1	current_phase = 1	next_phase = 0	reward = -0.788393	array([[-3.7320485, -2.2358558]], dtype=float32)

time = 36547	action = 0	current_phase = 0	next_phase = 1	reward = -0.081182	array([[-1.7317358, -2.811444 ]], dtype=float32)

time = 36552	action = 0	current_phase = 0	next_phase = 1	reward = 0.020336	array([[-2.1194248, -2.9161222]], dtype=float32)

time = 36557	action = 0	current_phase = 0	next_phase = 1	reward = 0.093637	array([[-2.5960848, -3.0484114]], dtype=float32)

time = 36562	action = 1	current_phase = 0	next_phase = 1	reward = -1.652052	array([[-6.1640406, -3.5096304]], dtype=float32)

time = 36570	action = 1	current_phase = 1	next_phase = 0	reward = -0.954381	array([[-3.7987905, -2.396834 ]], dtype=float32)

time = 36578	action = 0	current_phase = 0	next_phase = 1	reward = -0.055733	array([[-1.6468811, -2.731781 ]], dtype=float32)

time = 36583	action = 0	current_phase = 0	next_phase = 1	reward = 0.039897	array([[-2.2243717, -2.9785006]], dtype=float32)

time = 36588	action = 0	current_phase = 0	next_phase = 1	reward = 0.078445	array([[-2.7565138, -3.0303655]], dtype=float32)

time = 36593	action = 1	current_phase = 0	next_phase = 1	reward = -1.899050	array([[-6.047497 , -3.8483775]], dtype=float32)

time = 36601	action = 1	current_phase = 1	next_phase = 0	reward = -0.926224	array([[-4.013919 , -2.4659543]], dtype=float32)

time = 36609	action = 0	current_phase = 0	next_phase = 1	reward = -0.034760	array([[-1.4705952, -2.3503869]], dtype=float32)

time = 36614	action = 0	current_phase = 0	next_phase = 1	reward = 0.030097	array([[-1.8421004, -2.8369505]], dtype=float32)

time = 36619	action = 1	current_phase = 0	next_phase = 1	reward = -0.626081	array([[-2.364477 , -2.3483047]], dtype=float32)

time = 36627	action = 1	current_phase = 1	next_phase = 0	reward = -0.880620	array([[-3.2868447, -1.9727578]], dtype=float32)

time = 36635	action = 0	current_phase = 0	next_phase = 1	reward = 0.188045	array([[-1.3775272, -2.911329 ]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0313 - val_loss: 0.0127

Epoch 2/50

 - 3s - loss: 0.0283 - val_loss: 0.0129

Epoch 3/50

 - 3s - loss: 0.0249 - val_loss: 0.0133

Epoch 4/50

 - 3s - loss: 0.0278 - val_loss: 0.0141

Epoch 5/50

 - 3s - loss: 0.0211 - val_loss: 0.0128

Epoch 6/50

 - 3s - loss: 0.0286 - val_loss: 0.0123

Epoch 7/50

 - 3s - loss: 0.0245 - val_loss: 0.0119

Epoch 8/50

 - 3s - loss: 0.0273 - val_loss: 0.0119

Epoch 9/50

 - 3s - loss: 0.0209 - val_loss: 0.0156

Epoch 10/50

 - 3s - loss: 0.0260 - val_loss: 0.0146

Epoch 11/50

 - 3s - loss: 0.0312 - val_loss: 0.0128

Epoch 12/50

 - 3s - loss: 0.0254 - val_loss: 0.0108

Epoch 13/50

 - 3s - loss: 0.0254 - val_loss: 0.0137

Epoch 14/50

 - 3s - loss: 0.0259 - val_loss: 0.0131

Epoch 15/50

 - 3s - loss: 0.0238 - val_loss: 0.0122

Epoch 16/50

 - 3s - loss: 0.0218 - val_loss: 0.0129

Epoch 17/50

 - 3s - loss: 0.0261 - val_loss: 0.0116

Epoch 18/50

 - 3s - loss: 0.0239 - val_loss: 0.0110

Epoch 19/50

 - 3s - loss: 0.0249 - val_loss: 0.0156

Epoch 20/50

 - 3s - loss: 0.0260 - val_loss: 0.0135

Epoch 21/50

 - 3s - loss: 0.0267 - val_loss: 0.0147

Epoch 22/50

 - 3s - loss: 0.0280 - val_loss: 0.0124

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 36640	action = 0	current_phase = 0	next_phase = 1	reward = -0.025010	array([[-1.7777946, -2.361649 ]], dtype=float32)

time = 36645	action = 0	current_phase = 0	next_phase = 1	reward = -0.245217	array([[-2.3882828, -2.980803 ]], dtype=float32)

time = 36650	action = 1	current_phase = 0	next_phase = 1	reward = -0.990717	array([[-4.9747753, -2.7509792]], dtype=float32)

time = 36658	action = 1	current_phase = 1	next_phase = 0	reward = -0.709820	array([[-3.6175508, -2.192883 ]], dtype=float32)

time = 36666	action = 0	current_phase = 0	next_phase = 1	reward = -0.082296	array([[-1.6473758, -2.7924886]], dtype=float32)

time = 36671	action = 0	current_phase = 0	next_phase = 1	reward = -0.002344	array([[-1.9399045, -2.8402088]], dtype=float32)

time = 36676	action = 0	current_phase = 0	next_phase = 1	reward = 0.060925	array([[-2.4953356, -3.0391955]], dtype=float32)

time = 36681	action = 1	current_phase = 0	next_phase = 1	reward = -1.494998	array([[-6.129881 , -3.1713517]], dtype=float32)

time = 36689	action = 1	current_phase = 1	next_phase = 0	reward = -0.893301	array([[-3.772943 , -2.3100963]], dtype=float32)

time = 36697	action = 0	current_phase = 0	next_phase = 1	reward = -0.060049	array([[-1.7561542, -2.848345 ]], dtype=float32)

time = 36702	action = 0	current_phase = 0	next_phase = 1	reward = 0.009175	array([[-2.1211133, -2.918059 ]], dtype=float32)

time = 36707	action = 0	current_phase = 0	next_phase = 1	reward = 0.080445	array([[-2.8711567, -2.9758806]], dtype=float32)

time = 36712	action = 1	current_phase = 0	next_phase = 1	reward = -1.613827	array([[-6.1482334, -3.5489514]], dtype=float32)

time = 36720	action = 1	current_phase = 1	next_phase = 0	reward = -0.909498	array([[-3.8385034, -2.3876035]], dtype=float32)

time = 36728	action = 0	current_phase = 0	next_phase = 1	reward = -0.061338	array([[-1.7123095, -2.7741048]], dtype=float32)

time = 36733	action = 0	current_phase = 0	next_phase = 1	reward = 0.010169	array([[-2.1630023, -2.950987 ]], dtype=float32)

time = 36738	action = 0	current_phase = 0	next_phase = 1	reward = 0.074542	array([[-2.9036243, -2.9740906]], dtype=float32)

time = 36743	action = 1	current_phase = 0	next_phase = 1	reward = -1.858200	array([[-6.0347137, -3.8801818]], dtype=float32)

time = 36751	action = 1	current_phase = 1	next_phase = 0	reward = -1.089317	array([[-4.0384083, -2.4765496]], dtype=float32)

time = 36759	action = 0	current_phase = 0	next_phase = 1	reward = -0.042340	array([[-1.3128991, -2.3283951]], dtype=float32)

time = 36764	action = 0	current_phase = 0	next_phase = 1	reward = 0.019426	array([[-1.8578092, -2.8263514]], dtype=float32)

time = 36769	action = 1	current_phase = 0	next_phase = 1	reward = -0.757740	array([[-2.5686593, -2.213255 ]], dtype=float32)

time = 36777	action = 1	current_phase = 1	next_phase = 0	reward = -0.928555	array([[-3.6169543, -2.0760002]], dtype=float32)

time = 36785	action = 0	current_phase = 0	next_phase = 1	reward = 0.186958	array([[-1.3568261, -2.847037 ]], dtype=float32)

time = 36790	action = 0	current_phase = 0	next_phase = 1	reward = -0.022248	array([[-1.7802738, -2.361944 ]], dtype=float32)

time = 36795	action = 0	current_phase = 0	next_phase = 1	reward = -0.232176	array([[-2.3597882, -2.9690433]], dtype=float32)

time = 36800	action = 1	current_phase = 0	next_phase = 1	reward = -1.040850	array([[-5.0102158, -2.780611 ]], dtype=float32)

time = 36808	action = 1	current_phase = 1	next_phase = 0	reward = -0.716250	array([[-3.6687949, -2.1805348]], dtype=float32)

time = 36816	action = 0	current_phase = 0	next_phase = 1	reward = -0.088498	array([[-1.6708032, -2.7987869]], dtype=float32)

time = 36821	action = 0	current_phase = 0	next_phase = 1	reward = -0.006053	array([[-1.9605284, -2.8469896]], dtype=float32)

time = 36826	action = 0	current_phase = 0	next_phase = 1	reward = 0.052524	array([[-2.4441786, -2.993917 ]], dtype=float32)

time = 36831	action = 1	current_phase = 0	next_phase = 1	reward = -1.398088	array([[-6.131138 , -3.2344992]], dtype=float32)

time = 36839	action = 1	current_phase = 1	next_phase = 0	reward = -0.825446	array([[-3.7796679, -2.3291972]], dtype=float32)

time = 36847	action = 0	current_phase = 0	next_phase = 1	reward = -0.059058	array([[-1.7864233, -2.8435755]], dtype=float32)

time = 36852	action = 0	current_phase = 0	next_phase = 1	reward = 0.019987	array([[-2.2381816, -2.9763658]], dtype=float32)

time = 36857	action = 0	current_phase = 0	next_phase = 1	reward = 0.066079	array([[-2.729355, -3.007865]], dtype=float32)

time = 36862	action = 1	current_phase = 0	next_phase = 1	reward = -1.613194	array([[-6.1432323, -3.5204687]], dtype=float32)

time = 36870	action = 1	current_phase = 1	next_phase = 0	reward = -0.954555	array([[-3.8255787, -2.4158669]], dtype=float32)

time = 36878	action = 0	current_phase = 0	next_phase = 1	reward = -0.063810	array([[-1.8284638, -2.8213546]], dtype=float32)

time = 36883	action = 0	current_phase = 0	next_phase = 1	reward = 0.028411	array([[-2.1530678, -2.9573624]], dtype=float32)

time = 36888	action = 0	current_phase = 0	next_phase = 1	reward = 0.088656	array([[-2.8293757, -3.0012052]], dtype=float32)

time = 36893	action = 1	current_phase = 0	next_phase = 1	reward = -1.940872	array([[-6.0602207, -3.8322005]], dtype=float32)

time = 36901	action = 1	current_phase = 1	next_phase = 0	reward = -1.036771	array([[-4.0657043, -2.519064 ]], dtype=float32)

time = 36909	action = 0	current_phase = 0	next_phase = 1	reward = -0.031561	array([[-1.415223, -2.396887]], dtype=float32)

time = 36914	action = 0	current_phase = 0	next_phase = 1	reward = 0.049985	array([[-1.862141 , -2.8292975]], dtype=float32)

time = 36919	action = 1	current_phase = 0	next_phase = 1	reward = -1.176496	array([[-2.3841524, -2.2742329]], dtype=float32)

time = 36927	action = 1	current_phase = 1	next_phase = 0	reward = -0.758799	array([[-3.772045 , -2.1774015]], dtype=float32)

time = 36935	action = 0	current_phase = 0	next_phase = 1	reward = -0.093268	array([[-1.4175625, -2.819256 ]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0239 - val_loss: 0.0109

Epoch 2/50

 - 3s - loss: 0.0239 - val_loss: 0.0129

Epoch 3/50

 - 3s - loss: 0.0256 - val_loss: 0.0113

Epoch 4/50

 - 3s - loss: 0.0266 - val_loss: 0.0118

Epoch 5/50

 - 3s - loss: 0.0237 - val_loss: 0.0130

Epoch 6/50

 - 3s - loss: 0.0259 - val_loss: 0.0107

Epoch 7/50

 - 3s - loss: 0.0238 - val_loss: 0.0128

Epoch 8/50

 - 3s - loss: 0.0248 - val_loss: 0.0112

Epoch 9/50

 - 3s - loss: 0.0240 - val_loss: 0.0110

Epoch 10/50

 - 3s - loss: 0.0248 - val_loss: 0.0144

Epoch 11/50

 - 3s - loss: 0.0221 - val_loss: 0.0131

Epoch 12/50

 - 3s - loss: 0.0263 - val_loss: 0.0157

Epoch 13/50

 - 3s - loss: 0.0253 - val_loss: 0.0129

Epoch 14/50

 - 3s - loss: 0.0238 - val_loss: 0.0199

Epoch 15/50

 - 3s - loss: 0.0245 - val_loss: 0.0119

Epoch 16/50

 - 3s - loss: 0.0210 - val_loss: 0.0128

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 36940	action = 0	current_phase = 0	next_phase = 1	reward = 0.252981	array([[-1.7579823, -2.3402433]], dtype=float32)

time = 36945	action = 0	current_phase = 0	next_phase = 1	reward = -0.242155	array([[-2.4655  , -3.025395]], dtype=float32)

time = 36950	action = 1	current_phase = 0	next_phase = 1	reward = -1.150388	array([[-5.1727934, -2.8928614]], dtype=float32)

time = 36958	action = 1	current_phase = 1	next_phase = 0	reward = -0.708650	array([[-3.6383595, -2.099382 ]], dtype=float32)

time = 36966	action = 0	current_phase = 0	next_phase = 1	reward = -0.078604	array([[-1.7614099, -2.8247309]], dtype=float32)

time = 36971	action = 0	current_phase = 0	next_phase = 1	reward = -0.015867	array([[-1.9916177, -2.8772998]], dtype=float32)

time = 36976	action = 0	current_phase = 0	next_phase = 1	reward = 0.059458	array([[-2.5618916, -3.024267 ]], dtype=float32)

time = 36981	action = 1	current_phase = 0	next_phase = 1	reward = -1.490449	array([[-6.224782 , -3.3033862]], dtype=float32)

time = 36989	action = 1	current_phase = 1	next_phase = 0	reward = -0.763304	array([[-3.7346268, -2.1968796]], dtype=float32)

time = 36997	action = 0	current_phase = 0	next_phase = 1	reward = -0.050127	array([[-1.7751855, -2.8386936]], dtype=float32)

time = 37002	action = 0	current_phase = 0	next_phase = 1	reward = 0.023425	array([[-2.140576 , -2.9271123]], dtype=float32)

time = 37007	action = 0	current_phase = 0	next_phase = 1	reward = 0.080384	array([[-2.8370397, -3.0156512]], dtype=float32)

time = 37012	action = 1	current_phase = 0	next_phase = 1	reward = -1.672049	array([[-6.288394 , -3.5382988]], dtype=float32)

time = 37020	action = 1	current_phase = 1	next_phase = 0	reward = -1.256028	array([[-3.817484 , -2.3605669]], dtype=float32)

time = 37028	action = 0	current_phase = 0	next_phase = 1	reward = 0.227509	array([[-1.6905644, -2.7857075]], dtype=float32)

time = 37033	action = 0	current_phase = 0	next_phase = 1	reward = 0.017791	array([[-2.1086848, -2.9467132]], dtype=float32)

time = 37038	action = 0	current_phase = 0	next_phase = 1	reward = 0.081741	array([[-2.7724695, -2.9786038]], dtype=float32)

time = 37043	action = 1	current_phase = 0	next_phase = 1	reward = -1.909085	array([[-6.154298 , -3.6359348]], dtype=float32)

time = 37051	action = 1	current_phase = 1	next_phase = 0	reward = -1.106879	array([[-4.050112, -2.480442]], dtype=float32)

time = 37059	action = 0	current_phase = 0	next_phase = 1	reward = -0.038302	array([[-1.3476608, -2.3149705]], dtype=float32)

time = 37064	action = 0	current_phase = 0	next_phase = 1	reward = 0.045063	array([[-1.9555798, -2.868033 ]], dtype=float32)

time = 37069	action = 0	current_phase = 0	next_phase = 1	reward = 0.068150	array([[-2.3850968, -2.3977573]], dtype=float32)

time = 37074	action = 1	current_phase = 0	next_phase = 1	reward = -1.058902	array([[-5.948288, -2.969902]], dtype=float32)

time = 37082	action = 1	current_phase = 1	next_phase = 0	reward = -0.711420	array([[-3.790296 , -2.3344865]], dtype=float32)

time = 37090	action = 0	current_phase = 0	next_phase = 1	reward = -0.303213	array([[-1.7541604, -2.3212092]], dtype=float32)

time = 37095	action = 0	current_phase = 0	next_phase = 1	reward = 0.333029	array([[-2.1487994, -2.9155579]], dtype=float32)

time = 37100	action = 1	current_phase = 0	next_phase = 1	reward = -1.316253	array([[-5.076582 , -2.9525962]], dtype=float32)

time = 37108	action = 1	current_phase = 1	next_phase = 0	reward = -0.710369	array([[-3.5211477, -2.0303674]], dtype=float32)

time = 37116	action = 0	current_phase = 0	next_phase = 1	reward = -0.082971	array([[-1.7450535, -2.8204489]], dtype=float32)

time = 37121	action = 0	current_phase = 0	next_phase = 1	reward = 0.002109	array([[-1.8751761, -2.8382738]], dtype=float32)

time = 37126	action = 0	current_phase = 0	next_phase = 1	reward = 0.065112	array([[-2.594009 , -3.0354166]], dtype=float32)

time = 37131	action = 1	current_phase = 0	next_phase = 1	reward = -1.297656	array([[-6.213991 , -3.3529992]], dtype=float32)

time = 37139	action = 1	current_phase = 1	next_phase = 0	reward = -0.826357	array([[-3.6849337, -2.181179 ]], dtype=float32)

time = 37147	action = 0	current_phase = 0	next_phase = 1	reward = -0.067839	array([[-1.7493613, -2.8227296]], dtype=float32)

time = 37152	action = 0	current_phase = 0	next_phase = 1	reward = -0.002035	array([[-2.096539 , -2.9084945]], dtype=float32)

time = 37157	action = 0	current_phase = 0	next_phase = 1	reward = 0.062236	array([[-2.756351 , -3.0432048]], dtype=float32)

time = 37162	action = 1	current_phase = 0	next_phase = 1	reward = -1.538829	array([[-6.255172 , -3.5489864]], dtype=float32)

time = 37170	action = 1	current_phase = 1	next_phase = 0	reward = -0.946102	array([[-3.8483953, -2.3660645]], dtype=float32)

time = 37178	action = 0	current_phase = 0	next_phase = 1	reward = -0.053422	array([[-1.7920988, -2.81521  ]], dtype=float32)

time = 37183	action = 0	current_phase = 0	next_phase = 1	reward = 0.009235	array([[-2.1246982, -2.9467132]], dtype=float32)

time = 37188	action = 0	current_phase = 0	next_phase = 1	reward = 0.077114	array([[-2.912996 , -3.0207949]], dtype=float32)

time = 37193	action = 1	current_phase = 0	next_phase = 1	reward = -1.936027	array([[-6.147846, -3.764996]], dtype=float32)

time = 37201	action = 1	current_phase = 1	next_phase = 0	reward = -1.385604	array([[-4.02948  , -2.4639378]], dtype=float32)

time = 37209	action = 0	current_phase = 0	next_phase = 1	reward = 0.256108	array([[-1.3385776, -2.2955577]], dtype=float32)

time = 37214	action = 0	current_phase = 0	next_phase = 1	reward = 0.019973	array([[-1.8618468, -2.83527  ]], dtype=float32)

time = 37219	action = 1	current_phase = 0	next_phase = 1	reward = -0.647566	array([[-2.4286613, -2.2760966]], dtype=float32)

time = 37227	action = 1	current_phase = 1	next_phase = 0	reward = -1.134184	array([[-3.408298 , -1.9783177]], dtype=float32)

time = 37235	action = 0	current_phase = 0	next_phase = 1	reward = 0.470178	array([[-1.3972783, -2.9081972]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0225 - val_loss: 0.0124

Epoch 2/50

 - 3s - loss: 0.0235 - val_loss: 0.0163

Epoch 3/50

 - 3s - loss: 0.0204 - val_loss: 0.0135

Epoch 4/50

 - 3s - loss: 0.0213 - val_loss: 0.0132

Epoch 5/50

 - 3s - loss: 0.0256 - val_loss: 0.0121

Epoch 6/50

 - 3s - loss: 0.0218 - val_loss: 0.0169

Epoch 7/50

 - 3s - loss: 0.0257 - val_loss: 0.0175

Epoch 8/50

 - 3s - loss: 0.0224 - val_loss: 0.0136

Epoch 9/50

 - 3s - loss: 0.0256 - val_loss: 0.0165

Epoch 10/50

 - 3s - loss: 0.0214 - val_loss: 0.0146

Epoch 11/50

 - 3s - loss: 0.0215 - val_loss: 0.0141

Epoch 12/50

 - 3s - loss: 0.0193 - val_loss: 0.0160

Epoch 13/50

 - 3s - loss: 0.0189 - val_loss: 0.0159

Epoch 14/50

 - 3s - loss: 0.0195 - val_loss: 0.0150

Epoch 15/50

 - 3s - loss: 0.0182 - val_loss: 0.0158

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 37240	action = 0	current_phase = 0	next_phase = 1	reward = -0.018737	array([[-1.842451 , -2.3797112]], dtype=float32)

time = 37245	action = 0	current_phase = 0	next_phase = 1	reward = -0.226713	array([[-2.6870768, -2.9906979]], dtype=float32)

time = 37250	action = 1	current_phase = 0	next_phase = 1	reward = -1.103109	array([[-5.1814165, -2.7790208]], dtype=float32)

time = 37258	action = 1	current_phase = 1	next_phase = 0	reward = -0.716723	array([[-3.606782 , -2.1213412]], dtype=float32)

time = 37266	action = 0	current_phase = 0	next_phase = 1	reward = -0.079462	array([[-1.7209697, -2.8103995]], dtype=float32)

time = 37271	action = 0	current_phase = 0	next_phase = 1	reward = -0.003941	array([[-1.9862031, -2.8591423]], dtype=float32)

time = 37276	action = 0	current_phase = 0	next_phase = 1	reward = 0.080438	array([[-2.590448 , -3.0555675]], dtype=float32)

time = 37281	action = 1	current_phase = 0	next_phase = 1	reward = -1.443088	array([[-6.312084 , -3.2507358]], dtype=float32)

time = 37289	action = 1	current_phase = 1	next_phase = 0	reward = -0.785770	array([[-3.7100687, -2.2668686]], dtype=float32)

time = 37297	action = 0	current_phase = 0	next_phase = 1	reward = -0.073857	array([[-1.8144476, -2.8471096]], dtype=float32)

time = 37302	action = 0	current_phase = 0	next_phase = 1	reward = 0.004973	array([[-2.08253  , -2.9004917]], dtype=float32)

time = 37307	action = 0	current_phase = 0	next_phase = 1	reward = 0.063946	array([[-2.7780402, -2.9786963]], dtype=float32)

time = 37312	action = 1	current_phase = 0	next_phase = 1	reward = -1.540929	array([[-6.309244, -3.493235]], dtype=float32)

time = 37320	action = 1	current_phase = 1	next_phase = 0	reward = -1.358734	array([[-3.821103 , -2.4326034]], dtype=float32)

time = 37328	action = 0	current_phase = 0	next_phase = 1	reward = 0.232589	array([[-1.7531196, -2.7982783]], dtype=float32)

time = 37333	action = 0	current_phase = 0	next_phase = 1	reward = 0.030487	array([[-2.1392157, -2.9619381]], dtype=float32)

time = 37338	action = 0	current_phase = 0	next_phase = 1	reward = 0.071034	array([[-2.9229014, -3.074387 ]], dtype=float32)

time = 37343	action = 1	current_phase = 0	next_phase = 1	reward = -1.941107	array([[-6.135186 , -3.9686654]], dtype=float32)

time = 37351	action = 1	current_phase = 1	next_phase = 0	reward = -1.341712	array([[-4.0103188, -2.449304 ]], dtype=float32)

time = 37359	action = 0	current_phase = 0	next_phase = 1	reward = 0.263954	array([[-1.4364123, -2.3085458]], dtype=float32)

time = 37364	action = 0	current_phase = 0	next_phase = 1	reward = 0.038558	array([[-1.8444738, -2.8213372]], dtype=float32)

time = 37369	action = 1	current_phase = 0	next_phase = 1	reward = -0.744350	array([[-2.3966408, -2.3314064]], dtype=float32)

time = 37377	action = 1	current_phase = 1	next_phase = 0	reward = -0.659864	array([[-3.4583268, -1.9572731]], dtype=float32)

time = 37385	action = 0	current_phase = 0	next_phase = 1	reward = -0.388098	array([[-1.5039706, -2.8381078]], dtype=float32)

time = 37390	action = 0	current_phase = 0	next_phase = 1	reward = 0.254094	array([[-1.733379 , -2.3515828]], dtype=float32)

time = 37395	action = 0	current_phase = 0	next_phase = 1	reward = -0.232588	array([[-2.3898907, -2.9843633]], dtype=float32)

time = 37400	action = 1	current_phase = 0	next_phase = 1	reward = -1.025864	array([[-5.1399336, -2.7639198]], dtype=float32)

time = 37408	action = 1	current_phase = 1	next_phase = 0	reward = -0.696800	array([[-3.5995116, -2.1239648]], dtype=float32)

time = 37416	action = 0	current_phase = 0	next_phase = 1	reward = -0.077697	array([[-1.7651219, -2.8153055]], dtype=float32)

time = 37421	action = 0	current_phase = 0	next_phase = 1	reward = 0.012157	array([[-1.9114969, -2.8482475]], dtype=float32)

time = 37426	action = 0	current_phase = 0	next_phase = 1	reward = 0.058805	array([[-2.5942342, -3.0473375]], dtype=float32)

time = 37431	action = 1	current_phase = 0	next_phase = 1	reward = -1.510794	array([[-6.3292303, -3.2759721]], dtype=float32)

time = 37439	action = 1	current_phase = 1	next_phase = 0	reward = -0.893603	array([[-3.7023463, -2.2510793]], dtype=float32)

time = 37447	action = 0	current_phase = 0	next_phase = 1	reward = -0.047735	array([[-1.8038018, -2.8468268]], dtype=float32)

time = 37452	action = 0	current_phase = 0	next_phase = 1	reward = 0.026900	array([[-2.1248345, -2.9365387]], dtype=float32)

time = 37457	action = 0	current_phase = 0	next_phase = 1	reward = 0.081427	array([[-2.9636083, -2.9825623]], dtype=float32)

time = 37462	action = 1	current_phase = 0	next_phase = 1	reward = -1.517891	array([[-6.320813 , -3.5256355]], dtype=float32)

time = 37470	action = 1	current_phase = 1	next_phase = 0	reward = -1.299240	array([[-3.8089323, -2.4537184]], dtype=float32)

time = 37478	action = 0	current_phase = 0	next_phase = 1	reward = 0.239359	array([[-1.5990324, -2.736188 ]], dtype=float32)

time = 37483	action = 0	current_phase = 0	next_phase = 1	reward = 0.036038	array([[-2.1941814, -2.9790514]], dtype=float32)

time = 37488	action = 0	current_phase = 0	next_phase = 1	reward = 0.089050	array([[-2.8952847, -2.9707835]], dtype=float32)

time = 37493	action = 1	current_phase = 0	next_phase = 1	reward = -1.326582	array([[-6.2134886, -3.537517 ]], dtype=float32)

time = 37501	action = 1	current_phase = 1	next_phase = 0	reward = -1.240704	array([[-3.880083 , -2.4312348]], dtype=float32)

time = 37509	action = 0	current_phase = 0	next_phase = 1	reward = -0.032629	array([[-1.4088893, -2.3048437]], dtype=float32)

time = 37514	action = 0	current_phase = 0	next_phase = 1	reward = 0.044514	array([[-1.8978372, -2.8369038]], dtype=float32)

time = 37519	action = 1	current_phase = 0	next_phase = 1	reward = -0.750837	array([[-2.4626799, -2.325314 ]], dtype=float32)

time = 37527	action = 1	current_phase = 1	next_phase = 0	reward = -0.929467	array([[-3.5500197, -2.013043 ]], dtype=float32)

time = 37535	action = 0	current_phase = 0	next_phase = 1	reward = -0.108500	array([[-1.4447974, -2.8875368]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0263 - val_loss: 0.0135

Epoch 2/50

 - 3s - loss: 0.0195 - val_loss: 0.0146

Epoch 3/50

 - 3s - loss: 0.0277 - val_loss: 0.0177

Epoch 4/50

 - 3s - loss: 0.0199 - val_loss: 0.0160

Epoch 5/50

 - 3s - loss: 0.0225 - val_loss: 0.0164

Epoch 6/50

 - 3s - loss: 0.0180 - val_loss: 0.0139

Epoch 7/50

 - 3s - loss: 0.0228 - val_loss: 0.0137

Epoch 8/50

 - 3s - loss: 0.0199 - val_loss: 0.0134

Epoch 9/50

 - 3s - loss: 0.0240 - val_loss: 0.0149

Epoch 10/50

 - 3s - loss: 0.0272 - val_loss: 0.0136

Epoch 11/50

 - 3s - loss: 0.0207 - val_loss: 0.0169

Epoch 12/50

 - 3s - loss: 0.0219 - val_loss: 0.0137

Epoch 13/50

 - 3s - loss: 0.0254 - val_loss: 0.0146

Epoch 14/50

 - 3s - loss: 0.0169 - val_loss: 0.0170

Epoch 15/50

 - 3s - loss: 0.0200 - val_loss: 0.0140

Epoch 16/50

 - 3s - loss: 0.0188 - val_loss: 0.0147

Epoch 17/50

 - 3s - loss: 0.0191 - val_loss: 0.0174

Epoch 18/50

 - 3s - loss: 0.0177 - val_loss: 0.0173

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 37540	action = 0	current_phase = 0	next_phase = 1	reward = -0.011186	array([[-1.7078503, -2.325407 ]], dtype=float32)

time = 37545	action = 0	current_phase = 0	next_phase = 1	reward = 0.340043	array([[-2.4534492, -3.0069528]], dtype=float32)

time = 37550	action = 1	current_phase = 0	next_phase = 1	reward = -1.384339	array([[-5.2070494, -2.923603 ]], dtype=float32)

time = 37558	action = 1	current_phase = 1	next_phase = 0	reward = -0.705690	array([[-3.5453625, -2.121671 ]], dtype=float32)

time = 37566	action = 0	current_phase = 0	next_phase = 1	reward = -0.078004	array([[-1.6640761, -2.8039992]], dtype=float32)

time = 37571	action = 0	current_phase = 0	next_phase = 1	reward = -0.005285	array([[-1.9178457, -2.8331914]], dtype=float32)

time = 37576	action = 0	current_phase = 0	next_phase = 1	reward = 0.066362	array([[-2.5881019, -3.0553684]], dtype=float32)

time = 37581	action = 1	current_phase = 0	next_phase = 1	reward = -1.499503	array([[-6.407833 , -3.2656035]], dtype=float32)

time = 37589	action = 1	current_phase = 1	next_phase = 0	reward = -0.786977	array([[-3.6616497, -2.2262125]], dtype=float32)

time = 37597	action = 0	current_phase = 0	next_phase = 1	reward = -0.067992	array([[-1.7075438, -2.8163273]], dtype=float32)

time = 37602	action = 0	current_phase = 0	next_phase = 1	reward = 0.002500	array([[-2.0373914, -2.8989263]], dtype=float32)

time = 37607	action = 0	current_phase = 0	next_phase = 1	reward = 0.061696	array([[-2.782989 , -3.0287843]], dtype=float32)

time = 37612	action = 1	current_phase = 0	next_phase = 1	reward = -1.687705	array([[-6.3379564, -3.5658631]], dtype=float32)

time = 37620	action = 1	current_phase = 1	next_phase = 0	reward = -0.953560	array([[-3.8216796, -2.4411118]], dtype=float32)

time = 37628	action = 0	current_phase = 0	next_phase = 1	reward = -0.068581	array([[-1.7627392, -2.8099575]], dtype=float32)

time = 37633	action = 0	current_phase = 0	next_phase = 1	reward = -0.002250	array([[-2.1369123, -2.9845228]], dtype=float32)

time = 37638	action = 0	current_phase = 0	next_phase = 1	reward = 0.065102	array([[-2.8560438, -3.0363436]], dtype=float32)

time = 37643	action = 1	current_phase = 0	next_phase = 1	reward = -1.788600	array([[-6.282193, -3.70714 ]], dtype=float32)

time = 37651	action = 1	current_phase = 1	next_phase = 0	reward = -1.340312	array([[-4.0117583, -2.5023046]], dtype=float32)

time = 37659	action = 0	current_phase = 0	next_phase = 1	reward = 0.254421	array([[-1.280959 , -2.2933326]], dtype=float32)

time = 37664	action = 0	current_phase = 0	next_phase = 1	reward = 0.025448	array([[-1.8570986, -2.838008 ]], dtype=float32)

time = 37669	action = 1	current_phase = 0	next_phase = 1	reward = -0.648261	array([[-2.4965625, -2.369441 ]], dtype=float32)

time = 37677	action = 1	current_phase = 1	next_phase = 0	reward = -1.196038	array([[-3.1769996, -1.8961473]], dtype=float32)

time = 37685	action = 0	current_phase = 0	next_phase = 1	reward = 0.452740	array([[-1.4113781, -2.909256 ]], dtype=float32)

time = 37690	action = 0	current_phase = 0	next_phase = 1	reward = -0.041052	array([[-1.8038065, -2.3245134]], dtype=float32)

time = 37695	action = 0	current_phase = 0	next_phase = 1	reward = 0.021448	array([[-2.409295, -2.997911]], dtype=float32)

time = 37700	action = 1	current_phase = 0	next_phase = 1	reward = -1.198356	array([[-5.184514, -2.763207]], dtype=float32)

time = 37708	action = 1	current_phase = 1	next_phase = 0	reward = -0.695784	array([[-3.4148498, -2.0777621]], dtype=float32)

time = 37716	action = 0	current_phase = 0	next_phase = 1	reward = -0.080206	array([[-1.678426 , -2.8100715]], dtype=float32)

time = 37721	action = 0	current_phase = 0	next_phase = 1	reward = -0.001140	array([[-1.9097066, -2.8337545]], dtype=float32)

time = 37726	action = 0	current_phase = 0	next_phase = 1	reward = 0.071071	array([[-2.5603247, -3.043126 ]], dtype=float32)

time = 37731	action = 1	current_phase = 0	next_phase = 1	reward = -1.444762	array([[-6.3656845, -3.2078724]], dtype=float32)

time = 37739	action = 1	current_phase = 1	next_phase = 0	reward = -0.830047	array([[-3.6876988, -2.2607477]], dtype=float32)

time = 37747	action = 0	current_phase = 0	next_phase = 1	reward = -0.081137	array([[-1.7027967, -2.817481 ]], dtype=float32)

time = 37752	action = 0	current_phase = 0	next_phase = 1	reward = -0.007038	array([[-2.0576591, -2.890283 ]], dtype=float32)

time = 37757	action = 0	current_phase = 0	next_phase = 1	reward = 0.053734	array([[-2.8004603, -3.027431 ]], dtype=float32)

time = 37762	action = 1	current_phase = 0	next_phase = 1	reward = -1.586806	array([[-6.3839703, -3.5482798]], dtype=float32)

time = 37770	action = 1	current_phase = 1	next_phase = 0	reward = -0.839864	array([[-3.7883162, -2.4368916]], dtype=float32)

time = 37778	action = 0	current_phase = 0	next_phase = 1	reward = -0.052254	array([[-1.7189581, -2.776308 ]], dtype=float32)

time = 37783	action = 0	current_phase = 0	next_phase = 1	reward = 0.021515	array([[-2.1364274, -2.9721828]], dtype=float32)

time = 37788	action = 0	current_phase = 0	next_phase = 1	reward = 0.078681	array([[-2.9256928, -2.9950633]], dtype=float32)

time = 37793	action = 1	current_phase = 0	next_phase = 1	reward = -1.272209	array([[-6.23975  , -3.6729126]], dtype=float32)

time = 37801	action = 1	current_phase = 1	next_phase = 0	reward = -1.435746	array([[-3.8021302, -2.445623 ]], dtype=float32)

time = 37809	action = 0	current_phase = 0	next_phase = 1	reward = 0.261475	array([[-1.383178 , -2.2960987]], dtype=float32)

time = 37814	action = 0	current_phase = 0	next_phase = 1	reward = 0.038518	array([[-1.8161948, -2.8472438]], dtype=float32)

time = 37819	action = 0	current_phase = 0	next_phase = 1	reward = 0.060262	array([[-2.2808738, -2.4300923]], dtype=float32)

time = 37824	action = 1	current_phase = 0	next_phase = 1	reward = -1.180085	array([[-6.1716633, -3.051147 ]], dtype=float32)

time = 37832	action = 1	current_phase = 1	next_phase = 0	reward = -1.039605	array([[-3.678022, -2.300797]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0320 - val_loss: 0.0155

Epoch 2/50

 - 3s - loss: 0.0259 - val_loss: 0.0154

Epoch 3/50

 - 3s - loss: 0.0225 - val_loss: 0.0150

Epoch 4/50

 - 3s - loss: 0.0270 - val_loss: 0.0161

Epoch 5/50

 - 3s - loss: 0.0221 - val_loss: 0.0127

Epoch 6/50

 - 3s - loss: 0.0202 - val_loss: 0.0187

Epoch 7/50

 - 3s - loss: 0.0208 - val_loss: 0.0152

Epoch 8/50

 - 3s - loss: 0.0230 - val_loss: 0.0166

Epoch 9/50

 - 3s - loss: 0.0217 - val_loss: 0.0226

Epoch 10/50

 - 3s - loss: 0.0193 - val_loss: 0.0156

Epoch 11/50

 - 3s - loss: 0.0263 - val_loss: 0.0160

Epoch 12/50

 - 3s - loss: 0.0206 - val_loss: 0.0256

Epoch 13/50

 - 3s - loss: 0.0177 - val_loss: 0.0172

Epoch 14/50

 - 3s - loss: 0.0247 - val_loss: 0.0176

Epoch 15/50

 - 3s - loss: 0.0198 - val_loss: 0.0224

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 37840	action = 0	current_phase = 0	next_phase = 1	reward = -0.019033	array([[-1.7693503, -2.2937553]], dtype=float32)

time = 37845	action = 0	current_phase = 0	next_phase = 1	reward = 0.047645	array([[-2.158987 , -2.8860538]], dtype=float32)

time = 37850	action = 1	current_phase = 0	next_phase = 1	reward = -1.044418	array([[-5.2356124, -2.7116437]], dtype=float32)

time = 37858	action = 1	current_phase = 1	next_phase = 0	reward = -0.696620	array([[-3.4977555, -2.0334265]], dtype=float32)

time = 37866	action = 0	current_phase = 0	next_phase = 1	reward = -0.070909	array([[-1.6798714, -2.770538 ]], dtype=float32)

time = 37871	action = 0	current_phase = 0	next_phase = 1	reward = 0.017535	array([[-2.0866163, -2.8433986]], dtype=float32)

time = 37876	action = 0	current_phase = 0	next_phase = 1	reward = 0.066619	array([[-2.6080422, -2.9663556]], dtype=float32)

time = 37881	action = 1	current_phase = 0	next_phase = 1	reward = -1.512674	array([[-6.459458 , -3.2028553]], dtype=float32)

time = 37889	action = 1	current_phase = 1	next_phase = 0	reward = -0.846856	array([[-3.6706548, -2.2121224]], dtype=float32)

time = 37897	action = 0	current_phase = 0	next_phase = 1	reward = -0.068736	array([[-1.7201289, -2.782019 ]], dtype=float32)

time = 37902	action = 0	current_phase = 0	next_phase = 1	reward = -0.002998	array([[-2.2655952, -2.925463 ]], dtype=float32)

time = 37907	action = 1	current_phase = 0	next_phase = 1	reward = -1.612025	array([[-2.9506445, -2.949549 ]], dtype=float32)

time = 37915	action = 1	current_phase = 1	next_phase = 0	reward = -0.659151	array([[-3.6563096, -2.0511851]], dtype=float32)

time = 37923	action = 0	current_phase = 0	next_phase = 1	reward = -0.131871	array([[-1.5597018, -2.8070831]], dtype=float32)

time = 37928	action = 0	current_phase = 0	next_phase = 1	reward = -0.053477	array([[-1.8925847, -2.7972755]], dtype=float32)

time = 37933	action = 0	current_phase = 0	next_phase = 1	reward = 0.014705	array([[-2.304837, -2.974509]], dtype=float32)

time = 37938	action = 1	current_phase = 0	next_phase = 1	reward = -1.200554	array([[-3.392839 , -2.9253314]], dtype=float32)

time = 37946	action = 1	current_phase = 1	next_phase = 0	reward = -0.882067	array([[-3.5945158, -2.1266196]], dtype=float32)

time = 37954	action = 0	current_phase = 0	next_phase = 1	reward = -0.117252	array([[-1.4049647, -2.7572637]], dtype=float32)

time = 37959	action = 0	current_phase = 0	next_phase = 1	reward = -0.034606	array([[-1.7370399, -2.247259 ]], dtype=float32)

time = 37964	action = 0	current_phase = 0	next_phase = 1	reward = 0.034416	array([[-1.9376259, -2.795125 ]], dtype=float32)

time = 37969	action = 1	current_phase = 0	next_phase = 1	reward = -0.705766	array([[-3.1109653, -2.4180582]], dtype=float32)

time = 37977	action = 1	current_phase = 1	next_phase = 0	reward = -0.913313	array([[-3.5273304, -2.0873196]], dtype=float32)

time = 37985	action = 0	current_phase = 0	next_phase = 1	reward = 0.189215	array([[-1.4564922, -2.8060603]], dtype=float32)

time = 37990	action = 0	current_phase = 0	next_phase = 1	reward = -0.301006	array([[-1.9381796, -2.2989736]], dtype=float32)

time = 37995	action = 0	current_phase = 0	next_phase = 1	reward = 0.330671	array([[-2.364707 , -2.9443836]], dtype=float32)

time = 38000	action = 1	current_phase = 0	next_phase = 1	reward = -1.432084	array([[-5.2620287, -2.8023016]], dtype=float32)

time = 38008	action = 1	current_phase = 1	next_phase = 0	reward = -0.705239	array([[-3.564938, -2.069492]], dtype=float32)

time = 38016	action = 0	current_phase = 0	next_phase = 1	reward = -0.085364	array([[-1.7436086, -2.784454 ]], dtype=float32)

time = 38021	action = 0	current_phase = 0	next_phase = 1	reward = -0.001549	array([[-2.0829263, -2.8268344]], dtype=float32)

time = 38026	action = 0	current_phase = 0	next_phase = 1	reward = 0.072113	array([[-2.5217466, -2.9867222]], dtype=float32)

time = 38031	action = 1	current_phase = 0	next_phase = 1	reward = -1.503814	array([[-6.436503 , -3.1847157]], dtype=float32)

time = 38039	action = 1	current_phase = 1	next_phase = 0	reward = -0.782715	array([[-3.6269898, -2.153802 ]], dtype=float32)

time = 38047	action = 0	current_phase = 0	next_phase = 1	reward = -0.058448	array([[-1.8083102, -2.8012555]], dtype=float32)

time = 38052	action = 0	current_phase = 0	next_phase = 1	reward = 0.030995	array([[-2.3195004, -2.9370368]], dtype=float32)

time = 38057	action = 0	current_phase = 0	next_phase = 1	reward = 0.082888	array([[-2.8874726, -2.9352944]], dtype=float32)

time = 38062	action = 1	current_phase = 0	next_phase = 1	reward = -1.725681	array([[-6.4015174, -3.4653356]], dtype=float32)

time = 38070	action = 1	current_phase = 1	next_phase = 0	reward = -1.032853	array([[-3.7948647, -2.377752 ]], dtype=float32)

time = 38078	action = 0	current_phase = 0	next_phase = 1	reward = -0.057473	array([[-1.7556919, -2.7729852]], dtype=float32)

time = 38083	action = 0	current_phase = 0	next_phase = 1	reward = 0.015540	array([[-2.3118978, -2.9440682]], dtype=float32)

time = 38088	action = 0	current_phase = 0	next_phase = 1	reward = 0.081135	array([[-2.9789376, -2.997514 ]], dtype=float32)

time = 38093	action = 1	current_phase = 0	next_phase = 1	reward = -1.265303	array([[-6.3454666, -3.499639 ]], dtype=float32)

time = 38101	action = 1	current_phase = 1	next_phase = 0	reward = -1.513108	array([[-3.8443093, -2.4101741]], dtype=float32)

time = 38109	action = 0	current_phase = 0	next_phase = 1	reward = 0.235969	array([[-1.4113913, -2.262439 ]], dtype=float32)

time = 38114	action = 0	current_phase = 0	next_phase = 1	reward = 0.032270	array([[-1.915677 , -2.7823915]], dtype=float32)

time = 38119	action = 1	current_phase = 0	next_phase = 1	reward = -0.681594	array([[-2.5100784, -2.3381367]], dtype=float32)

time = 38127	action = 1	current_phase = 1	next_phase = 0	reward = -0.943205	array([[-3.4211216, -1.9714382]], dtype=float32)

time = 38135	action = 0	current_phase = 0	next_phase = 1	reward = 0.167731	array([[-1.4166952, -2.8393245]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0320 - val_loss: 0.0174

Epoch 2/50

 - 3s - loss: 0.0245 - val_loss: 0.0136

Epoch 3/50

 - 3s - loss: 0.0280 - val_loss: 0.0163

Epoch 4/50

 - 3s - loss: 0.0199 - val_loss: 0.0137

Epoch 5/50

 - 3s - loss: 0.0257 - val_loss: 0.0120

Epoch 6/50

 - 3s - loss: 0.0226 - val_loss: 0.0147

Epoch 7/50

 - 3s - loss: 0.0249 - val_loss: 0.0147

Epoch 8/50

 - 3s - loss: 0.0240 - val_loss: 0.0141

Epoch 9/50

 - 3s - loss: 0.0243 - val_loss: 0.0160

Epoch 10/50

 - 3s - loss: 0.0232 - val_loss: 0.0144

Epoch 11/50

 - 3s - loss: 0.0270 - val_loss: 0.0149

Epoch 12/50

 - 3s - loss: 0.0216 - val_loss: 0.0140

Epoch 13/50

 - 3s - loss: 0.0207 - val_loss: 0.0125

Epoch 14/50

 - 3s - loss: 0.0203 - val_loss: 0.0138

Epoch 15/50

 - 3s - loss: 0.0203 - val_loss: 0.0147

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 38140	action = 0	current_phase = 0	next_phase = 1	reward = -0.307011	array([[-1.9999689, -2.3204894]], dtype=float32)

time = 38145	action = 0	current_phase = 0	next_phase = 1	reward = 0.052345	array([[-2.2653413, -2.9315333]], dtype=float32)

time = 38150	action = 1	current_phase = 0	next_phase = 1	reward = -1.084594	array([[-5.2742376, -2.781405 ]], dtype=float32)

time = 38158	action = 1	current_phase = 1	next_phase = 0	reward = -0.723471	array([[-3.5679615, -2.1160572]], dtype=float32)

time = 38166	action = 0	current_phase = 0	next_phase = 1	reward = -0.082239	array([[-1.6184099, -2.7613945]], dtype=float32)

time = 38171	action = 0	current_phase = 0	next_phase = 1	reward = -0.012984	array([[-2.1557193, -2.8686314]], dtype=float32)

time = 38176	action = 0	current_phase = 0	next_phase = 1	reward = 0.062010	array([[-2.4233565, -2.9847674]], dtype=float32)

time = 38181	action = 1	current_phase = 0	next_phase = 1	reward = -1.488877	array([[-6.4764185, -3.2040744]], dtype=float32)

time = 38189	action = 1	current_phase = 1	next_phase = 0	reward = -0.785368	array([[-3.643173 , -2.1931813]], dtype=float32)

time = 38197	action = 0	current_phase = 0	next_phase = 1	reward = -0.068364	array([[-1.7745144, -2.7960548]], dtype=float32)

time = 38202	action = 0	current_phase = 0	next_phase = 1	reward = 0.011439	array([[-2.2430153, -2.92343  ]], dtype=float32)

time = 38207	action = 0	current_phase = 0	next_phase = 1	reward = 0.083961	array([[-2.748917, -2.981535]], dtype=float32)

time = 38212	action = 1	current_phase = 0	next_phase = 1	reward = -1.610656	array([[-6.4779825, -3.5113587]], dtype=float32)

time = 38220	action = 1	current_phase = 1	next_phase = 0	reward = -1.062356	array([[-3.7640853, -2.342822 ]], dtype=float32)

time = 38228	action = 0	current_phase = 0	next_phase = 1	reward = -0.061042	array([[-1.6935545, -2.753774 ]], dtype=float32)

time = 38233	action = 0	current_phase = 0	next_phase = 1	reward = 0.018163	array([[-2.1951075, -2.9273648]], dtype=float32)

time = 38238	action = 0	current_phase = 0	next_phase = 1	reward = 0.083141	array([[-2.924789 , -2.9816093]], dtype=float32)

time = 38243	action = 1	current_phase = 0	next_phase = 1	reward = -1.870704	array([[-6.3381386, -3.744326 ]], dtype=float32)

time = 38251	action = 1	current_phase = 1	next_phase = 0	reward = -0.937466	array([[-3.8733325, -2.3293564]], dtype=float32)

time = 38259	action = 0	current_phase = 0	next_phase = 1	reward = -0.036434	array([[-1.4332016, -2.228126 ]], dtype=float32)

time = 38264	action = 0	current_phase = 0	next_phase = 1	reward = 0.028370	array([[-1.8059993, -2.784453 ]], dtype=float32)

time = 38269	action = 1	current_phase = 0	next_phase = 1	reward = -0.693472	array([[-2.5360045, -2.2500257]], dtype=float32)

time = 38277	action = 1	current_phase = 1	next_phase = 0	reward = -0.575554	array([[-3.3331463, -1.956052 ]], dtype=float32)

time = 38285	action = 0	current_phase = 0	next_phase = 1	reward = -0.093973	array([[-1.396781 , -2.7997875]], dtype=float32)

time = 38290	action = 0	current_phase = 0	next_phase = 1	reward = -0.029365	array([[-2.0195255, -2.2882986]], dtype=float32)

time = 38295	action = 0	current_phase = 0	next_phase = 1	reward = 0.035310	array([[-2.325478, -2.956664]], dtype=float32)

time = 38300	action = 1	current_phase = 0	next_phase = 1	reward = -1.333472	array([[-5.357089 , -2.9093523]], dtype=float32)

time = 38308	action = 1	current_phase = 1	next_phase = 0	reward = -0.696756	array([[-3.5641415, -2.128544 ]], dtype=float32)

time = 38316	action = 0	current_phase = 0	next_phase = 1	reward = -0.076721	array([[-1.6666044, -2.7859635]], dtype=float32)

time = 38321	action = 0	current_phase = 0	next_phase = 1	reward = 0.010454	array([[-2.0436738, -2.8522887]], dtype=float32)

time = 38326	action = 0	current_phase = 0	next_phase = 1	reward = 0.063562	array([[-2.46764  , -2.9820642]], dtype=float32)

time = 38331	action = 1	current_phase = 0	next_phase = 1	reward = -1.562198	array([[-6.4451933, -3.288354 ]], dtype=float32)

time = 38339	action = 1	current_phase = 1	next_phase = 0	reward = -0.842247	array([[-3.6669657, -2.2293112]], dtype=float32)

time = 38347	action = 0	current_phase = 0	next_phase = 1	reward = -0.060394	array([[-1.7484261, -2.7910209]], dtype=float32)

time = 38352	action = 0	current_phase = 0	next_phase = 1	reward = 0.006469	array([[-2.2726424, -2.9165053]], dtype=float32)

time = 38357	action = 0	current_phase = 0	next_phase = 1	reward = 0.074396	array([[-2.748423, -2.978191]], dtype=float32)

time = 38362	action = 1	current_phase = 0	next_phase = 1	reward = -1.613426	array([[-6.478266 , -3.5102062]], dtype=float32)

time = 38370	action = 1	current_phase = 1	next_phase = 0	reward = -1.004292	array([[-3.770699 , -2.3531227]], dtype=float32)

time = 38378	action = 0	current_phase = 0	next_phase = 1	reward = -0.049568	array([[-1.7133324, -2.7503228]], dtype=float32)

time = 38383	action = 0	current_phase = 0	next_phase = 1	reward = 0.021614	array([[-2.185069 , -2.9227834]], dtype=float32)

time = 38388	action = 0	current_phase = 0	next_phase = 1	reward = 0.074294	array([[-2.936564 , -3.0065966]], dtype=float32)

time = 38393	action = 1	current_phase = 0	next_phase = 1	reward = -1.940530	array([[-6.353072, -3.666061]], dtype=float32)

time = 38401	action = 1	current_phase = 1	next_phase = 0	reward = -1.026763	array([[-3.896937 , -2.3443625]], dtype=float32)

time = 38409	action = 0	current_phase = 0	next_phase = 1	reward = -0.034159	array([[-1.4113426, -2.2191172]], dtype=float32)

time = 38414	action = 0	current_phase = 0	next_phase = 1	reward = 0.040795	array([[-1.9091269, -2.8041496]], dtype=float32)

time = 38419	action = 1	current_phase = 0	next_phase = 1	reward = -0.741107	array([[-2.568252, -2.250395]], dtype=float32)

time = 38427	action = 1	current_phase = 1	next_phase = 0	reward = -0.651172	array([[-3.4887364, -2.0457716]], dtype=float32)

time = 38435	action = 0	current_phase = 0	next_phase = 1	reward = -0.385339	array([[-1.3542228, -2.823838 ]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0237 - val_loss: 0.0112

Epoch 2/50

 - 3s - loss: 0.0261 - val_loss: 0.0097

Epoch 3/50

 - 3s - loss: 0.0210 - val_loss: 0.0106

Epoch 4/50

 - 3s - loss: 0.0253 - val_loss: 0.0095

Epoch 5/50

 - 3s - loss: 0.0238 - val_loss: 0.0102

Epoch 6/50

 - 3s - loss: 0.0262 - val_loss: 0.0091

Epoch 7/50

 - 3s - loss: 0.0283 - val_loss: 0.0102

Epoch 8/50

 - 3s - loss: 0.0225 - val_loss: 0.0104

Epoch 9/50

 - 3s - loss: 0.0218 - val_loss: 0.0104

Epoch 10/50

 - 3s - loss: 0.0251 - val_loss: 0.0107

Epoch 11/50

 - 3s - loss: 0.0235 - val_loss: 0.0101

Epoch 12/50

 - 3s - loss: 0.0238 - val_loss: 0.0102

Epoch 13/50

 - 3s - loss: 0.0234 - val_loss: 0.0102

Epoch 14/50

 - 3s - loss: 0.0235 - val_loss: 0.0103

Epoch 15/50

 - 3s - loss: 0.0263 - val_loss: 0.0100

Epoch 16/50

 - 3s - loss: 0.0213 - val_loss: 0.0093

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 38440	action = 0	current_phase = 0	next_phase = 1	reward = 0.260882	array([[-1.8701308, -2.242845 ]], dtype=float32)

time = 38445	action = 0	current_phase = 0	next_phase = 1	reward = -0.229888	array([[-2.3751125, -2.9474406]], dtype=float32)

time = 38450	action = 1	current_phase = 0	next_phase = 1	reward = -1.014925	array([[-5.293375 , -2.7633483]], dtype=float32)

time = 38458	action = 1	current_phase = 1	next_phase = 0	reward = -0.710159	array([[-3.5784867, -2.1393235]], dtype=float32)

time = 38466	action = 0	current_phase = 0	next_phase = 1	reward = -0.084384	array([[-1.5998683, -2.746907 ]], dtype=float32)

time = 38471	action = 0	current_phase = 0	next_phase = 1	reward = -0.022164	array([[-2.0107236, -2.8181314]], dtype=float32)

time = 38476	action = 0	current_phase = 0	next_phase = 1	reward = 0.062654	array([[-2.475026 , -2.9810443]], dtype=float32)

time = 38481	action = 1	current_phase = 0	next_phase = 1	reward = -1.434647	array([[-6.5119596, -3.245367 ]], dtype=float32)

time = 38489	action = 1	current_phase = 1	next_phase = 0	reward = -0.777919	array([[-3.632781 , -2.2174048]], dtype=float32)

time = 38497	action = 0	current_phase = 0	next_phase = 1	reward = -0.069690	array([[-1.6229291, -2.7530217]], dtype=float32)

time = 38502	action = 0	current_phase = 0	next_phase = 1	reward = 0.013459	array([[-2.2307277, -2.9160244]], dtype=float32)

time = 38507	action = 0	current_phase = 0	next_phase = 1	reward = 0.072495	array([[-2.743803 , -2.9795005]], dtype=float32)

time = 38512	action = 1	current_phase = 0	next_phase = 1	reward = -1.732914	array([[-6.4377823, -3.5910075]], dtype=float32)

time = 38520	action = 1	current_phase = 1	next_phase = 0	reward = -0.996024	array([[-3.798854 , -2.3944063]], dtype=float32)

time = 38528	action = 0	current_phase = 0	next_phase = 1	reward = -0.047707	array([[-1.6313113, -2.7178707]], dtype=float32)

time = 38533	action = 0	current_phase = 0	next_phase = 1	reward = 0.012662	array([[-2.162641 , -2.9076524]], dtype=float32)

time = 38538	action = 0	current_phase = 0	next_phase = 1	reward = 0.084132	array([[-2.9769406, -3.0100982]], dtype=float32)

time = 38543	action = 1	current_phase = 0	next_phase = 1	reward = -1.860825	array([[-6.3725834, -3.6851885]], dtype=float32)

time = 38551	action = 1	current_phase = 1	next_phase = 0	reward = -1.025764	array([[-3.897913 , -2.3497965]], dtype=float32)

time = 38559	action = 0	current_phase = 0	next_phase = 1	reward = -0.025353	array([[-1.4103472, -2.2104182]], dtype=float32)

time = 38564	action = 0	current_phase = 0	next_phase = 1	reward = 0.032814	array([[-1.9576931, -2.821366 ]], dtype=float32)

time = 38569	action = 1	current_phase = 0	next_phase = 1	reward = -0.737227	array([[-2.5163388, -2.2316039]], dtype=float32)

time = 38577	action = 1	current_phase = 1	next_phase = 0	reward = -0.924461	array([[-3.5721865, -2.0977068]], dtype=float32)

time = 38585	action = 0	current_phase = 0	next_phase = 1	reward = 0.183689	array([[-1.2797298, -2.8711143]], dtype=float32)

time = 38590	action = 0	current_phase = 0	next_phase = 1	reward = -0.023483	array([[-1.9899398, -2.2632456]], dtype=float32)

time = 38595	action = 0	current_phase = 0	next_phase = 1	reward = 0.041282	array([[-2.3542585, -2.9417148]], dtype=float32)

time = 38600	action = 1	current_phase = 0	next_phase = 1	reward = -1.191198	array([[-5.2848887, -2.7885854]], dtype=float32)

time = 38608	action = 1	current_phase = 1	next_phase = 0	reward = -0.702606	array([[-3.4684453, -2.127007 ]], dtype=float32)

time = 38616	action = 0	current_phase = 0	next_phase = 1	reward = -0.085888	array([[-1.573807 , -2.7743363]], dtype=float32)

time = 38621	action = 0	current_phase = 0	next_phase = 1	reward = -0.006120	array([[-1.9775769, -2.8106692]], dtype=float32)

time = 38626	action = 0	current_phase = 0	next_phase = 1	reward = 0.062252	array([[-2.4834626, -2.9866035]], dtype=float32)

time = 38631	action = 1	current_phase = 0	next_phase = 1	reward = -1.461765	array([[-6.512906 , -3.3168495]], dtype=float32)

time = 38639	action = 1	current_phase = 1	next_phase = 0	reward = -0.773880	array([[-3.6275866, -2.2121434]], dtype=float32)

time = 38647	action = 0	current_phase = 0	next_phase = 1	reward = -0.052921	array([[-1.7281312, -2.7859247]], dtype=float32)

time = 38652	action = 0	current_phase = 0	next_phase = 1	reward = 0.030731	array([[-2.2426934, -2.927973 ]], dtype=float32)

time = 38657	action = 0	current_phase = 0	next_phase = 1	reward = 0.072724	array([[-2.7412825, -2.972515 ]], dtype=float32)

time = 38662	action = 1	current_phase = 0	next_phase = 1	reward = -1.681521	array([[-6.466365 , -3.5776806]], dtype=float32)

time = 38670	action = 1	current_phase = 1	next_phase = 0	reward = -0.960756	array([[-3.7808506, -2.4090054]], dtype=float32)

time = 38678	action = 0	current_phase = 0	next_phase = 1	reward = -0.062542	array([[-1.6842494, -2.7507482]], dtype=float32)

time = 38683	action = 0	current_phase = 0	next_phase = 1	reward = 0.009932	array([[-2.2738156, -2.9459004]], dtype=float32)

time = 38688	action = 0	current_phase = 0	next_phase = 1	reward = 0.082947	array([[-2.9395595, -2.9898875]], dtype=float32)

time = 38693	action = 1	current_phase = 0	next_phase = 1	reward = -1.921946	array([[-6.4130535, -3.5570571]], dtype=float32)

time = 38701	action = 1	current_phase = 1	next_phase = 0	reward = -1.076339	array([[-4.0272427, -2.3760853]], dtype=float32)

time = 38709	action = 0	current_phase = 0	next_phase = 1	reward = -0.024000	array([[-1.3437577, -2.207042 ]], dtype=float32)

time = 38714	action = 0	current_phase = 0	next_phase = 1	reward = 0.049582	array([[-1.8491613, -2.7831929]], dtype=float32)

time = 38719	action = 1	current_phase = 0	next_phase = 1	reward = -0.693373	array([[-2.491193 , -2.2478304]], dtype=float32)

time = 38727	action = 1	current_phase = 1	next_phase = 0	reward = -0.663723	array([[-3.4387417, -2.1117768]], dtype=float32)

time = 38735	action = 0	current_phase = 0	next_phase = 1	reward = -0.388745	array([[-1.3998936, -2.85843  ]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0248 - val_loss: 0.0127

Epoch 2/50

 - 3s - loss: 0.0248 - val_loss: 0.0158

Epoch 3/50

 - 3s - loss: 0.0234 - val_loss: 0.0136

Epoch 4/50

 - 3s - loss: 0.0220 - val_loss: 0.0123

Epoch 5/50

 - 3s - loss: 0.0242 - val_loss: 0.0140

Epoch 6/50

 - 3s - loss: 0.0233 - val_loss: 0.0140

Epoch 7/50

 - 3s - loss: 0.0192 - val_loss: 0.0125

Epoch 8/50

 - 3s - loss: 0.0228 - val_loss: 0.0170

Epoch 9/50

 - 3s - loss: 0.0224 - val_loss: 0.0138

Epoch 10/50

 - 3s - loss: 0.0209 - val_loss: 0.0121

Epoch 11/50

 - 3s - loss: 0.0241 - val_loss: 0.0130

Epoch 12/50

 - 3s - loss: 0.0180 - val_loss: 0.0127

Epoch 13/50

 - 3s - loss: 0.0198 - val_loss: 0.0115

Epoch 14/50

 - 3s - loss: 0.0235 - val_loss: 0.0207

Epoch 15/50

 - 3s - loss: 0.0230 - val_loss: 0.0133

Epoch 16/50

 - 3s - loss: 0.0201 - val_loss: 0.0111

Epoch 17/50

 - 3s - loss: 0.0204 - val_loss: 0.0145

Epoch 18/50

 - 3s - loss: 0.0175 - val_loss: 0.0124

Epoch 19/50

 - 3s - loss: 0.0174 - val_loss: 0.0167

Epoch 20/50

 - 3s - loss: 0.0212 - val_loss: 0.0111

Epoch 21/50

 - 3s - loss: 0.0193 - val_loss: 0.0147

Epoch 22/50

 - 3s - loss: 0.0208 - val_loss: 0.0167

Epoch 23/50

 - 3s - loss: 0.0172 - val_loss: 0.0187

Epoch 24/50

 - 3s - loss: 0.0233 - val_loss: 0.0181

Epoch 25/50

 - 3s - loss: 0.0206 - val_loss: 0.0122

Epoch 26/50

 - 3s - loss: 0.0175 - val_loss: 0.0131

Epoch 27/50

 - 3s - loss: 0.0181 - val_loss: 0.0170

Epoch 28/50

 - 3s - loss: 0.0200 - val_loss: 0.0149

Epoch 29/50

 - 3s - loss: 0.0197 - val_loss: 0.0138

Epoch 30/50

 - 3s - loss: 0.0176 - val_loss: 0.0126

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 38740	action = 0	current_phase = 0	next_phase = 1	reward = -0.017599	array([[-1.9688603, -2.2312717]], dtype=float32)

time = 38745	action = 0	current_phase = 0	next_phase = 1	reward = 0.050570	array([[-2.291128, -2.913495]], dtype=float32)

time = 38750	action = 1	current_phase = 0	next_phase = 1	reward = -1.076359	array([[-5.4169173, -2.7731392]], dtype=float32)

time = 38758	action = 1	current_phase = 1	next_phase = 0	reward = -0.713355	array([[-3.5810037, -2.0931888]], dtype=float32)

time = 38766	action = 0	current_phase = 0	next_phase = 1	reward = -0.090909	array([[-1.6014585, -2.780056 ]], dtype=float32)

time = 38771	action = 0	current_phase = 0	next_phase = 1	reward = -0.016760	array([[-2.104794 , -2.8428936]], dtype=float32)

time = 38776	action = 0	current_phase = 0	next_phase = 1	reward = 0.050831	array([[-2.4366987, -2.947774 ]], dtype=float32)

time = 38781	action = 1	current_phase = 0	next_phase = 1	reward = -1.334434	array([[-6.516107 , -3.3122196]], dtype=float32)

time = 38789	action = 1	current_phase = 1	next_phase = 0	reward = -0.710160	array([[-3.5993702, -2.1570644]], dtype=float32)

time = 38797	action = 0	current_phase = 0	next_phase = 1	reward = -0.069359	array([[-1.7692893, -2.792035 ]], dtype=float32)

time = 38802	action = 0	current_phase = 0	next_phase = 1	reward = 0.016698	array([[-2.2833362, -2.9202585]], dtype=float32)

time = 38807	action = 0	current_phase = 0	next_phase = 1	reward = 0.075863	array([[-2.7656462, -2.996716 ]], dtype=float32)

time = 38812	action = 1	current_phase = 0	next_phase = 1	reward = -1.630576	array([[-6.539426, -3.496951]], dtype=float32)

time = 38820	action = 1	current_phase = 1	next_phase = 0	reward = -0.935928	array([[-3.724816 , -2.2780502]], dtype=float32)

time = 38828	action = 0	current_phase = 0	next_phase = 1	reward = -0.052815	array([[-1.8164165, -2.7830548]], dtype=float32)

time = 38833	action = 0	current_phase = 0	next_phase = 1	reward = 0.009374	array([[-2.2865214, -2.931676 ]], dtype=float32)

time = 38838	action = 1	current_phase = 0	next_phase = 1	reward = -1.676503	array([[-2.985926 , -2.9816465]], dtype=float32)

time = 38846	action = 1	current_phase = 1	next_phase = 0	reward = -0.820638	array([[-3.7930875, -2.1971264]], dtype=float32)

time = 38854	action = 0	current_phase = 0	next_phase = 1	reward = -0.113173	array([[-1.3167315, -2.746926 ]], dtype=float32)

time = 38859	action = 0	current_phase = 0	next_phase = 1	reward = 0.222680	array([[-1.6416671, -2.192488 ]], dtype=float32)

time = 38864	action = 0	current_phase = 0	next_phase = 1	reward = -0.266857	array([[-1.9667706, -2.797451 ]], dtype=float32)

time = 38869	action = 1	current_phase = 0	next_phase = 1	reward = -0.642040	array([[-2.948117 , -2.3047252]], dtype=float32)

time = 38877	action = 1	current_phase = 1	next_phase = 0	reward = -0.935369	array([[-3.5025768, -2.0863063]], dtype=float32)

time = 38885	action = 0	current_phase = 0	next_phase = 1	reward = -0.096133	array([[-1.3201761, -2.7775502]], dtype=float32)

time = 38890	action = 0	current_phase = 0	next_phase = 1	reward = 0.251770	array([[-1.978433 , -2.2550578]], dtype=float32)

time = 38895	action = 0	current_phase = 0	next_phase = 1	reward = 0.027199	array([[-2.230259 , -2.9042592]], dtype=float32)

time = 38900	action = 1	current_phase = 0	next_phase = 1	reward = -1.439831	array([[-5.4401307, -2.8936381]], dtype=float32)

time = 38908	action = 1	current_phase = 1	next_phase = 0	reward = -0.711483	array([[-3.568261 , -2.0922706]], dtype=float32)

time = 38916	action = 0	current_phase = 0	next_phase = 1	reward = -0.080589	array([[-1.6357744, -2.7563138]], dtype=float32)

time = 38921	action = 0	current_phase = 0	next_phase = 1	reward = 0.002319	array([[-2.0552285, -2.8315444]], dtype=float32)

time = 38926	action = 0	current_phase = 0	next_phase = 1	reward = 0.060045	array([[-2.464538 , -2.9529424]], dtype=float32)

time = 38931	action = 1	current_phase = 0	next_phase = 1	reward = -1.397805	array([[-6.4835157, -3.211968 ]], dtype=float32)

time = 38939	action = 1	current_phase = 1	next_phase = 0	reward = -0.844779	array([[-3.5996158, -2.1309872]], dtype=float32)

time = 38947	action = 0	current_phase = 0	next_phase = 1	reward = -0.060703	array([[-1.7010794, -2.7773871]], dtype=float32)

time = 38952	action = 0	current_phase = 0	next_phase = 1	reward = 0.004872	array([[-2.2978544, -2.9504585]], dtype=float32)

time = 38957	action = 0	current_phase = 0	next_phase = 1	reward = 0.072983	array([[-2.9063258, -3.0007248]], dtype=float32)

time = 38962	action = 1	current_phase = 0	next_phase = 1	reward = -1.545384	array([[-6.5308914, -3.4756622]], dtype=float32)

time = 38970	action = 1	current_phase = 1	next_phase = 0	reward = -0.911776	array([[-3.727254 , -2.3089619]], dtype=float32)

time = 38978	action = 0	current_phase = 0	next_phase = 1	reward = -0.058644	array([[-1.760989 , -2.7568192]], dtype=float32)

time = 38983	action = 0	current_phase = 0	next_phase = 1	reward = 0.025876	array([[-2.3056204, -2.9374862]], dtype=float32)

time = 38988	action = 0	current_phase = 0	next_phase = 1	reward = 0.072395	array([[-2.942714 , -3.0308895]], dtype=float32)

time = 38993	action = 1	current_phase = 0	next_phase = 1	reward = -1.932878	array([[-6.390866, -3.678165]], dtype=float32)

time = 39001	action = 1	current_phase = 1	next_phase = 0	reward = -1.389879	array([[-3.9626637, -2.4120078]], dtype=float32)

time = 39009	action = 0	current_phase = 0	next_phase = 1	reward = 0.254243	array([[-1.3750932, -2.2255535]], dtype=float32)

time = 39014	action = 0	current_phase = 0	next_phase = 1	reward = 0.030411	array([[-1.9142346, -2.8003757]], dtype=float32)

time = 39019	action = 1	current_phase = 0	next_phase = 1	reward = -0.761339	array([[-2.5529466, -2.3764124]], dtype=float32)

time = 39027	action = 1	current_phase = 1	next_phase = 0	reward = -1.199790	array([[-3.3555121, -2.093746 ]], dtype=float32)

time = 39035	action = 0	current_phase = 0	next_phase = 1	reward = 0.191064	array([[-1.2152793, -2.808239 ]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0214 - val_loss: 0.0121

Epoch 2/50

 - 3s - loss: 0.0251 - val_loss: 0.0120

Epoch 3/50

 - 3s - loss: 0.0248 - val_loss: 0.0133

Epoch 4/50

 - 3s - loss: 0.0183 - val_loss: 0.0102

Epoch 5/50

 - 3s - loss: 0.0248 - val_loss: 0.0116

Epoch 6/50

 - 3s - loss: 0.0224 - val_loss: 0.0150

Epoch 7/50

 - 3s - loss: 0.0260 - val_loss: 0.0176

Epoch 8/50

 - 3s - loss: 0.0211 - val_loss: 0.0153

Epoch 9/50

 - 3s - loss: 0.0242 - val_loss: 0.0091

Epoch 10/50

 - 3s - loss: 0.0195 - val_loss: 0.0134

Epoch 11/50

 - 3s - loss: 0.0201 - val_loss: 0.0102

Epoch 12/50

 - 3s - loss: 0.0229 - val_loss: 0.0109

Epoch 13/50

 - 3s - loss: 0.0209 - val_loss: 0.0093

Epoch 14/50

 - 3s - loss: 0.0205 - val_loss: 0.0087

Epoch 15/50

 - 3s - loss: 0.0264 - val_loss: 0.0096

Epoch 16/50

 - 3s - loss: 0.0188 - val_loss: 0.0148

Epoch 17/50

 - 3s - loss: 0.0212 - val_loss: 0.0089

Epoch 18/50

 - 3s - loss: 0.0184 - val_loss: 0.0118

Epoch 19/50

 - 3s - loss: 0.0205 - val_loss: 0.0104

Epoch 20/50

 - 3s - loss: 0.0201 - val_loss: 0.0120

Epoch 21/50

 - 3s - loss: 0.0181 - val_loss: 0.0157

Epoch 22/50

 - 3s - loss: 0.0209 - val_loss: 0.0117

Epoch 23/50

 - 3s - loss: 0.0232 - val_loss: 0.0115

Epoch 24/50

 - 3s - loss: 0.0260 - val_loss: 0.0176

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 39040	action = 0	current_phase = 0	next_phase = 1	reward = -0.003852	array([[-1.8728671, -2.2629156]], dtype=float32)

time = 39045	action = 0	current_phase = 0	next_phase = 1	reward = 0.073441	array([[-2.2229555, -2.920569 ]], dtype=float32)

time = 39050	action = 1	current_phase = 0	next_phase = 1	reward = -1.051188	array([[-5.399507 , -2.8893738]], dtype=float32)

time = 39058	action = 1	current_phase = 1	next_phase = 0	reward = -0.722985	array([[-3.5366056, -2.087571 ]], dtype=float32)

time = 39066	action = 0	current_phase = 0	next_phase = 1	reward = -0.077415	array([[-1.6880138, -2.8021355]], dtype=float32)

time = 39071	action = 0	current_phase = 0	next_phase = 1	reward = -0.011607	array([[-2.0520105, -2.8984005]], dtype=float32)

time = 39076	action = 0	current_phase = 0	next_phase = 1	reward = 0.055709	array([[-2.5679054, -3.0095546]], dtype=float32)

time = 39081	action = 1	current_phase = 0	next_phase = 1	reward = -1.396289	array([[-6.532211 , -3.4978344]], dtype=float32)

time = 39089	action = 1	current_phase = 1	next_phase = 0	reward = -0.829561	array([[-3.6988182, -2.2218208]], dtype=float32)

time = 39097	action = 0	current_phase = 0	next_phase = 1	reward = -0.048593	array([[-1.642337, -2.781227]], dtype=float32)

time = 39102	action = 0	current_phase = 0	next_phase = 1	reward = 0.003035	array([[-2.2512789, -2.965974 ]], dtype=float32)

time = 39107	action = 0	current_phase = 0	next_phase = 1	reward = 0.072973	array([[-2.8690429, -3.028992 ]], dtype=float32)

time = 39112	action = 1	current_phase = 0	next_phase = 1	reward = -1.713563	array([[-6.5101204, -3.7385788]], dtype=float32)

time = 39120	action = 1	current_phase = 1	next_phase = 0	reward = -1.251604	array([[-3.7935207, -2.4580898]], dtype=float32)

time = 39128	action = 0	current_phase = 0	next_phase = 1	reward = 0.241162	array([[-1.7381617, -2.7737846]], dtype=float32)

time = 39133	action = 0	current_phase = 0	next_phase = 1	reward = 0.007659	array([[-2.2378192, -2.9639804]], dtype=float32)

time = 39138	action = 0	current_phase = 0	next_phase = 1	reward = 0.069255	array([[-2.8237243, -3.0663748]], dtype=float32)

time = 39143	action = 1	current_phase = 0	next_phase = 1	reward = -1.893210	array([[-6.407684 , -3.7728179]], dtype=float32)

time = 39151	action = 1	current_phase = 1	next_phase = 0	reward = -1.343117	array([[-4.075885, -2.325768]], dtype=float32)

time = 39159	action = 0	current_phase = 0	next_phase = 1	reward = 0.254622	array([[-1.3636802, -2.2041957]], dtype=float32)

time = 39164	action = 0	current_phase = 0	next_phase = 1	reward = 0.030448	array([[-1.9729549, -2.843726 ]], dtype=float32)

time = 39169	action = 1	current_phase = 0	next_phase = 1	reward = -0.760184	array([[-2.5854888, -2.3236191]], dtype=float32)

time = 39177	action = 1	current_phase = 1	next_phase = 0	reward = -0.653930	array([[-3.6013877, -2.0178332]], dtype=float32)

time = 39185	action = 0	current_phase = 0	next_phase = 1	reward = -0.097991	array([[-1.335213 , -2.9036942]], dtype=float32)

time = 39190	action = 0	current_phase = 0	next_phase = 1	reward = -0.013252	array([[-1.9538006, -2.253813 ]], dtype=float32)

time = 39195	action = 0	current_phase = 0	next_phase = 1	reward = 0.052017	array([[-2.300147 , -2.9774816]], dtype=float32)

time = 39200	action = 1	current_phase = 0	next_phase = 1	reward = -1.363167	array([[-5.4355826, -2.9756012]], dtype=float32)

time = 39208	action = 1	current_phase = 1	next_phase = 0	reward = -0.713740	array([[-3.5760674, -2.1161404]], dtype=float32)

time = 39216	action = 0	current_phase = 0	next_phase = 1	reward = -0.081300	array([[-1.7128414, -2.837834 ]], dtype=float32)

time = 39221	action = 0	current_phase = 0	next_phase = 1	reward = -0.004058	array([[-2.1069694, -2.892771 ]], dtype=float32)

time = 39226	action = 0	current_phase = 0	next_phase = 1	reward = 0.062260	array([[-2.4238458, -3.001262 ]], dtype=float32)

time = 39231	action = 1	current_phase = 0	next_phase = 1	reward = -1.379452	array([[-6.5347295, -3.3698628]], dtype=float32)

time = 39239	action = 1	current_phase = 1	next_phase = 0	reward = -0.782071	array([[-3.6655955, -2.2115164]], dtype=float32)

time = 39247	action = 0	current_phase = 0	next_phase = 1	reward = -0.069689	array([[-1.6848049, -2.7886171]], dtype=float32)

time = 39252	action = 0	current_phase = 0	next_phase = 1	reward = 0.021748	array([[-2.2476149, -2.943105 ]], dtype=float32)

time = 39257	action = 0	current_phase = 0	next_phase = 1	reward = 0.077262	array([[-2.726914 , -3.0392916]], dtype=float32)

time = 39262	action = 1	current_phase = 0	next_phase = 1	reward = -1.610577	array([[-6.4866123, -3.5223205]], dtype=float32)

time = 39270	action = 1	current_phase = 1	next_phase = 0	reward = -1.021709	array([[-3.784055 , -2.3481812]], dtype=float32)

time = 39278	action = 0	current_phase = 0	next_phase = 1	reward = -0.052747	array([[-1.7945127, -2.8067675]], dtype=float32)

time = 39283	action = 0	current_phase = 0	next_phase = 1	reward = 0.031004	array([[-2.2446651, -2.9558005]], dtype=float32)

time = 39288	action = 0	current_phase = 0	next_phase = 1	reward = 0.088157	array([[-2.9156487, -3.076963 ]], dtype=float32)

time = 39293	action = 1	current_phase = 0	next_phase = 1	reward = -1.861264	array([[-6.412369, -3.754637]], dtype=float32)

time = 39301	action = 1	current_phase = 1	next_phase = 0	reward = -1.106869	array([[-4.573005 , -2.5492048]], dtype=float32)

time = 39309	action = 0	current_phase = 0	next_phase = 1	reward = -0.039662	array([[-1.5716428, -2.2794638]], dtype=float32)

time = 39314	action = 0	current_phase = 0	next_phase = 1	reward = 0.040492	array([[-1.9196789, -2.8265252]], dtype=float32)

time = 39319	action = 1	current_phase = 0	next_phase = 1	reward = -0.713428	array([[-2.506537 , -2.3261983]], dtype=float32)

time = 39327	action = 1	current_phase = 1	next_phase = 0	reward = -0.864182	array([[-3.4405007, -2.0347548]], dtype=float32)

time = 39335	action = 0	current_phase = 0	next_phase = 1	reward = 0.180204	array([[-1.1825527, -2.916508 ]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0294 - val_loss: 0.0143

Epoch 2/50

 - 3s - loss: 0.0304 - val_loss: 0.0172

Epoch 3/50

 - 3s - loss: 0.0233 - val_loss: 0.0153

Epoch 4/50

 - 3s - loss: 0.0241 - val_loss: 0.0148

Epoch 5/50

 - 3s - loss: 0.0248 - val_loss: 0.0157

Epoch 6/50

 - 3s - loss: 0.0238 - val_loss: 0.0139

Epoch 7/50

 - 3s - loss: 0.0278 - val_loss: 0.0133

Epoch 8/50

 - 3s - loss: 0.0210 - val_loss: 0.0159

Epoch 9/50

 - 3s - loss: 0.0268 - val_loss: 0.0141

Epoch 10/50

 - 3s - loss: 0.0236 - val_loss: 0.0176

Epoch 11/50

 - 3s - loss: 0.0251 - val_loss: 0.0185

Epoch 12/50

 - 3s - loss: 0.0261 - val_loss: 0.0154

Epoch 13/50

 - 3s - loss: 0.0205 - val_loss: 0.0135

Epoch 14/50

 - 3s - loss: 0.0262 - val_loss: 0.0152

Epoch 15/50

 - 3s - loss: 0.0274 - val_loss: 0.0182

Epoch 16/50

 - 3s - loss: 0.0237 - val_loss: 0.0152

Epoch 17/50

 - 3s - loss: 0.0224 - val_loss: 0.0141

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 39340	action = 0	current_phase = 0	next_phase = 1	reward = -0.017928	array([[-1.8417903, -2.2379677]], dtype=float32)

time = 39345	action = 0	current_phase = 0	next_phase = 1	reward = 0.040522	array([[-2.1518798, -2.9045074]], dtype=float32)

time = 39350	action = 1	current_phase = 0	next_phase = 1	reward = -1.315110	array([[-5.4014115, -2.91051  ]], dtype=float32)

time = 39358	action = 1	current_phase = 1	next_phase = 0	reward = -0.705963	array([[-3.629056 , -2.1046262]], dtype=float32)

time = 39366	action = 0	current_phase = 0	next_phase = 1	reward = -0.082465	array([[-1.7546083, -2.7800405]], dtype=float32)

time = 39371	action = 0	current_phase = 0	next_phase = 1	reward = -0.002647	array([[-1.9648287, -2.8409126]], dtype=float32)

time = 39376	action = 0	current_phase = 0	next_phase = 1	reward = 0.051759	array([[-2.704618 , -3.0149434]], dtype=float32)

time = 39381	action = 1	current_phase = 0	next_phase = 1	reward = -1.434980	array([[-6.4752307, -3.3129432]], dtype=float32)

time = 39389	action = 1	current_phase = 1	next_phase = 0	reward = -0.739762	array([[-3.6983933, -2.179377 ]], dtype=float32)

time = 39397	action = 0	current_phase = 0	next_phase = 1	reward = -0.073385	array([[-1.8361592, -2.8046832]], dtype=float32)

time = 39402	action = 0	current_phase = 0	next_phase = 1	reward = 0.010351	array([[-2.1132703, -2.8687906]], dtype=float32)

time = 39407	action = 0	current_phase = 0	next_phase = 1	reward = 0.073389	array([[-2.8328938, -3.0639715]], dtype=float32)

time = 39412	action = 1	current_phase = 0	next_phase = 1	reward = -1.546585	array([[-6.444337 , -3.4933574]], dtype=float32)

time = 39420	action = 1	current_phase = 1	next_phase = 0	reward = -1.303951	array([[-3.8684957, -2.4176278]], dtype=float32)

time = 39428	action = 0	current_phase = 0	next_phase = 1	reward = 0.232309	array([[-1.7421236, -2.7655606]], dtype=float32)

time = 39433	action = 0	current_phase = 0	next_phase = 1	reward = 0.032927	array([[-2.2162428, -2.9492452]], dtype=float32)

time = 39438	action = 0	current_phase = 0	next_phase = 1	reward = 0.075619	array([[-2.8048415, -3.0401154]], dtype=float32)

time = 39443	action = 1	current_phase = 0	next_phase = 1	reward = -1.935710	array([[-6.367499 , -3.7928305]], dtype=float32)

time = 39451	action = 1	current_phase = 1	next_phase = 0	reward = -0.986270	array([[-4.215268 , -2.4719214]], dtype=float32)

time = 39459	action = 0	current_phase = 0	next_phase = 1	reward = -0.036364	array([[-1.3915638, -2.2014837]], dtype=float32)

time = 39464	action = 0	current_phase = 0	next_phase = 1	reward = 0.033336	array([[-1.7967584, -2.7832675]], dtype=float32)

time = 39469	action = 1	current_phase = 0	next_phase = 1	reward = -0.701031	array([[-2.4162028, -2.336465 ]], dtype=float32)

time = 39477	action = 1	current_phase = 1	next_phase = 0	reward = -0.818135	array([[-3.421807 , -1.8952255]], dtype=float32)

time = 39485	action = 0	current_phase = 0	next_phase = 1	reward = 0.181555	array([[-1.407522 , -2.9205875]], dtype=float32)

time = 39490	action = 0	current_phase = 0	next_phase = 1	reward = -0.026367	array([[-1.78464  , -2.2339578]], dtype=float32)

time = 39495	action = 0	current_phase = 0	next_phase = 1	reward = -0.232362	array([[-2.2525687, -2.9274418]], dtype=float32)

time = 39500	action = 1	current_phase = 0	next_phase = 1	reward = -1.025388	array([[-5.3272853, -2.6941755]], dtype=float32)

time = 39508	action = 1	current_phase = 1	next_phase = 0	reward = -0.707282	array([[-3.556699 , -2.0258696]], dtype=float32)

time = 39516	action = 0	current_phase = 0	next_phase = 1	reward = -0.099620	array([[-1.782333 , -2.7985113]], dtype=float32)

time = 39521	action = 0	current_phase = 0	next_phase = 1	reward = -0.005787	array([[-1.9804143, -2.823856 ]], dtype=float32)

time = 39526	action = 0	current_phase = 0	next_phase = 1	reward = 0.065411	array([[-2.5701532, -2.991803 ]], dtype=float32)

time = 39531	action = 1	current_phase = 0	next_phase = 1	reward = -1.548619	array([[-6.507473, -3.239561]], dtype=float32)

time = 39539	action = 1	current_phase = 1	next_phase = 0	reward = -0.790395	array([[-3.712217 , -2.1769853]], dtype=float32)

time = 39547	action = 0	current_phase = 0	next_phase = 1	reward = -0.058364	array([[-1.8113189, -2.8399818]], dtype=float32)

time = 39552	action = 0	current_phase = 0	next_phase = 1	reward = 0.017150	array([[-2.1608653, -2.8939939]], dtype=float32)

time = 39557	action = 0	current_phase = 0	next_phase = 1	reward = 0.076187	array([[-2.8119016, -3.0424056]], dtype=float32)

time = 39562	action = 1	current_phase = 0	next_phase = 1	reward = -1.614347	array([[-6.458797 , -3.5185444]], dtype=float32)

time = 39570	action = 1	current_phase = 1	next_phase = 0	reward = -1.005266	array([[-3.861594 , -2.4016247]], dtype=float32)

time = 39578	action = 0	current_phase = 0	next_phase = 1	reward = -0.054904	array([[-1.7262557, -2.7548583]], dtype=float32)

time = 39583	action = 0	current_phase = 0	next_phase = 1	reward = 0.003615	array([[-2.2018056, -2.912311 ]], dtype=float32)

time = 39588	action = 0	current_phase = 0	next_phase = 1	reward = 0.064525	array([[-2.8386989, -3.0472813]], dtype=float32)

time = 39593	action = 1	current_phase = 0	next_phase = 1	reward = -1.254183	array([[-6.3856215, -3.6631145]], dtype=float32)

time = 39601	action = 1	current_phase = 1	next_phase = 0	reward = -1.536622	array([[-3.9546597, -2.5003042]], dtype=float32)

time = 39609	action = 0	current_phase = 0	next_phase = 1	reward = 0.246041	array([[-1.4080036, -2.21739  ]], dtype=float32)

time = 39614	action = 0	current_phase = 0	next_phase = 1	reward = 0.040774	array([[-1.8681245, -2.7939458]], dtype=float32)

time = 39619	action = 1	current_phase = 0	next_phase = 1	reward = -0.635900	array([[-2.4496746, -2.4313385]], dtype=float32)

time = 39627	action = 1	current_phase = 1	next_phase = 0	reward = -0.588150	array([[-3.4737866, -2.0260339]], dtype=float32)

time = 39635	action = 0	current_phase = 0	next_phase = 1	reward = -0.099729	array([[-1.4705714, -2.8437605]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0223 - val_loss: 0.0134

Epoch 2/50

 - 3s - loss: 0.0273 - val_loss: 0.0159

Epoch 3/50

 - 3s - loss: 0.0222 - val_loss: 0.0150

Epoch 4/50

 - 3s - loss: 0.0221 - val_loss: 0.0121

Epoch 5/50

 - 3s - loss: 0.0212 - val_loss: 0.0187

Epoch 6/50

 - 3s - loss: 0.0208 - val_loss: 0.0138

Epoch 7/50

 - 3s - loss: 0.0200 - val_loss: 0.0139

Epoch 8/50

 - 3s - loss: 0.0224 - val_loss: 0.0109

Epoch 9/50

 - 3s - loss: 0.0211 - val_loss: 0.0151

Epoch 10/50

 - 3s - loss: 0.0224 - val_loss: 0.0117

Epoch 11/50

 - 3s - loss: 0.0198 - val_loss: 0.0139

Epoch 12/50

 - 3s - loss: 0.0249 - val_loss: 0.0171

Epoch 13/50

 - 3s - loss: 0.0284 - val_loss: 0.0141

Epoch 14/50

 - 3s - loss: 0.0213 - val_loss: 0.0161

Epoch 15/50

 - 3s - loss: 0.0196 - val_loss: 0.0117

Epoch 16/50

 - 3s - loss: 0.0213 - val_loss: 0.0152

Epoch 17/50

 - 3s - loss: 0.0212 - val_loss: 0.0148

Epoch 18/50

 - 3s - loss: 0.0237 - val_loss: 0.0164

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 39640	action = 0	current_phase = 0	next_phase = 1	reward = -0.026368	array([[-1.7246845, -2.2319553]], dtype=float32)

time = 39645	action = 0	current_phase = 0	next_phase = 1	reward = -0.227521	array([[-2.2607172, -2.9345357]], dtype=float32)

time = 39650	action = 1	current_phase = 0	next_phase = 1	reward = -0.975847	array([[-5.38867 , -2.758768]], dtype=float32)

time = 39658	action = 1	current_phase = 1	next_phase = 0	reward = -0.703699	array([[-3.5980167, -2.1081605]], dtype=float32)

time = 39666	action = 0	current_phase = 0	next_phase = 1	reward = -0.091069	array([[-1.7250708, -2.818702 ]], dtype=float32)

time = 39671	action = 0	current_phase = 0	next_phase = 1	reward = -0.025377	array([[-1.98386  , -2.8458257]], dtype=float32)

time = 39676	action = 0	current_phase = 0	next_phase = 1	reward = 0.041710	array([[-2.594941, -3.007069]], dtype=float32)

time = 39681	action = 1	current_phase = 0	next_phase = 1	reward = -1.376839	array([[-6.466616, -3.313922]], dtype=float32)

time = 39689	action = 1	current_phase = 1	next_phase = 0	reward = -0.759951	array([[-3.77067  , -2.2574742]], dtype=float32)

time = 39697	action = 0	current_phase = 0	next_phase = 1	reward = -0.076498	array([[-1.7830756, -2.8056655]], dtype=float32)

time = 39702	action = 0	current_phase = 0	next_phase = 1	reward = 0.001633	array([[-2.1792955, -2.9209104]], dtype=float32)

time = 39707	action = 0	current_phase = 0	next_phase = 1	reward = 0.058464	array([[-2.8634057, -3.071484 ]], dtype=float32)

time = 39712	action = 1	current_phase = 0	next_phase = 1	reward = -1.653179	array([[-6.452142, -3.595965]], dtype=float32)

time = 39720	action = 1	current_phase = 1	next_phase = 0	reward = -0.914321	array([[-3.9690795, -2.5220277]], dtype=float32)

time = 39728	action = 0	current_phase = 0	next_phase = 1	reward = -0.053775	array([[-1.8211348, -2.8032324]], dtype=float32)

time = 39733	action = 0	current_phase = 0	next_phase = 1	reward = 0.024564	array([[-2.2660513, -2.9496176]], dtype=float32)

time = 39738	action = 0	current_phase = 0	next_phase = 1	reward = 0.078648	array([[-2.865356 , -3.1168013]], dtype=float32)

time = 39743	action = 1	current_phase = 0	next_phase = 1	reward = -1.877887	array([[-6.327136, -3.829456]], dtype=float32)

time = 39751	action = 1	current_phase = 1	next_phase = 0	reward = -1.136738	array([[-4.2616053, -2.5042937]], dtype=float32)

time = 39759	action = 0	current_phase = 0	next_phase = 1	reward = -0.040614	array([[-1.4376996, -2.2288883]], dtype=float32)

time = 39764	action = 0	current_phase = 0	next_phase = 1	reward = 0.021022	array([[-1.8131886, -2.777751 ]], dtype=float32)

time = 39769	action = 1	current_phase = 0	next_phase = 1	reward = -0.683583	array([[-2.412443 , -2.2595356]], dtype=float32)

time = 39777	action = 1	current_phase = 1	next_phase = 0	reward = -0.590143	array([[-3.3728676, -1.7370713]], dtype=float32)

time = 39785	action = 0	current_phase = 0	next_phase = 1	reward = -0.103081	array([[-1.4462268, -2.936091 ]], dtype=float32)

time = 39790	action = 0	current_phase = 0	next_phase = 1	reward = -0.021029	array([[-1.8181803, -2.2230978]], dtype=float32)

time = 39795	action = 0	current_phase = 0	next_phase = 1	reward = 0.045466	array([[-2.2546444, -2.942041 ]], dtype=float32)

time = 39800	action = 1	current_phase = 0	next_phase = 1	reward = -1.356417	array([[-5.3818984, -2.927975 ]], dtype=float32)

time = 39808	action = 1	current_phase = 1	next_phase = 0	reward = -0.708405	array([[-3.6646986, -2.1166246]], dtype=float32)

time = 39816	action = 0	current_phase = 0	next_phase = 1	reward = -0.077537	array([[-1.7008708, -2.8056135]], dtype=float32)

time = 39821	action = 0	current_phase = 0	next_phase = 1	reward = -0.011298	array([[-1.967911 , -2.8330305]], dtype=float32)

time = 39826	action = 0	current_phase = 0	next_phase = 1	reward = 0.051532	array([[-2.6383965, -3.0279331]], dtype=float32)

time = 39831	action = 1	current_phase = 0	next_phase = 1	reward = -1.387298	array([[-6.455347, -3.23106 ]], dtype=float32)

time = 39839	action = 1	current_phase = 1	next_phase = 0	reward = -0.776758	array([[-3.7418952, -2.2203102]], dtype=float32)

time = 39847	action = 0	current_phase = 0	next_phase = 1	reward = -0.049986	array([[-1.7525954, -2.8204343]], dtype=float32)

time = 39852	action = 0	current_phase = 0	next_phase = 1	reward = 0.027572	array([[-2.1502483, -2.918494 ]], dtype=float32)

time = 39857	action = 0	current_phase = 0	next_phase = 1	reward = 0.080633	array([[-2.9265425, -3.085074 ]], dtype=float32)

time = 39862	action = 1	current_phase = 0	next_phase = 1	reward = -1.686854	array([[-6.3997755, -3.596837 ]], dtype=float32)

time = 39870	action = 1	current_phase = 1	next_phase = 0	reward = -1.066943	array([[-3.9582233, -2.492042 ]], dtype=float32)

time = 39878	action = 0	current_phase = 0	next_phase = 1	reward = -0.038428	array([[-1.7278979, -2.7282178]], dtype=float32)

time = 39883	action = 0	current_phase = 0	next_phase = 1	reward = 0.039411	array([[-2.2295911, -2.9309554]], dtype=float32)

time = 39888	action = 0	current_phase = 0	next_phase = 1	reward = 0.071791	array([[-2.9006903, -3.1186776]], dtype=float32)

time = 39893	action = 1	current_phase = 0	next_phase = 1	reward = -1.252816	array([[-6.3622108, -3.542822 ]], dtype=float32)

time = 39901	action = 1	current_phase = 1	next_phase = 0	reward = -1.180052	array([[-4.0091543, -2.5213604]], dtype=float32)

time = 39909	action = 0	current_phase = 0	next_phase = 1	reward = -0.032317	array([[-1.3162867, -2.1876607]], dtype=float32)

time = 39914	action = 0	current_phase = 0	next_phase = 1	reward = 0.292497	array([[-1.8398169, -2.7907355]], dtype=float32)

time = 39919	action = 1	current_phase = 0	next_phase = 1	reward = -0.928780	array([[-2.427152 , -2.3408039]], dtype=float32)

time = 39927	action = 1	current_phase = 1	next_phase = 0	reward = -1.152832	array([[-3.3850837, -1.8636401]], dtype=float32)

time = 39935	action = 0	current_phase = 0	next_phase = 1	reward = -0.076051	array([[-1.3977197, -2.9329374]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0250 - val_loss: 0.0154

Epoch 2/50

 - 3s - loss: 0.0224 - val_loss: 0.0145

Epoch 3/50

 - 3s - loss: 0.0210 - val_loss: 0.0122

Epoch 4/50

 - 3s - loss: 0.0224 - val_loss: 0.0136

Epoch 5/50

 - 3s - loss: 0.0191 - val_loss: 0.0125

Epoch 6/50

 - 3s - loss: 0.0250 - val_loss: 0.0165

Epoch 7/50

 - 3s - loss: 0.0201 - val_loss: 0.0126

Epoch 8/50

 - 3s - loss: 0.0212 - val_loss: 0.0152

Epoch 9/50

 - 3s - loss: 0.0210 - val_loss: 0.0121

Epoch 10/50

 - 3s - loss: 0.0199 - val_loss: 0.0141

Epoch 11/50

 - 3s - loss: 0.0196 - val_loss: 0.0134

Epoch 12/50

 - 3s - loss: 0.0221 - val_loss: 0.0130

Epoch 13/50

 - 3s - loss: 0.0202 - val_loss: 0.0146

Epoch 14/50

 - 3s - loss: 0.0199 - val_loss: 0.0132

Epoch 15/50

 - 3s - loss: 0.0213 - val_loss: 0.0130

Epoch 16/50

 - 3s - loss: 0.0225 - val_loss: 0.0154

Epoch 17/50

 - 3s - loss: 0.0239 - val_loss: 0.0178

Epoch 18/50

 - 3s - loss: 0.0230 - val_loss: 0.0196

Epoch 19/50

 - 3s - loss: 0.0208 - val_loss: 0.0119

Epoch 20/50

 - 3s - loss: 0.0221 - val_loss: 0.0183

Epoch 21/50

 - 3s - loss: 0.0194 - val_loss: 0.0176

Epoch 22/50

 - 3s - loss: 0.0186 - val_loss: 0.0149

Epoch 23/50

 - 3s - loss: 0.0210 - val_loss: 0.0154

Epoch 24/50

 - 3s - loss: 0.0188 - val_loss: 0.0131

Epoch 25/50

 - 3s - loss: 0.0224 - val_loss: 0.0129

Epoch 26/50

 - 3s - loss: 0.0213 - val_loss: 0.0149

Epoch 27/50

 - 3s - loss: 0.0184 - val_loss: 0.0134

Epoch 28/50

 - 3s - loss: 0.0209 - val_loss: 0.0123

Epoch 29/50

 - 3s - loss: 0.0217 - val_loss: 0.0137

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 39940	action = 0	current_phase = 0	next_phase = 1	reward = 0.553374	array([[-1.7113631, -2.236672 ]], dtype=float32)

time = 39945	action = 0	current_phase = 0	next_phase = 1	reward = 0.048799	array([[-2.2967901, -2.9538424]], dtype=float32)

time = 39950	action = 1	current_phase = 0	next_phase = 1	reward = -1.261088	array([[-5.4622383, -3.0168831]], dtype=float32)

time = 39958	action = 1	current_phase = 1	next_phase = 0	reward = -0.694489	array([[-3.5499642, -2.0229058]], dtype=float32)

time = 39966	action = 0	current_phase = 0	next_phase = 1	reward = -0.086891	array([[-1.8453803, -2.8168123]], dtype=float32)

time = 39971	action = 0	current_phase = 0	next_phase = 1	reward = -0.020653	array([[-2.047556, -2.85613 ]], dtype=float32)

time = 39976	action = 0	current_phase = 0	next_phase = 1	reward = 0.035486	array([[-2.6274757, -3.0172467]], dtype=float32)

time = 39981	action = 1	current_phase = 0	next_phase = 1	reward = -1.544994	array([[-6.5118055, -3.3320086]], dtype=float32)

time = 39989	action = 1	current_phase = 1	next_phase = 0	reward = -0.779949	array([[-3.7448912, -2.2542043]], dtype=float32)

time = 39997	action = 0	current_phase = 0	next_phase = 1	reward = -0.064375	array([[-1.9028827, -2.8267539]], dtype=float32)

time = 40002	action = 0	current_phase = 0	next_phase = 1	reward = 0.006221	array([[-2.2211976, -2.9522471]], dtype=float32)

time = 40007	action = 0	current_phase = 0	next_phase = 1	reward = 0.058197	array([[-2.8276677, -3.0554354]], dtype=float32)

time = 40012	action = 1	current_phase = 0	next_phase = 1	reward = -1.565289	array([[-6.442194, -3.636916]], dtype=float32)

time = 40020	action = 1	current_phase = 1	next_phase = 0	reward = -1.196674	array([[-3.904125 , -2.4580545]], dtype=float32)

time = 40028	action = 0	current_phase = 0	next_phase = 1	reward = 0.239281	array([[-1.7815042, -2.778469 ]], dtype=float32)

time = 40033	action = 0	current_phase = 0	next_phase = 1	reward = 0.010966	array([[-2.3156404, -2.9662025]], dtype=float32)

time = 40038	action = 0	current_phase = 0	next_phase = 1	reward = 0.079393	array([[-2.9106822, -3.0990038]], dtype=float32)

time = 40043	action = 1	current_phase = 0	next_phase = 1	reward = -1.803631	array([[-6.4028716, -3.674771 ]], dtype=float32)

time = 40051	action = 1	current_phase = 1	next_phase = 0	reward = -1.088758	array([[-4.1902413, -2.4495943]], dtype=float32)

time = 40059	action = 0	current_phase = 0	next_phase = 1	reward = -0.026201	array([[-1.3464942, -2.2020855]], dtype=float32)

time = 40064	action = 0	current_phase = 0	next_phase = 1	reward = 0.052837	array([[-1.8754274, -2.8028214]], dtype=float32)

time = 40069	action = 1	current_phase = 0	next_phase = 1	reward = -0.633696	array([[-2.4190369, -2.3263917]], dtype=float32)

time = 40077	action = 1	current_phase = 1	next_phase = 0	reward = -1.208896	array([[-3.3022761, -1.8605556]], dtype=float32)

time = 40085	action = 0	current_phase = 0	next_phase = 1	reward = 0.471276	array([[-1.4821181, -2.9431908]], dtype=float32)

time = 40090	action = 0	current_phase = 0	next_phase = 1	reward = -0.291408	array([[-1.7819071, -2.258083 ]], dtype=float32)

time = 40095	action = 0	current_phase = 0	next_phase = 1	reward = 0.063685	array([[-2.3072746, -2.9373918]], dtype=float32)

time = 40100	action = 1	current_phase = 0	next_phase = 1	reward = -1.023240	array([[-5.4168434, -2.7979553]], dtype=float32)

time = 40108	action = 1	current_phase = 1	next_phase = 0	reward = -0.715229	array([[-3.5380979, -1.9972644]], dtype=float32)

time = 40116	action = 0	current_phase = 0	next_phase = 1	reward = -0.081771	array([[-1.8629291, -2.8170853]], dtype=float32)

time = 40121	action = 0	current_phase = 0	next_phase = 1	reward = -0.015822	array([[-2.0936775, -2.8669484]], dtype=float32)

time = 40126	action = 0	current_phase = 0	next_phase = 1	reward = 0.052777	array([[-2.5930467, -3.0139387]], dtype=float32)

time = 40131	action = 1	current_phase = 0	next_phase = 1	reward = -1.501437	array([[-6.5316267, -3.3133066]], dtype=float32)

time = 40139	action = 1	current_phase = 1	next_phase = 0	reward = -0.822831	array([[-3.7161179, -2.1997702]], dtype=float32)

time = 40147	action = 0	current_phase = 0	next_phase = 1	reward = -0.086086	array([[-1.8543463, -2.810335 ]], dtype=float32)

time = 40152	action = 0	current_phase = 0	next_phase = 1	reward = -0.007501	array([[-2.2165399, -2.94127  ]], dtype=float32)

time = 40157	action = 0	current_phase = 0	next_phase = 1	reward = 0.061042	array([[-2.8422418, -3.072872 ]], dtype=float32)

time = 40162	action = 1	current_phase = 0	next_phase = 1	reward = -1.582294	array([[-6.520483 , -3.4723392]], dtype=float32)

time = 40170	action = 1	current_phase = 1	next_phase = 0	reward = -1.236344	array([[-3.9028673, -2.4137926]], dtype=float32)

time = 40178	action = 0	current_phase = 0	next_phase = 1	reward = 0.241300	array([[-1.8016552, -2.7840145]], dtype=float32)

time = 40183	action = 0	current_phase = 0	next_phase = 1	reward = 0.017831	array([[-2.3266191, -2.9564402]], dtype=float32)

time = 40188	action = 0	current_phase = 0	next_phase = 1	reward = 0.069469	array([[-2.8827577, -3.1161342]], dtype=float32)

time = 40193	action = 1	current_phase = 0	next_phase = 1	reward = -1.816072	array([[-6.403365 , -3.7323754]], dtype=float32)

time = 40201	action = 1	current_phase = 1	next_phase = 0	reward = -1.275191	array([[-4.188687, -2.453945]], dtype=float32)

time = 40209	action = 0	current_phase = 0	next_phase = 1	reward = 0.276570	array([[-1.3774745, -2.2041805]], dtype=float32)

time = 40214	action = 0	current_phase = 0	next_phase = 1	reward = 0.043872	array([[-1.9193   , -2.8223128]], dtype=float32)

time = 40219	action = 1	current_phase = 0	next_phase = 1	reward = -0.688576	array([[-2.4335046, -2.3254137]], dtype=float32)

time = 40227	action = 1	current_phase = 1	next_phase = 0	reward = -0.601272	array([[-3.2914217, -1.6860116]], dtype=float32)

time = 40235	action = 0	current_phase = 0	next_phase = 1	reward = -0.118149	array([[-1.6350299, -2.907852 ]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0243 - val_loss: 0.0083

Epoch 2/50

 - 3s - loss: 0.0229 - val_loss: 0.0097

Epoch 3/50

 - 3s - loss: 0.0254 - val_loss: 0.0082

Epoch 4/50

 - 3s - loss: 0.0196 - val_loss: 0.0078

Epoch 5/50

 - 3s - loss: 0.0198 - val_loss: 0.0084

Epoch 6/50

 - 3s - loss: 0.0175 - val_loss: 0.0087

Epoch 7/50

 - 3s - loss: 0.0191 - val_loss: 0.0145

Epoch 8/50

 - 3s - loss: 0.0179 - val_loss: 0.0091

Epoch 9/50

 - 3s - loss: 0.0191 - val_loss: 0.0090

Epoch 10/50

 - 3s - loss: 0.0186 - val_loss: 0.0109

Epoch 11/50

 - 3s - loss: 0.0195 - val_loss: 0.0084

Epoch 12/50

 - 3s - loss: 0.0177 - val_loss: 0.0098

Epoch 13/50

 - 3s - loss: 0.0225 - val_loss: 0.0080

Epoch 14/50

 - 3s - loss: 0.0206 - val_loss: 0.0096

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 40240	action = 0	current_phase = 0	next_phase = 1	reward = -0.302717	array([[-1.7644075, -2.2185118]], dtype=float32)

time = 40245	action = 0	current_phase = 0	next_phase = 1	reward = 0.326909	array([[-2.1671834, -2.9107475]], dtype=float32)

time = 40250	action = 1	current_phase = 0	next_phase = 1	reward = -1.304873	array([[-5.453981 , -3.0132258]], dtype=float32)

time = 40258	action = 1	current_phase = 1	next_phase = 0	reward = -0.719206	array([[-3.6466317, -2.134492 ]], dtype=float32)

time = 40266	action = 0	current_phase = 0	next_phase = 1	reward = -0.084020	array([[-1.7247572, -2.799599 ]], dtype=float32)

time = 40271	action = 0	current_phase = 0	next_phase = 1	reward = 0.010806	array([[-1.9319643, -2.8376474]], dtype=float32)

time = 40276	action = 0	current_phase = 0	next_phase = 1	reward = 0.068043	array([[-2.5521839, -3.0179005]], dtype=float32)

time = 40281	action = 1	current_phase = 0	next_phase = 1	reward = -1.566297	array([[-6.492043 , -3.2390258]], dtype=float32)

time = 40289	action = 1	current_phase = 1	next_phase = 0	reward = -0.782891	array([[-3.7314577, -2.1973274]], dtype=float32)

time = 40297	action = 0	current_phase = 0	next_phase = 1	reward = -0.073553	array([[-1.772831 , -2.8240852]], dtype=float32)

time = 40302	action = 0	current_phase = 0	next_phase = 1	reward = 0.014592	array([[-2.2081745, -2.9737782]], dtype=float32)

time = 40307	action = 0	current_phase = 0	next_phase = 1	reward = 0.073108	array([[-2.8516936, -3.0594485]], dtype=float32)

time = 40312	action = 1	current_phase = 0	next_phase = 1	reward = -1.544759	array([[-6.430936 , -3.6200988]], dtype=float32)

time = 40320	action = 1	current_phase = 1	next_phase = 0	reward = -0.998478	array([[-3.8929694, -2.4294648]], dtype=float32)

time = 40328	action = 0	current_phase = 0	next_phase = 1	reward = -0.060535	array([[-1.8189036, -2.8119586]], dtype=float32)

time = 40333	action = 0	current_phase = 0	next_phase = 1	reward = 0.041839	array([[-2.2183702, -2.9531338]], dtype=float32)

time = 40338	action = 0	current_phase = 0	next_phase = 1	reward = 0.088718	array([[-2.8044274, -3.1396022]], dtype=float32)

time = 40343	action = 1	current_phase = 0	next_phase = 1	reward = -1.976804	array([[-6.3867307, -3.623478 ]], dtype=float32)

time = 40351	action = 1	current_phase = 1	next_phase = 0	reward = -1.030516	array([[-4.2837663, -2.4588394]], dtype=float32)

time = 40359	action = 0	current_phase = 0	next_phase = 1	reward = -0.033251	array([[-1.341862, -2.201076]], dtype=float32)

time = 40364	action = 0	current_phase = 0	next_phase = 1	reward = 0.034342	array([[-1.7125982, -2.7807117]], dtype=float32)

time = 40369	action = 1	current_phase = 0	next_phase = 1	reward = -0.696395	array([[-2.3683243, -2.2867837]], dtype=float32)

time = 40377	action = 1	current_phase = 1	next_phase = 0	reward = -0.598986	array([[-3.4446592, -1.8842916]], dtype=float32)

time = 40385	action = 0	current_phase = 0	next_phase = 1	reward = -0.391376	array([[-1.5414126, -2.9978912]], dtype=float32)

time = 40390	action = 0	current_phase = 0	next_phase = 1	reward = -0.028814	array([[-1.7462943, -2.2269692]], dtype=float32)

time = 40395	action = 0	current_phase = 0	next_phase = 1	reward = 0.042338	array([[-2.1968858, -2.9283028]], dtype=float32)

time = 40400	action = 1	current_phase = 0	next_phase = 1	reward = -0.967096	array([[-5.393162 , -2.7755897]], dtype=float32)

time = 40408	action = 1	current_phase = 1	next_phase = 0	reward = -0.658257	array([[-3.5514472, -2.0072217]], dtype=float32)

time = 40416	action = 0	current_phase = 0	next_phase = 1	reward = -0.091730	array([[-1.6858507, -2.8083684]], dtype=float32)

time = 40421	action = 0	current_phase = 0	next_phase = 1	reward = -0.015248	array([[-1.9184185, -2.854849 ]], dtype=float32)

time = 40426	action = 0	current_phase = 0	next_phase = 1	reward = 0.063072	array([[-2.3690872, -2.9772677]], dtype=float32)

time = 40431	action = 1	current_phase = 0	next_phase = 1	reward = -1.447219	array([[-6.4901633, -3.2542453]], dtype=float32)

time = 40439	action = 1	current_phase = 1	next_phase = 0	reward = -0.897309	array([[-3.7422388, -2.2098594]], dtype=float32)

time = 40447	action = 0	current_phase = 0	next_phase = 1	reward = -0.071998	array([[-1.7783973, -2.8336062]], dtype=float32)

time = 40452	action = 0	current_phase = 0	next_phase = 1	reward = -0.001797	array([[-2.1860046, -2.9548128]], dtype=float32)

time = 40457	action = 0	current_phase = 0	next_phase = 1	reward = 0.065734	array([[-2.7571144, -3.0796106]], dtype=float32)

time = 40462	action = 1	current_phase = 0	next_phase = 1	reward = -1.518028	array([[-6.4869833, -3.5542338]], dtype=float32)

time = 40470	action = 1	current_phase = 1	next_phase = 0	reward = -0.943640	array([[-3.8983803, -2.4489603]], dtype=float32)

time = 40478	action = 0	current_phase = 0	next_phase = 1	reward = -0.061632	array([[-1.8182642, -2.8103073]], dtype=float32)

time = 40483	action = 0	current_phase = 0	next_phase = 1	reward = 0.011068	array([[-2.0541883, -2.8721428]], dtype=float32)

time = 40488	action = 0	current_phase = 0	next_phase = 1	reward = 0.070478	array([[-2.8686924, -3.0745037]], dtype=float32)

time = 40493	action = 1	current_phase = 0	next_phase = 1	reward = -1.263240	array([[-6.4409933, -3.3646104]], dtype=float32)

time = 40501	action = 1	current_phase = 1	next_phase = 0	reward = -1.327634	array([[-4.0723333, -2.5003526]], dtype=float32)

time = 40509	action = 0	current_phase = 0	next_phase = 1	reward = 0.252828	array([[-1.3238872, -2.2180924]], dtype=float32)

time = 40514	action = 0	current_phase = 0	next_phase = 1	reward = 0.036603	array([[-1.7919773, -2.788357 ]], dtype=float32)

time = 40519	action = 1	current_phase = 0	next_phase = 1	reward = -0.650464	array([[-2.393715 , -2.2678115]], dtype=float32)

time = 40527	action = 1	current_phase = 1	next_phase = 0	reward = -0.923772	array([[-3.4374847, -1.9360192]], dtype=float32)

time = 40535	action = 0	current_phase = 0	next_phase = 1	reward = 0.186073	array([[-1.5088345, -2.9159966]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0225 - val_loss: 0.0102

Epoch 2/50

 - 3s - loss: 0.0236 - val_loss: 0.0090

Epoch 3/50

 - 3s - loss: 0.0211 - val_loss: 0.0122

Epoch 4/50

 - 3s - loss: 0.0195 - val_loss: 0.0100

Epoch 5/50

 - 3s - loss: 0.0190 - val_loss: 0.0086

Epoch 6/50

 - 3s - loss: 0.0207 - val_loss: 0.0101

Epoch 7/50

 - 3s - loss: 0.0203 - val_loss: 0.0092

Epoch 8/50

 - 3s - loss: 0.0235 - val_loss: 0.0099

Epoch 9/50

 - 3s - loss: 0.0167 - val_loss: 0.0112

Epoch 10/50

 - 3s - loss: 0.0232 - val_loss: 0.0091

Epoch 11/50

 - 3s - loss: 0.0157 - val_loss: 0.0118

Epoch 12/50

 - 3s - loss: 0.0187 - val_loss: 0.0115

Epoch 13/50

 - 3s - loss: 0.0213 - val_loss: 0.0153

Epoch 14/50

 - 3s - loss: 0.0174 - val_loss: 0.0141

Epoch 15/50

 - 3s - loss: 0.0164 - val_loss: 0.0096

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 40540	action = 0	current_phase = 0	next_phase = 1	reward = -0.010600	array([[-1.8330059, -2.202755 ]], dtype=float32)

time = 40545	action = 0	current_phase = 0	next_phase = 1	reward = -0.224493	array([[-2.2822402, -2.917789 ]], dtype=float32)

time = 40550	action = 1	current_phase = 0	next_phase = 1	reward = -1.041484	array([[-5.4301214, -2.73561  ]], dtype=float32)

time = 40558	action = 1	current_phase = 1	next_phase = 0	reward = -0.711654	array([[-3.6350813, -2.0803275]], dtype=float32)

time = 40566	action = 0	current_phase = 0	next_phase = 1	reward = -0.088093	array([[-1.8370585, -2.8179488]], dtype=float32)

time = 40571	action = 0	current_phase = 0	next_phase = 1	reward = -0.003640	array([[-2.0241044, -2.8395333]], dtype=float32)

time = 40576	action = 0	current_phase = 0	next_phase = 1	reward = 0.056698	array([[-2.5955458, -3.0329623]], dtype=float32)

time = 40581	action = 1	current_phase = 0	next_phase = 1	reward = -1.502238	array([[-6.4831376, -3.2448134]], dtype=float32)

time = 40589	action = 1	current_phase = 1	next_phase = 0	reward = -1.118655	array([[-3.7973166, -2.2048264]], dtype=float32)

time = 40597	action = 0	current_phase = 0	next_phase = 1	reward = 0.207412	array([[-1.8052766, -2.8123593]], dtype=float32)

time = 40602	action = 0	current_phase = 0	next_phase = 1	reward = -0.014069	array([[-2.175914 , -2.9079475]], dtype=float32)

time = 40607	action = 0	current_phase = 0	next_phase = 1	reward = 0.047961	array([[-2.803222 , -3.0529494]], dtype=float32)

time = 40612	action = 1	current_phase = 0	next_phase = 1	reward = -1.578076	array([[-6.503368, -3.419393]], dtype=float32)

time = 40620	action = 1	current_phase = 1	next_phase = 0	reward = -1.009422	array([[-3.9410172, -2.4014285]], dtype=float32)

time = 40628	action = 0	current_phase = 0	next_phase = 1	reward = -0.056729	array([[-1.8359466, -2.7811718]], dtype=float32)

time = 40633	action = 0	current_phase = 0	next_phase = 1	reward = 0.009173	array([[-2.254585 , -2.9351091]], dtype=float32)

time = 40638	action = 0	current_phase = 0	next_phase = 1	reward = 0.083621	array([[-2.8865907, -3.1292105]], dtype=float32)

time = 40643	action = 1	current_phase = 0	next_phase = 1	reward = -1.931418	array([[-6.4256964, -3.6219144]], dtype=float32)

time = 40651	action = 1	current_phase = 1	next_phase = 0	reward = -0.977886	array([[-4.318036 , -2.4866977]], dtype=float32)

time = 40659	action = 0	current_phase = 0	next_phase = 1	reward = -0.035566	array([[-1.4947605, -2.183824 ]], dtype=float32)

time = 40664	action = 0	current_phase = 0	next_phase = 1	reward = 0.031357	array([[-1.8399367, -2.7781398]], dtype=float32)

time = 40669	action = 1	current_phase = 0	next_phase = 1	reward = -0.699077	array([[-2.3924236, -2.2607613]], dtype=float32)

time = 40677	action = 1	current_phase = 1	next_phase = 0	reward = -0.916267	array([[-3.4398136, -1.7712191]], dtype=float32)

time = 40685	action = 0	current_phase = 0	next_phase = 1	reward = -0.089344	array([[-1.5442576, -2.9450493]], dtype=float32)

time = 40690	action = 0	current_phase = 0	next_phase = 1	reward = 0.263185	array([[-1.7356963, -2.192132 ]], dtype=float32)

time = 40695	action = 0	current_phase = 0	next_phase = 1	reward = 0.041031	array([[-2.2847607, -2.922164 ]], dtype=float32)

time = 40700	action = 1	current_phase = 0	next_phase = 1	reward = -1.201585	array([[-5.455558, -2.784006]], dtype=float32)

time = 40708	action = 1	current_phase = 1	next_phase = 0	reward = -0.698526	array([[-3.6273313, -2.0803292]], dtype=float32)

time = 40716	action = 0	current_phase = 0	next_phase = 1	reward = -0.074775	array([[-1.7946587, -2.845799 ]], dtype=float32)

time = 40721	action = 0	current_phase = 0	next_phase = 1	reward = 0.006313	array([[-2.0452433, -2.8621001]], dtype=float32)

time = 40726	action = 0	current_phase = 0	next_phase = 1	reward = 0.068387	array([[-2.704965 , -3.0226984]], dtype=float32)

time = 40731	action = 1	current_phase = 0	next_phase = 1	reward = -1.567808	array([[-6.507801 , -3.2840805]], dtype=float32)

time = 40739	action = 1	current_phase = 1	next_phase = 0	reward = -0.784332	array([[-3.7801352, -2.165115 ]], dtype=float32)

time = 40747	action = 0	current_phase = 0	next_phase = 1	reward = -0.062952	array([[-1.8116114, -2.828339 ]], dtype=float32)

time = 40752	action = 0	current_phase = 0	next_phase = 1	reward = 0.003644	array([[-2.2589931, -2.9324422]], dtype=float32)

time = 40757	action = 0	current_phase = 0	next_phase = 1	reward = 0.075138	array([[-2.8241467, -3.0845594]], dtype=float32)

time = 40762	action = 1	current_phase = 0	next_phase = 1	reward = -1.647852	array([[-6.484851 , -3.5704107]], dtype=float32)

time = 40770	action = 1	current_phase = 1	next_phase = 0	reward = -0.894373	array([[-3.998877 , -2.4479353]], dtype=float32)

time = 40778	action = 0	current_phase = 0	next_phase = 1	reward = -0.041575	array([[-1.9076469, -2.810967 ]], dtype=float32)

time = 40783	action = 0	current_phase = 0	next_phase = 1	reward = 0.039651	array([[-2.1959963, -2.9160066]], dtype=float32)

time = 40788	action = 0	current_phase = 0	next_phase = 1	reward = 0.077422	array([[-2.7875829, -3.1718798]], dtype=float32)

time = 40793	action = 1	current_phase = 0	next_phase = 1	reward = -1.455090	array([[-6.4423027, -3.3151155]], dtype=float32)

time = 40801	action = 1	current_phase = 1	next_phase = 0	reward = -1.150148	array([[-4.0995154, -2.454368 ]], dtype=float32)

time = 40809	action = 0	current_phase = 0	next_phase = 1	reward = -0.035156	array([[-1.4462867, -2.1731915]], dtype=float32)

time = 40814	action = 0	current_phase = 0	next_phase = 1	reward = 0.045943	array([[-1.924063 , -2.8237722]], dtype=float32)

time = 40819	action = 0	current_phase = 0	next_phase = 1	reward = 0.055880	array([[-2.3275223, -2.3382149]], dtype=float32)

time = 40824	action = 1	current_phase = 0	next_phase = 1	reward = -1.006880	array([[-6.368232 , -2.9597545]], dtype=float32)

time = 40832	action = 1	current_phase = 1	next_phase = 0	reward = -0.775462	array([[-3.7471323, -2.3097343]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0230 - val_loss: 0.0127

Epoch 2/50

 - 3s - loss: 0.0266 - val_loss: 0.0126

Epoch 3/50

 - 3s - loss: 0.0265 - val_loss: 0.0134

Epoch 4/50

 - 3s - loss: 0.0245 - val_loss: 0.0151

Epoch 5/50

 - 3s - loss: 0.0238 - val_loss: 0.0121

Epoch 6/50

 - 3s - loss: 0.0224 - val_loss: 0.0176

Epoch 7/50

 - 3s - loss: 0.0208 - val_loss: 0.0127

Epoch 8/50

 - 3s - loss: 0.0242 - val_loss: 0.0119

Epoch 9/50

 - 3s - loss: 0.0228 - val_loss: 0.0135

Epoch 10/50

 - 3s - loss: 0.0207 - val_loss: 0.0144

Epoch 11/50

 - 3s - loss: 0.0212 - val_loss: 0.0144

Epoch 12/50

 - 3s - loss: 0.0207 - val_loss: 0.0132

Epoch 13/50

 - 3s - loss: 0.0204 - val_loss: 0.0134

Epoch 14/50

 - 3s - loss: 0.0205 - val_loss: 0.0123

Epoch 15/50

 - 3s - loss: 0.0220 - val_loss: 0.0130

Epoch 16/50

 - 3s - loss: 0.0340 - val_loss: 0.0143

Epoch 17/50

 - 3s - loss: 0.0184 - val_loss: 0.0128

Epoch 18/50

 - 3s - loss: 0.0212 - val_loss: 0.0132

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 40840	action = 0	current_phase = 0	next_phase = 1	reward = -0.042681	array([[-1.7157204, -2.2145739]], dtype=float32)

time = 40845	action = 0	current_phase = 0	next_phase = 1	reward = -0.255905	array([[-2.0774212, -2.9030006]], dtype=float32)

time = 40850	action = 1	current_phase = 0	next_phase = 1	reward = -0.977239	array([[-5.4178343, -2.639715 ]], dtype=float32)

time = 40858	action = 1	current_phase = 1	next_phase = 0	reward = -0.644824	array([[-3.5203538, -2.0550964]], dtype=float32)

time = 40866	action = 0	current_phase = 0	next_phase = 1	reward = -0.069227	array([[-1.6134678, -2.7910044]], dtype=float32)

time = 40871	action = 0	current_phase = 0	next_phase = 1	reward = -0.000642	array([[-2.0775938, -2.8677053]], dtype=float32)

time = 40876	action = 0	current_phase = 0	next_phase = 1	reward = 0.065841	array([[-2.517241 , -2.9959264]], dtype=float32)

time = 40881	action = 1	current_phase = 0	next_phase = 1	reward = -1.494822	array([[-6.5497656, -3.2444963]], dtype=float32)

time = 40889	action = 1	current_phase = 1	next_phase = 0	reward = -0.842792	array([[-3.7571485, -2.2656693]], dtype=float32)

time = 40897	action = 0	current_phase = 0	next_phase = 1	reward = -0.076281	array([[-1.7400709, -2.7851057]], dtype=float32)

time = 40902	action = 0	current_phase = 0	next_phase = 1	reward = 0.010695	array([[-2.2079246, -2.9435148]], dtype=float32)

time = 40907	action = 0	current_phase = 0	next_phase = 1	reward = 0.079676	array([[-2.779121, -3.06925 ]], dtype=float32)

time = 40912	action = 1	current_phase = 0	next_phase = 1	reward = -1.601692	array([[-6.5158653, -3.434183 ]], dtype=float32)

time = 40920	action = 1	current_phase = 1	next_phase = 0	reward = -1.001150	array([[-3.9876187, -2.4552991]], dtype=float32)

time = 40928	action = 0	current_phase = 0	next_phase = 1	reward = -0.050719	array([[-1.7360716, -2.7907922]], dtype=float32)

time = 40933	action = 0	current_phase = 0	next_phase = 1	reward = 0.019501	array([[-2.1518593, -2.906123 ]], dtype=float32)

time = 40938	action = 0	current_phase = 0	next_phase = 1	reward = 0.079948	array([[-2.8220317, -3.1286988]], dtype=float32)

time = 40943	action = 1	current_phase = 0	next_phase = 1	reward = -1.986897	array([[-6.4303007, -3.6832843]], dtype=float32)

time = 40951	action = 1	current_phase = 1	next_phase = 0	reward = -1.455039	array([[-4.2684984, -2.4896407]], dtype=float32)

time = 40959	action = 0	current_phase = 0	next_phase = 1	reward = 0.266206	array([[-1.2315176, -2.1463585]], dtype=float32)

time = 40964	action = 0	current_phase = 0	next_phase = 1	reward = 0.049768	array([[-1.6668568, -2.7499237]], dtype=float32)

time = 40969	action = 1	current_phase = 0	next_phase = 1	reward = -0.650510	array([[-2.390019 , -2.2670095]], dtype=float32)

time = 40977	action = 1	current_phase = 1	next_phase = 0	reward = -0.641609	array([[-3.3741095, -1.9503229]], dtype=float32)

time = 40985	action = 0	current_phase = 0	next_phase = 1	reward = -0.097921	array([[-1.3780756, -2.9693863]], dtype=float32)

time = 40990	action = 0	current_phase = 0	next_phase = 1	reward = -0.559829	array([[-1.8434787, -2.1895475]], dtype=float32)

time = 40995	action = 0	current_phase = 0	next_phase = 1	reward = 0.626937	array([[-2.0782948, -2.8490784]], dtype=float32)

time = 41000	action = 1	current_phase = 0	next_phase = 1	reward = -1.320265	array([[-5.4771156, -2.912969 ]], dtype=float32)

time = 41008	action = 1	current_phase = 1	next_phase = 0	reward = -0.716028	array([[-3.6781616, -2.1787958]], dtype=float32)

time = 41016	action = 0	current_phase = 0	next_phase = 1	reward = -0.082915	array([[-1.6809793, -2.7832096]], dtype=float32)

time = 41021	action = 0	current_phase = 0	next_phase = 1	reward = 0.000846	array([[-2.0409603, -2.8523111]], dtype=float32)

time = 41026	action = 0	current_phase = 0	next_phase = 1	reward = 0.068765	array([[-2.5307453, -2.9869945]], dtype=float32)

time = 41031	action = 1	current_phase = 0	next_phase = 1	reward = -1.451413	array([[-6.531211 , -3.2294114]], dtype=float32)

time = 41039	action = 1	current_phase = 1	next_phase = 0	reward = -0.833947	array([[-3.7819078, -2.2886643]], dtype=float32)

time = 41047	action = 0	current_phase = 0	next_phase = 1	reward = -0.073536	array([[-1.7482319, -2.796436 ]], dtype=float32)

time = 41052	action = 0	current_phase = 0	next_phase = 1	reward = 0.000566	array([[-2.1982102, -2.9234018]], dtype=float32)

time = 41057	action = 0	current_phase = 0	next_phase = 1	reward = 0.057606	array([[-2.7916245, -3.0514858]], dtype=float32)

time = 41062	action = 1	current_phase = 0	next_phase = 1	reward = -1.601407	array([[-6.4804373, -3.4165108]], dtype=float32)

time = 41070	action = 1	current_phase = 1	next_phase = 0	reward = -0.881283	array([[-3.854194, -2.395422]], dtype=float32)

time = 41078	action = 0	current_phase = 0	next_phase = 1	reward = -0.049248	array([[-1.8002028, -2.787125 ]], dtype=float32)

time = 41083	action = 0	current_phase = 0	next_phase = 1	reward = 0.023057	array([[-2.2345583, -2.934926 ]], dtype=float32)

time = 41088	action = 0	current_phase = 0	next_phase = 1	reward = 0.060230	array([[-2.7751098, -3.106678 ]], dtype=float32)

time = 41093	action = 1	current_phase = 0	next_phase = 1	reward = -1.898140	array([[-6.402932 , -3.7483132]], dtype=float32)

time = 41101	action = 1	current_phase = 1	next_phase = 0	reward = -0.966067	array([[-4.1991925, -2.4245872]], dtype=float32)

time = 41109	action = 0	current_phase = 0	next_phase = 1	reward = -0.029332	array([[-1.3868883, -2.1534095]], dtype=float32)

time = 41114	action = 0	current_phase = 0	next_phase = 1	reward = 0.030237	array([[-1.7006048, -2.7811475]], dtype=float32)

time = 41119	action = 1	current_phase = 0	next_phase = 1	reward = -0.642603	array([[-2.2315202, -2.218097 ]], dtype=float32)

time = 41127	action = 1	current_phase = 1	next_phase = 0	reward = -0.913224	array([[-3.3583696, -1.9577031]], dtype=float32)

time = 41135	action = 0	current_phase = 0	next_phase = 1	reward = 0.183117	array([[-1.360048 , -3.0471454]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0213 - val_loss: 0.0120

Epoch 2/50

 - 3s - loss: 0.0198 - val_loss: 0.0119

Epoch 3/50

 - 3s - loss: 0.0187 - val_loss: 0.0109

Epoch 4/50

 - 3s - loss: 0.0186 - val_loss: 0.0101

Epoch 5/50

 - 3s - loss: 0.0197 - val_loss: 0.0114

Epoch 6/50

 - 3s - loss: 0.0189 - val_loss: 0.0119

Epoch 7/50

 - 3s - loss: 0.0168 - val_loss: 0.0128

Epoch 8/50

 - 3s - loss: 0.0199 - val_loss: 0.0103

Epoch 9/50

 - 3s - loss: 0.0171 - val_loss: 0.0127

Epoch 10/50

 - 3s - loss: 0.0194 - val_loss: 0.0111

Epoch 11/50

 - 3s - loss: 0.0148 - val_loss: 0.0138

Epoch 12/50

 - 3s - loss: 0.0177 - val_loss: 0.0112

Epoch 13/50

 - 3s - loss: 0.0191 - val_loss: 0.0117

Epoch 14/50

 - 3s - loss: 0.0178 - val_loss: 0.0154

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 41140	action = 0	current_phase = 0	next_phase = 1	reward = -0.031963	array([[-1.8862408, -2.1652718]], dtype=float32)

time = 41145	action = 0	current_phase = 0	next_phase = 1	reward = 0.027200	array([[-2.1919503, -2.9174273]], dtype=float32)

time = 41150	action = 1	current_phase = 0	next_phase = 1	reward = -1.320691	array([[-5.5198736, -3.0154321]], dtype=float32)

time = 41158	action = 1	current_phase = 1	next_phase = 0	reward = -0.696374	array([[-3.8097858, -2.225274 ]], dtype=float32)

time = 41166	action = 0	current_phase = 0	next_phase = 1	reward = -0.090713	array([[-1.6816096, -2.7886853]], dtype=float32)

time = 41171	action = 0	current_phase = 0	next_phase = 1	reward = -0.016047	array([[-2.1744046, -2.8935618]], dtype=float32)

time = 41176	action = 0	current_phase = 0	next_phase = 1	reward = 0.043044	array([[-2.6199229, -3.0673985]], dtype=float32)

time = 41181	action = 1	current_phase = 0	next_phase = 1	reward = -1.492212	array([[-6.505703, -3.240583]], dtype=float32)

time = 41189	action = 1	current_phase = 1	next_phase = 0	reward = -1.052689	array([[-3.8044815, -2.2558532]], dtype=float32)

time = 41197	action = 0	current_phase = 0	next_phase = 1	reward = 0.211760	array([[-1.8195044, -2.7888844]], dtype=float32)

time = 41202	action = 0	current_phase = 0	next_phase = 1	reward = 0.014549	array([[-2.2515268, -2.918417 ]], dtype=float32)

time = 41207	action = 0	current_phase = 0	next_phase = 1	reward = 0.084683	array([[-2.740829 , -3.2038307]], dtype=float32)

time = 41212	action = 1	current_phase = 0	next_phase = 1	reward = -1.656194	array([[-6.475989 , -3.5948112]], dtype=float32)

time = 41220	action = 1	current_phase = 1	next_phase = 0	reward = -0.961441	array([[-4.038007 , -2.4863958]], dtype=float32)

time = 41228	action = 0	current_phase = 0	next_phase = 1	reward = -0.072675	array([[-1.9248619, -2.8367171]], dtype=float32)

time = 41233	action = 0	current_phase = 0	next_phase = 1	reward = 0.023749	array([[-2.2959545, -2.9486372]], dtype=float32)

time = 41238	action = 0	current_phase = 0	next_phase = 1	reward = 0.081961	array([[-2.7426326, -3.2552738]], dtype=float32)

time = 41243	action = 1	current_phase = 0	next_phase = 1	reward = -1.307963	array([[-6.453532, -3.679512]], dtype=float32)

time = 41251	action = 1	current_phase = 1	next_phase = 0	reward = -1.336051	array([[-4.135049 , -2.4327533]], dtype=float32)

time = 41259	action = 0	current_phase = 0	next_phase = 1	reward = 0.248001	array([[-1.4496602, -2.1438353]], dtype=float32)

time = 41264	action = 0	current_phase = 0	next_phase = 1	reward = 0.017433	array([[-1.8258551, -2.7779958]], dtype=float32)

time = 41269	action = 1	current_phase = 0	next_phase = 1	reward = -0.701726	array([[-2.3454404, -2.2323194]], dtype=float32)

time = 41277	action = 1	current_phase = 1	next_phase = 0	reward = -0.654108	array([[-3.6118665, -2.0845337]], dtype=float32)

time = 41285	action = 0	current_phase = 0	next_phase = 1	reward = -0.104772	array([[-1.5168812, -2.9361887]], dtype=float32)

time = 41290	action = 0	current_phase = 0	next_phase = 1	reward = -0.299886	array([[-1.8845035, -2.1482623]], dtype=float32)

time = 41295	action = 0	current_phase = 0	next_phase = 1	reward = 0.329800	array([[-2.1712294, -2.8808827]], dtype=float32)

time = 41300	action = 1	current_phase = 0	next_phase = 1	reward = -1.315366	array([[-5.4805746, -2.9241214]], dtype=float32)

time = 41308	action = 1	current_phase = 1	next_phase = 0	reward = -1.010550	array([[-3.8409266, -2.198267 ]], dtype=float32)

time = 41316	action = 0	current_phase = 0	next_phase = 1	reward = 0.202727	array([[-1.66178  , -2.8187206]], dtype=float32)

time = 41321	action = 0	current_phase = 0	next_phase = 1	reward = -0.027095	array([[-2.1569343, -2.8756778]], dtype=float32)

time = 41326	action = 0	current_phase = 0	next_phase = 1	reward = 0.045413	array([[-2.5881996, -3.0408864]], dtype=float32)

time = 41331	action = 1	current_phase = 0	next_phase = 1	reward = -1.446524	array([[-6.554788 , -3.3202384]], dtype=float32)

time = 41339	action = 1	current_phase = 1	next_phase = 0	reward = -0.791754	array([[-3.8535614, -2.30655  ]], dtype=float32)

time = 41347	action = 0	current_phase = 0	next_phase = 1	reward = -0.046360	array([[-1.8685877, -2.8124077]], dtype=float32)

time = 41352	action = 0	current_phase = 0	next_phase = 1	reward = 0.023104	array([[-2.2823963, -2.9338722]], dtype=float32)

time = 41357	action = 0	current_phase = 0	next_phase = 1	reward = 0.070663	array([[-2.7818544, -3.0808253]], dtype=float32)

time = 41362	action = 1	current_phase = 0	next_phase = 1	reward = -1.680685	array([[-6.4835606, -3.67145  ]], dtype=float32)

time = 41370	action = 1	current_phase = 1	next_phase = 0	reward = -0.946842	array([[-4.0009933, -2.4510345]], dtype=float32)

time = 41378	action = 0	current_phase = 0	next_phase = 1	reward = -0.054469	array([[-1.8545625, -2.8045292]], dtype=float32)

time = 41383	action = 0	current_phase = 0	next_phase = 1	reward = 0.025178	array([[-2.2155788, -2.9080858]], dtype=float32)

time = 41388	action = 0	current_phase = 0	next_phase = 1	reward = 0.077634	array([[-2.6964538, -3.1523085]], dtype=float32)

time = 41393	action = 1	current_phase = 0	next_phase = 1	reward = -1.887707	array([[-6.418964 , -3.7873302]], dtype=float32)

time = 41401	action = 1	current_phase = 1	next_phase = 0	reward = -1.082443	array([[-4.3236556, -2.4323273]], dtype=float32)

time = 41409	action = 0	current_phase = 0	next_phase = 1	reward = -0.023327	array([[-1.4082086, -2.1268327]], dtype=float32)

time = 41414	action = 0	current_phase = 0	next_phase = 1	reward = 0.048270	array([[-1.8459617, -2.790263 ]], dtype=float32)

time = 41419	action = 1	current_phase = 0	next_phase = 1	reward = -0.757543	array([[-2.3795362, -2.2689548]], dtype=float32)

time = 41427	action = 1	current_phase = 1	next_phase = 0	reward = -1.193001	array([[-3.8136945, -2.145842 ]], dtype=float32)

time = 41435	action = 0	current_phase = 0	next_phase = 1	reward = 0.472837	array([[-1.480773 , -2.9754777]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0236 - val_loss: 0.0095

Epoch 2/50

 - 3s - loss: 0.0273 - val_loss: 0.0105

Epoch 3/50

 - 3s - loss: 0.0235 - val_loss: 0.0108

Epoch 4/50

 - 3s - loss: 0.0236 - val_loss: 0.0103

Epoch 5/50

 - 3s - loss: 0.0200 - val_loss: 0.0101

Epoch 6/50

 - 3s - loss: 0.0240 - val_loss: 0.0120

Epoch 7/50

 - 3s - loss: 0.0211 - val_loss: 0.0111

Epoch 8/50

 - 3s - loss: 0.0186 - val_loss: 0.0104

Epoch 9/50

 - 3s - loss: 0.0222 - val_loss: 0.0111

Epoch 10/50

 - 3s - loss: 0.0185 - val_loss: 0.0114

Epoch 11/50

 - 3s - loss: 0.0268 - val_loss: 0.0126

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 41440	action = 0	current_phase = 0	next_phase = 1	reward = -0.008287	array([[-1.9236885, -2.1121254]], dtype=float32)

time = 41445	action = 0	current_phase = 0	next_phase = 1	reward = 0.055525	array([[-2.2883928, -2.9204428]], dtype=float32)

time = 41450	action = 1	current_phase = 0	next_phase = 1	reward = -1.384629	array([[-5.473652 , -2.8406544]], dtype=float32)

time = 41458	action = 1	current_phase = 1	next_phase = 0	reward = -0.709277	array([[-3.7841103, -2.2137182]], dtype=float32)

time = 41466	action = 0	current_phase = 0	next_phase = 1	reward = -0.077041	array([[-1.7169912, -2.7828066]], dtype=float32)

time = 41471	action = 0	current_phase = 0	next_phase = 1	reward = -0.003949	array([[-2.1505203, -2.8552816]], dtype=float32)

time = 41476	action = 0	current_phase = 0	next_phase = 1	reward = 0.047612	array([[-2.5270858, -2.9973783]], dtype=float32)

time = 41481	action = 1	current_phase = 0	next_phase = 1	reward = -1.565924	array([[-6.5321174, -3.2142053]], dtype=float32)

time = 41489	action = 1	current_phase = 1	next_phase = 0	reward = -0.804250	array([[-3.8621123, -2.3105927]], dtype=float32)

time = 41497	action = 0	current_phase = 0	next_phase = 1	reward = -0.059649	array([[-1.8345222, -2.7798557]], dtype=float32)

time = 41502	action = 0	current_phase = 0	next_phase = 1	reward = 0.023107	array([[-2.2518063, -2.9260511]], dtype=float32)

time = 41507	action = 0	current_phase = 0	next_phase = 1	reward = 0.069017	array([[-2.7505689, -3.0623424]], dtype=float32)

time = 41512	action = 1	current_phase = 0	next_phase = 1	reward = -1.712849	array([[-6.486912, -3.454392]], dtype=float32)

time = 41520	action = 1	current_phase = 1	next_phase = 0	reward = -0.995996	array([[-3.9771674, -2.4610472]], dtype=float32)

time = 41528	action = 0	current_phase = 0	next_phase = 1	reward = -0.045541	array([[-1.8745227, -2.7865467]], dtype=float32)

time = 41533	action = 0	current_phase = 0	next_phase = 1	reward = 0.019272	array([[-2.1968174, -2.8863187]], dtype=float32)

time = 41538	action = 0	current_phase = 0	next_phase = 1	reward = 0.081123	array([[-2.772696 , -3.1088405]], dtype=float32)

time = 41543	action = 1	current_phase = 0	next_phase = 1	reward = -1.884701	array([[-6.4033546, -3.7667058]], dtype=float32)

time = 41551	action = 1	current_phase = 1	next_phase = 0	reward = -1.083842	array([[-4.31951  , -2.3895032]], dtype=float32)

time = 41559	action = 0	current_phase = 0	next_phase = 1	reward = -0.021212	array([[-1.3688277, -2.0766773]], dtype=float32)

time = 41564	action = 0	current_phase = 0	next_phase = 1	reward = 0.052530	array([[-1.7503772, -2.7598217]], dtype=float32)

time = 41569	action = 1	current_phase = 0	next_phase = 1	reward = -0.707389	array([[-2.3440533, -2.1536164]], dtype=float32)

time = 41577	action = 1	current_phase = 1	next_phase = 0	reward = -1.196523	array([[-3.5594497, -2.1113126]], dtype=float32)

time = 41585	action = 0	current_phase = 0	next_phase = 1	reward = 0.465528	array([[-1.3659345, -2.9166207]], dtype=float32)

time = 41590	action = 0	current_phase = 0	next_phase = 1	reward = -0.017038	array([[-1.9438965, -2.096147 ]], dtype=float32)

time = 41595	action = 0	current_phase = 0	next_phase = 1	reward = 0.050933	array([[-2.2381544, -2.9066448]], dtype=float32)

time = 41600	action = 1	current_phase = 0	next_phase = 1	reward = -1.407837	array([[-5.5014215, -2.8014781]], dtype=float32)

time = 41608	action = 1	current_phase = 1	next_phase = 0	reward = -0.992190	array([[-3.827945 , -2.2569923]], dtype=float32)

time = 41616	action = 0	current_phase = 0	next_phase = 1	reward = 0.200250	array([[-1.7419235, -2.781498 ]], dtype=float32)

time = 41621	action = 0	current_phase = 0	next_phase = 1	reward = -0.010818	array([[-2.1220517, -2.8449438]], dtype=float32)

time = 41626	action = 0	current_phase = 0	next_phase = 1	reward = 0.055183	array([[-2.4806263, -2.9692502]], dtype=float32)

time = 41631	action = 1	current_phase = 0	next_phase = 1	reward = -1.494327	array([[-6.52494 , -3.170881]], dtype=float32)

time = 41639	action = 1	current_phase = 1	next_phase = 0	reward = -1.118222	array([[-3.9106126, -2.380159 ]], dtype=float32)

time = 41647	action = 0	current_phase = 0	next_phase = 1	reward = 0.211263	array([[-1.7875046, -2.7614949]], dtype=float32)

time = 41652	action = 0	current_phase = 0	next_phase = 1	reward = 0.010005	array([[-2.23106 , -2.927083]], dtype=float32)

time = 41657	action = 0	current_phase = 0	next_phase = 1	reward = 0.063979	array([[-2.7686863, -3.0868237]], dtype=float32)

time = 41662	action = 1	current_phase = 0	next_phase = 1	reward = -1.614295	array([[-6.4966607, -3.4581728]], dtype=float32)

time = 41670	action = 1	current_phase = 1	next_phase = 0	reward = -0.945014	array([[-3.9528832, -2.44974  ]], dtype=float32)

time = 41678	action = 0	current_phase = 0	next_phase = 1	reward = -0.042362	array([[-1.7645063, -2.7591143]], dtype=float32)

time = 41683	action = 0	current_phase = 0	next_phase = 1	reward = 0.035177	array([[-2.2843008, -2.9254305]], dtype=float32)

time = 41688	action = 0	current_phase = 0	next_phase = 1	reward = 0.084590	array([[-2.8008988, -3.070358 ]], dtype=float32)

time = 41693	action = 1	current_phase = 0	next_phase = 1	reward = -1.889440	array([[-6.419836 , -3.7656465]], dtype=float32)

time = 41701	action = 1	current_phase = 1	next_phase = 0	reward = -1.339522	array([[-4.440545 , -2.3947415]], dtype=float32)

time = 41709	action = 0	current_phase = 0	next_phase = 1	reward = 0.269579	array([[-1.4464101, -2.0721989]], dtype=float32)

time = 41714	action = 0	current_phase = 0	next_phase = 1	reward = 0.036918	array([[-1.7623503, -2.7502742]], dtype=float32)

time = 41719	action = 1	current_phase = 0	next_phase = 1	reward = -0.705693	array([[-2.3903685, -2.152166 ]], dtype=float32)

time = 41727	action = 1	current_phase = 1	next_phase = 0	reward = -1.141744	array([[-3.6048203, -2.1361728]], dtype=float32)

time = 41735	action = 0	current_phase = 0	next_phase = 1	reward = 0.471189	array([[-1.3290743, -2.9387505]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0237 - val_loss: 0.0171

Epoch 2/50

 - 3s - loss: 0.0223 - val_loss: 0.0147

Epoch 3/50

 - 3s - loss: 0.0240 - val_loss: 0.0146

Epoch 4/50

 - 3s - loss: 0.0207 - val_loss: 0.0168

Epoch 5/50

 - 3s - loss: 0.0255 - val_loss: 0.0137

Epoch 6/50

 - 3s - loss: 0.0227 - val_loss: 0.0155

Epoch 7/50

 - 3s - loss: 0.0226 - val_loss: 0.0169

Epoch 8/50

 - 3s - loss: 0.0207 - val_loss: 0.0144

Epoch 9/50

 - 3s - loss: 0.0196 - val_loss: 0.0154

Epoch 10/50

 - 3s - loss: 0.0192 - val_loss: 0.0150

Epoch 11/50

 - 3s - loss: 0.0183 - val_loss: 0.0155

Epoch 12/50

 - 3s - loss: 0.0224 - val_loss: 0.0151

Epoch 13/50

 - 3s - loss: 0.0228 - val_loss: 0.0162

Epoch 14/50

 - 3s - loss: 0.0195 - val_loss: 0.0175

Epoch 15/50

 - 3s - loss: 0.0182 - val_loss: 0.0166

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 41740	action = 0	current_phase = 0	next_phase = 1	reward = -0.028859	array([[-1.8056867, -2.0836341]], dtype=float32)

time = 41745	action = 0	current_phase = 0	next_phase = 1	reward = 0.032468	array([[-2.1470914, -2.8736746]], dtype=float32)

time = 41750	action = 1	current_phase = 0	next_phase = 1	reward = -1.348591	array([[-5.5346003, -2.8289201]], dtype=float32)

time = 41758	action = 1	current_phase = 1	next_phase = 0	reward = -0.713834	array([[-3.8257418, -2.1885073]], dtype=float32)

time = 41766	action = 0	current_phase = 0	next_phase = 1	reward = -0.086380	array([[-1.7627774, -2.77013  ]], dtype=float32)

time = 41771	action = 0	current_phase = 0	next_phase = 1	reward = 0.004921	array([[-2.1311638, -2.8706522]], dtype=float32)

time = 41776	action = 0	current_phase = 0	next_phase = 1	reward = 0.082847	array([[-2.5771968, -3.0102663]], dtype=float32)

time = 41781	action = 1	current_phase = 0	next_phase = 1	reward = -1.500750	array([[-6.609318, -3.262389]], dtype=float32)

time = 41789	action = 1	current_phase = 1	next_phase = 0	reward = -0.879533	array([[-3.8970952, -2.2953224]], dtype=float32)

time = 41797	action = 0	current_phase = 0	next_phase = 1	reward = -0.063902	array([[-1.8649759, -2.7879808]], dtype=float32)

time = 41802	action = 0	current_phase = 0	next_phase = 1	reward = 0.021495	array([[-2.2368493, -2.9346964]], dtype=float32)

time = 41807	action = 0	current_phase = 0	next_phase = 1	reward = 0.081230	array([[-2.817245 , -3.0882266]], dtype=float32)

time = 41812	action = 1	current_phase = 0	next_phase = 1	reward = -1.726711	array([[-6.5296435, -3.5721276]], dtype=float32)

time = 41820	action = 1	current_phase = 1	next_phase = 0	reward = -1.071724	array([[-4.054815, -2.428152]], dtype=float32)

time = 41828	action = 0	current_phase = 0	next_phase = 1	reward = -0.050589	array([[-1.8351762, -2.7752001]], dtype=float32)

time = 41833	action = 0	current_phase = 0	next_phase = 1	reward = 0.027144	array([[-2.263682, -2.911382]], dtype=float32)

time = 41838	action = 0	current_phase = 0	next_phase = 1	reward = 0.078040	array([[-2.7784445, -3.204018 ]], dtype=float32)

time = 41843	action = 1	current_phase = 0	next_phase = 1	reward = -1.992792	array([[-6.556633 , -3.4252656]], dtype=float32)

time = 41851	action = 1	current_phase = 1	next_phase = 0	reward = -1.037793	array([[-4.4176354, -2.4115248]], dtype=float32)

time = 41859	action = 0	current_phase = 0	next_phase = 1	reward = -0.027475	array([[-1.4440839, -2.0724313]], dtype=float32)

time = 41864	action = 0	current_phase = 0	next_phase = 1	reward = 0.042659	array([[-1.7434516, -2.7427194]], dtype=float32)

time = 41869	action = 1	current_phase = 0	next_phase = 1	reward = -0.775493	array([[-2.4189506, -2.193832 ]], dtype=float32)

time = 41877	action = 1	current_phase = 1	next_phase = 0	reward = -0.926924	array([[-3.6785526, -2.0920842]], dtype=float32)

time = 41885	action = 0	current_phase = 0	next_phase = 1	reward = -0.092431	array([[-1.440515 , -2.8798933]], dtype=float32)

time = 41890	action = 0	current_phase = 0	next_phase = 1	reward = -0.010296	array([[-1.8847055, -2.0928392]], dtype=float32)

time = 41895	action = 0	current_phase = 0	next_phase = 1	reward = 0.338167	array([[-2.201615 , -2.8746336]], dtype=float32)

time = 41900	action = 1	current_phase = 0	next_phase = 1	reward = -1.360958	array([[-5.530937 , -2.7580485]], dtype=float32)

time = 41908	action = 1	current_phase = 1	next_phase = 0	reward = -0.720429	array([[-3.8233094, -2.2164204]], dtype=float32)

time = 41916	action = 0	current_phase = 0	next_phase = 1	reward = -0.091236	array([[-1.7667915, -2.7794635]], dtype=float32)

time = 41921	action = 0	current_phase = 0	next_phase = 1	reward = -0.007486	array([[-2.1941621, -2.87961  ]], dtype=float32)

time = 41926	action = 0	current_phase = 0	next_phase = 1	reward = 0.067473	array([[-2.5103807, -2.974704 ]], dtype=float32)

time = 41931	action = 1	current_phase = 0	next_phase = 1	reward = -1.402258	array([[-6.6016173, -3.2171082]], dtype=float32)

time = 41939	action = 1	current_phase = 1	next_phase = 0	reward = -1.074273	array([[-3.8448534, -2.251334 ]], dtype=float32)

time = 41947	action = 0	current_phase = 0	next_phase = 1	reward = 0.227727	array([[-1.8008735, -2.7578557]], dtype=float32)

time = 41952	action = 0	current_phase = 0	next_phase = 1	reward = 0.015038	array([[-2.2559376, -2.9644501]], dtype=float32)

time = 41957	action = 0	current_phase = 0	next_phase = 1	reward = 0.066232	array([[-2.7356856, -3.067617 ]], dtype=float32)

time = 41962	action = 1	current_phase = 0	next_phase = 1	reward = -1.549168	array([[-6.5575953, -3.5743613]], dtype=float32)

time = 41970	action = 1	current_phase = 1	next_phase = 0	reward = -1.008486	array([[-4.012457 , -2.4185748]], dtype=float32)

time = 41978	action = 0	current_phase = 0	next_phase = 1	reward = -0.068948	array([[-1.8788488, -2.7539406]], dtype=float32)

time = 41983	action = 0	current_phase = 0	next_phase = 1	reward = 0.011968	array([[-2.3046532, -2.9243798]], dtype=float32)

time = 41988	action = 0	current_phase = 0	next_phase = 1	reward = 0.078189	array([[-2.8029826, -3.1722336]], dtype=float32)

time = 41993	action = 1	current_phase = 0	next_phase = 1	reward = -1.834073	array([[-6.541282 , -3.5795755]], dtype=float32)

time = 42001	action = 1	current_phase = 1	next_phase = 0	reward = -1.038513	array([[-4.3452516, -2.40243  ]], dtype=float32)

time = 42009	action = 0	current_phase = 0	next_phase = 1	reward = -0.031703	array([[-1.392071, -2.078016]], dtype=float32)

time = 42014	action = 0	current_phase = 0	next_phase = 1	reward = 0.038507	array([[-1.8868654, -2.767511 ]], dtype=float32)

time = 42019	action = 1	current_phase = 0	next_phase = 1	reward = -0.690183	array([[-2.4424162, -2.206416 ]], dtype=float32)

time = 42027	action = 1	current_phase = 1	next_phase = 0	reward = -0.650668	array([[-3.5784526, -2.1320364]], dtype=float32)

time = 42035	action = 0	current_phase = 0	next_phase = 1	reward = -0.103378	array([[-1.4177556, -2.9543018]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0191 - val_loss: 0.0114

Epoch 2/50

 - 3s - loss: 0.0210 - val_loss: 0.0106

Epoch 3/50

 - 3s - loss: 0.0227 - val_loss: 0.0116

Epoch 4/50

 - 3s - loss: 0.0189 - val_loss: 0.0099

Epoch 5/50

 - 3s - loss: 0.0227 - val_loss: 0.0118

Epoch 6/50

 - 3s - loss: 0.0189 - val_loss: 0.0108

Epoch 7/50

 - 3s - loss: 0.0184 - val_loss: 0.0107

Epoch 8/50

 - 3s - loss: 0.0208 - val_loss: 0.0160

Epoch 9/50

 - 3s - loss: 0.0192 - val_loss: 0.0155

Epoch 10/50

 - 3s - loss: 0.0174 - val_loss: 0.0123

Epoch 11/50

 - 3s - loss: 0.0227 - val_loss: 0.0113

Epoch 12/50

 - 3s - loss: 0.0239 - val_loss: 0.0132

Epoch 13/50

 - 3s - loss: 0.0193 - val_loss: 0.0103

Epoch 14/50

 - 3s - loss: 0.0189 - val_loss: 0.0111

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 42040	action = 0	current_phase = 0	next_phase = 1	reward = -0.295535	array([[-1.8330977, -2.1027675]], dtype=float32)

time = 42045	action = 0	current_phase = 0	next_phase = 1	reward = 0.330496	array([[-2.1647449, -2.894813 ]], dtype=float32)

time = 42050	action = 1	current_phase = 0	next_phase = 1	reward = -1.369717	array([[-5.542064 , -2.9132447]], dtype=float32)

time = 42058	action = 1	current_phase = 1	next_phase = 0	reward = -0.703167	array([[-3.8489132, -2.1978045]], dtype=float32)

time = 42066	action = 0	current_phase = 0	next_phase = 1	reward = -0.086126	array([[-1.748517 , -2.8163028]], dtype=float32)

time = 42071	action = 0	current_phase = 0	next_phase = 1	reward = -0.000966	array([[-2.1018107, -2.8612514]], dtype=float32)

time = 42076	action = 0	current_phase = 0	next_phase = 1	reward = 0.056792	array([[-2.544578 , -3.0129755]], dtype=float32)

time = 42081	action = 1	current_phase = 0	next_phase = 1	reward = -1.349903	array([[-6.575223 , -3.1815333]], dtype=float32)

time = 42089	action = 1	current_phase = 1	next_phase = 0	reward = -1.176727	array([[-3.875382 , -2.2780101]], dtype=float32)

time = 42097	action = 0	current_phase = 0	next_phase = 1	reward = 0.215702	array([[-1.6744707, -2.7840738]], dtype=float32)

time = 42102	action = 0	current_phase = 0	next_phase = 1	reward = 0.011648	array([[-2.22998 , -2.932343]], dtype=float32)

time = 42107	action = 0	current_phase = 0	next_phase = 1	reward = 0.073770	array([[-2.7261598, -3.0790663]], dtype=float32)

time = 42112	action = 1	current_phase = 0	next_phase = 1	reward = -1.584740	array([[-6.5815067, -3.4717305]], dtype=float32)

time = 42120	action = 1	current_phase = 1	next_phase = 0	reward = -1.244871	array([[-4.0270257, -2.4101453]], dtype=float32)

time = 42128	action = 0	current_phase = 0	next_phase = 1	reward = 0.241668	array([[-1.7842836, -2.7903419]], dtype=float32)

time = 42133	action = 0	current_phase = 0	next_phase = 1	reward = 0.025939	array([[-2.19886 , -2.925006]], dtype=float32)

time = 42138	action = 0	current_phase = 0	next_phase = 1	reward = 0.075866	array([[-2.7799451, -3.1120024]], dtype=float32)

time = 42143	action = 1	current_phase = 0	next_phase = 1	reward = -1.822829	array([[-6.4845314, -3.723056 ]], dtype=float32)

time = 42151	action = 1	current_phase = 1	next_phase = 0	reward = -1.386863	array([[-4.4330516, -2.4216459]], dtype=float32)

time = 42159	action = 0	current_phase = 0	next_phase = 1	reward = 0.258243	array([[-1.3138592, -2.092561 ]], dtype=float32)

time = 42164	action = 0	current_phase = 0	next_phase = 1	reward = 0.027497	array([[-1.7854302, -2.7751272]], dtype=float32)

time = 42169	action = 1	current_phase = 0	next_phase = 1	reward = -0.698602	array([[-2.3489048, -2.2627027]], dtype=float32)

time = 42177	action = 1	current_phase = 1	next_phase = 0	reward = -1.160912	array([[-3.6930137, -2.1524136]], dtype=float32)

time = 42185	action = 0	current_phase = 0	next_phase = 1	reward = 0.451237	array([[-1.2983923, -3.029014 ]], dtype=float32)

time = 42190	action = 0	current_phase = 0	next_phase = 1	reward = -0.574509	array([[-1.8911343, -2.1197748]], dtype=float32)

time = 42195	action = 0	current_phase = 0	next_phase = 1	reward = 0.598858	array([[-2.1113713, -2.9042408]], dtype=float32)

time = 42200	action = 1	current_phase = 0	next_phase = 1	reward = -1.325098	array([[-5.4999814, -2.9000134]], dtype=float32)

time = 42208	action = 1	current_phase = 1	next_phase = 0	reward = -0.707553	array([[-3.8842282, -2.178179 ]], dtype=float32)

time = 42216	action = 0	current_phase = 0	next_phase = 1	reward = -0.078237	array([[-1.6730459, -2.8179889]], dtype=float32)

time = 42221	action = 0	current_phase = 0	next_phase = 1	reward = 0.022986	array([[-1.9989555, -2.8269572]], dtype=float32)

time = 42226	action = 0	current_phase = 0	next_phase = 1	reward = 0.074084	array([[-2.5323799, -3.0010462]], dtype=float32)

time = 42231	action = 1	current_phase = 0	next_phase = 1	reward = -1.451069	array([[-6.5730953, -3.237965 ]], dtype=float32)

time = 42239	action = 1	current_phase = 1	next_phase = 0	reward = -0.836866	array([[-3.9542737, -2.307683 ]], dtype=float32)

time = 42247	action = 0	current_phase = 0	next_phase = 1	reward = -0.088347	array([[-1.7893605, -2.8307247]], dtype=float32)

time = 42252	action = 0	current_phase = 0	next_phase = 1	reward = -0.012929	array([[-2.211388 , -2.9261365]], dtype=float32)

time = 42257	action = 0	current_phase = 0	next_phase = 1	reward = 0.059838	array([[-2.6534626, -3.0754466]], dtype=float32)

time = 42262	action = 1	current_phase = 0	next_phase = 1	reward = -1.570895	array([[-6.593846, -3.363389]], dtype=float32)

time = 42270	action = 1	current_phase = 1	next_phase = 0	reward = -0.957156	array([[-4.036454 , -2.4395714]], dtype=float32)

time = 42278	action = 0	current_phase = 0	next_phase = 1	reward = -0.047463	array([[-1.8271687, -2.7969499]], dtype=float32)

time = 42283	action = 0	current_phase = 0	next_phase = 1	reward = 0.010771	array([[-2.2684839, -2.9424555]], dtype=float32)

time = 42288	action = 0	current_phase = 0	next_phase = 1	reward = 0.082093	array([[-2.838241 , -3.1904178]], dtype=float32)

time = 42293	action = 1	current_phase = 0	next_phase = 1	reward = -1.316316	array([[-6.581133 , -3.3636472]], dtype=float32)

time = 42301	action = 1	current_phase = 1	next_phase = 0	reward = -1.126343	array([[-4.114471 , -2.4699218]], dtype=float32)

time = 42309	action = 0	current_phase = 0	next_phase = 1	reward = -0.037542	array([[-1.3710636, -2.0863118]], dtype=float32)

time = 42314	action = 0	current_phase = 0	next_phase = 1	reward = 0.025846	array([[-1.7546055, -2.7803984]], dtype=float32)

time = 42319	action = 1	current_phase = 0	next_phase = 1	reward = -0.781128	array([[-2.2802334, -2.2396448]], dtype=float32)

time = 42327	action = 1	current_phase = 1	next_phase = 0	reward = -0.932616	array([[-3.75274  , -2.1403198]], dtype=float32)

time = 42335	action = 0	current_phase = 0	next_phase = 1	reward = 0.192108	array([[-1.4055382, -2.9065797]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0232 - val_loss: 0.0120

Epoch 2/50

 - 3s - loss: 0.0200 - val_loss: 0.0141

Epoch 3/50

 - 3s - loss: 0.0185 - val_loss: 0.0123

Epoch 4/50

 - 3s - loss: 0.0202 - val_loss: 0.0105

Epoch 5/50

 - 3s - loss: 0.0188 - val_loss: 0.0112

Epoch 6/50

 - 3s - loss: 0.0201 - val_loss: 0.0116

Epoch 7/50

 - 3s - loss: 0.0202 - val_loss: 0.0114

Epoch 8/50

 - 3s - loss: 0.0203 - val_loss: 0.0116

Epoch 9/50

 - 3s - loss: 0.0184 - val_loss: 0.0129

Epoch 10/50

 - 3s - loss: 0.0191 - val_loss: 0.0101

Epoch 11/50

 - 3s - loss: 0.0208 - val_loss: 0.0109

Epoch 12/50

 - 3s - loss: 0.0203 - val_loss: 0.0115

Epoch 13/50

 - 3s - loss: 0.0179 - val_loss: 0.0123

Epoch 14/50

 - 3s - loss: 0.0187 - val_loss: 0.0104

Epoch 15/50

 - 3s - loss: 0.0189 - val_loss: 0.0121

Epoch 16/50

 - 3s - loss: 0.0210 - val_loss: 0.0117

Epoch 17/50

 - 3s - loss: 0.0237 - val_loss: 0.0118

Epoch 18/50

 - 3s - loss: 0.0196 - val_loss: 0.0105

Epoch 19/50

 - 3s - loss: 0.0181 - val_loss: 0.0145

Epoch 20/50

 - 3s - loss: 0.0189 - val_loss: 0.0125

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 42340	action = 0	current_phase = 0	next_phase = 1	reward = -0.282291	array([[-1.6705896, -2.1669888]], dtype=float32)

time = 42345	action = 0	current_phase = 0	next_phase = 1	reward = 0.069677	array([[-2.2519228, -2.9079971]], dtype=float32)

time = 42350	action = 1	current_phase = 0	next_phase = 1	reward = -1.055723	array([[-5.510919 , -2.7954164]], dtype=float32)

time = 42358	action = 1	current_phase = 1	next_phase = 0	reward = -0.686513	array([[-3.7045465, -2.0299468]], dtype=float32)

time = 42366	action = 0	current_phase = 0	next_phase = 1	reward = -0.071439	array([[-1.7335868, -2.760394 ]], dtype=float32)

time = 42371	action = 0	current_phase = 0	next_phase = 1	reward = 0.004414	array([[-1.9361463, -2.8113527]], dtype=float32)

time = 42376	action = 0	current_phase = 0	next_phase = 1	reward = 0.052918	array([[-2.5160303, -3.0110774]], dtype=float32)

time = 42381	action = 1	current_phase = 0	next_phase = 1	reward = -1.395924	array([[-6.560998 , -3.2400265]], dtype=float32)

time = 42389	action = 1	current_phase = 1	next_phase = 0	reward = -0.837261	array([[-3.904163 , -2.2556052]], dtype=float32)

time = 42397	action = 0	current_phase = 0	next_phase = 1	reward = -0.082131	array([[-1.7261809, -2.7598796]], dtype=float32)

time = 42402	action = 0	current_phase = 0	next_phase = 1	reward = 0.004207	array([[-2.159923 , -2.9151683]], dtype=float32)

time = 42407	action = 0	current_phase = 0	next_phase = 1	reward = 0.066764	array([[-2.733603 , -3.0806556]], dtype=float32)

time = 42412	action = 1	current_phase = 0	next_phase = 1	reward = -1.581287	array([[-6.5451374, -3.461359 ]], dtype=float32)

time = 42420	action = 1	current_phase = 1	next_phase = 0	reward = -0.926230	array([[-4.0240717, -2.3957973]], dtype=float32)

time = 42428	action = 0	current_phase = 0	next_phase = 1	reward = -0.052149	array([[-1.7169641, -2.7680807]], dtype=float32)

time = 42433	action = 0	current_phase = 0	next_phase = 1	reward = 0.022108	array([[-2.172189 , -2.9100056]], dtype=float32)

time = 42438	action = 0	current_phase = 0	next_phase = 1	reward = 0.082984	array([[-2.7927847, -3.157034 ]], dtype=float32)

time = 42443	action = 1	current_phase = 0	next_phase = 1	reward = -1.347662	array([[-6.5032535, -3.5675106]], dtype=float32)

time = 42451	action = 1	current_phase = 1	next_phase = 0	reward = -1.415213	array([[-4.0267467, -2.4157624]], dtype=float32)

time = 42459	action = 0	current_phase = 0	next_phase = 1	reward = 0.262460	array([[-1.2091695, -2.0959902]], dtype=float32)

time = 42464	action = 0	current_phase = 0	next_phase = 1	reward = 0.048178	array([[-1.7664865, -2.7690635]], dtype=float32)

time = 42469	action = 0	current_phase = 0	next_phase = 1	reward = 0.047839	array([[-2.1632307, -2.3342156]], dtype=float32)

time = 42474	action = 1	current_phase = 0	next_phase = 1	reward = -1.136925	array([[-6.2723575, -3.0002432]], dtype=float32)

time = 42482	action = 1	current_phase = 1	next_phase = 0	reward = -0.694109	array([[-3.841865 , -2.2813253]], dtype=float32)

time = 42490	action = 0	current_phase = 0	next_phase = 1	reward = -0.017537	array([[-1.6226317, -2.1497397]], dtype=float32)

time = 42495	action = 0	current_phase = 0	next_phase = 1	reward = 0.058582	array([[-2.28345  , -2.9969082]], dtype=float32)

time = 42500	action = 1	current_phase = 0	next_phase = 1	reward = -1.378999	array([[-5.4942136, -2.9823608]], dtype=float32)

time = 42508	action = 1	current_phase = 1	next_phase = 0	reward = -0.699997	array([[-3.7355375, -2.0774546]], dtype=float32)

time = 42516	action = 0	current_phase = 0	next_phase = 1	reward = -0.084416	array([[-1.693263 , -2.7539024]], dtype=float32)

time = 42521	action = 0	current_phase = 0	next_phase = 1	reward = 0.000447	array([[-1.9828454, -2.825756 ]], dtype=float32)

time = 42526	action = 0	current_phase = 0	next_phase = 1	reward = 0.061342	array([[-2.5197322, -3.0233893]], dtype=float32)

time = 42531	action = 1	current_phase = 0	next_phase = 1	reward = -1.541207	array([[-6.596099 , -3.2615037]], dtype=float32)

time = 42539	action = 1	current_phase = 1	next_phase = 0	reward = -0.895354	array([[-3.935114 , -2.3039012]], dtype=float32)

time = 42547	action = 0	current_phase = 0	next_phase = 1	reward = -0.061000	array([[-1.7639176, -2.7841697]], dtype=float32)

time = 42552	action = 0	current_phase = 0	next_phase = 1	reward = 0.004212	array([[-2.1439428, -2.906918 ]], dtype=float32)

time = 42557	action = 0	current_phase = 0	next_phase = 1	reward = 0.066054	array([[-2.772013 , -3.1016426]], dtype=float32)

time = 42562	action = 1	current_phase = 0	next_phase = 1	reward = -1.655780	array([[-6.5121984, -3.5871673]], dtype=float32)

time = 42570	action = 1	current_phase = 1	next_phase = 0	reward = -0.959383	array([[-4.0684557, -2.4606886]], dtype=float32)

time = 42578	action = 0	current_phase = 0	next_phase = 1	reward = -0.042102	array([[-1.7047039, -2.7565455]], dtype=float32)

time = 42583	action = 0	current_phase = 0	next_phase = 1	reward = 0.025425	array([[-2.1262393, -2.9028091]], dtype=float32)

time = 42588	action = 0	current_phase = 0	next_phase = 1	reward = 0.075347	array([[-2.8057127, -3.158585 ]], dtype=float32)

time = 42593	action = 1	current_phase = 0	next_phase = 1	reward = -2.011352	array([[-6.4633813, -3.7668858]], dtype=float32)

time = 42601	action = 1	current_phase = 1	next_phase = 0	reward = -0.970162	array([[-4.4827127, -2.3721564]], dtype=float32)

time = 42609	action = 0	current_phase = 0	next_phase = 1	reward = -0.006358	array([[-1.2745042, -2.107492 ]], dtype=float32)

time = 42614	action = 0	current_phase = 0	next_phase = 1	reward = 0.329111	array([[-1.7409911, -2.7653928]], dtype=float32)

time = 42619	action = 0	current_phase = 0	next_phase = 1	reward = -0.229139	array([[-2.3430376, -2.3787446]], dtype=float32)

time = 42624	action = 1	current_phase = 0	next_phase = 1	reward = -1.141450	array([[-6.420515 , -3.0325246]], dtype=float32)

time = 42632	action = 1	current_phase = 1	next_phase = 0	reward = -0.697431	array([[-4.0431275, -2.3273294]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0169 - val_loss: 0.0107

Epoch 2/50

 - 3s - loss: 0.0178 - val_loss: 0.0100

Epoch 3/50

 - 3s - loss: 0.0203 - val_loss: 0.0129

Epoch 4/50

 - 3s - loss: 0.0167 - val_loss: 0.0136

Epoch 5/50

 - 3s - loss: 0.0191 - val_loss: 0.0186

Epoch 6/50

 - 3s - loss: 0.0207 - val_loss: 0.0133

Epoch 7/50

 - 3s - loss: 0.0173 - val_loss: 0.0144

Epoch 8/50

 - 3s - loss: 0.0217 - val_loss: 0.0131

Epoch 9/50

 - 3s - loss: 0.0155 - val_loss: 0.0160

Epoch 10/50

 - 3s - loss: 0.0177 - val_loss: 0.0134

Epoch 11/50

 - 3s - loss: 0.0181 - val_loss: 0.0168

Epoch 12/50

 - 3s - loss: 0.0145 - val_loss: 0.0170

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 42640	action = 0	current_phase = 0	next_phase = 1	reward = -0.036642	array([[-1.66506  , -2.1783044]], dtype=float32)

time = 42645	action = 0	current_phase = 0	next_phase = 1	reward = -0.246562	array([[-1.9989818, -2.8737967]], dtype=float32)

time = 42650	action = 1	current_phase = 0	next_phase = 1	reward = -1.082433	array([[-5.413531 , -2.8811953]], dtype=float32)

time = 42658	action = 1	current_phase = 1	next_phase = 0	reward = -0.707072	array([[-3.6815672, -2.0275135]], dtype=float32)

time = 42666	action = 0	current_phase = 0	next_phase = 1	reward = -0.086270	array([[-1.7193655, -2.7942383]], dtype=float32)

time = 42671	action = 0	current_phase = 0	next_phase = 1	reward = -0.013403	array([[-1.9493359, -2.8222048]], dtype=float32)

time = 42676	action = 0	current_phase = 0	next_phase = 1	reward = 0.060116	array([[-2.43295 , -2.999197]], dtype=float32)

time = 42681	action = 1	current_phase = 0	next_phase = 1	reward = -1.495886	array([[-6.5312757, -3.3562467]], dtype=float32)

time = 42689	action = 1	current_phase = 1	next_phase = 0	reward = -0.821276	array([[-3.871697 , -2.2219052]], dtype=float32)

time = 42697	action = 0	current_phase = 0	next_phase = 1	reward = -0.074309	array([[-1.740255, -2.78411 ]], dtype=float32)

time = 42702	action = 0	current_phase = 0	next_phase = 1	reward = 0.007139	array([[-2.1177816, -2.9027665]], dtype=float32)

time = 42707	action = 0	current_phase = 0	next_phase = 1	reward = 0.070709	array([[-2.6722107, -3.119478 ]], dtype=float32)

time = 42712	action = 1	current_phase = 0	next_phase = 1	reward = -1.553885	array([[-6.4839263, -3.545002 ]], dtype=float32)

time = 42720	action = 1	current_phase = 1	next_phase = 0	reward = -1.303837	array([[-4.095748 , -2.5199983]], dtype=float32)

time = 42728	action = 0	current_phase = 0	next_phase = 1	reward = 0.234297	array([[-1.6588246, -2.7673151]], dtype=float32)

time = 42733	action = 0	current_phase = 0	next_phase = 1	reward = 0.009215	array([[-2.1092205, -2.8894389]], dtype=float32)

time = 42738	action = 0	current_phase = 0	next_phase = 1	reward = 0.071997	array([[-2.7540507, -3.1846852]], dtype=float32)

time = 42743	action = 1	current_phase = 0	next_phase = 1	reward = -1.863267	array([[-6.434403, -3.696811]], dtype=float32)

time = 42751	action = 1	current_phase = 1	next_phase = 0	reward = -0.980903	array([[-4.3821783, -2.372666 ]], dtype=float32)

time = 42759	action = 0	current_phase = 0	next_phase = 1	reward = -0.033287	array([[-1.229716 , -2.1279647]], dtype=float32)

time = 42764	action = 0	current_phase = 0	next_phase = 1	reward = 0.035439	array([[-1.6869406, -2.763788 ]], dtype=float32)

time = 42769	action = 0	current_phase = 0	next_phase = 1	reward = 0.061307	array([[-2.3173156, -2.3547153]], dtype=float32)

time = 42774	action = 1	current_phase = 0	next_phase = 1	reward = -1.197367	array([[-6.419353 , -3.0724914]], dtype=float32)

time = 42782	action = 1	current_phase = 1	next_phase = 0	reward = -0.759007	array([[-4.0582066, -2.3361192]], dtype=float32)

time = 42790	action = 0	current_phase = 0	next_phase = 1	reward = -0.042585	array([[-1.6216474, -2.1900163]], dtype=float32)

time = 42795	action = 0	current_phase = 0	next_phase = 1	reward = -0.248961	array([[-1.9651487, -2.8663392]], dtype=float32)

time = 42800	action = 1	current_phase = 0	next_phase = 1	reward = -1.086376	array([[-5.4211745, -2.8641856]], dtype=float32)

time = 42808	action = 1	current_phase = 1	next_phase = 0	reward = -0.708685	array([[-3.6575255, -2.0463042]], dtype=float32)

time = 42816	action = 0	current_phase = 0	next_phase = 1	reward = -0.099620	array([[-1.6228929, -2.7798285]], dtype=float32)

time = 42821	action = 0	current_phase = 0	next_phase = 1	reward = -0.015771	array([[-1.9478035, -2.8232336]], dtype=float32)

time = 42826	action = 0	current_phase = 0	next_phase = 1	reward = 0.052653	array([[-2.4494853, -3.017275 ]], dtype=float32)

time = 42831	action = 1	current_phase = 0	next_phase = 1	reward = -1.382433	array([[-6.506618 , -3.2700255]], dtype=float32)

time = 42839	action = 1	current_phase = 1	next_phase = 0	reward = -0.820370	array([[-3.8534267, -2.1989727]], dtype=float32)

time = 42847	action = 0	current_phase = 0	next_phase = 1	reward = -0.061658	array([[-1.6952155, -2.778902 ]], dtype=float32)

time = 42852	action = 0	current_phase = 0	next_phase = 1	reward = 0.008029	array([[-2.1265368, -2.9194853]], dtype=float32)

time = 42857	action = 0	current_phase = 0	next_phase = 1	reward = 0.069445	array([[-2.7552712, -3.1118581]], dtype=float32)

time = 42862	action = 1	current_phase = 0	next_phase = 1	reward = -1.660577	array([[-6.447142 , -3.6680024]], dtype=float32)

time = 42870	action = 1	current_phase = 1	next_phase = 0	reward = -0.960165	array([[-4.100009, -2.481978]], dtype=float32)

time = 42878	action = 0	current_phase = 0	next_phase = 1	reward = -0.045403	array([[-1.704177 , -2.7668939]], dtype=float32)

time = 42883	action = 0	current_phase = 0	next_phase = 1	reward = 0.030691	array([[-2.1704245, -2.9119697]], dtype=float32)

time = 42888	action = 0	current_phase = 0	next_phase = 1	reward = 0.090601	array([[-2.7604637, -3.2135594]], dtype=float32)

time = 42893	action = 1	current_phase = 0	next_phase = 1	reward = -1.887329	array([[-6.3976307, -3.822283 ]], dtype=float32)

time = 42901	action = 1	current_phase = 1	next_phase = 0	reward = -1.104744	array([[-4.3107953, -2.3354645]], dtype=float32)

time = 42909	action = 0	current_phase = 0	next_phase = 1	reward = -0.036058	array([[-1.2649012, -2.1498463]], dtype=float32)

time = 42914	action = 0	current_phase = 0	next_phase = 1	reward = 0.040834	array([[-1.6769034, -2.767441 ]], dtype=float32)

time = 42919	action = 0	current_phase = 0	next_phase = 1	reward = 0.067221	array([[-2.2518969, -2.3373985]], dtype=float32)

time = 42924	action = 1	current_phase = 0	next_phase = 1	reward = -1.049071	array([[-6.4229555, -3.013072 ]], dtype=float32)

time = 42932	action = 1	current_phase = 1	next_phase = 0	reward = -0.993222	array([[-3.960454 , -2.3486478]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0194 - val_loss: 0.0097

Epoch 2/50

 - 3s - loss: 0.0208 - val_loss: 0.0113

Epoch 3/50

 - 3s - loss: 0.0196 - val_loss: 0.0107

Epoch 4/50

 - 3s - loss: 0.0218 - val_loss: 0.0075

Epoch 5/50

 - 3s - loss: 0.0251 - val_loss: 0.0096

Epoch 6/50

 - 3s - loss: 0.0235 - val_loss: 0.0094

Epoch 7/50

 - 3s - loss: 0.0242 - val_loss: 0.0077

Epoch 8/50

 - 3s - loss: 0.0184 - val_loss: 0.0131

Epoch 9/50

 - 3s - loss: 0.0233 - val_loss: 0.0096

Epoch 10/50

 - 3s - loss: 0.0182 - val_loss: 0.0087

Epoch 11/50

 - 3s - loss: 0.0183 - val_loss: 0.0102

Epoch 12/50

 - 3s - loss: 0.0205 - val_loss: 0.0163

Epoch 13/50

 - 3s - loss: 0.0202 - val_loss: 0.0080

Epoch 14/50

 - 3s - loss: 0.0171 - val_loss: 0.0137

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 42940	action = 0	current_phase = 0	next_phase = 1	reward = -0.024308	array([[-1.649189 , -2.1818607]], dtype=float32)

time = 42945	action = 0	current_phase = 0	next_phase = 1	reward = 0.052593	array([[-1.9924288, -2.8057277]], dtype=float32)

time = 42950	action = 1	current_phase = 0	next_phase = 1	reward = -1.148608	array([[-5.388256 , -2.7820215]], dtype=float32)

time = 42958	action = 1	current_phase = 1	next_phase = 0	reward = -0.708196	array([[-3.7610044, -2.0620844]], dtype=float32)

time = 42966	action = 0	current_phase = 0	next_phase = 1	reward = -0.087875	array([[-1.7381458, -2.7706602]], dtype=float32)

time = 42971	action = 0	current_phase = 0	next_phase = 1	reward = 0.002693	array([[-1.9761516, -2.8056643]], dtype=float32)

time = 42976	action = 0	current_phase = 0	next_phase = 1	reward = 0.069082	array([[-2.430533 , -2.9961827]], dtype=float32)

time = 42981	action = 1	current_phase = 0	next_phase = 1	reward = -1.546293	array([[-6.4863443, -3.2724676]], dtype=float32)

time = 42989	action = 1	current_phase = 1	next_phase = 0	reward = -0.815086	array([[-3.888103 , -2.2265167]], dtype=float32)

time = 42997	action = 0	current_phase = 0	next_phase = 1	reward = -0.059561	array([[-1.8278371, -2.7820144]], dtype=float32)

time = 43002	action = 0	current_phase = 0	next_phase = 1	reward = 0.022413	array([[-2.147243 , -2.8834555]], dtype=float32)

time = 43007	action = 0	current_phase = 0	next_phase = 1	reward = 0.067944	array([[-2.7245054, -3.0998547]], dtype=float32)

time = 43012	action = 1	current_phase = 0	next_phase = 1	reward = -1.725639	array([[-6.4519405, -3.5647361]], dtype=float32)

time = 43020	action = 1	current_phase = 1	next_phase = 0	reward = -0.915224	array([[-4.1485767, -2.5432158]], dtype=float32)

time = 43028	action = 0	current_phase = 0	next_phase = 1	reward = -0.052822	array([[-1.821908 , -2.7832208]], dtype=float32)

time = 43033	action = 0	current_phase = 0	next_phase = 1	reward = 0.025673	array([[-2.1430957, -2.8730967]], dtype=float32)

time = 43038	action = 0	current_phase = 0	next_phase = 1	reward = 0.092945	array([[-2.756174 , -3.1630294]], dtype=float32)

time = 43043	action = 1	current_phase = 0	next_phase = 1	reward = -1.404895	array([[-6.4484386, -3.3432717]], dtype=float32)

time = 43051	action = 1	current_phase = 1	next_phase = 0	reward = -1.427886	array([[-4.1649456, -2.3889143]], dtype=float32)

time = 43059	action = 0	current_phase = 0	next_phase = 1	reward = 0.263030	array([[-1.2114531, -2.1195123]], dtype=float32)

time = 43064	action = 0	current_phase = 0	next_phase = 1	reward = 0.058562	array([[-1.749067 , -2.7407231]], dtype=float32)

time = 43069	action = 0	current_phase = 0	next_phase = 1	reward = 0.042814	array([[-1.8843646, -2.3683598]], dtype=float32)

time = 43074	action = 1	current_phase = 0	next_phase = 1	reward = -1.101881	array([[-6.405093, -2.992924]], dtype=float32)

time = 43082	action = 1	current_phase = 1	next_phase = 0	reward = -0.691551	array([[-3.977622 , -2.2868953]], dtype=float32)

time = 43090	action = 0	current_phase = 0	next_phase = 1	reward = -0.284048	array([[-1.7569909, -2.177287 ]], dtype=float32)

time = 43095	action = 0	current_phase = 0	next_phase = 1	reward = 0.339724	array([[-2.020615 , -2.8139846]], dtype=float32)

time = 43100	action = 1	current_phase = 0	next_phase = 1	reward = -1.388703	array([[-5.45006  , -2.9753788]], dtype=float32)

time = 43108	action = 1	current_phase = 1	next_phase = 0	reward = -0.717792	array([[-3.7830734, -2.1402795]], dtype=float32)

time = 43116	action = 0	current_phase = 0	next_phase = 1	reward = -0.076213	array([[-1.738436 , -2.7602777]], dtype=float32)

time = 43121	action = 0	current_phase = 0	next_phase = 1	reward = 0.010666	array([[-2.003626 , -2.8115942]], dtype=float32)

time = 43126	action = 0	current_phase = 0	next_phase = 1	reward = 0.082430	array([[-2.507823 , -3.0141854]], dtype=float32)

time = 43131	action = 1	current_phase = 0	next_phase = 1	reward = -1.517733	array([[-6.496416 , -3.2698872]], dtype=float32)

time = 43139	action = 1	current_phase = 1	next_phase = 0	reward = -0.795308	array([[-3.9351306, -2.2895098]], dtype=float32)

time = 43147	action = 0	current_phase = 0	next_phase = 1	reward = -0.068004	array([[-1.8237408, -2.7851264]], dtype=float32)

time = 43152	action = 0	current_phase = 0	next_phase = 1	reward = 0.011702	array([[-2.1350026, -2.8855698]], dtype=float32)

time = 43157	action = 0	current_phase = 0	next_phase = 1	reward = 0.082792	array([[-2.676446 , -3.0823073]], dtype=float32)

time = 43162	action = 1	current_phase = 0	next_phase = 1	reward = -1.700692	array([[-6.4651613, -3.565664 ]], dtype=float32)

time = 43170	action = 1	current_phase = 1	next_phase = 0	reward = -0.963424	array([[-4.2176547, -2.5809345]], dtype=float32)

time = 43178	action = 0	current_phase = 0	next_phase = 1	reward = -0.062807	array([[-1.780279, -2.76856 ]], dtype=float32)

time = 43183	action = 0	current_phase = 0	next_phase = 1	reward = 0.009334	array([[-2.171254 , -2.8797922]], dtype=float32)

time = 43188	action = 0	current_phase = 0	next_phase = 1	reward = 0.067789	array([[-2.7880821, -3.1980767]], dtype=float32)

time = 43193	action = 1	current_phase = 0	next_phase = 1	reward = -1.856943	array([[-6.421913 , -3.6898313]], dtype=float32)

time = 43201	action = 1	current_phase = 1	next_phase = 0	reward = -1.242984	array([[-4.4730577, -2.4462824]], dtype=float32)

time = 43209	action = 0	current_phase = 0	next_phase = 1	reward = 0.262967	array([[-1.244017, -2.134201]], dtype=float32)

time = 43214	action = 0	current_phase = 0	next_phase = 1	reward = 0.051392	array([[-1.8667684, -2.7948532]], dtype=float32)

time = 43219	action = 0	current_phase = 0	next_phase = 1	reward = 0.041542	array([[-2.3777404, -2.389733 ]], dtype=float32)

time = 43224	action = 1	current_phase = 0	next_phase = 1	reward = -1.130272	array([[-6.4853163, -3.108628 ]], dtype=float32)

time = 43232	action = 1	current_phase = 1	next_phase = 0	reward = -0.920936	array([[-4.010804, -2.339592]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0195 - val_loss: 0.0136

Epoch 2/50

 - 3s - loss: 0.0194 - val_loss: 0.0124

Epoch 3/50

 - 3s - loss: 0.0203 - val_loss: 0.0116

Epoch 4/50

 - 3s - loss: 0.0202 - val_loss: 0.0126

Epoch 5/50

 - 3s - loss: 0.0196 - val_loss: 0.0171

Epoch 6/50

 - 3s - loss: 0.0236 - val_loss: 0.0131

Epoch 7/50

 - 3s - loss: 0.0200 - val_loss: 0.0121

Epoch 8/50

 - 3s - loss: 0.0177 - val_loss: 0.0138

Epoch 9/50

 - 3s - loss: 0.0192 - val_loss: 0.0127

Epoch 10/50

 - 3s - loss: 0.0161 - val_loss: 0.0140

Epoch 11/50

 - 3s - loss: 0.0174 - val_loss: 0.0111

Epoch 12/50

 - 3s - loss: 0.0171 - val_loss: 0.0137

Epoch 13/50

 - 3s - loss: 0.0155 - val_loss: 0.0151

Epoch 14/50

 - 3s - loss: 0.0191 - val_loss: 0.0159

Epoch 15/50

 - 3s - loss: 0.0250 - val_loss: 0.0116

Epoch 16/50

 - 3s - loss: 0.0173 - val_loss: 0.0144

Epoch 17/50

 - 3s - loss: 0.0162 - val_loss: 0.0140

Epoch 18/50

 - 3s - loss: 0.0154 - val_loss: 0.0126

Epoch 19/50

 - 3s - loss: 0.0216 - val_loss: 0.0139

Epoch 20/50

 - 3s - loss: 0.0162 - val_loss: 0.0176

Epoch 21/50

 - 3s - loss: 0.0186 - val_loss: 0.0146

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 43240	action = 0	current_phase = 0	next_phase = 1	reward = 0.247028	array([[-1.6035821, -2.1839695]], dtype=float32)

time = 43245	action = 0	current_phase = 0	next_phase = 1	reward = -0.247902	array([[-2.0770528, -2.8155117]], dtype=float32)

time = 43250	action = 1	current_phase = 0	next_phase = 1	reward = -1.017708	array([[-5.3500824, -2.7755237]], dtype=float32)

time = 43258	action = 1	current_phase = 1	next_phase = 0	reward = -0.711964	array([[-3.728683 , -2.0929306]], dtype=float32)

time = 43266	action = 0	current_phase = 0	next_phase = 1	reward = -0.084203	array([[-1.7326598, -2.753784 ]], dtype=float32)

time = 43271	action = 0	current_phase = 0	next_phase = 1	reward = -0.006067	array([[-2.0879302, -2.807484 ]], dtype=float32)

time = 43276	action = 0	current_phase = 0	next_phase = 1	reward = 0.046930	array([[-2.5092053, -2.9915934]], dtype=float32)

time = 43281	action = 1	current_phase = 0	next_phase = 1	reward = -1.513300	array([[-6.5352864, -3.2921638]], dtype=float32)

time = 43289	action = 1	current_phase = 1	next_phase = 0	reward = -0.784803	array([[-3.897758, -2.21309 ]], dtype=float32)

time = 43297	action = 0	current_phase = 0	next_phase = 1	reward = -0.072181	array([[-1.8611666, -2.7527192]], dtype=float32)

time = 43302	action = 0	current_phase = 0	next_phase = 1	reward = 0.016122	array([[-2.1855268, -2.8713112]], dtype=float32)

time = 43307	action = 0	current_phase = 0	next_phase = 1	reward = 0.071290	array([[-2.7810526, -3.1276414]], dtype=float32)

time = 43312	action = 1	current_phase = 0	next_phase = 1	reward = -1.538721	array([[-6.466009 , -3.5377884]], dtype=float32)

time = 43320	action = 1	current_phase = 1	next_phase = 0	reward = -1.306021	array([[-3.9791365, -2.3014972]], dtype=float32)

time = 43328	action = 0	current_phase = 0	next_phase = 1	reward = 0.243481	array([[-1.8308966, -2.756422 ]], dtype=float32)

time = 43333	action = 0	current_phase = 0	next_phase = 1	reward = 0.020475	array([[-2.2622337, -2.8724802]], dtype=float32)

time = 43338	action = 0	current_phase = 0	next_phase = 1	reward = 0.082648	array([[-2.7971642, -3.1388059]], dtype=float32)

time = 43343	action = 1	current_phase = 0	next_phase = 1	reward = -1.928548	array([[-6.4482484, -3.6873717]], dtype=float32)

time = 43351	action = 1	current_phase = 1	next_phase = 0	reward = -1.036766	array([[-4.3457184, -2.2221384]], dtype=float32)

time = 43359	action = 0	current_phase = 0	next_phase = 1	reward = -0.036560	array([[-1.4143205, -2.135051 ]], dtype=float32)

time = 43364	action = 0	current_phase = 0	next_phase = 1	reward = 0.025469	array([[-1.8606156, -2.7419553]], dtype=float32)

time = 43369	action = 0	current_phase = 0	next_phase = 1	reward = 0.058720	array([[-2.3281732, -2.357726 ]], dtype=float32)

time = 43374	action = 1	current_phase = 0	next_phase = 1	reward = -1.035606	array([[-6.480709 , -3.0577245]], dtype=float32)

time = 43382	action = 1	current_phase = 1	next_phase = 0	reward = -1.044830	array([[-3.9397454, -2.4160533]], dtype=float32)

time = 43390	action = 0	current_phase = 0	next_phase = 1	reward = -0.024296	array([[-1.6582094, -2.1716895]], dtype=float32)

time = 43395	action = 0	current_phase = 0	next_phase = 1	reward = -0.224804	array([[-2.1251829, -2.8348145]], dtype=float32)

time = 43400	action = 1	current_phase = 0	next_phase = 1	reward = -0.738485	array([[-5.350442 , -2.5844831]], dtype=float32)

time = 43408	action = 1	current_phase = 1	next_phase = 0	reward = -0.720880	array([[-3.6837502, -2.0515666]], dtype=float32)

time = 43416	action = 0	current_phase = 0	next_phase = 1	reward = -0.096040	array([[-1.7331083, -2.7364683]], dtype=float32)

time = 43421	action = 0	current_phase = 0	next_phase = 1	reward = -0.021387	array([[-2.1588936, -2.8258724]], dtype=float32)

time = 43426	action = 0	current_phase = 0	next_phase = 1	reward = 0.036718	array([[-2.3591807, -2.9021263]], dtype=float32)

time = 43431	action = 1	current_phase = 0	next_phase = 1	reward = -1.482478	array([[-6.54478  , -3.3097832]], dtype=float32)

time = 43439	action = 1	current_phase = 1	next_phase = 0	reward = -0.828695	array([[-3.909914 , -2.2102418]], dtype=float32)

time = 43447	action = 0	current_phase = 0	next_phase = 1	reward = -0.084934	array([[-1.8388577, -2.7707448]], dtype=float32)

time = 43452	action = 0	current_phase = 0	next_phase = 1	reward = -0.003249	array([[-2.2062995, -2.87387  ]], dtype=float32)

time = 43457	action = 0	current_phase = 0	next_phase = 1	reward = 0.060453	array([[-2.7650461, -3.0973618]], dtype=float32)

time = 43462	action = 1	current_phase = 0	next_phase = 1	reward = -1.577949	array([[-6.4761972, -3.5380087]], dtype=float32)

time = 43470	action = 1	current_phase = 1	next_phase = 0	reward = -0.933061	array([[-4.0531263, -2.414655 ]], dtype=float32)

time = 43478	action = 0	current_phase = 0	next_phase = 1	reward = -0.051092	array([[-1.8417839, -2.744935 ]], dtype=float32)

time = 43483	action = 0	current_phase = 0	next_phase = 1	reward = 0.023823	array([[-2.2032638, -2.8526676]], dtype=float32)

time = 43488	action = 0	current_phase = 0	next_phase = 1	reward = 0.069275	array([[-2.8472714, -3.1494753]], dtype=float32)

time = 43493	action = 1	current_phase = 0	next_phase = 1	reward = -1.339998	array([[-6.4855394, -3.3197598]], dtype=float32)

time = 43501	action = 1	current_phase = 1	next_phase = 0	reward = -1.503966	array([[-4.12678 , -2.346432]], dtype=float32)

time = 43509	action = 0	current_phase = 0	next_phase = 1	reward = 0.256559	array([[-1.4160588, -2.1673799]], dtype=float32)

time = 43514	action = 0	current_phase = 0	next_phase = 1	reward = 0.041037	array([[-1.8314639, -2.7659562]], dtype=float32)

time = 43519	action = 0	current_phase = 0	next_phase = 1	reward = 0.071735	array([[-2.249372 , -2.3681147]], dtype=float32)

time = 43524	action = 1	current_phase = 0	next_phase = 1	reward = -1.081652	array([[-6.4088383, -3.0141726]], dtype=float32)

time = 43532	action = 1	current_phase = 1	next_phase = 0	reward = -0.760544	array([[-4.062727 , -2.2976787]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0212 - val_loss: 0.0118

Epoch 2/50

 - 3s - loss: 0.0231 - val_loss: 0.0105

Epoch 3/50

 - 3s - loss: 0.0240 - val_loss: 0.0107

Epoch 4/50

 - 3s - loss: 0.0197 - val_loss: 0.0095

Epoch 5/50

 - 3s - loss: 0.0167 - val_loss: 0.0098

Epoch 6/50

 - 3s - loss: 0.0189 - val_loss: 0.0106

Epoch 7/50

 - 3s - loss: 0.0213 - val_loss: 0.0096

Epoch 8/50

 - 3s - loss: 0.0203 - val_loss: 0.0124

Epoch 9/50

 - 3s - loss: 0.0178 - val_loss: 0.0121

Epoch 10/50

 - 3s - loss: 0.0196 - val_loss: 0.0104

Epoch 11/50

 - 3s - loss: 0.0209 - val_loss: 0.0131

Epoch 12/50

 - 3s - loss: 0.0184 - val_loss: 0.0124

Epoch 13/50

 - 3s - loss: 0.0237 - val_loss: 0.0112

Epoch 14/50

 - 3s - loss: 0.0195 - val_loss: 0.0107

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 43540	action = 0	current_phase = 0	next_phase = 1	reward = -0.300926	array([[-1.6467373, -2.158359 ]], dtype=float32)

time = 43545	action = 0	current_phase = 0	next_phase = 1	reward = 0.061572	array([[-2.0405085, -2.7990808]], dtype=float32)

time = 43550	action = 1	current_phase = 0	next_phase = 1	reward = -1.097107	array([[-5.4411373, -2.6964567]], dtype=float32)

time = 43558	action = 1	current_phase = 1	next_phase = 0	reward = -0.709711	array([[-3.7234206, -2.070366 ]], dtype=float32)

time = 43566	action = 0	current_phase = 0	next_phase = 1	reward = -0.091961	array([[-1.7571913, -2.7525303]], dtype=float32)

time = 43571	action = 0	current_phase = 0	next_phase = 1	reward = -0.019256	array([[-2.0023522, -2.783995 ]], dtype=float32)

time = 43576	action = 0	current_phase = 0	next_phase = 1	reward = 0.055857	array([[-2.3558035, -2.9186532]], dtype=float32)

time = 43581	action = 1	current_phase = 0	next_phase = 1	reward = -1.364293	array([[-6.510089, -3.205952]], dtype=float32)

time = 43589	action = 1	current_phase = 1	next_phase = 0	reward = -0.788421	array([[-3.894649, -2.242796]], dtype=float32)

time = 43597	action = 0	current_phase = 0	next_phase = 1	reward = -0.081123	array([[-1.7711797, -2.7366543]], dtype=float32)

time = 43602	action = 0	current_phase = 0	next_phase = 1	reward = -0.000687	array([[-2.1893756, -2.892422 ]], dtype=float32)

time = 43607	action = 0	current_phase = 0	next_phase = 1	reward = 0.068598	array([[-2.7189171, -3.089821 ]], dtype=float32)

time = 43612	action = 1	current_phase = 0	next_phase = 1	reward = -1.573935	array([[-6.4662313, -3.5587523]], dtype=float32)

time = 43620	action = 1	current_phase = 1	next_phase = 0	reward = -0.967684	array([[-4.026574, -2.381876]], dtype=float32)

time = 43628	action = 0	current_phase = 0	next_phase = 1	reward = -0.079466	array([[-1.7090578, -2.714307 ]], dtype=float32)

time = 43633	action = 0	current_phase = 0	next_phase = 1	reward = 0.005581	array([[-2.2247953, -2.8624506]], dtype=float32)

time = 43638	action = 0	current_phase = 0	next_phase = 1	reward = 0.069921	array([[-2.8026104, -3.145641 ]], dtype=float32)

time = 43643	action = 1	current_phase = 0	next_phase = 1	reward = -1.721927	array([[-6.4814615, -3.5007002]], dtype=float32)

time = 43651	action = 1	current_phase = 1	next_phase = 0	reward = -1.090695	array([[-4.318892 , -2.3545072]], dtype=float32)

time = 43659	action = 0	current_phase = 0	next_phase = 1	reward = -0.036536	array([[-1.2850195, -2.129262 ]], dtype=float32)

time = 43664	action = 0	current_phase = 0	next_phase = 1	reward = 0.037695	array([[-1.6887566, -2.7057264]], dtype=float32)

time = 43669	action = 0	current_phase = 0	next_phase = 1	reward = 0.070384	array([[-2.364826, -2.39986 ]], dtype=float32)

time = 43674	action = 1	current_phase = 0	next_phase = 1	reward = -1.063847	array([[-6.469105 , -3.0240645]], dtype=float32)

time = 43682	action = 1	current_phase = 1	next_phase = 0	reward = -0.760863	array([[-4.02708  , -2.3306623]], dtype=float32)

time = 43690	action = 0	current_phase = 0	next_phase = 1	reward = -0.286987	array([[-1.6659799, -2.1497462]], dtype=float32)

time = 43695	action = 0	current_phase = 0	next_phase = 1	reward = 0.343749	array([[-2.061626 , -2.8071127]], dtype=float32)

time = 43700	action = 1	current_phase = 0	next_phase = 1	reward = -1.415095	array([[-5.4956255, -2.8863187]], dtype=float32)

time = 43708	action = 1	current_phase = 1	next_phase = 0	reward = -0.709027	array([[-3.749093, -2.061063]], dtype=float32)

time = 43716	action = 0	current_phase = 0	next_phase = 1	reward = -0.077520	array([[-1.7725704, -2.7420242]], dtype=float32)

time = 43721	action = 0	current_phase = 0	next_phase = 1	reward = -0.006889	array([[-2.059666 , -2.8066852]], dtype=float32)

time = 43726	action = 0	current_phase = 0	next_phase = 1	reward = 0.053636	array([[-2.468993 , -2.9704123]], dtype=float32)

time = 43731	action = 1	current_phase = 0	next_phase = 1	reward = -1.415797	array([[-6.538514 , -3.2246468]], dtype=float32)

time = 43739	action = 1	current_phase = 1	next_phase = 0	reward = -1.047053	array([[-3.891675 , -2.2326066]], dtype=float32)

time = 43747	action = 0	current_phase = 0	next_phase = 1	reward = 0.215071	array([[-1.7151749, -2.7256534]], dtype=float32)

time = 43752	action = 0	current_phase = 0	next_phase = 1	reward = 0.014075	array([[-2.1988416, -2.8613305]], dtype=float32)

time = 43757	action = 0	current_phase = 0	next_phase = 1	reward = 0.071323	array([[-2.7279577, -3.094112 ]], dtype=float32)

time = 43762	action = 1	current_phase = 0	next_phase = 1	reward = -1.541898	array([[-6.482669 , -3.5162516]], dtype=float32)

time = 43770	action = 1	current_phase = 1	next_phase = 0	reward = -0.963960	array([[-4.049262 , -2.3898988]], dtype=float32)

time = 43778	action = 0	current_phase = 0	next_phase = 1	reward = -0.062118	array([[-1.8504889, -2.769758 ]], dtype=float32)

time = 43783	action = 0	current_phase = 0	next_phase = 1	reward = 0.026210	array([[-2.2047935, -2.86033  ]], dtype=float32)

time = 43788	action = 0	current_phase = 0	next_phase = 1	reward = 0.084190	array([[-2.782639 , -3.1355493]], dtype=float32)

time = 43793	action = 1	current_phase = 0	next_phase = 1	reward = -1.912063	array([[-6.4383926, -3.661863 ]], dtype=float32)

time = 43801	action = 1	current_phase = 1	next_phase = 0	reward = -1.152911	array([[-4.3211327, -2.3162386]], dtype=float32)

time = 43809	action = 0	current_phase = 0	next_phase = 1	reward = -0.040297	array([[-1.2741725, -2.1245818]], dtype=float32)

time = 43814	action = 0	current_phase = 0	next_phase = 1	reward = 0.024537	array([[-1.778128 , -2.7376401]], dtype=float32)

time = 43819	action = 1	current_phase = 0	next_phase = 1	reward = -0.717641	array([[-2.3687832, -2.3525274]], dtype=float32)

time = 43827	action = 1	current_phase = 1	next_phase = 0	reward = -0.926781	array([[-3.5418005, -1.9246002]], dtype=float32)

time = 43835	action = 0	current_phase = 0	next_phase = 1	reward = 0.185115	array([[-1.4485152, -2.8975036]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0210 - val_loss: 0.0092

Epoch 2/50

 - 3s - loss: 0.0185 - val_loss: 0.0098

Epoch 3/50

 - 3s - loss: 0.0218 - val_loss: 0.0101

Epoch 4/50

 - 3s - loss: 0.0178 - val_loss: 0.0097

Epoch 5/50

 - 3s - loss: 0.0202 - val_loss: 0.0088

Epoch 6/50

 - 3s - loss: 0.0211 - val_loss: 0.0086

Epoch 7/50

 - 3s - loss: 0.0162 - val_loss: 0.0098

Epoch 8/50

 - 3s - loss: 0.0204 - val_loss: 0.0126

Epoch 9/50

 - 3s - loss: 0.0181 - val_loss: 0.0107

Epoch 10/50

 - 3s - loss: 0.0178 - val_loss: 0.0098

Epoch 11/50

 - 3s - loss: 0.0197 - val_loss: 0.0092

Epoch 12/50

 - 3s - loss: 0.0169 - val_loss: 0.0130

Epoch 13/50

 - 3s - loss: 0.0160 - val_loss: 0.0089

Epoch 14/50

 - 3s - loss: 0.0177 - val_loss: 0.0086

Epoch 15/50

 - 3s - loss: 0.0161 - val_loss: 0.0113

Epoch 16/50

 - 3s - loss: 0.0187 - val_loss: 0.0100

Epoch 17/50

 - 3s - loss: 0.0170 - val_loss: 0.0089

Epoch 18/50

 - 3s - loss: 0.0175 - val_loss: 0.0114

Epoch 19/50

 - 3s - loss: 0.0156 - val_loss: 0.0093

Epoch 20/50

 - 3s - loss: 0.0166 - val_loss: 0.0122

Epoch 21/50

 - 3s - loss: 0.0202 - val_loss: 0.0106

Epoch 22/50

 - 3s - loss: 0.0171 - val_loss: 0.0099

Epoch 23/50

 - 3s - loss: 0.0175 - val_loss: 0.0100

Epoch 24/50

 - 3s - loss: 0.0155 - val_loss: 0.0093

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 43840	action = 0	current_phase = 0	next_phase = 1	reward = -0.563528	array([[-1.8081911, -2.190438 ]], dtype=float32)

time = 43845	action = 0	current_phase = 0	next_phase = 1	reward = 0.633597	array([[-2.0642798, -2.8240776]], dtype=float32)

time = 43850	action = 1	current_phase = 0	next_phase = 1	reward = -1.371424	array([[-5.479946, -3.014552]], dtype=float32)

time = 43858	action = 1	current_phase = 1	next_phase = 0	reward = -0.707393	array([[-3.7754297, -2.1246817]], dtype=float32)

time = 43866	action = 0	current_phase = 0	next_phase = 1	reward = -0.091856	array([[-1.8219237, -2.7697575]], dtype=float32)

time = 43871	action = 0	current_phase = 0	next_phase = 1	reward = -0.011056	array([[-2.0104818, -2.8191137]], dtype=float32)

time = 43876	action = 0	current_phase = 0	next_phase = 1	reward = 0.047206	array([[-2.61445  , -3.0407038]], dtype=float32)

time = 43881	action = 1	current_phase = 0	next_phase = 1	reward = -1.557402	array([[-6.545533, -3.364155]], dtype=float32)

time = 43889	action = 1	current_phase = 1	next_phase = 0	reward = -1.141935	array([[-3.8984146, -2.2682357]], dtype=float32)

time = 43897	action = 0	current_phase = 0	next_phase = 1	reward = 0.216416	array([[-1.7335618, -2.7611465]], dtype=float32)

time = 43902	action = 0	current_phase = 0	next_phase = 1	reward = 0.011869	array([[-2.1579003, -2.9201763]], dtype=float32)

time = 43907	action = 0	current_phase = 0	next_phase = 1	reward = 0.057428	array([[-2.7780051, -3.138686 ]], dtype=float32)

time = 43912	action = 1	current_phase = 0	next_phase = 1	reward = -1.589594	array([[-6.474095 , -3.6558263]], dtype=float32)

time = 43920	action = 1	current_phase = 1	next_phase = 0	reward = -0.894892	array([[-3.997089 , -2.3575969]], dtype=float32)

time = 43928	action = 0	current_phase = 0	next_phase = 1	reward = -0.040176	array([[-1.8740692, -2.8146608]], dtype=float32)

time = 43933	action = 0	current_phase = 0	next_phase = 1	reward = 0.030038	array([[-2.2327309, -2.9049094]], dtype=float32)

time = 43938	action = 0	current_phase = 0	next_phase = 1	reward = 0.079727	array([[-2.8533134, -3.2203379]], dtype=float32)

time = 43943	action = 1	current_phase = 0	next_phase = 1	reward = -0.802576	array([[-6.507811, -3.106676]], dtype=float32)

time = 43951	action = 1	current_phase = 1	next_phase = 0	reward = -1.285554	array([[-3.9099364, -2.3980126]], dtype=float32)

time = 43959	action = 0	current_phase = 0	next_phase = 1	reward = -0.035628	array([[-1.2890351, -2.1547177]], dtype=float32)

time = 43964	action = 0	current_phase = 0	next_phase = 1	reward = 0.035414	array([[-1.7799969, -2.7666972]], dtype=float32)

time = 43969	action = 0	current_phase = 0	next_phase = 1	reward = 0.070039	array([[-2.099071 , -2.4045675]], dtype=float32)

time = 43974	action = 1	current_phase = 0	next_phase = 1	reward = -1.035627	array([[-6.403857 , -2.9710705]], dtype=float32)

time = 43982	action = 1	current_phase = 1	next_phase = 0	reward = -0.699819	array([[-3.8076148, -2.279088 ]], dtype=float32)

time = 43990	action = 0	current_phase = 0	next_phase = 1	reward = -0.027049	array([[-1.6845734, -2.2350454]], dtype=float32)

time = 43995	action = 0	current_phase = 0	next_phase = 1	reward = 0.040406	array([[-2.132812, -2.841854]], dtype=float32)

time = 44000	action = 1	current_phase = 0	next_phase = 1	reward = -1.373138	array([[-5.49644  , -2.9263973]], dtype=float32)

time = 44008	action = 1	current_phase = 1	next_phase = 0	reward = -0.713047	array([[-3.7179255, -2.089416 ]], dtype=float32)

time = 44016	action = 0	current_phase = 0	next_phase = 1	reward = -0.081072	array([[-1.7241051, -2.760326 ]], dtype=float32)

time = 44021	action = 0	current_phase = 0	next_phase = 1	reward = -0.000370	array([[-2.0590296, -2.8375936]], dtype=float32)

time = 44026	action = 0	current_phase = 0	next_phase = 1	reward = 0.060222	array([[-2.5943108, -3.035671 ]], dtype=float32)

time = 44031	action = 1	current_phase = 0	next_phase = 1	reward = -1.401377	array([[-6.531387 , -3.3431764]], dtype=float32)

time = 44039	action = 1	current_phase = 1	next_phase = 0	reward = -0.776461	array([[-3.846641 , -2.2150354]], dtype=float32)

time = 44047	action = 0	current_phase = 0	next_phase = 1	reward = -0.072467	array([[-1.8245537, -2.7871234]], dtype=float32)

time = 44052	action = 0	current_phase = 0	next_phase = 1	reward = 0.008930	array([[-2.2029152, -2.9002662]], dtype=float32)

time = 44057	action = 0	current_phase = 0	next_phase = 1	reward = 0.073300	array([[-2.8328574, -3.1334164]], dtype=float32)

time = 44062	action = 1	current_phase = 0	next_phase = 1	reward = -1.634083	array([[-6.517598, -3.555878]], dtype=float32)

time = 44070	action = 1	current_phase = 1	next_phase = 0	reward = -1.216292	array([[-4.042117 , -2.3902407]], dtype=float32)

time = 44078	action = 0	current_phase = 0	next_phase = 1	reward = 0.231002	array([[-1.890457 , -2.8058848]], dtype=float32)

time = 44083	action = 0	current_phase = 0	next_phase = 1	reward = 0.029544	array([[-2.2514713, -2.897308 ]], dtype=float32)

time = 44088	action = 0	current_phase = 0	next_phase = 1	reward = 0.080216	array([[-2.829749 , -3.2664483]], dtype=float32)

time = 44093	action = 1	current_phase = 0	next_phase = 1	reward = -1.337833	array([[-6.519087, -3.431197]], dtype=float32)

time = 44101	action = 1	current_phase = 1	next_phase = 0	reward = -1.428112	array([[-3.9875803, -2.3727012]], dtype=float32)

time = 44109	action = 0	current_phase = 0	next_phase = 1	reward = 0.247128	array([[-1.2128688, -2.154623 ]], dtype=float32)

time = 44114	action = 0	current_phase = 0	next_phase = 1	reward = 0.027664	array([[-1.6642019, -2.7620153]], dtype=float32)

time = 44119	action = 0	current_phase = 0	next_phase = 1	reward = 0.068503	array([[-2.303502 , -2.4318945]], dtype=float32)

time = 44124	action = 1	current_phase = 0	next_phase = 1	reward = -1.009199	array([[-6.4590335, -2.9822414]], dtype=float32)

time = 44132	action = 1	current_phase = 1	next_phase = 0	reward = -0.703353	array([[-3.7861605, -2.2684152]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0272 - val_loss: 0.0099

Epoch 2/50

 - 3s - loss: 0.0204 - val_loss: 0.0081

Epoch 3/50

 - 3s - loss: 0.0202 - val_loss: 0.0111

Epoch 4/50

 - 3s - loss: 0.0217 - val_loss: 0.0125

Epoch 5/50

 - 3s - loss: 0.0259 - val_loss: 0.0104

Epoch 6/50

 - 3s - loss: 0.0191 - val_loss: 0.0133

Epoch 7/50

 - 3s - loss: 0.0213 - val_loss: 0.0106

Epoch 8/50

 - 3s - loss: 0.0216 - val_loss: 0.0084

Epoch 9/50

 - 3s - loss: 0.0183 - val_loss: 0.0101

Epoch 10/50

 - 3s - loss: 0.0215 - val_loss: 0.0113

Epoch 11/50

 - 3s - loss: 0.0239 - val_loss: 0.0095

Epoch 12/50

 - 3s - loss: 0.0195 - val_loss: 0.0108

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 44140	action = 0	current_phase = 0	next_phase = 1	reward = -0.024623	array([[-1.7747414, -2.227239 ]], dtype=float32)

time = 44145	action = 0	current_phase = 0	next_phase = 1	reward = -0.510946	array([[-2.124318 , -2.9166994]], dtype=float32)

time = 44150	action = 1	current_phase = 0	next_phase = 1	reward = -0.861660	array([[-5.4941545, -2.6046019]], dtype=float32)

time = 44158	action = 1	current_phase = 1	next_phase = 0	reward = -0.715587	array([[-3.7148194, -2.0546699]], dtype=float32)

time = 44166	action = 0	current_phase = 0	next_phase = 1	reward = -0.091464	array([[-1.7440503, -2.7939851]], dtype=float32)

time = 44171	action = 0	current_phase = 0	next_phase = 1	reward = -0.015936	array([[-1.9302752, -2.8286512]], dtype=float32)

time = 44176	action = 0	current_phase = 0	next_phase = 1	reward = 0.053740	array([[-2.4941974, -3.0272613]], dtype=float32)

time = 44181	action = 1	current_phase = 0	next_phase = 1	reward = -1.377643	array([[-6.5614605, -3.3007715]], dtype=float32)

time = 44189	action = 1	current_phase = 1	next_phase = 0	reward = -0.826891	array([[-3.8930464, -2.2293317]], dtype=float32)

time = 44197	action = 0	current_phase = 0	next_phase = 1	reward = -0.083011	array([[-1.7711372, -2.8032298]], dtype=float32)

time = 44202	action = 0	current_phase = 0	next_phase = 1	reward = -0.001986	array([[-2.0972352, -2.9162827]], dtype=float32)

time = 44207	action = 0	current_phase = 0	next_phase = 1	reward = 0.054378	array([[-2.7144365, -3.1520796]], dtype=float32)

time = 44212	action = 1	current_phase = 0	next_phase = 1	reward = -1.627801	array([[-6.514237 , -3.6070137]], dtype=float32)

time = 44220	action = 1	current_phase = 1	next_phase = 0	reward = -1.213019	array([[-3.9767041, -2.3244884]], dtype=float32)

time = 44228	action = 0	current_phase = 0	next_phase = 1	reward = 0.223693	array([[-1.8111941, -2.8026414]], dtype=float32)

time = 44233	action = 0	current_phase = 0	next_phase = 1	reward = 0.016261	array([[-2.185713, -2.919609]], dtype=float32)

time = 44238	action = 0	current_phase = 0	next_phase = 1	reward = 0.078998	array([[-2.8036   , -3.2233222]], dtype=float32)

time = 44243	action = 1	current_phase = 0	next_phase = 1	reward = -1.851602	array([[-6.5018992, -3.716971 ]], dtype=float32)

time = 44251	action = 1	current_phase = 1	next_phase = 0	reward = -0.970070	array([[-4.32401 , -2.288385]], dtype=float32)

time = 44259	action = 0	current_phase = 0	next_phase = 1	reward = -0.036877	array([[-1.3393624, -2.1658092]], dtype=float32)

time = 44264	action = 0	current_phase = 0	next_phase = 1	reward = 0.023485	array([[-1.8971169, -2.8137462]], dtype=float32)

time = 44269	action = 1	current_phase = 0	next_phase = 1	reward = -0.639541	array([[-2.4366431, -2.3466876]], dtype=float32)

time = 44277	action = 1	current_phase = 1	next_phase = 0	reward = -0.594633	array([[-3.5145216, -1.9900815]], dtype=float32)

time = 44285	action = 0	current_phase = 0	next_phase = 1	reward = -0.374877	array([[-1.3997383, -2.9707754]], dtype=float32)

time = 44290	action = 0	current_phase = 0	next_phase = 1	reward = 0.280311	array([[-1.6658568, -2.1926591]], dtype=float32)

time = 44295	action = 0	current_phase = 0	next_phase = 1	reward = -0.206927	array([[-2.1377985, -2.887106 ]], dtype=float32)

time = 44300	action = 1	current_phase = 0	next_phase = 1	reward = -1.149015	array([[-5.478735 , -2.8054712]], dtype=float32)

time = 44308	action = 1	current_phase = 1	next_phase = 0	reward = -0.717385	array([[-3.7987118, -2.1091218]], dtype=float32)

time = 44316	action = 0	current_phase = 0	next_phase = 1	reward = -0.080287	array([[-1.7565684, -2.803554 ]], dtype=float32)

time = 44321	action = 0	current_phase = 0	next_phase = 1	reward = -0.004726	array([[-2.043386 , -2.8748124]], dtype=float32)

time = 44326	action = 0	current_phase = 0	next_phase = 1	reward = 0.061765	array([[-2.5685573, -3.0817037]], dtype=float32)

time = 44331	action = 1	current_phase = 0	next_phase = 1	reward = -1.456397	array([[-6.532946 , -3.3025193]], dtype=float32)

time = 44339	action = 1	current_phase = 1	next_phase = 0	reward = -1.088485	array([[-3.8807006, -2.2061543]], dtype=float32)

time = 44347	action = 0	current_phase = 0	next_phase = 1	reward = 0.221135	array([[-1.7106416, -2.8013608]], dtype=float32)

time = 44352	action = 0	current_phase = 0	next_phase = 1	reward = 0.014643	array([[-2.1744494, -2.9144812]], dtype=float32)

time = 44357	action = 0	current_phase = 0	next_phase = 1	reward = 0.081960	array([[-2.7585418, -3.1721091]], dtype=float32)

time = 44362	action = 1	current_phase = 0	next_phase = 1	reward = -1.715546	array([[-6.5250745, -3.599873 ]], dtype=float32)

time = 44370	action = 1	current_phase = 1	next_phase = 0	reward = -0.972489	array([[-4.0003386, -2.322948 ]], dtype=float32)

time = 44378	action = 0	current_phase = 0	next_phase = 1	reward = -0.073276	array([[-1.84935  , -2.8298612]], dtype=float32)

time = 44383	action = 0	current_phase = 0	next_phase = 1	reward = 0.032165	array([[-2.2189302, -2.9364238]], dtype=float32)

time = 44388	action = 0	current_phase = 0	next_phase = 1	reward = 0.083923	array([[-2.7691302, -3.2804768]], dtype=float32)

time = 44393	action = 1	current_phase = 0	next_phase = 1	reward = -1.913075	array([[-6.522922, -3.607805]], dtype=float32)

time = 44401	action = 1	current_phase = 1	next_phase = 0	reward = -1.023235	array([[-4.3726106, -2.2644372]], dtype=float32)

time = 44409	action = 0	current_phase = 0	next_phase = 1	reward = -0.026549	array([[-1.2923912, -2.1574357]], dtype=float32)

time = 44414	action = 0	current_phase = 0	next_phase = 1	reward = 0.038085	array([[-1.8465645, -2.801374 ]], dtype=float32)

time = 44419	action = 1	current_phase = 0	next_phase = 1	reward = -1.178813	array([[-2.3582633, -2.3493094]], dtype=float32)

time = 44427	action = 1	current_phase = 1	next_phase = 0	reward = -1.127004	array([[-3.5300522, -1.9033446]], dtype=float32)

time = 44435	action = 0	current_phase = 0	next_phase = 1	reward = 0.202664	array([[-1.2106311, -2.9386094]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0193 - val_loss: 0.0074

Epoch 2/50

 - 3s - loss: 0.0184 - val_loss: 0.0086

Epoch 3/50

 - 3s - loss: 0.0193 - val_loss: 0.0078

Epoch 4/50

 - 3s - loss: 0.0180 - val_loss: 0.0080

Epoch 5/50

 - 3s - loss: 0.0226 - val_loss: 0.0090

Epoch 6/50

 - 3s - loss: 0.0200 - val_loss: 0.0084

Epoch 7/50

 - 3s - loss: 0.0214 - val_loss: 0.0079

Epoch 8/50

 - 3s - loss: 0.0180 - val_loss: 0.0077

Epoch 9/50

 - 3s - loss: 0.0176 - val_loss: 0.0104

Epoch 10/50

 - 3s - loss: 0.0169 - val_loss: 0.0101

Epoch 11/50

 - 3s - loss: 0.0183 - val_loss: 0.0091

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 44440	action = 0	current_phase = 0	next_phase = 1	reward = 0.256381	array([[-1.75206  , -2.1905286]], dtype=float32)

time = 44445	action = 0	current_phase = 0	next_phase = 1	reward = -0.230171	array([[-2.3820472, -2.9725873]], dtype=float32)

time = 44450	action = 1	current_phase = 0	next_phase = 1	reward = -1.073936	array([[-5.6321535, -2.773868 ]], dtype=float32)

time = 44458	action = 1	current_phase = 1	next_phase = 0	reward = -0.706079	array([[-3.790387 , -2.0611346]], dtype=float32)

time = 44466	action = 0	current_phase = 0	next_phase = 1	reward = -0.073050	array([[-1.7522421, -2.8429856]], dtype=float32)

time = 44471	action = 0	current_phase = 0	next_phase = 1	reward = 0.012516	array([[-2.0886967, -2.8727293]], dtype=float32)

time = 44476	action = 0	current_phase = 0	next_phase = 1	reward = 0.072863	array([[-2.62998  , -3.0993626]], dtype=float32)

time = 44481	action = 1	current_phase = 0	next_phase = 1	reward = -1.500519	array([[-6.5967484, -3.3116822]], dtype=float32)

time = 44489	action = 1	current_phase = 1	next_phase = 0	reward = -0.836360	array([[-3.9328423, -2.2894547]], dtype=float32)

time = 44497	action = 0	current_phase = 0	next_phase = 1	reward = -0.072644	array([[-1.8011527, -2.825907 ]], dtype=float32)

time = 44502	action = 0	current_phase = 0	next_phase = 1	reward = 0.009720	array([[-2.2303238, -2.9458053]], dtype=float32)

time = 44507	action = 0	current_phase = 0	next_phase = 1	reward = 0.078336	array([[-2.800669 , -3.2252138]], dtype=float32)

time = 44512	action = 1	current_phase = 0	next_phase = 1	reward = -1.660479	array([[-6.545207 , -3.5737822]], dtype=float32)

time = 44520	action = 1	current_phase = 1	next_phase = 0	reward = -0.954126	array([[-4.0598545, -2.4177814]], dtype=float32)

time = 44528	action = 0	current_phase = 0	next_phase = 1	reward = -0.036812	array([[-1.8274904, -2.838421 ]], dtype=float32)

time = 44533	action = 0	current_phase = 0	next_phase = 1	reward = 0.035116	array([[-2.2725868, -2.9487305]], dtype=float32)

time = 44538	action = 0	current_phase = 0	next_phase = 1	reward = 0.078898	array([[-2.8365445, -3.2663488]], dtype=float32)

time = 44543	action = 1	current_phase = 0	next_phase = 1	reward = -1.884989	array([[-6.523263 , -3.7680304]], dtype=float32)

time = 44551	action = 1	current_phase = 1	next_phase = 0	reward = -1.141203	array([[-4.363754, -2.270348]], dtype=float32)

time = 44559	action = 0	current_phase = 0	next_phase = 1	reward = -0.042440	array([[-1.3625555, -2.1542888]], dtype=float32)

time = 44564	action = 0	current_phase = 0	next_phase = 1	reward = 0.021690	array([[-1.9202642, -2.8124845]], dtype=float32)

time = 44569	action = 1	current_phase = 0	next_phase = 1	reward = -0.759625	array([[-2.4119728, -2.325842 ]], dtype=float32)

time = 44577	action = 1	current_phase = 1	next_phase = 0	reward = -0.919168	array([[-3.629146 , -1.9492381]], dtype=float32)

time = 44585	action = 0	current_phase = 0	next_phase = 1	reward = 0.167694	array([[-1.4232061, -2.9546916]], dtype=float32)

time = 44590	action = 0	current_phase = 0	next_phase = 1	reward = -0.045196	array([[-1.7678232, -2.1804404]], dtype=float32)

time = 44595	action = 0	current_phase = 0	next_phase = 1	reward = -0.258475	array([[-2.3366992, -2.9491434]], dtype=float32)

time = 44600	action = 1	current_phase = 0	next_phase = 1	reward = -1.041216	array([[-5.4952316, -2.7644045]], dtype=float32)

time = 44608	action = 1	current_phase = 1	next_phase = 0	reward = -0.720708	array([[-3.7580376, -2.0383992]], dtype=float32)

time = 44616	action = 0	current_phase = 0	next_phase = 1	reward = -0.100463	array([[-1.7780943, -2.834243 ]], dtype=float32)

time = 44621	action = 0	current_phase = 0	next_phase = 1	reward = -0.017692	array([[-1.9557889, -2.8486457]], dtype=float32)

time = 44626	action = 0	current_phase = 0	next_phase = 1	reward = 0.051362	array([[-2.556265, -3.057795]], dtype=float32)

time = 44631	action = 1	current_phase = 0	next_phase = 1	reward = -1.438658	array([[-6.6208878, -3.2888837]], dtype=float32)

time = 44639	action = 1	current_phase = 1	next_phase = 0	reward = -1.119861	array([[-3.9309824, -2.2481356]], dtype=float32)

time = 44647	action = 0	current_phase = 0	next_phase = 1	reward = 0.224552	array([[-1.7681165, -2.8107197]], dtype=float32)

time = 44652	action = 0	current_phase = 0	next_phase = 1	reward = 0.005427	array([[-2.2279007, -2.9331307]], dtype=float32)

time = 44657	action = 0	current_phase = 0	next_phase = 1	reward = 0.073658	array([[-2.8240151, -3.2096121]], dtype=float32)

time = 44662	action = 1	current_phase = 0	next_phase = 1	reward = -1.665509	array([[-6.564682, -3.532512]], dtype=float32)

time = 44670	action = 1	current_phase = 1	next_phase = 0	reward = -1.065390	array([[-4.094761 , -2.3897114]], dtype=float32)

time = 44678	action = 0	current_phase = 0	next_phase = 1	reward = -0.044044	array([[-1.8401337, -2.8280535]], dtype=float32)

time = 44683	action = 0	current_phase = 0	next_phase = 1	reward = 0.028973	array([[-2.2469637, -2.94272  ]], dtype=float32)

time = 44688	action = 0	current_phase = 0	next_phase = 1	reward = 0.076082	array([[-2.8207753, -3.2773721]], dtype=float32)

time = 44693	action = 1	current_phase = 0	next_phase = 1	reward = -1.933551	array([[-6.5452166, -3.7003489]], dtype=float32)

time = 44701	action = 1	current_phase = 1	next_phase = 0	reward = -1.021226	array([[-4.375345 , -2.2796307]], dtype=float32)

time = 44709	action = 0	current_phase = 0	next_phase = 1	reward = -0.024113	array([[-1.3950127, -2.1625297]], dtype=float32)

time = 44714	action = 0	current_phase = 0	next_phase = 1	reward = 0.050112	array([[-1.9099958, -2.8270607]], dtype=float32)

time = 44719	action = 1	current_phase = 0	next_phase = 1	reward = -0.699844	array([[-2.3986635, -2.324976 ]], dtype=float32)

time = 44727	action = 1	current_phase = 1	next_phase = 0	reward = -0.931458	array([[-3.5017164, -1.867229 ]], dtype=float32)

time = 44735	action = 0	current_phase = 0	next_phase = 1	reward = -0.106400	array([[-1.4024763, -2.9294333]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0201 - val_loss: 0.0079

Epoch 2/50

 - 3s - loss: 0.0174 - val_loss: 0.0073

Epoch 3/50

 - 3s - loss: 0.0176 - val_loss: 0.0083

Epoch 4/50

 - 3s - loss: 0.0198 - val_loss: 0.0073

Epoch 5/50

 - 3s - loss: 0.0174 - val_loss: 0.0074

Epoch 6/50

 - 3s - loss: 0.0210 - val_loss: 0.0092

Epoch 7/50

 - 3s - loss: 0.0172 - val_loss: 0.0081

Epoch 8/50

 - 3s - loss: 0.0211 - val_loss: 0.0079

Epoch 9/50

 - 3s - loss: 0.0179 - val_loss: 0.0083

Epoch 10/50

 - 3s - loss: 0.0195 - val_loss: 0.0099

Epoch 11/50

 - 3s - loss: 0.0216 - val_loss: 0.0086

Epoch 12/50

 - 3s - loss: 0.0194 - val_loss: 0.0079

Epoch 13/50

 - 3s - loss: 0.0183 - val_loss: 0.0091

Epoch 14/50

 - 3s - loss: 0.0192 - val_loss: 0.0092

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 44740	action = 0	current_phase = 0	next_phase = 1	reward = 0.245181	array([[-1.7452556, -2.1506672]], dtype=float32)

time = 44745	action = 0	current_phase = 0	next_phase = 1	reward = -0.512468	array([[-2.1959968, -2.9416773]], dtype=float32)

time = 44750	action = 1	current_phase = 0	next_phase = 1	reward = -0.759339	array([[-5.4784565, -2.6491132]], dtype=float32)

time = 44758	action = 1	current_phase = 1	next_phase = 0	reward = -0.707174	array([[-3.7170606, -2.0587482]], dtype=float32)

time = 44766	action = 0	current_phase = 0	next_phase = 1	reward = -0.073331	array([[-1.667902, -2.808921]], dtype=float32)

time = 44771	action = 0	current_phase = 0	next_phase = 1	reward = -0.007541	array([[-2.0137374, -2.8716075]], dtype=float32)

time = 44776	action = 0	current_phase = 0	next_phase = 1	reward = 0.057525	array([[-2.4927576, -3.096118 ]], dtype=float32)

time = 44781	action = 1	current_phase = 0	next_phase = 1	reward = -1.359549	array([[-6.576791 , -3.2132642]], dtype=float32)

time = 44789	action = 1	current_phase = 1	next_phase = 0	reward = -0.830382	array([[-3.8527718, -2.1938128]], dtype=float32)

time = 44797	action = 0	current_phase = 0	next_phase = 1	reward = -0.081400	array([[-1.6689428, -2.7972307]], dtype=float32)

time = 44802	action = 0	current_phase = 0	next_phase = 1	reward = -0.013124	array([[-2.092251 , -2.9048839]], dtype=float32)

time = 44807	action = 0	current_phase = 0	next_phase = 1	reward = 0.075327	array([[-2.647068 , -3.1375606]], dtype=float32)

time = 44812	action = 1	current_phase = 0	next_phase = 1	reward = -1.485882	array([[-6.535541 , -3.4556353]], dtype=float32)

time = 44820	action = 1	current_phase = 1	next_phase = 0	reward = -0.965167	array([[-4.022619 , -2.3898573]], dtype=float32)

time = 44828	action = 0	current_phase = 0	next_phase = 1	reward = -0.070625	array([[-1.759138 , -2.8115144]], dtype=float32)

time = 44833	action = 0	current_phase = 0	next_phase = 1	reward = 0.009959	array([[-2.1804135, -2.9246972]], dtype=float32)

time = 44838	action = 0	current_phase = 0	next_phase = 1	reward = 0.071283	array([[-2.744092 , -3.2652276]], dtype=float32)

time = 44843	action = 1	current_phase = 0	next_phase = 1	reward = -1.280469	array([[-6.570566 , -3.3503556]], dtype=float32)

time = 44851	action = 1	current_phase = 1	next_phase = 0	reward = -1.439396	array([[-4.023462 , -2.3841538]], dtype=float32)

time = 44859	action = 0	current_phase = 0	next_phase = 1	reward = 0.250512	array([[-1.1646516, -2.1180503]], dtype=float32)

time = 44864	action = 0	current_phase = 0	next_phase = 1	reward = 0.037004	array([[-1.7607627, -2.7940326]], dtype=float32)

time = 44869	action = 0	current_phase = 0	next_phase = 1	reward = 0.058958	array([[-2.2709322, -2.3461049]], dtype=float32)

time = 44874	action = 1	current_phase = 0	next_phase = 1	reward = -1.026991	array([[-6.493282 , -2.9882412]], dtype=float32)

time = 44882	action = 1	current_phase = 1	next_phase = 0	reward = -0.708828	array([[-3.7244377, -2.228993 ]], dtype=float32)

time = 44890	action = 0	current_phase = 0	next_phase = 1	reward = -0.302644	array([[-1.7598412, -2.1868439]], dtype=float32)

time = 44895	action = 0	current_phase = 0	next_phase = 1	reward = 0.054234	array([[-2.2108989, -2.9696426]], dtype=float32)

time = 44900	action = 1	current_phase = 0	next_phase = 1	reward = -1.085324	array([[-5.5008836, -2.6775913]], dtype=float32)

time = 44908	action = 1	current_phase = 1	next_phase = 0	reward = -0.676916	array([[-3.6824021, -2.0995836]], dtype=float32)

time = 44916	action = 0	current_phase = 0	next_phase = 1	reward = -0.068404	array([[-1.6993388, -2.7991893]], dtype=float32)

time = 44921	action = 0	current_phase = 0	next_phase = 1	reward = -0.011503	array([[-2.0402284, -2.8821352]], dtype=float32)

time = 44926	action = 0	current_phase = 0	next_phase = 1	reward = 0.063640	array([[-2.47927  , -3.0777571]], dtype=float32)

time = 44931	action = 1	current_phase = 0	next_phase = 1	reward = -1.451300	array([[-6.583838, -3.319155]], dtype=float32)

time = 44939	action = 1	current_phase = 1	next_phase = 0	reward = -0.833436	array([[-3.9103217, -2.2296124]], dtype=float32)

time = 44947	action = 0	current_phase = 0	next_phase = 1	reward = -0.082625	array([[-1.6522467, -2.7920403]], dtype=float32)

time = 44952	action = 0	current_phase = 0	next_phase = 1	reward = 0.016778	array([[-2.0960593, -2.9133916]], dtype=float32)

time = 44957	action = 0	current_phase = 0	next_phase = 1	reward = 0.081851	array([[-2.7273803, -3.219591 ]], dtype=float32)

time = 44962	action = 1	current_phase = 0	next_phase = 1	reward = -1.655414	array([[-6.5441847, -3.4980223]], dtype=float32)

time = 44970	action = 1	current_phase = 1	next_phase = 0	reward = -1.305075	array([[-4.0634303, -2.4038758]], dtype=float32)

time = 44978	action = 0	current_phase = 0	next_phase = 1	reward = 0.246467	array([[-1.6772842, -2.7883332]], dtype=float32)

time = 44983	action = 0	current_phase = 0	next_phase = 1	reward = 0.014330	array([[-2.1503725, -2.9369545]], dtype=float32)

time = 44988	action = 0	current_phase = 0	next_phase = 1	reward = 0.062361	array([[-2.7733526, -3.2879906]], dtype=float32)

time = 44993	action = 1	current_phase = 0	next_phase = 1	reward = -1.252532	array([[-6.568114 , -3.2352195]], dtype=float32)

time = 45001	action = 1	current_phase = 1	next_phase = 0	reward = -1.480028	array([[-3.9762454, -2.3364406]], dtype=float32)

time = 45009	action = 0	current_phase = 0	next_phase = 1	reward = 0.247213	array([[-1.1293038, -2.111544 ]], dtype=float32)

time = 45014	action = 0	current_phase = 0	next_phase = 1	reward = 0.030051	array([[-1.8293881, -2.8275309]], dtype=float32)

time = 45019	action = 0	current_phase = 0	next_phase = 1	reward = 0.072214	array([[-2.1494608, -2.3170128]], dtype=float32)

time = 45024	action = 1	current_phase = 0	next_phase = 1	reward = -1.023338	array([[-6.464566, -2.877062]], dtype=float32)

time = 45032	action = 1	current_phase = 1	next_phase = 0	reward = -0.972341	array([[-3.6457376, -2.2351778]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0190 - val_loss: 0.0122

Epoch 2/50

 - 3s - loss: 0.0203 - val_loss: 0.0096

Epoch 3/50

 - 3s - loss: 0.0175 - val_loss: 0.0143

Epoch 4/50

 - 3s - loss: 0.0237 - val_loss: 0.0113

Epoch 5/50

 - 3s - loss: 0.0184 - val_loss: 0.0094

Epoch 6/50

 - 3s - loss: 0.0176 - val_loss: 0.0107

Epoch 7/50

 - 3s - loss: 0.0178 - val_loss: 0.0124

Epoch 8/50

 - 3s - loss: 0.0152 - val_loss: 0.0101

Epoch 9/50

 - 3s - loss: 0.0162 - val_loss: 0.0091

Epoch 10/50

 - 3s - loss: 0.0170 - val_loss: 0.0109

Epoch 11/50

 - 3s - loss: 0.0167 - val_loss: 0.0107

Epoch 12/50

 - 3s - loss: 0.0161 - val_loss: 0.0094

Epoch 13/50

 - 3s - loss: 0.0180 - val_loss: 0.0113

Epoch 14/50

 - 3s - loss: 0.0159 - val_loss: 0.0119

Epoch 15/50

 - 3s - loss: 0.0205 - val_loss: 0.0135

Epoch 16/50

 - 3s - loss: 0.0189 - val_loss: 0.0141

Epoch 17/50

 - 3s - loss: 0.0159 - val_loss: 0.0158

Epoch 18/50

 - 3s - loss: 0.0158 - val_loss: 0.0146

Epoch 19/50

 - 3s - loss: 0.0227 - val_loss: 0.0107

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 45040	action = 0	current_phase = 0	next_phase = 1	reward = -0.016186	array([[-1.6582261, -2.1751652]], dtype=float32)

time = 45045	action = 0	current_phase = 0	next_phase = 1	reward = 0.326343	array([[-2.007855, -2.852127]], dtype=float32)

time = 45050	action = 1	current_phase = 0	next_phase = 1	reward = -1.247788	array([[-5.5315123, -2.8472295]], dtype=float32)

time = 45058	action = 1	current_phase = 1	next_phase = 0	reward = -0.711369	array([[-3.6646411, -2.1274316]], dtype=float32)

time = 45066	action = 0	current_phase = 0	next_phase = 1	reward = -0.102617	array([[-1.5859225, -2.789135 ]], dtype=float32)

time = 45071	action = 0	current_phase = 0	next_phase = 1	reward = -0.006960	array([[-1.9648324, -2.832645 ]], dtype=float32)

time = 45076	action = 0	current_phase = 0	next_phase = 1	reward = 0.061645	array([[-2.5387764, -3.0491438]], dtype=float32)

time = 45081	action = 1	current_phase = 0	next_phase = 1	reward = -1.439968	array([[-6.625121, -3.305997]], dtype=float32)

time = 45089	action = 1	current_phase = 1	next_phase = 0	reward = -0.845615	array([[-3.8992217, -2.3056693]], dtype=float32)

time = 45097	action = 0	current_phase = 0	next_phase = 1	reward = -0.061748	array([[-1.7522551, -2.7985356]], dtype=float32)

time = 45102	action = 0	current_phase = 0	next_phase = 1	reward = -0.005624	array([[-2.129867 , -2.9171262]], dtype=float32)

time = 45107	action = 0	current_phase = 0	next_phase = 1	reward = 0.057287	array([[-2.723804 , -3.1644678]], dtype=float32)

time = 45112	action = 1	current_phase = 0	next_phase = 1	reward = -1.636578	array([[-6.5516434, -3.5229445]], dtype=float32)

time = 45120	action = 1	current_phase = 1	next_phase = 0	reward = -0.832574	array([[-4.0431204, -2.45337  ]], dtype=float32)

time = 45128	action = 0	current_phase = 0	next_phase = 1	reward = -0.056728	array([[-1.7352686, -2.770458 ]], dtype=float32)

time = 45133	action = 0	current_phase = 0	next_phase = 1	reward = 0.013012	array([[-2.1686037, -2.8977685]], dtype=float32)

time = 45138	action = 0	current_phase = 0	next_phase = 1	reward = 0.072435	array([[-2.8011365, -3.213552 ]], dtype=float32)

time = 45143	action = 1	current_phase = 0	next_phase = 1	reward = -1.834819	array([[-6.5381746, -3.6047115]], dtype=float32)

time = 45151	action = 1	current_phase = 1	next_phase = 0	reward = -1.032091	array([[-4.397298, -2.306567]], dtype=float32)

time = 45159	action = 0	current_phase = 0	next_phase = 1	reward = -0.040110	array([[-1.2532606, -2.1103282]], dtype=float32)

time = 45164	action = 0	current_phase = 0	next_phase = 1	reward = 0.028081	array([[-1.7795501, -2.7837272]], dtype=float32)

time = 45169	action = 1	current_phase = 0	next_phase = 1	reward = -0.714440	array([[-2.2981362, -2.2572527]], dtype=float32)

time = 45177	action = 1	current_phase = 1	next_phase = 0	reward = -1.201565	array([[-3.5286386, -1.9663688]], dtype=float32)

time = 45185	action = 0	current_phase = 0	next_phase = 1	reward = 0.459527	array([[-1.0379692, -3.0288734]], dtype=float32)

time = 45190	action = 0	current_phase = 0	next_phase = 1	reward = -0.038956	array([[-1.787497, -2.135404]], dtype=float32)

time = 45195	action = 0	current_phase = 0	next_phase = 1	reward = 0.031040	array([[-2.1593451, -2.8874197]], dtype=float32)

time = 45200	action = 1	current_phase = 0	next_phase = 1	reward = -1.283291	array([[-5.512585 , -2.9389114]], dtype=float32)

time = 45208	action = 1	current_phase = 1	next_phase = 0	reward = -0.700762	array([[-3.7090907, -2.0988958]], dtype=float32)

time = 45216	action = 0	current_phase = 0	next_phase = 1	reward = -0.088328	array([[-1.7303375, -2.8121958]], dtype=float32)

time = 45221	action = 0	current_phase = 0	next_phase = 1	reward = -0.018847	array([[-1.9856595, -2.8505387]], dtype=float32)

time = 45226	action = 0	current_phase = 0	next_phase = 1	reward = 0.049800	array([[-2.4609222, -3.0222974]], dtype=float32)

time = 45231	action = 1	current_phase = 0	next_phase = 1	reward = -1.461869	array([[-6.562885 , -3.2551827]], dtype=float32)

time = 45239	action = 1	current_phase = 1	next_phase = 0	reward = -0.756728	array([[-3.862524 , -2.2650442]], dtype=float32)

time = 45247	action = 0	current_phase = 0	next_phase = 1	reward = -0.075013	array([[-1.7652385, -2.796939 ]], dtype=float32)

time = 45252	action = 0	current_phase = 0	next_phase = 1	reward = 0.015811	array([[-2.1248317, -2.9110985]], dtype=float32)

time = 45257	action = 0	current_phase = 0	next_phase = 1	reward = 0.078285	array([[-2.7174373, -3.157886 ]], dtype=float32)

time = 45262	action = 1	current_phase = 0	next_phase = 1	reward = -1.635502	array([[-6.5284424, -3.4662147]], dtype=float32)

time = 45270	action = 1	current_phase = 1	next_phase = 0	reward = -1.008491	array([[-4.0734797, -2.4790869]], dtype=float32)

time = 45278	action = 0	current_phase = 0	next_phase = 1	reward = -0.050599	array([[-1.7574939, -2.7994475]], dtype=float32)

time = 45283	action = 0	current_phase = 0	next_phase = 1	reward = 0.025902	array([[-2.2193837, -2.92596  ]], dtype=float32)

time = 45288	action = 0	current_phase = 0	next_phase = 1	reward = 0.084313	array([[-2.8008451, -3.256575 ]], dtype=float32)

time = 45293	action = 1	current_phase = 0	next_phase = 1	reward = -1.328042	array([[-6.5714455, -3.4845533]], dtype=float32)

time = 45301	action = 1	current_phase = 1	next_phase = 0	reward = -1.196238	array([[-4.028064 , -2.2939494]], dtype=float32)

time = 45309	action = 0	current_phase = 0	next_phase = 1	reward = -0.042649	array([[-1.1838715, -2.1114893]], dtype=float32)

time = 45314	action = 0	current_phase = 0	next_phase = 1	reward = 0.024303	array([[-1.8295555, -2.8098395]], dtype=float32)

time = 45319	action = 0	current_phase = 0	next_phase = 1	reward = 0.063123	array([[-2.2193923, -2.3962803]], dtype=float32)

time = 45324	action = 1	current_phase = 0	next_phase = 1	reward = -0.946223	array([[-6.5129476, -2.8852882]], dtype=float32)

time = 45332	action = 1	current_phase = 1	next_phase = 0	reward = -0.964218	array([[-3.757902 , -2.2661428]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0230 - val_loss: 0.0090

Epoch 2/50

 - 3s - loss: 0.0198 - val_loss: 0.0126

Epoch 3/50

 - 3s - loss: 0.0190 - val_loss: 0.0098

Epoch 4/50

 - 3s - loss: 0.0206 - val_loss: 0.0125

Epoch 5/50

 - 3s - loss: 0.0176 - val_loss: 0.0099

Epoch 6/50

 - 3s - loss: 0.0221 - val_loss: 0.0099

Epoch 7/50

 - 3s - loss: 0.0161 - val_loss: 0.0126

Epoch 8/50

 - 3s - loss: 0.0163 - val_loss: 0.0127

Epoch 9/50

 - 3s - loss: 0.0172 - val_loss: 0.0122

Epoch 10/50

 - 3s - loss: 0.0161 - val_loss: 0.0112

Epoch 11/50

 - 3s - loss: 0.0182 - val_loss: 0.0101

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 45340	action = 0	current_phase = 0	next_phase = 1	reward = 0.258438	array([[-1.7413127, -2.219902 ]], dtype=float32)

time = 45345	action = 0	current_phase = 0	next_phase = 1	reward = 0.045908	array([[-2.097422, -2.891014]], dtype=float32)

time = 45350	action = 1	current_phase = 0	next_phase = 1	reward = -1.329065	array([[-5.5333757, -2.903429 ]], dtype=float32)

time = 45358	action = 1	current_phase = 1	next_phase = 0	reward = -0.690997	array([[-3.6442473, -2.0747452]], dtype=float32)

time = 45366	action = 0	current_phase = 0	next_phase = 1	reward = -0.083131	array([[-1.7169769, -2.808136 ]], dtype=float32)

time = 45371	action = 0	current_phase = 0	next_phase = 1	reward = -0.006343	array([[-2.0968645, -2.8692284]], dtype=float32)

time = 45376	action = 0	current_phase = 0	next_phase = 1	reward = 0.064959	array([[-2.4916227, -3.0612626]], dtype=float32)

time = 45381	action = 1	current_phase = 0	next_phase = 1	reward = -1.409290	array([[-6.6460495, -3.320753 ]], dtype=float32)

time = 45389	action = 1	current_phase = 1	next_phase = 0	reward = -0.781244	array([[-3.8550365, -2.2238998]], dtype=float32)

time = 45397	action = 0	current_phase = 0	next_phase = 1	reward = -0.067749	array([[-1.7965221, -2.8119006]], dtype=float32)

time = 45402	action = 0	current_phase = 0	next_phase = 1	reward = -0.001622	array([[-2.1433077, -2.9108386]], dtype=float32)

time = 45407	action = 0	current_phase = 0	next_phase = 1	reward = 0.053851	array([[-2.7436674, -3.1853168]], dtype=float32)

time = 45412	action = 1	current_phase = 0	next_phase = 1	reward = -1.641897	array([[-6.5951557, -3.5788264]], dtype=float32)

time = 45420	action = 1	current_phase = 1	next_phase = 0	reward = -0.945676	array([[-4.027066 , -2.4266176]], dtype=float32)

time = 45428	action = 0	current_phase = 0	next_phase = 1	reward = -0.051289	array([[-1.8712018, -2.8304062]], dtype=float32)

time = 45433	action = 0	current_phase = 0	next_phase = 1	reward = 0.022641	array([[-2.2398722, -2.9361413]], dtype=float32)

time = 45438	action = 0	current_phase = 0	next_phase = 1	reward = 0.078077	array([[-2.802937 , -3.2985895]], dtype=float32)

time = 45443	action = 1	current_phase = 0	next_phase = 1	reward = -1.886649	array([[-6.577163 , -3.7638488]], dtype=float32)

time = 45451	action = 1	current_phase = 1	next_phase = 0	reward = -1.102650	array([[-4.307237 , -2.2881007]], dtype=float32)

time = 45459	action = 0	current_phase = 0	next_phase = 1	reward = -0.042581	array([[-1.3584557, -2.1351826]], dtype=float32)

time = 45464	action = 0	current_phase = 0	next_phase = 1	reward = 0.030307	array([[-1.7998643, -2.7786589]], dtype=float32)

time = 45469	action = 1	current_phase = 0	next_phase = 1	reward = -0.636131	array([[-2.3967671, -2.3572092]], dtype=float32)

time = 45477	action = 1	current_phase = 1	next_phase = 0	reward = -0.867728	array([[-3.4935303, -1.8898852]], dtype=float32)

time = 45485	action = 0	current_phase = 0	next_phase = 1	reward = 0.158454	array([[-1.4646778, -2.9638546]], dtype=float32)

time = 45490	action = 0	current_phase = 0	next_phase = 1	reward = -0.312939	array([[-1.8366299, -2.1678483]], dtype=float32)

time = 45495	action = 0	current_phase = 0	next_phase = 1	reward = 0.325061	array([[-2.1144547, -2.877655 ]], dtype=float32)

time = 45500	action = 1	current_phase = 0	next_phase = 1	reward = -1.269394	array([[-5.5420427, -3.03118  ]], dtype=float32)

time = 45508	action = 1	current_phase = 1	next_phase = 0	reward = -0.704518	array([[-3.719127 , -2.1058745]], dtype=float32)

time = 45516	action = 0	current_phase = 0	next_phase = 1	reward = -0.082006	array([[-1.7696363, -2.81048  ]], dtype=float32)

time = 45521	action = 0	current_phase = 0	next_phase = 1	reward = -0.012927	array([[-1.9959445, -2.8472462]], dtype=float32)

time = 45526	action = 0	current_phase = 0	next_phase = 1	reward = 0.063299	array([[-2.4681268, -3.0517712]], dtype=float32)

time = 45531	action = 1	current_phase = 0	next_phase = 1	reward = -1.435067	array([[-6.628033, -3.347697]], dtype=float32)

time = 45539	action = 1	current_phase = 1	next_phase = 0	reward = -0.827297	array([[-3.8690503, -2.2415588]], dtype=float32)

time = 45547	action = 0	current_phase = 0	next_phase = 1	reward = -0.064667	array([[-1.8183794, -2.8095644]], dtype=float32)

time = 45552	action = 0	current_phase = 0	next_phase = 1	reward = 0.006511	array([[-2.1640131, -2.9354432]], dtype=float32)

time = 45557	action = 0	current_phase = 0	next_phase = 1	reward = 0.070809	array([[-2.7429242, -3.1978471]], dtype=float32)

time = 45562	action = 1	current_phase = 0	next_phase = 1	reward = -1.658548	array([[-6.5859613, -3.6307862]], dtype=float32)

time = 45570	action = 1	current_phase = 1	next_phase = 0	reward = -1.002982	array([[-4.035681 , -2.4370205]], dtype=float32)

time = 45578	action = 0	current_phase = 0	next_phase = 1	reward = -0.061275	array([[-1.8122003, -2.8010201]], dtype=float32)

time = 45583	action = 0	current_phase = 0	next_phase = 1	reward = 0.009819	array([[-2.2098618, -2.9251926]], dtype=float32)

time = 45588	action = 0	current_phase = 0	next_phase = 1	reward = 0.070245	array([[-2.8194566, -3.3100336]], dtype=float32)

time = 45593	action = 1	current_phase = 0	next_phase = 1	reward = -1.818073	array([[-6.5640874, -3.7528808]], dtype=float32)

time = 45601	action = 1	current_phase = 1	next_phase = 0	reward = -1.092059	array([[-4.332467 , -2.4222908]], dtype=float32)

time = 45609	action = 0	current_phase = 0	next_phase = 1	reward = -0.038011	array([[-1.3658712, -2.147944 ]], dtype=float32)

time = 45614	action = 0	current_phase = 0	next_phase = 1	reward = 0.033285	array([[-1.8261664, -2.789515 ]], dtype=float32)

time = 45619	action = 1	current_phase = 0	next_phase = 1	reward = -0.712536	array([[-2.3796368, -2.360633 ]], dtype=float32)

time = 45627	action = 1	current_phase = 1	next_phase = 0	reward = -1.199865	array([[-3.5011752, -2.0091925]], dtype=float32)

time = 45635	action = 0	current_phase = 0	next_phase = 1	reward = 0.476430	array([[-1.3323085, -2.958221 ]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0289 - val_loss: 0.0152

Epoch 2/50

 - 3s - loss: 0.0200 - val_loss: 0.0106

Epoch 3/50

 - 3s - loss: 0.0200 - val_loss: 0.0129

Epoch 4/50

 - 3s - loss: 0.0197 - val_loss: 0.0077

Epoch 5/50

 - 3s - loss: 0.0238 - val_loss: 0.0082

Epoch 6/50

 - 3s - loss: 0.0211 - val_loss: 0.0089

Epoch 7/50

 - 3s - loss: 0.0180 - val_loss: 0.0113

Epoch 8/50

 - 3s - loss: 0.0202 - val_loss: 0.0084

Epoch 9/50

 - 3s - loss: 0.0197 - val_loss: 0.0123

Epoch 10/50

 - 3s - loss: 0.0191 - val_loss: 0.0121

Epoch 11/50

 - 3s - loss: 0.0164 - val_loss: 0.0102

Epoch 12/50

 - 3s - loss: 0.0222 - val_loss: 0.0113

Epoch 13/50

 - 3s - loss: 0.0213 - val_loss: 0.0105

Epoch 14/50

 - 3s - loss: 0.0156 - val_loss: 0.0090

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 45640	action = 0	current_phase = 0	next_phase = 1	reward = -0.009420	array([[-1.7349842, -2.196806 ]], dtype=float32)

time = 45645	action = 0	current_phase = 0	next_phase = 1	reward = 0.064034	array([[-2.2898128, -2.9794593]], dtype=float32)

time = 45650	action = 1	current_phase = 0	next_phase = 1	reward = -1.419800	array([[-5.575356 , -3.0203633]], dtype=float32)

time = 45658	action = 1	current_phase = 1	next_phase = 0	reward = -0.713688	array([[-3.7924185, -2.1326923]], dtype=float32)

time = 45666	action = 0	current_phase = 0	next_phase = 1	reward = -0.072065	array([[-1.6733875, -2.7876484]], dtype=float32)

time = 45671	action = 0	current_phase = 0	next_phase = 1	reward = -0.003126	array([[-2.0454793, -2.8750684]], dtype=float32)

time = 45676	action = 0	current_phase = 0	next_phase = 1	reward = 0.062538	array([[-2.6389723, -3.1632836]], dtype=float32)

time = 45681	action = 1	current_phase = 0	next_phase = 1	reward = -1.498696	array([[-6.6430383, -3.4032352]], dtype=float32)

time = 45689	action = 1	current_phase = 1	next_phase = 0	reward = -1.143765	array([[-3.885736 , -2.2437062]], dtype=float32)

time = 45697	action = 0	current_phase = 0	next_phase = 1	reward = 0.223549	array([[-1.6132605, -2.7686703]], dtype=float32)

time = 45702	action = 0	current_phase = 0	next_phase = 1	reward = 0.013340	array([[-2.145519 , -2.9411132]], dtype=float32)

time = 45707	action = 0	current_phase = 0	next_phase = 1	reward = 0.067901	array([[-2.7628212, -3.226966 ]], dtype=float32)

time = 45712	action = 1	current_phase = 0	next_phase = 1	reward = -1.584384	array([[-6.610857 , -3.5485747]], dtype=float32)

time = 45720	action = 1	current_phase = 1	next_phase = 0	reward = -1.067327	array([[-4.0367956, -2.3732142]], dtype=float32)

time = 45728	action = 0	current_phase = 0	next_phase = 1	reward = -0.056829	array([[-1.732354 , -2.8029563]], dtype=float32)

time = 45733	action = 0	current_phase = 0	next_phase = 1	reward = 0.023527	array([[-2.117495 , -2.8998816]], dtype=float32)

time = 45738	action = 0	current_phase = 0	next_phase = 1	reward = 0.069856	array([[-2.7686992, -3.2669744]], dtype=float32)

time = 45743	action = 1	current_phase = 0	next_phase = 1	reward = -1.272687	array([[-6.632217 , -3.4467971]], dtype=float32)

time = 45751	action = 1	current_phase = 1	next_phase = 0	reward = -1.480935	array([[-3.9502387, -2.3008842]], dtype=float32)

time = 45759	action = 0	current_phase = 0	next_phase = 1	reward = 0.255672	array([[-1.1347632, -2.136655 ]], dtype=float32)

time = 45764	action = 0	current_phase = 0	next_phase = 1	reward = 0.048316	array([[-1.9318647, -2.8477325]], dtype=float32)

time = 45769	action = 0	current_phase = 0	next_phase = 1	reward = 0.044737	array([[-2.2800703, -2.4146686]], dtype=float32)

time = 45774	action = 1	current_phase = 0	next_phase = 1	reward = -1.157967	array([[-6.5878506, -3.0000527]], dtype=float32)

time = 45782	action = 1	current_phase = 1	next_phase = 0	reward = -0.672289	array([[-3.7975698, -2.2256026]], dtype=float32)

time = 45790	action = 0	current_phase = 0	next_phase = 1	reward = -0.273188	array([[-1.6953099, -2.179154 ]], dtype=float32)

time = 45795	action = 0	current_phase = 0	next_phase = 1	reward = 0.344257	array([[-2.2168026, -2.9934099]], dtype=float32)

time = 45800	action = 1	current_phase = 0	next_phase = 1	reward = -1.326110	array([[-5.555319 , -3.0566118]], dtype=float32)

time = 45808	action = 1	current_phase = 1	next_phase = 0	reward = -0.710846	array([[-3.721528 , -2.0947404]], dtype=float32)

time = 45816	action = 0	current_phase = 0	next_phase = 1	reward = -0.080339	array([[-1.7541832, -2.7956705]], dtype=float32)

time = 45821	action = 0	current_phase = 0	next_phase = 1	reward = -0.006655	array([[-2.0198665, -2.8583262]], dtype=float32)

time = 45826	action = 0	current_phase = 0	next_phase = 1	reward = 0.057068	array([[-2.601224 , -3.1337278]], dtype=float32)

time = 45831	action = 1	current_phase = 0	next_phase = 1	reward = -1.569436	array([[-6.643773 , -3.3888423]], dtype=float32)

time = 45839	action = 1	current_phase = 1	next_phase = 0	reward = -0.771448	array([[-3.8675928, -2.2102015]], dtype=float32)

time = 45847	action = 0	current_phase = 0	next_phase = 1	reward = -0.070326	array([[-1.6415321, -2.7797241]], dtype=float32)

time = 45852	action = 0	current_phase = 0	next_phase = 1	reward = 0.013652	array([[-2.1697307, -2.930831 ]], dtype=float32)

time = 45857	action = 0	current_phase = 0	next_phase = 1	reward = 0.071364	array([[-2.737041 , -3.2003772]], dtype=float32)

time = 45862	action = 1	current_phase = 0	next_phase = 1	reward = -1.582568	array([[-6.6151457, -3.545192 ]], dtype=float32)

time = 45870	action = 1	current_phase = 1	next_phase = 0	reward = -1.072890	array([[-4.039803 , -2.4095654]], dtype=float32)

time = 45878	action = 0	current_phase = 0	next_phase = 1	reward = -0.058935	array([[-1.7898941, -2.81632  ]], dtype=float32)

time = 45883	action = 0	current_phase = 0	next_phase = 1	reward = 0.010988	array([[-2.1603942, -2.9113367]], dtype=float32)

time = 45888	action = 0	current_phase = 0	next_phase = 1	reward = 0.081407	array([[-2.85854  , -3.2847323]], dtype=float32)

time = 45893	action = 1	current_phase = 0	next_phase = 1	reward = -1.312384	array([[-6.6315584, -3.4618266]], dtype=float32)

time = 45901	action = 1	current_phase = 1	next_phase = 0	reward = -1.567129	array([[-4.1307316, -2.3028479]], dtype=float32)

time = 45909	action = 0	current_phase = 0	next_phase = 1	reward = 0.246990	array([[-1.243454 , -2.1561394]], dtype=float32)

time = 45914	action = 0	current_phase = 0	next_phase = 1	reward = 0.026699	array([[-1.776304 , -2.7849083]], dtype=float32)

time = 45919	action = 0	current_phase = 0	next_phase = 1	reward = 0.065709	array([[-2.2872753, -2.3880968]], dtype=float32)

time = 45924	action = 1	current_phase = 0	next_phase = 1	reward = -1.014479	array([[-6.58746  , -3.0257146]], dtype=float32)

time = 45932	action = 1	current_phase = 1	next_phase = 0	reward = -0.770426	array([[-3.8838592, -2.2319565]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0194 - val_loss: 0.0115

Epoch 2/50

 - 3s - loss: 0.0169 - val_loss: 0.0108

Epoch 3/50

 - 3s - loss: 0.0188 - val_loss: 0.0102

Epoch 4/50

 - 3s - loss: 0.0188 - val_loss: 0.0118

Epoch 5/50

 - 3s - loss: 0.0166 - val_loss: 0.0128

Epoch 6/50

 - 3s - loss: 0.0168 - val_loss: 0.0086

Epoch 7/50

 - 3s - loss: 0.0177 - val_loss: 0.0088

Epoch 8/50

 - 3s - loss: 0.0178 - val_loss: 0.0121

Epoch 9/50

 - 3s - loss: 0.0150 - val_loss: 0.0109

Epoch 10/50

 - 3s - loss: 0.0181 - val_loss: 0.0094

Epoch 11/50

 - 3s - loss: 0.0155 - val_loss: 0.0146

Epoch 12/50

 - 3s - loss: 0.0205 - val_loss: 0.0112

Epoch 13/50

 - 3s - loss: 0.0177 - val_loss: 0.0104

Epoch 14/50

 - 3s - loss: 0.0142 - val_loss: 0.0117

Epoch 15/50

 - 3s - loss: 0.0165 - val_loss: 0.0143

Epoch 16/50

 - 3s - loss: 0.0162 - val_loss: 0.0107

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 45940	action = 0	current_phase = 0	next_phase = 1	reward = -0.312451	array([[-1.7884883, -2.1954489]], dtype=float32)

time = 45945	action = 0	current_phase = 0	next_phase = 1	reward = 0.312758	array([[-2.0967653, -2.888185 ]], dtype=float32)

time = 45950	action = 1	current_phase = 0	next_phase = 1	reward = -1.439853	array([[-5.563326 , -3.0590253]], dtype=float32)

time = 45958	action = 1	current_phase = 1	next_phase = 0	reward = -0.717789	array([[-3.612399 , -2.0007823]], dtype=float32)

time = 45966	action = 0	current_phase = 0	next_phase = 1	reward = -0.078190	array([[-1.6536019, -2.8093243]], dtype=float32)

time = 45971	action = 0	current_phase = 0	next_phase = 1	reward = -0.010029	array([[-2.1243277, -2.8775465]], dtype=float32)

time = 45976	action = 0	current_phase = 0	next_phase = 1	reward = 0.062063	array([[-2.5335073, -3.0861597]], dtype=float32)

time = 45981	action = 1	current_phase = 0	next_phase = 1	reward = -1.443192	array([[-6.663426 , -3.2231731]], dtype=float32)

time = 45989	action = 1	current_phase = 1	next_phase = 0	reward = -0.833156	array([[-3.8479323, -2.2093368]], dtype=float32)

time = 45997	action = 0	current_phase = 0	next_phase = 1	reward = -0.066921	array([[-1.7286142, -2.7946672]], dtype=float32)

time = 46002	action = 0	current_phase = 0	next_phase = 1	reward = 0.007640	array([[-2.1796176, -2.9150872]], dtype=float32)

time = 46007	action = 0	current_phase = 0	next_phase = 1	reward = 0.076879	array([[-2.785161 , -3.2266889]], dtype=float32)

time = 46012	action = 1	current_phase = 0	next_phase = 1	reward = -1.618393	array([[-6.6355524, -3.549419 ]], dtype=float32)

time = 46020	action = 1	current_phase = 1	next_phase = 0	reward = -1.348906	array([[-3.9936104, -2.4066339]], dtype=float32)

time = 46028	action = 0	current_phase = 0	next_phase = 1	reward = 0.243066	array([[-1.6294254, -2.7616665]], dtype=float32)

time = 46033	action = 0	current_phase = 0	next_phase = 1	reward = 0.015603	array([[-2.213986 , -2.9172935]], dtype=float32)

time = 46038	action = 0	current_phase = 0	next_phase = 1	reward = 0.076623	array([[-2.7802038, -3.2468405]], dtype=float32)

time = 46043	action = 1	current_phase = 0	next_phase = 1	reward = -1.929656	array([[-6.648594 , -3.5760164]], dtype=float32)

time = 46051	action = 1	current_phase = 1	next_phase = 0	reward = -1.085109	array([[-4.3360004, -2.170097 ]], dtype=float32)

time = 46059	action = 0	current_phase = 0	next_phase = 1	reward = -0.045688	array([[-1.2729462, -2.1295757]], dtype=float32)

time = 46064	action = 0	current_phase = 0	next_phase = 1	reward = 0.030055	array([[-1.8010914, -2.8111372]], dtype=float32)

time = 46069	action = 0	current_phase = 0	next_phase = 1	reward = 0.346179	array([[-2.300362 , -2.3572888]], dtype=float32)

time = 46074	action = 1	current_phase = 0	next_phase = 1	reward = -1.351035	array([[-6.675651 , -3.1127725]], dtype=float32)

time = 46082	action = 1	current_phase = 1	next_phase = 0	reward = -0.769029	array([[-3.8320127, -2.2017806]], dtype=float32)

time = 46090	action = 0	current_phase = 0	next_phase = 1	reward = -0.037102	array([[-1.9131254, -2.1666467]], dtype=float32)

time = 46095	action = 0	current_phase = 0	next_phase = 1	reward = 0.030725	array([[-2.149313 , -2.9328432]], dtype=float32)

time = 46100	action = 1	current_phase = 0	next_phase = 1	reward = -1.257774	array([[-5.6267242, -2.9665618]], dtype=float32)

time = 46108	action = 1	current_phase = 1	next_phase = 0	reward = -0.715491	array([[-3.5755415, -1.948333 ]], dtype=float32)

time = 46116	action = 0	current_phase = 0	next_phase = 1	reward = -0.090309	array([[-1.6194597, -2.7942996]], dtype=float32)

time = 46121	action = 0	current_phase = 0	next_phase = 1	reward = 0.004217	array([[-2.0149584, -2.8443232]], dtype=float32)

time = 46126	action = 0	current_phase = 0	next_phase = 1	reward = 0.068374	array([[-2.5512862, -3.0868835]], dtype=float32)

time = 46131	action = 1	current_phase = 0	next_phase = 1	reward = -1.388167	array([[-6.6475525, -3.344614 ]], dtype=float32)

time = 46139	action = 1	current_phase = 1	next_phase = 0	reward = -0.822708	array([[-3.8401198, -2.1954908]], dtype=float32)

time = 46147	action = 0	current_phase = 0	next_phase = 1	reward = -0.085295	array([[-1.7767  , -2.795127]], dtype=float32)

time = 46152	action = 0	current_phase = 0	next_phase = 1	reward = -0.006208	array([[-2.2160537, -2.924458 ]], dtype=float32)

time = 46157	action = 0	current_phase = 0	next_phase = 1	reward = 0.054360	array([[-2.7854857, -3.2193637]], dtype=float32)

time = 46162	action = 1	current_phase = 0	next_phase = 1	reward = -1.517611	array([[-6.651316, -3.499733]], dtype=float32)

time = 46170	action = 1	current_phase = 1	next_phase = 0	reward = -0.988919	array([[-3.9732704, -2.3841667]], dtype=float32)

time = 46178	action = 0	current_phase = 0	next_phase = 1	reward = -0.047781	array([[-1.8473912, -2.83004  ]], dtype=float32)

time = 46183	action = 0	current_phase = 0	next_phase = 1	reward = 0.020909	array([[-2.2362144, -2.9257221]], dtype=float32)

time = 46188	action = 0	current_phase = 0	next_phase = 1	reward = 0.079176	array([[-2.7493753, -3.2651868]], dtype=float32)

time = 46193	action = 1	current_phase = 0	next_phase = 1	reward = -1.325275	array([[-6.672505 , -3.3641562]], dtype=float32)

time = 46201	action = 1	current_phase = 1	next_phase = 0	reward = -1.416124	array([[-4.0522523, -2.2028317]], dtype=float32)

time = 46209	action = 0	current_phase = 0	next_phase = 1	reward = 0.258974	array([[-1.2627975, -2.1316147]], dtype=float32)

time = 46214	action = 0	current_phase = 0	next_phase = 1	reward = 0.041113	array([[-1.9151149, -2.831594 ]], dtype=float32)

time = 46219	action = 0	current_phase = 0	next_phase = 1	reward = 0.050132	array([[-2.3003314, -2.3811488]], dtype=float32)

time = 46224	action = 1	current_phase = 0	next_phase = 1	reward = -1.158139	array([[-6.625809 , -3.0610619]], dtype=float32)

time = 46232	action = 1	current_phase = 1	next_phase = 0	reward = -1.186145	array([[-3.759851 , -2.2200558]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0201 - val_loss: 0.0095

Epoch 2/50

 - 3s - loss: 0.0184 - val_loss: 0.0093

Epoch 3/50

 - 3s - loss: 0.0189 - val_loss: 0.0077

Epoch 4/50

 - 3s - loss: 0.0210 - val_loss: 0.0080

Epoch 5/50

 - 3s - loss: 0.0192 - val_loss: 0.0091

Epoch 6/50

 - 3s - loss: 0.0204 - val_loss: 0.0103

Epoch 7/50

 - 3s - loss: 0.0195 - val_loss: 0.0078

Epoch 8/50

 - 3s - loss: 0.0162 - val_loss: 0.0077

Epoch 9/50

 - 3s - loss: 0.0178 - val_loss: 0.0082

Epoch 10/50

 - 3s - loss: 0.0178 - val_loss: 0.0089

Epoch 11/50

 - 3s - loss: 0.0194 - val_loss: 0.0093

Epoch 12/50

 - 3s - loss: 0.0224 - val_loss: 0.0088

Epoch 13/50

 - 3s - loss: 0.0175 - val_loss: 0.0094

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 46240	action = 0	current_phase = 0	next_phase = 1	reward = 0.254107	array([[-1.6535084, -2.2205157]], dtype=float32)

time = 46245	action = 0	current_phase = 0	next_phase = 1	reward = 0.043169	array([[-2.141048 , -2.9705415]], dtype=float32)

time = 46250	action = 1	current_phase = 0	next_phase = 1	reward = -0.974002	array([[-5.5758243, -2.7894473]], dtype=float32)

time = 46258	action = 1	current_phase = 1	next_phase = 0	reward = -0.712846	array([[-3.4915514, -1.9996885]], dtype=float32)

time = 46266	action = 0	current_phase = 0	next_phase = 1	reward = -0.076472	array([[-1.6611184, -2.8225222]], dtype=float32)

time = 46271	action = 0	current_phase = 0	next_phase = 1	reward = 0.025758	array([[-2.086343 , -2.9013352]], dtype=float32)

time = 46276	action = 0	current_phase = 0	next_phase = 1	reward = 0.079789	array([[-2.6256924, -3.158193 ]], dtype=float32)

time = 46281	action = 1	current_phase = 0	next_phase = 1	reward = -1.459572	array([[-6.688481 , -3.4190416]], dtype=float32)

time = 46289	action = 1	current_phase = 1	next_phase = 0	reward = -0.789308	array([[-3.8097835, -2.2184255]], dtype=float32)

time = 46297	action = 0	current_phase = 0	next_phase = 1	reward = -0.081395	array([[-1.7373278, -2.819675 ]], dtype=float32)

time = 46302	action = 0	current_phase = 0	next_phase = 1	reward = -0.005708	array([[-2.2168508, -2.9510655]], dtype=float32)

time = 46307	action = 0	current_phase = 0	next_phase = 1	reward = 0.063023	array([[-2.6992598, -3.2236881]], dtype=float32)

time = 46312	action = 1	current_phase = 0	next_phase = 1	reward = -1.638862	array([[-6.6649337, -3.5625381]], dtype=float32)

time = 46320	action = 1	current_phase = 1	next_phase = 0	reward = -0.895251	array([[-3.9903154, -2.4282162]], dtype=float32)

time = 46328	action = 0	current_phase = 0	next_phase = 1	reward = -0.064364	array([[-1.8687187, -2.8413992]], dtype=float32)

time = 46333	action = 0	current_phase = 0	next_phase = 1	reward = 0.000999	array([[-2.1885028, -2.930553 ]], dtype=float32)

time = 46338	action = 0	current_phase = 0	next_phase = 1	reward = 0.073016	array([[-2.757815 , -3.2959733]], dtype=float32)

time = 46343	action = 1	current_phase = 0	next_phase = 1	reward = -1.283392	array([[-6.681081 , -3.4851832]], dtype=float32)

time = 46351	action = 1	current_phase = 1	next_phase = 0	reward = -1.090491	array([[-3.9536319, -2.2761147]], dtype=float32)

time = 46359	action = 0	current_phase = 0	next_phase = 1	reward = -0.045061	array([[-1.3334755, -2.156602 ]], dtype=float32)

time = 46364	action = 0	current_phase = 0	next_phase = 1	reward = 0.035342	array([[-1.9678533, -2.8882775]], dtype=float32)

time = 46369	action = 0	current_phase = 0	next_phase = 1	reward = 0.068556	array([[-2.3711534, -2.4587135]], dtype=float32)

time = 46374	action = 1	current_phase = 0	next_phase = 1	reward = -1.595821	array([[-6.606828 , -2.9336548]], dtype=float32)

time = 46382	action = 1	current_phase = 1	next_phase = 0	reward = -0.890079	array([[-3.8512497, -2.2013097]], dtype=float32)

time = 46390	action = 0	current_phase = 0	next_phase = 1	reward = -0.026136	array([[-1.7106404, -2.205528 ]], dtype=float32)

time = 46395	action = 0	current_phase = 0	next_phase = 1	reward = 0.332956	array([[-2.1574116, -2.9551072]], dtype=float32)

time = 46400	action = 1	current_phase = 0	next_phase = 1	reward = -1.300497	array([[-5.4429674, -2.950457 ]], dtype=float32)

time = 46408	action = 1	current_phase = 1	next_phase = 0	reward = -0.712035	array([[-3.5735564, -1.9889082]], dtype=float32)

time = 46416	action = 0	current_phase = 0	next_phase = 1	reward = -0.088392	array([[-1.6589341, -2.8175397]], dtype=float32)

time = 46421	action = 0	current_phase = 0	next_phase = 1	reward = -0.015473	array([[-2.0749555, -2.8948188]], dtype=float32)

time = 46426	action = 0	current_phase = 0	next_phase = 1	reward = 0.050160	array([[-2.5802023, -3.1428018]], dtype=float32)

time = 46431	action = 1	current_phase = 0	next_phase = 1	reward = -1.450413	array([[-6.686666 , -3.3986669]], dtype=float32)

time = 46439	action = 1	current_phase = 1	next_phase = 0	reward = -0.768629	array([[-3.8166494, -2.2236307]], dtype=float32)

time = 46447	action = 0	current_phase = 0	next_phase = 1	reward = -0.072840	array([[-1.7705802, -2.8328376]], dtype=float32)

time = 46452	action = 0	current_phase = 0	next_phase = 1	reward = 0.003775	array([[-2.1609664, -2.945232 ]], dtype=float32)

time = 46457	action = 0	current_phase = 0	next_phase = 1	reward = 0.072126	array([[-2.7269545, -3.2643843]], dtype=float32)

time = 46462	action = 1	current_phase = 0	next_phase = 1	reward = -1.606219	array([[-6.655166 , -3.5324974]], dtype=float32)

time = 46470	action = 1	current_phase = 1	next_phase = 0	reward = -1.369067	array([[-3.9735956, -2.3838205]], dtype=float32)

time = 46478	action = 0	current_phase = 0	next_phase = 1	reward = 0.245907	array([[-1.6187606, -2.8202853]], dtype=float32)

time = 46483	action = 0	current_phase = 0	next_phase = 1	reward = 0.016867	array([[-2.2197022, -2.9456472]], dtype=float32)

time = 46488	action = 0	current_phase = 0	next_phase = 1	reward = 0.082644	array([[-2.7747445, -3.302318 ]], dtype=float32)

time = 46493	action = 1	current_phase = 0	next_phase = 1	reward = -1.910485	array([[-6.6176953, -3.6899238]], dtype=float32)

time = 46501	action = 1	current_phase = 1	next_phase = 0	reward = -1.106909	array([[-4.281186 , -2.2327003]], dtype=float32)

time = 46509	action = 0	current_phase = 0	next_phase = 1	reward = -0.053583	array([[-1.3171448, -2.152733 ]], dtype=float32)

time = 46514	action = 0	current_phase = 0	next_phase = 1	reward = -0.000434	array([[-1.9619325, -2.8616061]], dtype=float32)

time = 46519	action = 1	current_phase = 0	next_phase = 1	reward = -0.650600	array([[-2.4473538, -2.3941078]], dtype=float32)

time = 46527	action = 1	current_phase = 1	next_phase = 0	reward = -0.623040	array([[-3.2621295, -1.7234766]], dtype=float32)

time = 46535	action = 0	current_phase = 0	next_phase = 1	reward = -0.094954	array([[-1.2294835, -3.0633807]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0169 - val_loss: 0.0084

Epoch 2/50

 - 3s - loss: 0.0191 - val_loss: 0.0098

Epoch 3/50

 - 3s - loss: 0.0197 - val_loss: 0.0093

Epoch 4/50

 - 3s - loss: 0.0181 - val_loss: 0.0100

Epoch 5/50

 - 3s - loss: 0.0158 - val_loss: 0.0074

Epoch 6/50

 - 3s - loss: 0.0176 - val_loss: 0.0095

Epoch 7/50

 - 3s - loss: 0.0185 - val_loss: 0.0112

Epoch 8/50

 - 3s - loss: 0.0167 - val_loss: 0.0152

Epoch 9/50

 - 3s - loss: 0.0184 - val_loss: 0.0077

Epoch 10/50

 - 3s - loss: 0.0173 - val_loss: 0.0115

Epoch 11/50

 - 3s - loss: 0.0190 - val_loss: 0.0078

Epoch 12/50

 - 3s - loss: 0.0163 - val_loss: 0.0081

Epoch 13/50

 - 3s - loss: 0.0190 - val_loss: 0.0132

Epoch 14/50

 - 3s - loss: 0.0167 - val_loss: 0.0096

Epoch 15/50

 - 3s - loss: 0.0206 - val_loss: 0.0097

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 46540	action = 0	current_phase = 0	next_phase = 1	reward = -0.023586	array([[-1.8949237, -2.205008 ]], dtype=float32)

time = 46545	action = 0	current_phase = 0	next_phase = 1	reward = 0.029595	array([[-2.331803 , -2.9863842]], dtype=float32)

time = 46550	action = 1	current_phase = 0	next_phase = 1	reward = -1.349636	array([[-5.7031174, -3.0236595]], dtype=float32)

time = 46558	action = 1	current_phase = 1	next_phase = 0	reward = -0.997784	array([[-3.669588 , -2.0386944]], dtype=float32)

time = 46566	action = 0	current_phase = 0	next_phase = 1	reward = 0.193645	array([[-1.6366184, -2.843536 ]], dtype=float32)

time = 46571	action = 0	current_phase = 0	next_phase = 1	reward = -0.015799	array([[-2.1231902, -2.9145913]], dtype=float32)

time = 46576	action = 0	current_phase = 0	next_phase = 1	reward = 0.052818	array([[-2.6189368, -3.1290083]], dtype=float32)

time = 46581	action = 1	current_phase = 0	next_phase = 1	reward = -1.435540	array([[-6.7764015, -3.3917444]], dtype=float32)

time = 46589	action = 1	current_phase = 1	next_phase = 0	reward = -0.830053	array([[-3.7948115, -2.1996467]], dtype=float32)

time = 46597	action = 0	current_phase = 0	next_phase = 1	reward = -0.063187	array([[-1.8063283, -2.828945 ]], dtype=float32)

time = 46602	action = 0	current_phase = 0	next_phase = 1	reward = 0.021749	array([[-2.2424176, -2.9559765]], dtype=float32)

time = 46607	action = 0	current_phase = 0	next_phase = 1	reward = 0.065056	array([[-2.8036487, -3.2698047]], dtype=float32)

time = 46612	action = 1	current_phase = 0	next_phase = 1	reward = -1.637781	array([[-6.6993403, -3.6247685]], dtype=float32)

time = 46620	action = 1	current_phase = 1	next_phase = 0	reward = -0.841206	array([[-3.9228704, -2.3501678]], dtype=float32)

time = 46628	action = 0	current_phase = 0	next_phase = 1	reward = -0.040109	array([[-1.8140167, -2.849994 ]], dtype=float32)

time = 46633	action = 0	current_phase = 0	next_phase = 1	reward = 0.027333	array([[-2.1800342, -2.9322922]], dtype=float32)

time = 46638	action = 0	current_phase = 0	next_phase = 1	reward = 0.085651	array([[-2.825784 , -3.2856805]], dtype=float32)

time = 46643	action = 1	current_phase = 0	next_phase = 1	reward = -1.344823	array([[-6.735124 , -3.4404628]], dtype=float32)

time = 46651	action = 1	current_phase = 1	next_phase = 0	reward = -1.451539	array([[-3.939638, -2.250471]], dtype=float32)

time = 46659	action = 0	current_phase = 0	next_phase = 1	reward = 0.245903	array([[-1.3654664, -2.1677842]], dtype=float32)

time = 46664	action = 0	current_phase = 0	next_phase = 1	reward = 0.010481	array([[-1.78768  , -2.8073742]], dtype=float32)

time = 46669	action = 1	current_phase = 0	next_phase = 1	reward = -0.628611	array([[-2.4046042, -2.4000552]], dtype=float32)

time = 46677	action = 1	current_phase = 1	next_phase = 0	reward = -1.207008	array([[-3.4097977, -1.8553815]], dtype=float32)

time = 46685	action = 0	current_phase = 0	next_phase = 1	reward = 0.186135	array([[-1.1589651, -3.1695678]], dtype=float32)

time = 46690	action = 0	current_phase = 0	next_phase = 1	reward = 0.248512	array([[-1.8387136, -2.2087228]], dtype=float32)

time = 46695	action = 0	current_phase = 0	next_phase = 1	reward = -0.246103	array([[-2.268881 , -2.9702117]], dtype=float32)

time = 46700	action = 1	current_phase = 0	next_phase = 1	reward = -1.087310	array([[-5.6864095, -2.8349197]], dtype=float32)

time = 46708	action = 1	current_phase = 1	next_phase = 0	reward = -0.706134	array([[-3.6585755, -1.9895325]], dtype=float32)

time = 46716	action = 0	current_phase = 0	next_phase = 1	reward = -0.085250	array([[-1.7103075, -2.8407066]], dtype=float32)

time = 46721	action = 0	current_phase = 0	next_phase = 1	reward = -0.014970	array([[-2.0670319, -2.8878236]], dtype=float32)

time = 46726	action = 0	current_phase = 0	next_phase = 1	reward = 0.053179	array([[-2.6551952, -3.1785455]], dtype=float32)

time = 46731	action = 1	current_phase = 0	next_phase = 1	reward = -1.431998	array([[-6.7743917, -3.3008106]], dtype=float32)

time = 46739	action = 1	current_phase = 1	next_phase = 0	reward = -0.840498	array([[-3.7753108, -2.1881273]], dtype=float32)

time = 46747	action = 0	current_phase = 0	next_phase = 1	reward = -0.093098	array([[-1.7423742, -2.8175035]], dtype=float32)

time = 46752	action = 0	current_phase = 0	next_phase = 1	reward = -0.002963	array([[-2.2229626, -2.9488704]], dtype=float32)

time = 46757	action = 0	current_phase = 0	next_phase = 1	reward = 0.063939	array([[-2.7696297, -3.2068975]], dtype=float32)

time = 46762	action = 1	current_phase = 0	next_phase = 1	reward = -1.518397	array([[-6.728831 , -3.5227659]], dtype=float32)

time = 46770	action = 1	current_phase = 1	next_phase = 0	reward = -1.050767	array([[-3.9413228, -2.3446298]], dtype=float32)

time = 46778	action = 0	current_phase = 0	next_phase = 1	reward = -0.053652	array([[-1.8701406, -2.8472042]], dtype=float32)

time = 46783	action = 0	current_phase = 0	next_phase = 1	reward = 0.018538	array([[-2.3148806, -2.9812233]], dtype=float32)

time = 46788	action = 0	current_phase = 0	next_phase = 1	reward = 0.080667	array([[-2.8720725, -3.3410132]], dtype=float32)

time = 46793	action = 1	current_phase = 0	next_phase = 1	reward = -1.409024	array([[-6.70552  , -3.6365023]], dtype=float32)

time = 46801	action = 1	current_phase = 1	next_phase = 0	reward = -1.496269	array([[-3.923018, -2.285364]], dtype=float32)

time = 46809	action = 0	current_phase = 0	next_phase = 1	reward = 0.232306	array([[-1.2962224, -2.192723 ]], dtype=float32)

time = 46814	action = 0	current_phase = 0	next_phase = 1	reward = 0.018675	array([[-1.8255067, -2.8347807]], dtype=float32)

time = 46819	action = 0	current_phase = 0	next_phase = 1	reward = 0.072256	array([[-2.3937228, -2.4501157]], dtype=float32)

time = 46824	action = 1	current_phase = 0	next_phase = 1	reward = -0.950493	array([[-6.7167983, -2.9733655]], dtype=float32)

time = 46832	action = 1	current_phase = 1	next_phase = 0	reward = -0.715845	array([[-3.6523128, -2.2248783]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0228 - val_loss: 0.0114

Epoch 2/50

 - 3s - loss: 0.0156 - val_loss: 0.0106

Epoch 3/50

 - 3s - loss: 0.0194 - val_loss: 0.0098

Epoch 4/50

 - 3s - loss: 0.0202 - val_loss: 0.0107

Epoch 5/50

 - 3s - loss: 0.0191 - val_loss: 0.0107

Epoch 6/50

 - 3s - loss: 0.0180 - val_loss: 0.0151

Epoch 7/50

 - 3s - loss: 0.0191 - val_loss: 0.0122

Epoch 8/50

 - 3s - loss: 0.0152 - val_loss: 0.0096

Epoch 9/50

 - 3s - loss: 0.0200 - val_loss: 0.0106

Epoch 10/50

 - 3s - loss: 0.0153 - val_loss: 0.0134

Epoch 11/50

 - 3s - loss: 0.0191 - val_loss: 0.0120

Epoch 12/50

 - 3s - loss: 0.0182 - val_loss: 0.0125

Epoch 13/50

 - 3s - loss: 0.0164 - val_loss: 0.0097

Epoch 14/50

 - 3s - loss: 0.0168 - val_loss: 0.0130

Epoch 15/50

 - 3s - loss: 0.0172 - val_loss: 0.0096

Epoch 16/50

 - 3s - loss: 0.0143 - val_loss: 0.0104

Epoch 17/50

 - 3s - loss: 0.0164 - val_loss: 0.0103

Epoch 18/50

 - 3s - loss: 0.0146 - val_loss: 0.0106

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 46840	action = 0	current_phase = 0	next_phase = 1	reward = -0.033073	array([[-1.7577783, -2.1378932]], dtype=float32)

time = 46845	action = 0	current_phase = 0	next_phase = 1	reward = -0.235169	array([[-2.355045 , -2.9964159]], dtype=float32)

time = 46850	action = 1	current_phase = 0	next_phase = 1	reward = -1.081757	array([[-5.6481004, -2.6391687]], dtype=float32)

time = 46858	action = 1	current_phase = 1	next_phase = 0	reward = -0.728805	array([[-3.5781033, -2.0761266]], dtype=float32)

time = 46866	action = 0	current_phase = 0	next_phase = 1	reward = -0.085991	array([[-1.6973307, -2.8301404]], dtype=float32)

time = 46871	action = 0	current_phase = 0	next_phase = 1	reward = 0.008411	array([[-2.0830424, -2.8829122]], dtype=float32)

time = 46876	action = 0	current_phase = 0	next_phase = 1	reward = 0.067236	array([[-2.6032207, -3.1102169]], dtype=float32)

time = 46881	action = 1	current_phase = 0	next_phase = 1	reward = -1.507347	array([[-6.7510896, -3.202125 ]], dtype=float32)

time = 46889	action = 1	current_phase = 1	next_phase = 0	reward = -0.778216	array([[-3.7207093, -2.255807 ]], dtype=float32)

time = 46897	action = 0	current_phase = 0	next_phase = 1	reward = -0.071942	array([[-1.8412421, -2.8217325]], dtype=float32)

time = 46902	action = 0	current_phase = 0	next_phase = 1	reward = 0.003608	array([[-2.204574, -2.923312]], dtype=float32)

time = 46907	action = 0	current_phase = 0	next_phase = 1	reward = 0.067523	array([[-2.7794473, -3.2012873]], dtype=float32)

time = 46912	action = 1	current_phase = 0	next_phase = 1	reward = -1.578361	array([[-6.7273407, -3.4016337]], dtype=float32)

time = 46920	action = 1	current_phase = 1	next_phase = 0	reward = -0.901969	array([[-3.9008048, -2.4315152]], dtype=float32)

time = 46928	action = 0	current_phase = 0	next_phase = 1	reward = -0.068774	array([[-1.8307824, -2.8371723]], dtype=float32)

time = 46933	action = 0	current_phase = 0	next_phase = 1	reward = 0.004077	array([[-2.225794 , -2.9094722]], dtype=float32)

time = 46938	action = 0	current_phase = 0	next_phase = 1	reward = 0.057721	array([[-2.7990525, -3.3026621]], dtype=float32)

time = 46943	action = 1	current_phase = 0	next_phase = 1	reward = -1.347546	array([[-6.7486043, -3.2290158]], dtype=float32)

time = 46951	action = 1	current_phase = 1	next_phase = 0	reward = -1.382963	array([[-4.0117216, -2.3978117]], dtype=float32)

time = 46959	action = 0	current_phase = 0	next_phase = 1	reward = 0.238198	array([[-1.3045512, -2.1067045]], dtype=float32)

time = 46964	action = 0	current_phase = 0	next_phase = 1	reward = 0.298057	array([[-1.930579 , -2.8214035]], dtype=float32)

time = 46969	action = 1	current_phase = 0	next_phase = 1	reward = -1.007674	array([[-2.410564, -2.318476]], dtype=float32)

time = 46977	action = 1	current_phase = 1	next_phase = 0	reward = -0.646713	array([[-3.3148923, -1.8993421]], dtype=float32)

time = 46985	action = 0	current_phase = 0	next_phase = 1	reward = -0.107126	array([[-1.4932921, -3.1189585]], dtype=float32)

time = 46990	action = 0	current_phase = 0	next_phase = 1	reward = -0.296263	array([[-1.8731301, -2.1498976]], dtype=float32)

time = 46995	action = 0	current_phase = 0	next_phase = 1	reward = 0.328950	array([[-2.339341 , -2.9421687]], dtype=float32)

time = 47000	action = 1	current_phase = 0	next_phase = 1	reward = -1.309085	array([[-5.681781 , -2.8569927]], dtype=float32)

time = 47008	action = 1	current_phase = 1	next_phase = 0	reward = -0.687671	array([[-3.5507154, -2.0616186]], dtype=float32)

time = 47016	action = 0	current_phase = 0	next_phase = 1	reward = -0.072854	array([[-1.8076992, -2.8470995]], dtype=float32)

time = 47021	action = 0	current_phase = 0	next_phase = 1	reward = -0.016435	array([[-2.0907786, -2.8657362]], dtype=float32)

time = 47026	action = 0	current_phase = 0	next_phase = 1	reward = 0.063162	array([[-2.6333926, -3.136148 ]], dtype=float32)

time = 47031	action = 1	current_phase = 0	next_phase = 1	reward = -1.556811	array([[-6.7618103, -3.237036 ]], dtype=float32)

time = 47039	action = 1	current_phase = 1	next_phase = 0	reward = -0.889623	array([[-3.7485259, -2.2919097]], dtype=float32)

time = 47047	action = 0	current_phase = 0	next_phase = 1	reward = -0.054396	array([[-1.7901952, -2.806702 ]], dtype=float32)

time = 47052	action = 0	current_phase = 0	next_phase = 1	reward = 0.034133	array([[-2.2145646, -2.915633 ]], dtype=float32)

time = 47057	action = 0	current_phase = 0	next_phase = 1	reward = 0.088993	array([[-2.8170037, -3.2228253]], dtype=float32)

time = 47062	action = 1	current_phase = 0	next_phase = 1	reward = -1.698772	array([[-6.7245874, -3.461387 ]], dtype=float32)

time = 47070	action = 1	current_phase = 1	next_phase = 0	reward = -0.956224	array([[-3.884445 , -2.3737292]], dtype=float32)

time = 47078	action = 0	current_phase = 0	next_phase = 1	reward = -0.061812	array([[-1.7796199, -2.8208234]], dtype=float32)

time = 47083	action = 0	current_phase = 0	next_phase = 1	reward = 0.000735	array([[-2.1865094, -2.9035966]], dtype=float32)

time = 47088	action = 0	current_phase = 0	next_phase = 1	reward = 0.064621	array([[-2.7991302, -3.2291892]], dtype=float32)

time = 47093	action = 1	current_phase = 0	next_phase = 1	reward = -1.304043	array([[-6.7488933, -3.3000777]], dtype=float32)

time = 47101	action = 1	current_phase = 1	next_phase = 0	reward = -1.132071	array([[-3.8918927, -2.3859558]], dtype=float32)

time = 47109	action = 0	current_phase = 0	next_phase = 1	reward = -0.035082	array([[-1.4638753, -2.1012073]], dtype=float32)

time = 47114	action = 0	current_phase = 0	next_phase = 1	reward = 0.031942	array([[-1.8680696, -2.8068798]], dtype=float32)

time = 47119	action = 1	current_phase = 0	next_phase = 1	reward = -0.636206	array([[-2.3957744, -2.2734585]], dtype=float32)

time = 47127	action = 1	current_phase = 1	next_phase = 0	reward = -0.643497	array([[-3.3044353, -1.9690124]], dtype=float32)

time = 47135	action = 0	current_phase = 0	next_phase = 1	reward = -0.099267	array([[-1.4398267, -3.1663895]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0190 - val_loss: 0.0121

Epoch 2/50

 - 3s - loss: 0.0239 - val_loss: 0.0100

Epoch 3/50

 - 3s - loss: 0.0271 - val_loss: 0.0123

Epoch 4/50

 - 3s - loss: 0.0152 - val_loss: 0.0129

Epoch 5/50

 - 3s - loss: 0.0192 - val_loss: 0.0168

Epoch 6/50

 - 3s - loss: 0.0153 - val_loss: 0.0120

Epoch 7/50

 - 3s - loss: 0.0204 - val_loss: 0.0120

Epoch 8/50

 - 3s - loss: 0.0175 - val_loss: 0.0103

Epoch 9/50

 - 3s - loss: 0.0148 - val_loss: 0.0094

Epoch 10/50

 - 3s - loss: 0.0187 - val_loss: 0.0123

Epoch 11/50

 - 3s - loss: 0.0178 - val_loss: 0.0154

Epoch 12/50

 - 3s - loss: 0.0218 - val_loss: 0.0130

Epoch 13/50

 - 3s - loss: 0.0144 - val_loss: 0.0182

Epoch 14/50

 - 3s - loss: 0.0165 - val_loss: 0.0159

Epoch 15/50

 - 3s - loss: 0.0178 - val_loss: 0.0129

Epoch 16/50

 - 3s - loss: 0.0158 - val_loss: 0.0117

Epoch 17/50

 - 3s - loss: 0.0164 - val_loss: 0.0147

Epoch 18/50

 - 3s - loss: 0.0174 - val_loss: 0.0103

Epoch 19/50

 - 3s - loss: 0.0155 - val_loss: 0.0124

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 47140	action = 0	current_phase = 0	next_phase = 1	reward = -0.303732	array([[-1.9883895, -2.1087139]], dtype=float32)

time = 47145	action = 0	current_phase = 0	next_phase = 1	reward = 0.331998	array([[-2.443533 , -3.0102973]], dtype=float32)

time = 47150	action = 1	current_phase = 0	next_phase = 1	reward = -1.361633	array([[-5.74597  , -2.9198647]], dtype=float32)

time = 47158	action = 1	current_phase = 1	next_phase = 0	reward = -0.698994	array([[-3.585085 , -2.1216352]], dtype=float32)

time = 47166	action = 0	current_phase = 0	next_phase = 1	reward = -0.090371	array([[-1.7603366, -2.826375 ]], dtype=float32)

time = 47171	action = 0	current_phase = 0	next_phase = 1	reward = -0.003438	array([[-2.1000564, -2.8941784]], dtype=float32)

time = 47176	action = 0	current_phase = 0	next_phase = 1	reward = 0.053823	array([[-2.5985723, -3.0915012]], dtype=float32)

time = 47181	action = 1	current_phase = 0	next_phase = 1	reward = -1.492348	array([[-6.7982097, -3.2271595]], dtype=float32)

time = 47189	action = 1	current_phase = 1	next_phase = 0	reward = -0.829951	array([[-3.7223296, -2.3056355]], dtype=float32)

time = 47197	action = 0	current_phase = 0	next_phase = 1	reward = -0.067546	array([[-1.7666204, -2.8176928]], dtype=float32)

time = 47202	action = 0	current_phase = 0	next_phase = 1	reward = 0.005353	array([[-2.2701387, -2.9548645]], dtype=float32)

time = 47207	action = 0	current_phase = 0	next_phase = 1	reward = 0.076574	array([[-2.834258 , -3.2601075]], dtype=float32)

time = 47212	action = 1	current_phase = 0	next_phase = 1	reward = -1.699391	array([[-6.7948265, -3.513136 ]], dtype=float32)

time = 47220	action = 1	current_phase = 1	next_phase = 0	reward = -1.353193	array([[-3.9616838, -2.5125217]], dtype=float32)

time = 47228	action = 0	current_phase = 0	next_phase = 1	reward = 0.238436	array([[-1.7741323, -2.8150206]], dtype=float32)

time = 47233	action = 0	current_phase = 0	next_phase = 1	reward = 0.028068	array([[-2.3056674, -2.9628282]], dtype=float32)

time = 47238	action = 0	current_phase = 0	next_phase = 1	reward = 0.069872	array([[-2.8523166, -3.257393 ]], dtype=float32)

time = 47243	action = 1	current_phase = 0	next_phase = 1	reward = -1.834200	array([[-6.735083 , -3.6316733]], dtype=float32)

time = 47251	action = 1	current_phase = 1	next_phase = 0	reward = -1.031440	array([[-4.2538395, -2.340257 ]], dtype=float32)

time = 47259	action = 0	current_phase = 0	next_phase = 1	reward = -0.039500	array([[-1.5430244, -2.0617867]], dtype=float32)

time = 47264	action = 0	current_phase = 0	next_phase = 1	reward = 0.031975	array([[-1.8589535, -2.7994108]], dtype=float32)

time = 47269	action = 1	current_phase = 0	next_phase = 1	reward = -0.669713	array([[-2.5238416, -2.1813326]], dtype=float32)

time = 47277	action = 1	current_phase = 1	next_phase = 0	reward = -1.204417	array([[-3.2511456, -1.8898374]], dtype=float32)

time = 47285	action = 0	current_phase = 0	next_phase = 1	reward = 0.183057	array([[-1.3919673, -3.0827127]], dtype=float32)

time = 47290	action = 0	current_phase = 0	next_phase = 1	reward = -0.024139	array([[-1.9666386, -2.1038232]], dtype=float32)

time = 47295	action = 0	current_phase = 0	next_phase = 1	reward = 0.329460	array([[-2.3320286, -2.9663749]], dtype=float32)

time = 47300	action = 1	current_phase = 0	next_phase = 1	reward = -1.364119	array([[-5.7378526, -2.9297504]], dtype=float32)

time = 47308	action = 1	current_phase = 1	next_phase = 0	reward = -0.703777	array([[-3.566394 , -2.0888653]], dtype=float32)

time = 47316	action = 0	current_phase = 0	next_phase = 1	reward = -0.087976	array([[-1.8096728, -2.8435507]], dtype=float32)

time = 47321	action = 0	current_phase = 0	next_phase = 1	reward = -0.008513	array([[-2.124502 , -2.9030614]], dtype=float32)

time = 47326	action = 0	current_phase = 0	next_phase = 1	reward = 0.067889	array([[-2.703394 , -3.1608024]], dtype=float32)

time = 47331	action = 1	current_phase = 0	next_phase = 1	reward = -1.430112	array([[-6.79508 , -3.219338]], dtype=float32)

time = 47339	action = 1	current_phase = 1	next_phase = 0	reward = -0.834970	array([[-3.7316654, -2.3126454]], dtype=float32)

time = 47347	action = 0	current_phase = 0	next_phase = 1	reward = -0.069327	array([[-1.8260481, -2.8265848]], dtype=float32)

time = 47352	action = 0	current_phase = 0	next_phase = 1	reward = 0.008941	array([[-2.2473273, -2.9579668]], dtype=float32)

time = 47357	action = 0	current_phase = 0	next_phase = 1	reward = 0.080584	array([[-2.8267405, -3.2803836]], dtype=float32)

time = 47362	action = 1	current_phase = 0	next_phase = 1	reward = -1.664563	array([[-6.7869   , -3.5035672]], dtype=float32)

time = 47370	action = 1	current_phase = 1	next_phase = 0	reward = -0.892422	array([[-3.930071, -2.451344]], dtype=float32)

time = 47378	action = 0	current_phase = 0	next_phase = 1	reward = -0.057758	array([[-1.8181314, -2.8262324]], dtype=float32)

time = 47383	action = 0	current_phase = 0	next_phase = 1	reward = -0.005018	array([[-2.1955137, -2.9038177]], dtype=float32)

time = 47388	action = 0	current_phase = 0	next_phase = 1	reward = 0.080074	array([[-2.8404417, -3.2645798]], dtype=float32)

time = 47393	action = 1	current_phase = 0	next_phase = 1	reward = -1.335357	array([[-6.802892 , -3.1425848]], dtype=float32)

time = 47401	action = 1	current_phase = 1	next_phase = 0	reward = -1.140866	array([[-3.9322026, -2.3965821]], dtype=float32)

time = 47409	action = 0	current_phase = 0	next_phase = 1	reward = -0.039678	array([[-1.3599188, -2.0882523]], dtype=float32)

time = 47414	action = 0	current_phase = 0	next_phase = 1	reward = 0.033202	array([[-1.9298365, -2.8232975]], dtype=float32)

time = 47419	action = 1	current_phase = 0	next_phase = 1	reward = -0.635748	array([[-2.4220002, -2.2508972]], dtype=float32)

time = 47427	action = 1	current_phase = 1	next_phase = 0	reward = -0.599030	array([[-3.2135394, -1.7931052]], dtype=float32)

time = 47435	action = 0	current_phase = 0	next_phase = 1	reward = -0.397343	array([[-1.6622093, -3.1284766]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0225 - val_loss: 0.0082

Epoch 2/50

 - 3s - loss: 0.0159 - val_loss: 0.0100

Epoch 3/50

 - 3s - loss: 0.0195 - val_loss: 0.0107

Epoch 4/50

 - 3s - loss: 0.0181 - val_loss: 0.0111

Epoch 5/50

 - 3s - loss: 0.0201 - val_loss: 0.0091

Epoch 6/50

 - 3s - loss: 0.0209 - val_loss: 0.0076

Epoch 7/50

 - 3s - loss: 0.0150 - val_loss: 0.0115

Epoch 8/50

 - 3s - loss: 0.0157 - val_loss: 0.0092

Epoch 9/50

 - 3s - loss: 0.0157 - val_loss: 0.0093

Epoch 10/50

 - 3s - loss: 0.0190 - val_loss: 0.0100

Epoch 11/50

 - 3s - loss: 0.0160 - val_loss: 0.0112

Epoch 12/50

 - 3s - loss: 0.0169 - val_loss: 0.0122

Epoch 13/50

 - 3s - loss: 0.0201 - val_loss: 0.0117

Epoch 14/50

 - 3s - loss: 0.0160 - val_loss: 0.0103

Epoch 15/50

 - 3s - loss: 0.0148 - val_loss: 0.0106

Epoch 16/50

 - 3s - loss: 0.0157 - val_loss: 0.0104

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 47440	action = 0	current_phase = 0	next_phase = 1	reward = 0.251226	array([[-1.7322482, -2.0524063]], dtype=float32)

time = 47445	action = 0	current_phase = 0	next_phase = 1	reward = -0.230540	array([[-2.3933482, -3.0125523]], dtype=float32)

time = 47450	action = 1	current_phase = 0	next_phase = 1	reward = -0.973831	array([[-5.7152524, -2.6748137]], dtype=float32)

time = 47458	action = 1	current_phase = 1	next_phase = 0	reward = -0.718920	array([[-3.5221047, -2.1358507]], dtype=float32)

time = 47466	action = 0	current_phase = 0	next_phase = 1	reward = -0.097339	array([[-1.7125442, -2.8548915]], dtype=float32)

time = 47471	action = 0	current_phase = 0	next_phase = 1	reward = -0.010759	array([[-2.048012 , -2.8714323]], dtype=float32)

time = 47476	action = 0	current_phase = 0	next_phase = 1	reward = 0.045598	array([[-2.5903852, -3.1102414]], dtype=float32)

time = 47481	action = 1	current_phase = 0	next_phase = 1	reward = -1.435176	array([[-6.8072724, -3.1245778]], dtype=float32)

time = 47489	action = 1	current_phase = 1	next_phase = 0	reward = -0.782438	array([[-3.7133694, -2.3107743]], dtype=float32)

time = 47497	action = 0	current_phase = 0	next_phase = 1	reward = -0.066141	array([[-1.7090787, -2.8173153]], dtype=float32)

time = 47502	action = 0	current_phase = 0	next_phase = 1	reward = 0.016285	array([[-2.188838 , -2.9265924]], dtype=float32)

time = 47507	action = 0	current_phase = 0	next_phase = 1	reward = 0.073794	array([[-2.757673 , -3.2215624]], dtype=float32)

time = 47512	action = 1	current_phase = 0	next_phase = 1	reward = -1.601050	array([[-6.7822633, -3.4461884]], dtype=float32)

time = 47520	action = 1	current_phase = 1	next_phase = 0	reward = -0.952697	array([[-3.8616018, -2.4282975]], dtype=float32)

time = 47528	action = 0	current_phase = 0	next_phase = 1	reward = -0.065435	array([[-1.7630339, -2.8291516]], dtype=float32)

time = 47533	action = 0	current_phase = 0	next_phase = 1	reward = 0.017477	array([[-2.1769395, -2.926404 ]], dtype=float32)

time = 47538	action = 0	current_phase = 0	next_phase = 1	reward = 0.081783	array([[-2.816035 , -3.2581778]], dtype=float32)

time = 47543	action = 1	current_phase = 0	next_phase = 1	reward = -1.892307	array([[-6.73583  , -3.5554326]], dtype=float32)

time = 47551	action = 1	current_phase = 1	next_phase = 0	reward = -1.039175	array([[-4.2140794, -2.2490366]], dtype=float32)

time = 47559	action = 0	current_phase = 0	next_phase = 1	reward = -0.024556	array([[-1.374753 , -2.0603878]], dtype=float32)

time = 47564	action = 0	current_phase = 0	next_phase = 1	reward = 0.048117	array([[-1.8239558, -2.807502 ]], dtype=float32)

time = 47569	action = 1	current_phase = 0	next_phase = 1	reward = -0.652326	array([[-2.4098282, -2.2123163]], dtype=float32)

time = 47577	action = 1	current_phase = 1	next_phase = 0	reward = -0.822179	array([[-3.2523227, -1.834959 ]], dtype=float32)

time = 47585	action = 0	current_phase = 0	next_phase = 1	reward = -0.102725	array([[-1.4173775, -3.0629907]], dtype=float32)

time = 47590	action = 0	current_phase = 0	next_phase = 1	reward = -0.016795	array([[-1.8532405, -2.0521617]], dtype=float32)

time = 47595	action = 0	current_phase = 0	next_phase = 1	reward = 0.330403	array([[-2.29439 , -2.957963]], dtype=float32)

time = 47600	action = 1	current_phase = 0	next_phase = 1	reward = -1.362496	array([[-5.7419386, -2.835718 ]], dtype=float32)

time = 47608	action = 1	current_phase = 1	next_phase = 0	reward = -0.724476	array([[-3.6081362, -2.1447084]], dtype=float32)

time = 47616	action = 0	current_phase = 0	next_phase = 1	reward = -0.101399	array([[-1.714016 , -2.8374832]], dtype=float32)

time = 47621	action = 0	current_phase = 0	next_phase = 1	reward = -0.014565	array([[-1.9819984, -2.8676105]], dtype=float32)

time = 47626	action = 0	current_phase = 0	next_phase = 1	reward = 0.068789	array([[-2.5856795, -3.1282954]], dtype=float32)

time = 47631	action = 1	current_phase = 0	next_phase = 1	reward = -1.541971	array([[-6.7923336, -3.1311007]], dtype=float32)

time = 47639	action = 1	current_phase = 1	next_phase = 0	reward = -0.835193	array([[-3.6869946, -2.282072 ]], dtype=float32)

time = 47647	action = 0	current_phase = 0	next_phase = 1	reward = -0.068528	array([[-1.7500243, -2.8228343]], dtype=float32)

time = 47652	action = 0	current_phase = 0	next_phase = 1	reward = -0.012293	array([[-2.2020667, -2.9328897]], dtype=float32)

time = 47657	action = 0	current_phase = 0	next_phase = 1	reward = 0.066520	array([[-2.74825  , -3.1989684]], dtype=float32)

time = 47662	action = 1	current_phase = 0	next_phase = 1	reward = -1.648540	array([[-6.778396, -3.352217]], dtype=float32)

time = 47670	action = 1	current_phase = 1	next_phase = 0	reward = -0.901280	array([[-3.8545876, -2.4460628]], dtype=float32)

time = 47678	action = 0	current_phase = 0	next_phase = 1	reward = -0.052681	array([[-1.7447035, -2.8074007]], dtype=float32)

time = 47683	action = 0	current_phase = 0	next_phase = 1	reward = 0.017141	array([[-2.1802611, -2.9168339]], dtype=float32)

time = 47688	action = 0	current_phase = 0	next_phase = 1	reward = 0.075719	array([[-2.7363555, -3.251007 ]], dtype=float32)

time = 47693	action = 1	current_phase = 0	next_phase = 1	reward = -1.329184	array([[-6.7915354, -3.0903327]], dtype=float32)

time = 47701	action = 1	current_phase = 1	next_phase = 0	reward = -1.147992	array([[-3.9688802, -2.4070642]], dtype=float32)

time = 47709	action = 0	current_phase = 0	next_phase = 1	reward = -0.055814	array([[-1.376862 , -2.0766969]], dtype=float32)

time = 47714	action = 0	current_phase = 0	next_phase = 1	reward = 0.027868	array([[-1.793649 , -2.7971578]], dtype=float32)

time = 47719	action = 1	current_phase = 0	next_phase = 1	reward = -0.696853	array([[-2.445954, -2.163892]], dtype=float32)

time = 47727	action = 1	current_phase = 1	next_phase = 0	reward = -1.141082	array([[-3.29104  , -1.8172925]], dtype=float32)

time = 47735	action = 0	current_phase = 0	next_phase = 1	reward = 0.473709	array([[-1.3913355, -3.0012903]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0135 - val_loss: 0.0073

Epoch 2/50

 - 3s - loss: 0.0219 - val_loss: 0.0127

Epoch 3/50

 - 3s - loss: 0.0170 - val_loss: 0.0076

Epoch 4/50

 - 3s - loss: 0.0135 - val_loss: 0.0092

Epoch 5/50

 - 3s - loss: 0.0126 - val_loss: 0.0088

Epoch 6/50

 - 3s - loss: 0.0143 - val_loss: 0.0100

Epoch 7/50

 - 3s - loss: 0.0135 - val_loss: 0.0109

Epoch 8/50

 - 3s - loss: 0.0165 - val_loss: 0.0092

Epoch 9/50

 - 3s - loss: 0.0132 - val_loss: 0.0080

Epoch 10/50

 - 3s - loss: 0.0161 - val_loss: 0.0095

Epoch 11/50

 - 3s - loss: 0.0136 - val_loss: 0.0091

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 47740	action = 0	current_phase = 0	next_phase = 1	reward = -0.021948	array([[-1.8909738, -2.0740607]], dtype=float32)

time = 47745	action = 0	current_phase = 0	next_phase = 1	reward = -0.240453	array([[-2.451743 , -3.0308123]], dtype=float32)

time = 47750	action = 1	current_phase = 0	next_phase = 1	reward = -1.043946	array([[-5.744547, -2.753245]], dtype=float32)

time = 47758	action = 1	current_phase = 1	next_phase = 0	reward = -0.698532	array([[-3.4727685, -2.0627048]], dtype=float32)

time = 47766	action = 0	current_phase = 0	next_phase = 1	reward = -0.098203	array([[-1.7912623, -2.814339 ]], dtype=float32)

time = 47771	action = 0	current_phase = 0	next_phase = 1	reward = -0.021536	array([[-2.12741 , -2.880061]], dtype=float32)

time = 47776	action = 0	current_phase = 0	next_phase = 1	reward = 0.047979	array([[-2.608459 , -3.1098635]], dtype=float32)

time = 47781	action = 1	current_phase = 0	next_phase = 1	reward = -1.483687	array([[-6.8617263, -3.2471042]], dtype=float32)

time = 47789	action = 1	current_phase = 1	next_phase = 0	reward = -0.841428	array([[-3.6895473, -2.3301058]], dtype=float32)

time = 47797	action = 0	current_phase = 0	next_phase = 1	reward = -0.066065	array([[-1.8196938, -2.8173275]], dtype=float32)

time = 47802	action = 0	current_phase = 0	next_phase = 1	reward = 0.003803	array([[-2.2249842, -2.942969 ]], dtype=float32)

time = 47807	action = 0	current_phase = 0	next_phase = 1	reward = 0.072972	array([[-2.791782 , -3.2692223]], dtype=float32)

time = 47812	action = 1	current_phase = 0	next_phase = 1	reward = -1.588127	array([[-6.821544 , -3.4785922]], dtype=float32)

time = 47820	action = 1	current_phase = 1	next_phase = 0	reward = -0.898653	array([[-3.8687377, -2.4860404]], dtype=float32)

time = 47828	action = 0	current_phase = 0	next_phase = 1	reward = -0.066580	array([[-1.789463 , -2.8227205]], dtype=float32)

time = 47833	action = 0	current_phase = 0	next_phase = 1	reward = 0.013926	array([[-2.2688706, -2.939622 ]], dtype=float32)

time = 47838	action = 0	current_phase = 0	next_phase = 1	reward = 0.075793	array([[-2.8416703, -3.2677364]], dtype=float32)

time = 47843	action = 1	current_phase = 0	next_phase = 1	reward = -1.812038	array([[-6.778995 , -3.6264532]], dtype=float32)

time = 47851	action = 1	current_phase = 1	next_phase = 0	reward = -1.035449	array([[-4.1832495, -2.3727713]], dtype=float32)

time = 47859	action = 0	current_phase = 0	next_phase = 1	reward = -0.039844	array([[-1.4252856, -2.0417123]], dtype=float32)

time = 47864	action = 0	current_phase = 0	next_phase = 1	reward = 0.024085	array([[-1.839607 , -2.7958765]], dtype=float32)

time = 47869	action = 1	current_phase = 0	next_phase = 1	reward = -0.629746	array([[-2.5165439, -2.1270566]], dtype=float32)

time = 47877	action = 1	current_phase = 1	next_phase = 0	reward = -0.824084	array([[-3.1199214, -1.7987146]], dtype=float32)

time = 47885	action = 0	current_phase = 0	next_phase = 1	reward = 0.185262	array([[-1.4091012, -3.0385094]], dtype=float32)

time = 47890	action = 0	current_phase = 0	next_phase = 1	reward = -0.019241	array([[-1.8951409, -2.0334673]], dtype=float32)

time = 47895	action = 0	current_phase = 0	next_phase = 1	reward = -0.223375	array([[-2.4829488, -3.0527613]], dtype=float32)

time = 47900	action = 1	current_phase = 0	next_phase = 1	reward = -0.982877	array([[-5.771871 , -2.6824105]], dtype=float32)

time = 47908	action = 1	current_phase = 1	next_phase = 0	reward = -0.707222	array([[-3.4574964, -2.0939467]], dtype=float32)

time = 47916	action = 0	current_phase = 0	next_phase = 1	reward = -0.088288	array([[-1.7499685, -2.8113825]], dtype=float32)

time = 47921	action = 0	current_phase = 0	next_phase = 1	reward = -0.021272	array([[-2.124021 , -2.8795557]], dtype=float32)

time = 47926	action = 0	current_phase = 0	next_phase = 1	reward = 0.032345	array([[-2.626788 , -3.1337662]], dtype=float32)

time = 47931	action = 1	current_phase = 0	next_phase = 1	reward = -1.493646	array([[-6.829901, -3.372932]], dtype=float32)

time = 47939	action = 1	current_phase = 1	next_phase = 0	reward = -0.775340	array([[-3.650356 , -2.2600641]], dtype=float32)

time = 47947	action = 0	current_phase = 0	next_phase = 1	reward = -0.064769	array([[-1.8225154, -2.8093631]], dtype=float32)

time = 47952	action = 0	current_phase = 0	next_phase = 1	reward = -0.006678	array([[-2.2356024, -2.9575438]], dtype=float32)

time = 47957	action = 0	current_phase = 0	next_phase = 1	reward = 0.055273	array([[-2.776186 , -3.2305598]], dtype=float32)

time = 47962	action = 1	current_phase = 0	next_phase = 1	reward = -1.575012	array([[-6.8228908, -3.4838088]], dtype=float32)

time = 47970	action = 1	current_phase = 1	next_phase = 0	reward = -1.251425	array([[-3.77435  , -2.4174771]], dtype=float32)

time = 47978	action = 0	current_phase = 0	next_phase = 1	reward = 0.232206	array([[-1.7511537, -2.8005917]], dtype=float32)

time = 47983	action = 0	current_phase = 0	next_phase = 1	reward = 0.025233	array([[-2.2531388, -2.9304175]], dtype=float32)

time = 47988	action = 0	current_phase = 0	next_phase = 1	reward = 0.070453	array([[-2.8288069, -3.3846374]], dtype=float32)

time = 47993	action = 1	current_phase = 0	next_phase = 1	reward = -1.396174	array([[-6.836334, -3.305524]], dtype=float32)

time = 48001	action = 1	current_phase = 1	next_phase = 0	reward = -1.387073	array([[-3.892434 , -2.4910436]], dtype=float32)

time = 48009	action = 0	current_phase = 0	next_phase = 1	reward = 0.248719	array([[-1.3631532, -2.0510435]], dtype=float32)

time = 48014	action = 0	current_phase = 0	next_phase = 1	reward = 0.021293	array([[-1.8285713, -2.806077 ]], dtype=float32)

time = 48019	action = 1	current_phase = 0	next_phase = 1	reward = -0.672493	array([[-2.337889 , -2.1907218]], dtype=float32)

time = 48027	action = 1	current_phase = 1	next_phase = 0	reward = -0.643630	array([[-3.222362 , -1.9055436]], dtype=float32)

time = 48035	action = 0	current_phase = 0	next_phase = 1	reward = -0.375281	array([[-1.5609853, -3.0468168]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0168 - val_loss: 0.0077

Epoch 2/50

 - 3s - loss: 0.0156 - val_loss: 0.0098

Epoch 3/50

 - 3s - loss: 0.0152 - val_loss: 0.0074

Epoch 4/50

 - 3s - loss: 0.0147 - val_loss: 0.0084

Epoch 5/50

 - 3s - loss: 0.0165 - val_loss: 0.0080

Epoch 6/50

 - 3s - loss: 0.0145 - val_loss: 0.0138

Epoch 7/50

 - 3s - loss: 0.0156 - val_loss: 0.0088

Epoch 8/50

 - 3s - loss: 0.0140 - val_loss: 0.0084

Epoch 9/50

 - 3s - loss: 0.0153 - val_loss: 0.0122

Epoch 10/50

 - 3s - loss: 0.0203 - val_loss: 0.0097

Epoch 11/50

 - 3s - loss: 0.0172 - val_loss: 0.0113

Epoch 12/50

 - 3s - loss: 0.0175 - val_loss: 0.0084

Epoch 13/50

 - 3s - loss: 0.0148 - val_loss: 0.0092

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 48040	action = 0	current_phase = 0	next_phase = 1	reward = 0.265779	array([[-1.7614175, -2.0921998]], dtype=float32)

time = 48045	action = 0	current_phase = 0	next_phase = 1	reward = -0.237267	array([[-2.4615946, -3.0430293]], dtype=float32)

time = 48050	action = 1	current_phase = 0	next_phase = 1	reward = -1.047425	array([[-5.787626, -2.66504 ]], dtype=float32)

time = 48058	action = 1	current_phase = 1	next_phase = 0	reward = -0.697757	array([[-3.5289211, -2.1369615]], dtype=float32)

time = 48066	action = 0	current_phase = 0	next_phase = 1	reward = -0.065983	array([[-1.7865922, -2.831022 ]], dtype=float32)

time = 48071	action = 0	current_phase = 0	next_phase = 1	reward = 0.008851	array([[-2.1017783, -2.888652 ]], dtype=float32)

time = 48076	action = 0	current_phase = 0	next_phase = 1	reward = 0.070243	array([[-2.6571407, -3.1951814]], dtype=float32)

time = 48081	action = 1	current_phase = 0	next_phase = 1	reward = -1.518279	array([[-6.856399 , -3.2618377]], dtype=float32)

time = 48089	action = 1	current_phase = 1	next_phase = 0	reward = -0.762249	array([[-3.6712184, -2.3095884]], dtype=float32)

time = 48097	action = 0	current_phase = 0	next_phase = 1	reward = -0.059328	array([[-1.782354 , -2.8197646]], dtype=float32)

time = 48102	action = 0	current_phase = 0	next_phase = 1	reward = 0.003196	array([[-2.2404733, -2.9321966]], dtype=float32)

time = 48107	action = 0	current_phase = 0	next_phase = 1	reward = 0.063822	array([[-2.7528172, -3.2262414]], dtype=float32)

time = 48112	action = 1	current_phase = 0	next_phase = 1	reward = -1.563357	array([[-6.8363843, -3.4788156]], dtype=float32)

time = 48120	action = 1	current_phase = 1	next_phase = 0	reward = -1.207608	array([[-3.9163444, -2.5161948]], dtype=float32)

time = 48128	action = 0	current_phase = 0	next_phase = 1	reward = 0.231808	array([[-1.7087803, -2.8031714]], dtype=float32)

time = 48133	action = 0	current_phase = 0	next_phase = 1	reward = 0.008392	array([[-2.244247 , -2.9296107]], dtype=float32)

time = 48138	action = 0	current_phase = 0	next_phase = 1	reward = 0.077138	array([[-2.8040276, -3.2624218]], dtype=float32)

time = 48143	action = 1	current_phase = 0	next_phase = 1	reward = -1.862794	array([[-6.812659 , -3.6073346]], dtype=float32)

time = 48151	action = 1	current_phase = 1	next_phase = 0	reward = -1.353271	array([[-4.3259697, -2.3423142]], dtype=float32)

time = 48159	action = 0	current_phase = 0	next_phase = 1	reward = 0.254842	array([[-1.3542356, -2.0470068]], dtype=float32)

time = 48164	action = 0	current_phase = 0	next_phase = 1	reward = 0.030982	array([[-1.9116396, -2.8179846]], dtype=float32)

time = 48169	action = 1	current_phase = 0	next_phase = 1	reward = -0.625482	array([[-2.4802575, -2.1410232]], dtype=float32)

time = 48177	action = 1	current_phase = 1	next_phase = 0	reward = -0.538504	array([[-3.2017095, -1.8529854]], dtype=float32)

time = 48185	action = 0	current_phase = 0	next_phase = 1	reward = -0.108986	array([[-1.459908 , -3.0781286]], dtype=float32)

time = 48190	action = 0	current_phase = 0	next_phase = 1	reward = -0.298742	array([[-1.8622013, -2.0399582]], dtype=float32)

time = 48195	action = 0	current_phase = 0	next_phase = 1	reward = 0.061988	array([[-2.318582, -2.973636]], dtype=float32)

time = 48200	action = 1	current_phase = 0	next_phase = 1	reward = -0.967826	array([[-5.7804275, -2.6700044]], dtype=float32)

time = 48208	action = 1	current_phase = 1	next_phase = 0	reward = -0.714439	array([[-3.486879 , -2.1541696]], dtype=float32)

time = 48216	action = 0	current_phase = 0	next_phase = 1	reward = -0.075390	array([[-1.6767498, -2.824579 ]], dtype=float32)

time = 48221	action = 0	current_phase = 0	next_phase = 1	reward = 0.019260	array([[-2.1349812, -2.8967621]], dtype=float32)

time = 48226	action = 0	current_phase = 0	next_phase = 1	reward = 0.083324	array([[-2.6646376, -3.1934822]], dtype=float32)

time = 48231	action = 1	current_phase = 0	next_phase = 1	reward = -1.516952	array([[-6.8577423, -3.2642987]], dtype=float32)

time = 48239	action = 1	current_phase = 1	next_phase = 0	reward = -0.810482	array([[-3.6806538, -2.3267007]], dtype=float32)

time = 48247	action = 0	current_phase = 0	next_phase = 1	reward = -0.079456	array([[-1.8129125, -2.8186195]], dtype=float32)

time = 48252	action = 0	current_phase = 0	next_phase = 1	reward = 0.003617	array([[-2.2507293, -2.9450552]], dtype=float32)

time = 48257	action = 0	current_phase = 0	next_phase = 1	reward = 0.073394	array([[-2.7917938, -3.239714 ]], dtype=float32)

time = 48262	action = 1	current_phase = 0	next_phase = 1	reward = -1.591355	array([[-6.8216085, -3.5283601]], dtype=float32)

time = 48270	action = 1	current_phase = 1	next_phase = 0	reward = -0.999471	array([[-3.8849158, -2.5220513]], dtype=float32)

time = 48278	action = 0	current_phase = 0	next_phase = 1	reward = -0.030249	array([[-1.7901313, -2.787802 ]], dtype=float32)

time = 48283	action = 0	current_phase = 0	next_phase = 1	reward = 0.012851	array([[-2.2857208, -2.9507365]], dtype=float32)

time = 48288	action = 0	current_phase = 0	next_phase = 1	reward = 0.076384	array([[-2.8278437, -3.3258846]], dtype=float32)

time = 48293	action = 1	current_phase = 0	next_phase = 1	reward = -1.362719	array([[-6.8708086, -3.1830165]], dtype=float32)

time = 48301	action = 1	current_phase = 1	next_phase = 0	reward = -1.505215	array([[-3.8883872, -2.4020922]], dtype=float32)

time = 48309	action = 0	current_phase = 0	next_phase = 1	reward = 0.242612	array([[-1.2645477, -2.0482862]], dtype=float32)

time = 48314	action = 0	current_phase = 0	next_phase = 1	reward = 0.017192	array([[-1.8746153, -2.8332152]], dtype=float32)

time = 48319	action = 1	current_phase = 0	next_phase = 1	reward = -0.654826	array([[-2.2963352, -2.2180777]], dtype=float32)

time = 48327	action = 1	current_phase = 1	next_phase = 0	reward = -0.878576	array([[-3.323333 , -2.0478477]], dtype=float32)

time = 48335	action = 0	current_phase = 0	next_phase = 1	reward = -0.108551	array([[-1.5462627, -3.0082853]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0201 - val_loss: 0.0133

Epoch 2/50

 - 3s - loss: 0.0168 - val_loss: 0.0178

Epoch 3/50

 - 3s - loss: 0.0183 - val_loss: 0.0093

Epoch 4/50

 - 3s - loss: 0.0163 - val_loss: 0.0120

Epoch 5/50

 - 3s - loss: 0.0205 - val_loss: 0.0102

Epoch 6/50

 - 3s - loss: 0.0250 - val_loss: 0.0095

Epoch 7/50

 - 3s - loss: 0.0180 - val_loss: 0.0124

Epoch 8/50

 - 3s - loss: 0.0206 - val_loss: 0.0101

Epoch 9/50

 - 3s - loss: 0.0175 - val_loss: 0.0130

Epoch 10/50

 - 3s - loss: 0.0154 - val_loss: 0.0135

Epoch 11/50

 - 3s - loss: 0.0225 - val_loss: 0.0120

Epoch 12/50

 - 3s - loss: 0.0157 - val_loss: 0.0105

Epoch 13/50

 - 3s - loss: 0.0188 - val_loss: 0.0103

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 48340	action = 0	current_phase = 0	next_phase = 1	reward = -0.026313	array([[-1.8809353, -2.0858305]], dtype=float32)

time = 48345	action = 0	current_phase = 0	next_phase = 1	reward = 0.323772	array([[-2.1371546, -2.934384 ]], dtype=float32)

time = 48350	action = 1	current_phase = 0	next_phase = 1	reward = -1.427965	array([[-5.8296475, -3.008762 ]], dtype=float32)

time = 48358	action = 1	current_phase = 1	next_phase = 0	reward = -0.716870	array([[-3.5914962, -2.09391  ]], dtype=float32)

time = 48366	action = 0	current_phase = 0	next_phase = 1	reward = -0.092652	array([[-1.7963089, -2.8380892]], dtype=float32)

time = 48371	action = 0	current_phase = 0	next_phase = 1	reward = -0.028488	array([[-2.0622716, -2.8759086]], dtype=float32)

time = 48376	action = 0	current_phase = 0	next_phase = 1	reward = 0.048400	array([[-2.5209897, -3.1139402]], dtype=float32)

time = 48381	action = 1	current_phase = 0	next_phase = 1	reward = -1.484958	array([[-6.887376, -3.296253]], dtype=float32)

time = 48389	action = 1	current_phase = 1	next_phase = 0	reward = -0.841220	array([[-3.674208 , -2.2871542]], dtype=float32)

time = 48397	action = 0	current_phase = 0	next_phase = 1	reward = -0.080676	array([[-1.7959201, -2.823497 ]], dtype=float32)

time = 48402	action = 0	current_phase = 0	next_phase = 1	reward = 0.012221	array([[-2.2093735, -2.9377966]], dtype=float32)

time = 48407	action = 0	current_phase = 0	next_phase = 1	reward = 0.071604	array([[-2.6836984, -3.2616084]], dtype=float32)

time = 48412	action = 1	current_phase = 0	next_phase = 1	reward = -1.648585	array([[-6.858326 , -3.5617237]], dtype=float32)

time = 48420	action = 1	current_phase = 1	next_phase = 0	reward = -1.314936	array([[-3.9193492, -2.5453815]], dtype=float32)

time = 48428	action = 0	current_phase = 0	next_phase = 1	reward = 0.238296	array([[-1.7833319, -2.8183208]], dtype=float32)

time = 48433	action = 0	current_phase = 0	next_phase = 1	reward = 0.000950	array([[-2.2232752, -2.932138 ]], dtype=float32)

time = 48438	action = 0	current_phase = 0	next_phase = 1	reward = 0.064389	array([[-2.7954197, -3.3771653]], dtype=float32)

time = 48443	action = 1	current_phase = 0	next_phase = 1	reward = -1.895983	array([[-6.84637  , -3.6240416]], dtype=float32)

time = 48451	action = 1	current_phase = 1	next_phase = 0	reward = -1.094014	array([[-4.3185425, -2.3646755]], dtype=float32)

time = 48459	action = 0	current_phase = 0	next_phase = 1	reward = -0.040154	array([[-1.5509003, -2.021841 ]], dtype=float32)

time = 48464	action = 0	current_phase = 0	next_phase = 1	reward = 0.046528	array([[-1.8297807, -2.8080347]], dtype=float32)

time = 48469	action = 1	current_phase = 0	next_phase = 1	reward = -0.710369	array([[-2.4615116, -2.2257726]], dtype=float32)

time = 48477	action = 1	current_phase = 1	next_phase = 0	reward = -1.152723	array([[-3.384279 , -1.9705355]], dtype=float32)

time = 48485	action = 0	current_phase = 0	next_phase = 1	reward = 0.456529	array([[-1.3867731, -3.1828828]], dtype=float32)

time = 48490	action = 0	current_phase = 0	next_phase = 1	reward = -0.310807	array([[-1.8904003, -2.0504863]], dtype=float32)

time = 48495	action = 0	current_phase = 0	next_phase = 1	reward = 0.319986	array([[-2.1560757, -2.9111562]], dtype=float32)

time = 48500	action = 1	current_phase = 0	next_phase = 1	reward = -1.366276	array([[-5.8194146, -3.0599062]], dtype=float32)

time = 48508	action = 1	current_phase = 1	next_phase = 0	reward = -0.693781	array([[-3.6061363, -2.105361 ]], dtype=float32)

time = 48516	action = 0	current_phase = 0	next_phase = 1	reward = -0.082407	array([[-1.7581991, -2.8278394]], dtype=float32)

time = 48521	action = 0	current_phase = 0	next_phase = 1	reward = -0.004433	array([[-2.0895638, -2.9029102]], dtype=float32)

time = 48526	action = 0	current_phase = 0	next_phase = 1	reward = 0.051016	array([[-2.5130374, -3.1397974]], dtype=float32)

time = 48531	action = 1	current_phase = 0	next_phase = 1	reward = -1.446802	array([[-6.874556 , -3.2931442]], dtype=float32)

time = 48539	action = 1	current_phase = 1	next_phase = 0	reward = -0.818499	array([[-3.646273 , -2.2532096]], dtype=float32)

time = 48547	action = 0	current_phase = 0	next_phase = 1	reward = -0.064780	array([[-1.8152223, -2.816025 ]], dtype=float32)

time = 48552	action = 0	current_phase = 0	next_phase = 1	reward = 0.018095	array([[-2.1869252, -2.9249058]], dtype=float32)

time = 48557	action = 0	current_phase = 0	next_phase = 1	reward = 0.086437	array([[-2.7541037, -3.29827  ]], dtype=float32)

time = 48562	action = 1	current_phase = 0	next_phase = 1	reward = -1.719323	array([[-6.87553  , -3.5461235]], dtype=float32)

time = 48570	action = 1	current_phase = 1	next_phase = 0	reward = -1.015335	array([[-3.929731 , -2.5029843]], dtype=float32)

time = 48578	action = 0	current_phase = 0	next_phase = 1	reward = -0.067497	array([[-1.8614275, -2.8222244]], dtype=float32)

time = 48583	action = 0	current_phase = 0	next_phase = 1	reward = 0.010322	array([[-2.2701192, -2.9475443]], dtype=float32)

time = 48588	action = 0	current_phase = 0	next_phase = 1	reward = 0.074917	array([[-2.796271, -3.304643]], dtype=float32)

time = 48593	action = 1	current_phase = 0	next_phase = 1	reward = -1.809872	array([[-6.8430114, -3.6745865]], dtype=float32)

time = 48601	action = 1	current_phase = 1	next_phase = 0	reward = -1.335884	array([[-4.399211 , -2.2950306]], dtype=float32)

time = 48609	action = 0	current_phase = 0	next_phase = 1	reward = 0.259124	array([[-1.3683658, -2.065578 ]], dtype=float32)

time = 48614	action = 0	current_phase = 0	next_phase = 1	reward = 0.035251	array([[-1.8458554, -2.8093503]], dtype=float32)

time = 48619	action = 1	current_phase = 0	next_phase = 1	reward = -0.640475	array([[-2.482573 , -2.2305238]], dtype=float32)

time = 48627	action = 1	current_phase = 1	next_phase = 0	reward = -0.883445	array([[-3.308348 , -1.9765036]], dtype=float32)

time = 48635	action = 0	current_phase = 0	next_phase = 1	reward = -0.111581	array([[-1.3541801, -3.0779686]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 4s - loss: 0.0198 - val_loss: 0.0107

Epoch 2/50

 - 3s - loss: 0.0187 - val_loss: 0.0098

Epoch 3/50

 - 3s - loss: 0.0175 - val_loss: 0.0104

Epoch 4/50

 - 3s - loss: 0.0164 - val_loss: 0.0085

Epoch 5/50

 - 3s - loss: 0.0179 - val_loss: 0.0114

Epoch 6/50

 - 3s - loss: 0.0140 - val_loss: 0.0106

Epoch 7/50

 - 3s - loss: 0.0188 - val_loss: 0.0102

Epoch 8/50

 - 3s - loss: 0.0158 - val_loss: 0.0091

Epoch 9/50

 - 3s - loss: 0.0183 - val_loss: 0.0097

Epoch 10/50

 - 3s - loss: 0.0193 - val_loss: 0.0113

Epoch 11/50

 - 3s - loss: 0.0148 - val_loss: 0.0087

Epoch 12/50

 - 3s - loss: 0.0156 - val_loss: 0.0107

Epoch 13/50

 - 3s - loss: 0.0151 - val_loss: 0.0104

Epoch 14/50

 - 3s - loss: 0.0171 - val_loss: 0.0108

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 48640	action = 0	current_phase = 0	next_phase = 1	reward = 0.258451	array([[-1.72022  , -2.0548265]], dtype=float32)

time = 48645	action = 0	current_phase = 0	next_phase = 1	reward = -0.232397	array([[-2.2852812, -2.9742303]], dtype=float32)

time = 48650	action = 1	current_phase = 0	next_phase = 1	reward = -1.013758	array([[-5.8386397, -2.8309262]], dtype=float32)

time = 48658	action = 1	current_phase = 1	next_phase = 0	reward = -0.711466	array([[-3.5775514, -2.131588 ]], dtype=float32)

time = 48666	action = 0	current_phase = 0	next_phase = 1	reward = -0.083108	array([[-1.742598, -2.827845]], dtype=float32)

time = 48671	action = 0	current_phase = 0	next_phase = 1	reward = -0.013482	array([[-2.1105695, -2.865479 ]], dtype=float32)

time = 48676	action = 0	current_phase = 0	next_phase = 1	reward = 0.045441	array([[-2.4614182, -3.12905  ]], dtype=float32)

time = 48681	action = 1	current_phase = 0	next_phase = 1	reward = -1.387096	array([[-6.9144726, -3.306989 ]], dtype=float32)

time = 48689	action = 1	current_phase = 1	next_phase = 0	reward = -0.815859	array([[-3.6143146, -2.2076185]], dtype=float32)

time = 48697	action = 0	current_phase = 0	next_phase = 1	reward = -0.080747	array([[-1.8219061, -2.7982218]], dtype=float32)

time = 48702	action = 0	current_phase = 0	next_phase = 1	reward = -0.003176	array([[-2.141189, -2.90781 ]], dtype=float32)

time = 48707	action = 0	current_phase = 0	next_phase = 1	reward = 0.058147	array([[-2.6940455, -3.2379885]], dtype=float32)

time = 48712	action = 1	current_phase = 0	next_phase = 1	reward = -1.585422	array([[-6.8526745, -3.5894775]], dtype=float32)

time = 48720	action = 1	current_phase = 1	next_phase = 0	reward = -0.916324	array([[-3.8672376, -2.4391787]], dtype=float32)

time = 48728	action = 0	current_phase = 0	next_phase = 1	reward = -0.063468	array([[-1.7683579, -2.8225935]], dtype=float32)

time = 48733	action = 0	current_phase = 0	next_phase = 1	reward = 0.021017	array([[-2.2378502, -2.9248238]], dtype=float32)

time = 48738	action = 0	current_phase = 0	next_phase = 1	reward = 0.081729	array([[-2.7296414, -3.2932937]], dtype=float32)

time = 48743	action = 1	current_phase = 0	next_phase = 1	reward = -1.734880	array([[-6.847367 , -3.6997664]], dtype=float32)

time = 48751	action = 1	current_phase = 1	next_phase = 0	reward = -1.143234	array([[-4.1498504, -2.276483 ]], dtype=float32)

time = 48759	action = 0	current_phase = 0	next_phase = 1	reward = -0.032264	array([[-1.4547207, -2.0356731]], dtype=float32)

time = 48764	action = 0	current_phase = 0	next_phase = 1	reward = 0.043713	array([[-1.8373402, -2.8058777]], dtype=float32)

time = 48769	action = 1	current_phase = 0	next_phase = 1	reward = -0.634487	array([[-2.5045877, -2.2504919]], dtype=float32)

time = 48777	action = 1	current_phase = 1	next_phase = 0	reward = -0.573741	array([[-3.3433366, -1.9853758]], dtype=float32)

time = 48785	action = 0	current_phase = 0	next_phase = 1	reward = -0.104178	array([[-1.6273994, -3.0485804]], dtype=float32)

time = 48790	action = 0	current_phase = 0	next_phase = 1	reward = -0.015552	array([[-1.881363, -2.04699 ]], dtype=float32)

time = 48795	action = 0	current_phase = 0	next_phase = 1	reward = 0.046968	array([[-2.389803 , -3.0531306]], dtype=float32)

time = 48800	action = 1	current_phase = 0	next_phase = 1	reward = -1.317286	array([[-5.8407035, -3.013136 ]], dtype=float32)

time = 48808	action = 1	current_phase = 1	next_phase = 0	reward = -0.700659	array([[-3.4950109, -2.0912166]], dtype=float32)

time = 48816	action = 0	current_phase = 0	next_phase = 1	reward = -0.086993	array([[-1.8078114, -2.8036702]], dtype=float32)

time = 48821	action = 0	current_phase = 0	next_phase = 1	reward = -0.016270	array([[-2.0659409, -2.8450334]], dtype=float32)

time = 48826	action = 0	current_phase = 0	next_phase = 1	reward = 0.052479	array([[-2.5792303, -3.178562 ]], dtype=float32)

time = 48831	action = 1	current_phase = 0	next_phase = 1	reward = -1.453415	array([[-6.911633 , -3.3482354]], dtype=float32)

time = 48839	action = 1	current_phase = 1	next_phase = 0	reward = -1.069414	array([[-3.6692562, -2.2606735]], dtype=float32)

time = 48847	action = 0	current_phase = 0	next_phase = 1	reward = 0.216261	array([[-1.7883655, -2.7982705]], dtype=float32)

time = 48852	action = 0	current_phase = 0	next_phase = 1	reward = 0.013208	array([[-2.201381 , -2.9091573]], dtype=float32)

time = 48857	action = 0	current_phase = 0	next_phase = 1	reward = 0.068121	array([[-2.7301764, -3.2698495]], dtype=float32)

time = 48862	action = 1	current_phase = 0	next_phase = 1	reward = -1.550027	array([[-6.8627653, -3.5884714]], dtype=float32)

time = 48870	action = 1	current_phase = 1	next_phase = 0	reward = -1.004037	array([[-3.9412985, -2.4964523]], dtype=float32)

time = 48878	action = 0	current_phase = 0	next_phase = 1	reward = -0.043527	array([[-1.8472167, -2.8190186]], dtype=float32)

time = 48883	action = 0	current_phase = 0	next_phase = 1	reward = 0.024856	array([[-2.1892738, -2.9113426]], dtype=float32)

time = 48888	action = 0	current_phase = 0	next_phase = 1	reward = 0.075697	array([[-2.7258177, -3.3217318]], dtype=float32)

time = 48893	action = 1	current_phase = 0	next_phase = 1	reward = -1.877620	array([[-6.8481307, -3.7242568]], dtype=float32)

time = 48901	action = 1	current_phase = 1	next_phase = 0	reward = -1.033306	array([[-4.224353 , -2.2358124]], dtype=float32)

time = 48909	action = 0	current_phase = 0	next_phase = 1	reward = -0.034820	array([[-1.5112509, -2.0458724]], dtype=float32)

time = 48914	action = 0	current_phase = 0	next_phase = 1	reward = 0.045051	array([[-1.7748481, -2.7976074]], dtype=float32)

time = 48919	action = 1	current_phase = 0	next_phase = 1	reward = -1.292637	array([[-2.4164248, -2.2341754]], dtype=float32)

time = 48927	action = 1	current_phase = 1	next_phase = 0	reward = -0.819180	array([[-3.7251015, -1.9251609]], dtype=float32)

time = 48935	action = 0	current_phase = 0	next_phase = 1	reward = 0.178475	array([[-1.4740658, -2.9309638]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0157 - val_loss: 0.0098

Epoch 2/50

 - 3s - loss: 0.0196 - val_loss: 0.0093

Epoch 3/50

 - 3s - loss: 0.0184 - val_loss: 0.0173

Epoch 4/50

 - 3s - loss: 0.0164 - val_loss: 0.0104

Epoch 5/50

 - 3s - loss: 0.0146 - val_loss: 0.0077

Epoch 6/50

 - 3s - loss: 0.0172 - val_loss: 0.0106

Epoch 7/50

 - 3s - loss: 0.0140 - val_loss: 0.0091

Epoch 8/50

 - 3s - loss: 0.0148 - val_loss: 0.0108

Epoch 9/50

 - 3s - loss: 0.0201 - val_loss: 0.0111

Epoch 10/50

 - 3s - loss: 0.0164 - val_loss: 0.0106

Epoch 11/50

 - 3s - loss: 0.0135 - val_loss: 0.0077

Epoch 12/50

 - 3s - loss: 0.0158 - val_loss: 0.0086

Epoch 13/50

 - 3s - loss: 0.0142 - val_loss: 0.0113

Epoch 14/50

 - 3s - loss: 0.0150 - val_loss: 0.0090

Epoch 15/50

 - 3s - loss: 0.0157 - val_loss: 0.0097

Epoch 16/50

 - 3s - loss: 0.0156 - val_loss: 0.0131

Epoch 17/50

 - 3s - loss: 0.0157 - val_loss: 0.0125

Epoch 18/50

 - 3s - loss: 0.0156 - val_loss: 0.0083

Epoch 19/50

 - 3s - loss: 0.0210 - val_loss: 0.0083

Epoch 20/50

 - 3s - loss: 0.0119 - val_loss: 0.0107

Epoch 21/50

 - 3s - loss: 0.0161 - val_loss: 0.0105

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 48940	action = 0	current_phase = 0	next_phase = 1	reward = -0.295914	array([[-1.9616021, -2.014704 ]], dtype=float32)

time = 48945	action = 0	current_phase = 0	next_phase = 1	reward = 0.335341	array([[-2.2033434, -2.9340963]], dtype=float32)

time = 48950	action = 1	current_phase = 0	next_phase = 1	reward = -1.425403	array([[-5.8384943, -3.02634  ]], dtype=float32)

time = 48958	action = 1	current_phase = 1	next_phase = 0	reward = -0.710074	array([[-3.561076 , -2.1589007]], dtype=float32)

time = 48966	action = 0	current_phase = 0	next_phase = 1	reward = -0.094231	array([[-1.7719922, -2.80698  ]], dtype=float32)

time = 48971	action = 0	current_phase = 0	next_phase = 1	reward = -0.011651	array([[-2.072888 , -2.8538425]], dtype=float32)

time = 48976	action = 0	current_phase = 0	next_phase = 1	reward = 0.047754	array([[-2.5501842, -3.1384673]], dtype=float32)

time = 48981	action = 1	current_phase = 0	next_phase = 1	reward = -1.552105	array([[-6.915595, -3.250776]], dtype=float32)

time = 48989	action = 1	current_phase = 1	next_phase = 0	reward = -0.778938	array([[-3.6735942, -2.2860608]], dtype=float32)

time = 48997	action = 0	current_phase = 0	next_phase = 1	reward = -0.057013	array([[-1.8204226, -2.7773588]], dtype=float32)

time = 49002	action = 0	current_phase = 0	next_phase = 1	reward = 0.011117	array([[-2.2005725, -2.892372 ]], dtype=float32)

time = 49007	action = 0	current_phase = 0	next_phase = 1	reward = 0.071782	array([[-2.6777453, -3.1993384]], dtype=float32)

time = 49012	action = 1	current_phase = 0	next_phase = 1	reward = -1.614482	array([[-6.856142, -3.596106]], dtype=float32)

time = 49020	action = 1	current_phase = 1	next_phase = 0	reward = -1.319300	array([[-3.8909013, -2.5114594]], dtype=float32)

time = 49028	action = 0	current_phase = 0	next_phase = 1	reward = 0.234333	array([[-1.7454298, -2.764965 ]], dtype=float32)

time = 49033	action = 0	current_phase = 0	next_phase = 1	reward = 0.017628	array([[-2.2440448, -2.9060192]], dtype=float32)

time = 49038	action = 0	current_phase = 0	next_phase = 1	reward = 0.073913	array([[-2.7273016, -3.2989376]], dtype=float32)

time = 49043	action = 1	current_phase = 0	next_phase = 1	reward = -1.791482	array([[-6.8642507, -3.684612 ]], dtype=float32)

time = 49051	action = 1	current_phase = 1	next_phase = 0	reward = -1.092871	array([[-4.1210675, -2.3280325]], dtype=float32)

time = 49059	action = 0	current_phase = 0	next_phase = 1	reward = -0.048806	array([[-1.5311172, -2.0147314]], dtype=float32)

time = 49064	action = 0	current_phase = 0	next_phase = 1	reward = 0.025571	array([[-1.7267888, -2.7683594]], dtype=float32)

time = 49069	action = 1	current_phase = 0	next_phase = 1	reward = -0.696339	array([[-2.5080762, -2.205245 ]], dtype=float32)

time = 49077	action = 1	current_phase = 1	next_phase = 0	reward = -0.595525	array([[-3.2871141, -1.9597112]], dtype=float32)

time = 49085	action = 0	current_phase = 0	next_phase = 1	reward = -0.117253	array([[-1.4937779, -2.9817185]], dtype=float32)

time = 49090	action = 0	current_phase = 0	next_phase = 1	reward = -0.319896	array([[-1.9920139, -2.0308619]], dtype=float32)

time = 49095	action = 0	current_phase = 0	next_phase = 1	reward = 0.051889	array([[-2.2642088, -2.965916 ]], dtype=float32)

time = 49100	action = 1	current_phase = 0	next_phase = 1	reward = -1.136383	array([[-5.7885046, -2.741397 ]], dtype=float32)

time = 49108	action = 1	current_phase = 1	next_phase = 0	reward = -0.720687	array([[-3.558884 , -2.1300263]], dtype=float32)

time = 49116	action = 0	current_phase = 0	next_phase = 1	reward = -0.090510	array([[-1.7562503, -2.8092237]], dtype=float32)

time = 49121	action = 0	current_phase = 0	next_phase = 1	reward = 0.001018	array([[-2.0574589, -2.8263915]], dtype=float32)

time = 49126	action = 0	current_phase = 0	next_phase = 1	reward = 0.055488	array([[-2.5188618, -3.115291 ]], dtype=float32)

time = 49131	action = 1	current_phase = 0	next_phase = 1	reward = -1.482000	array([[-6.9389305, -3.2007215]], dtype=float32)

time = 49139	action = 1	current_phase = 1	next_phase = 0	reward = -0.836974	array([[-3.66802  , -2.2792616]], dtype=float32)

time = 49147	action = 0	current_phase = 0	next_phase = 1	reward = -0.068870	array([[-1.8296797, -2.7825775]], dtype=float32)

time = 49152	action = 0	current_phase = 0	next_phase = 1	reward = 0.004973	array([[-2.202939 , -2.9015214]], dtype=float32)

time = 49157	action = 0	current_phase = 0	next_phase = 1	reward = 0.067770	array([[-2.7122238, -3.2213287]], dtype=float32)

time = 49162	action = 1	current_phase = 0	next_phase = 1	reward = -1.711643	array([[-6.8609357, -3.603882 ]], dtype=float32)

time = 49170	action = 1	current_phase = 1	next_phase = 0	reward = -1.058575	array([[-3.918577 , -2.5229425]], dtype=float32)

time = 49178	action = 0	current_phase = 0	next_phase = 1	reward = -0.039153	array([[-1.7624443, -2.757507 ]], dtype=float32)

time = 49183	action = 0	current_phase = 0	next_phase = 1	reward = 0.020131	array([[-2.2066894, -2.8876967]], dtype=float32)

time = 49188	action = 0	current_phase = 0	next_phase = 1	reward = 0.071279	array([[-2.7265978, -3.265759 ]], dtype=float32)

time = 49193	action = 1	current_phase = 0	next_phase = 1	reward = -1.839487	array([[-6.8706727, -3.672764 ]], dtype=float32)

time = 49201	action = 1	current_phase = 1	next_phase = 0	reward = -1.386732	array([[-4.247794 , -2.3890226]], dtype=float32)

time = 49209	action = 0	current_phase = 0	next_phase = 1	reward = 0.264020	array([[-1.3883616, -2.0312772]], dtype=float32)

time = 49214	action = 0	current_phase = 0	next_phase = 1	reward = 0.019941	array([[-1.8200959, -2.8054903]], dtype=float32)

time = 49219	action = 1	current_phase = 0	next_phase = 1	reward = -0.719138	array([[-2.4162464, -2.2372906]], dtype=float32)

time = 49227	action = 1	current_phase = 1	next_phase = 0	reward = -0.924115	array([[-3.4069662, -2.0611992]], dtype=float32)

time = 49235	action = 0	current_phase = 0	next_phase = 1	reward = -0.084522	array([[-1.279665 , -3.0260115]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0281 - val_loss: 0.0115

Epoch 2/50

 - 3s - loss: 0.0159 - val_loss: 0.0106

Epoch 3/50

 - 3s - loss: 0.0189 - val_loss: 0.0100

Epoch 4/50

 - 3s - loss: 0.0174 - val_loss: 0.0123

Epoch 5/50

 - 3s - loss: 0.0184 - val_loss: 0.0088

Epoch 6/50

 - 3s - loss: 0.0283 - val_loss: 0.0134

Epoch 7/50

 - 3s - loss: 0.0160 - val_loss: 0.0120

Epoch 8/50

 - 3s - loss: 0.0171 - val_loss: 0.0126

Epoch 9/50

 - 3s - loss: 0.0193 - val_loss: 0.0105

Epoch 10/50

 - 3s - loss: 0.0143 - val_loss: 0.0103

Epoch 11/50

 - 3s - loss: 0.0175 - val_loss: 0.0101

Epoch 12/50

 - 3s - loss: 0.0185 - val_loss: 0.0109

Epoch 13/50

 - 3s - loss: 0.0152 - val_loss: 0.0109

Epoch 14/50

 - 3s - loss: 0.0203 - val_loss: 0.0100

Epoch 15/50

 - 3s - loss: 0.0158 - val_loss: 0.0112

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 49240	action = 0	current_phase = 0	next_phase = 1	reward = 0.270093	array([[-1.9714222, -2.0453646]], dtype=float32)

time = 49245	action = 0	current_phase = 0	next_phase = 1	reward = 0.047117	array([[-2.4354124, -3.0748372]], dtype=float32)

time = 49250	action = 1	current_phase = 0	next_phase = 1	reward = -1.302593	array([[-5.852401 , -2.9088178]], dtype=float32)

time = 49258	action = 1	current_phase = 1	next_phase = 0	reward = -0.708990	array([[-3.5350442, -2.1330202]], dtype=float32)

time = 49266	action = 0	current_phase = 0	next_phase = 1	reward = -0.076396	array([[-1.7960047, -2.764206 ]], dtype=float32)

time = 49271	action = 0	current_phase = 0	next_phase = 1	reward = -0.002223	array([[-2.1455336, -2.8619285]], dtype=float32)

time = 49276	action = 0	current_phase = 0	next_phase = 1	reward = 0.066097	array([[-2.5883074, -3.1875396]], dtype=float32)

time = 49281	action = 1	current_phase = 0	next_phase = 1	reward = -1.458330	array([[-6.957798 , -3.3785367]], dtype=float32)

time = 49289	action = 1	current_phase = 1	next_phase = 0	reward = -0.790728	array([[-3.648172 , -2.2541208]], dtype=float32)

time = 49297	action = 0	current_phase = 0	next_phase = 1	reward = -0.080323	array([[-1.86052  , -2.7692409]], dtype=float32)

time = 49302	action = 0	current_phase = 0	next_phase = 1	reward = 0.019825	array([[-2.2115378, -2.8861053]], dtype=float32)

time = 49307	action = 0	current_phase = 0	next_phase = 1	reward = 0.070636	array([[-2.732234 , -3.2440724]], dtype=float32)

time = 49312	action = 1	current_phase = 0	next_phase = 1	reward = -1.600795	array([[-6.9014153, -3.5523157]], dtype=float32)

time = 49320	action = 1	current_phase = 1	next_phase = 0	reward = -0.998650	array([[-3.8193574, -2.4140081]], dtype=float32)

time = 49328	action = 0	current_phase = 0	next_phase = 1	reward = -0.029028	array([[-1.9249505, -2.8018975]], dtype=float32)

time = 49333	action = 0	current_phase = 0	next_phase = 1	reward = 0.045114	array([[-2.28989  , -2.9275994]], dtype=float32)

time = 49338	action = 0	current_phase = 0	next_phase = 1	reward = 0.065138	array([[-2.75063  , -3.3152575]], dtype=float32)

time = 49343	action = 1	current_phase = 0	next_phase = 1	reward = -1.921418	array([[-6.869751 , -3.7962656]], dtype=float32)

time = 49351	action = 1	current_phase = 1	next_phase = 0	reward = -1.077154	array([[-4.098955, -2.342602]], dtype=float32)

time = 49359	action = 0	current_phase = 0	next_phase = 1	reward = -0.039006	array([[-1.5198696, -2.0231104]], dtype=float32)

time = 49364	action = 0	current_phase = 0	next_phase = 1	reward = 0.018217	array([[-1.8220618, -2.80092  ]], dtype=float32)

time = 49369	action = 1	current_phase = 0	next_phase = 1	reward = -0.690619	array([[-2.5060177, -2.2002711]], dtype=float32)

time = 49377	action = 1	current_phase = 1	next_phase = 0	reward = -0.875143	array([[-3.3635912, -1.9375166]], dtype=float32)

time = 49385	action = 0	current_phase = 0	next_phase = 1	reward = -0.102951	array([[-1.4471023, -2.900694 ]], dtype=float32)

time = 49390	action = 0	current_phase = 0	next_phase = 1	reward = 0.267389	array([[-1.9527568, -2.006365 ]], dtype=float32)

time = 49395	action = 0	current_phase = 0	next_phase = 1	reward = 0.050097	array([[-2.3246546, -2.9877887]], dtype=float32)

time = 49400	action = 1	current_phase = 0	next_phase = 1	reward = -1.376360	array([[-5.8545995, -2.9829063]], dtype=float32)

time = 49408	action = 1	current_phase = 1	next_phase = 0	reward = -0.704588	array([[-3.49785  , -2.1037037]], dtype=float32)

time = 49416	action = 0	current_phase = 0	next_phase = 1	reward = -0.079246	array([[-1.8452307, -2.7893357]], dtype=float32)

time = 49421	action = 0	current_phase = 0	next_phase = 1	reward = -0.014520	array([[-2.14599  , -2.8600106]], dtype=float32)

time = 49426	action = 0	current_phase = 0	next_phase = 1	reward = 0.047453	array([[-2.590177 , -3.2041101]], dtype=float32)

time = 49431	action = 1	current_phase = 0	next_phase = 1	reward = -1.418753	array([[-6.9528255, -3.3420825]], dtype=float32)

time = 49439	action = 1	current_phase = 1	next_phase = 0	reward = -1.131253	array([[-3.6630507, -2.2742825]], dtype=float32)

time = 49447	action = 0	current_phase = 0	next_phase = 1	reward = 0.221461	array([[-1.788441 , -2.7626266]], dtype=float32)

time = 49452	action = 0	current_phase = 0	next_phase = 1	reward = -0.001748	array([[-2.2208762, -2.8988194]], dtype=float32)

time = 49457	action = 0	current_phase = 0	next_phase = 1	reward = 0.057272	array([[-2.7386522, -3.2324686]], dtype=float32)

time = 49462	action = 1	current_phase = 0	next_phase = 1	reward = -1.624859	array([[-6.920982 , -3.5662746]], dtype=float32)

time = 49470	action = 1	current_phase = 1	next_phase = 0	reward = -0.897289	array([[-3.7857285, -2.3784697]], dtype=float32)

time = 49478	action = 0	current_phase = 0	next_phase = 1	reward = -0.047266	array([[-1.924991 , -2.8076768]], dtype=float32)

time = 49483	action = 0	current_phase = 0	next_phase = 1	reward = 0.028938	array([[-2.2231898, -2.8833313]], dtype=float32)

time = 49488	action = 0	current_phase = 0	next_phase = 1	reward = 0.072169	array([[-2.7501736, -3.2949982]], dtype=float32)

time = 49493	action = 1	current_phase = 0	next_phase = 1	reward = -1.875782	array([[-6.900172 , -3.7017646]], dtype=float32)

time = 49501	action = 1	current_phase = 1	next_phase = 0	reward = -0.879245	array([[-4.131207 , -2.2591302]], dtype=float32)

time = 49509	action = 0	current_phase = 0	next_phase = 1	reward = -0.037489	array([[-1.5135875, -2.00754  ]], dtype=float32)

time = 49514	action = 0	current_phase = 0	next_phase = 1	reward = 0.034835	array([[-1.769096 , -2.7790961]], dtype=float32)

time = 49519	action = 1	current_phase = 0	next_phase = 1	reward = -0.702823	array([[-2.4760387, -2.2536511]], dtype=float32)

time = 49527	action = 1	current_phase = 1	next_phase = 0	reward = -0.631957	array([[-3.3441958, -1.9466856]], dtype=float32)

time = 49535	action = 0	current_phase = 0	next_phase = 1	reward = -0.087246	array([[-1.5801239, -2.967185 ]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0192 - val_loss: 0.0098

Epoch 2/50

 - 3s - loss: 0.0165 - val_loss: 0.0137

Epoch 3/50

 - 3s - loss: 0.0172 - val_loss: 0.0091

Epoch 4/50

 - 3s - loss: 0.0137 - val_loss: 0.0104

Epoch 5/50

 - 3s - loss: 0.0142 - val_loss: 0.0074

Epoch 6/50

 - 3s - loss: 0.0142 - val_loss: 0.0139

Epoch 7/50

 - 3s - loss: 0.0178 - val_loss: 0.0088

Epoch 8/50

 - 3s - loss: 0.0180 - val_loss: 0.0109

Epoch 9/50

 - 3s - loss: 0.0152 - val_loss: 0.0109

Epoch 10/50

 - 3s - loss: 0.0142 - val_loss: 0.0117

Epoch 11/50

 - 3s - loss: 0.0142 - val_loss: 0.0125

Epoch 12/50

 - 3s - loss: 0.0140 - val_loss: 0.0109

Epoch 13/50

 - 3s - loss: 0.0154 - val_loss: 0.0109

Epoch 14/50

 - 3s - loss: 0.0122 - val_loss: 0.0129

Epoch 15/50

 - 3s - loss: 0.0163 - val_loss: 0.0121

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 49540	action = 1	current_phase = 0	next_phase = 1	reward = -2.168089	array([[-2.071006 , -2.0128155]], dtype=float32)

time = 49548	action = 1	current_phase = 1	next_phase = 0	reward = -0.605651	array([[-2.3896236, -1.3517436]], dtype=float32)

time = 49556	action = 1	current_phase = 0	next_phase = 1	reward = -2.255685	array([[-5.5695148, -3.2339435]], dtype=float32)

time = 49564	action = 1	current_phase = 1	next_phase = 0	reward = -0.708818	array([[-4.0558643, -2.483168 ]], dtype=float32)

time = 49572	action = 0	current_phase = 0	next_phase = 1	reward = 0.024930	array([[-1.8385901, -2.8434105]], dtype=float32)

time = 49577	action = 0	current_phase = 0	next_phase = 1	reward = 0.088158	array([[-2.4684677, -3.2405639]], dtype=float32)

time = 49582	action = 1	current_phase = 0	next_phase = 1	reward = -1.681109	array([[-6.3539286, -3.2746673]], dtype=float32)

time = 49590	action = 1	current_phase = 1	next_phase = 0	reward = -0.964285	array([[-3.6105285, -2.1986318]], dtype=float32)

time = 49598	action = 0	current_phase = 0	next_phase = 1	reward = -0.064754	array([[-1.8075745, -2.8142023]], dtype=float32)

time = 49603	action = 0	current_phase = 0	next_phase = 1	reward = 0.031770	array([[-2.1825843, -2.9072294]], dtype=float32)

time = 49608	action = 0	current_phase = 0	next_phase = 1	reward = 0.078126	array([[-2.755475 , -3.3423624]], dtype=float32)

time = 49613	action = 1	current_phase = 0	next_phase = 1	reward = -1.809565	array([[-6.8881283, -3.7293816]], dtype=float32)

time = 49621	action = 1	current_phase = 1	next_phase = 0	reward = -1.313922	array([[-4.039688 , -2.2957616]], dtype=float32)

time = 49629	action = 0	current_phase = 0	next_phase = 1	reward = 0.255894	array([[-1.4418635, -2.0139165]], dtype=float32)

time = 49634	action = 0	current_phase = 0	next_phase = 1	reward = 0.043674	array([[-1.7397294, -2.7965312]], dtype=float32)

time = 49639	action = 1	current_phase = 0	next_phase = 1	reward = -0.704585	array([[-2.4372962, -2.2378478]], dtype=float32)

time = 49647	action = 1	current_phase = 1	next_phase = 0	reward = -0.926379	array([[-3.3599532, -1.9249933]], dtype=float32)

time = 49655	action = 0	current_phase = 0	next_phase = 1	reward = 0.175765	array([[-1.4994068, -2.9248352]], dtype=float32)

time = 49660	action = 0	current_phase = 0	next_phase = 1	reward = -0.049780	array([[-2.0088174, -2.0151672]], dtype=float32)

time = 49665	action = 0	current_phase = 0	next_phase = 1	reward = -0.259906	array([[-2.3328497, -3.0454073]], dtype=float32)

time = 49670	action = 1	current_phase = 0	next_phase = 1	reward = -1.025889	array([[-5.8484163, -2.773584 ]], dtype=float32)

time = 49678	action = 1	current_phase = 1	next_phase = 0	reward = -0.699707	array([[-3.430967 , -2.0153391]], dtype=float32)

time = 49686	action = 0	current_phase = 0	next_phase = 1	reward = -0.085919	array([[-1.7817049, -2.812914 ]], dtype=float32)

time = 49691	action = 0	current_phase = 0	next_phase = 1	reward = -0.012828	array([[-2.078984 , -2.8607883]], dtype=float32)

time = 49696	action = 0	current_phase = 0	next_phase = 1	reward = 0.044357	array([[-2.4919538, -3.1581182]], dtype=float32)

time = 49701	action = 1	current_phase = 0	next_phase = 1	reward = -1.330291	array([[-6.977084 , -3.3294082]], dtype=float32)

time = 49709	action = 1	current_phase = 1	next_phase = 0	reward = -0.750937	array([[-3.6241817, -2.2336025]], dtype=float32)

time = 49717	action = 0	current_phase = 0	next_phase = 1	reward = -0.054906	array([[-1.7774119, -2.7828279]], dtype=float32)

time = 49722	action = 0	current_phase = 0	next_phase = 1	reward = 0.001511	array([[-2.151858 , -2.9050612]], dtype=float32)

time = 49727	action = 0	current_phase = 0	next_phase = 1	reward = 0.073213	array([[-2.6808038, -3.2240028]], dtype=float32)

time = 49732	action = 1	current_phase = 0	next_phase = 1	reward = -1.554731	array([[-6.9027157, -3.621076 ]], dtype=float32)

time = 49740	action = 1	current_phase = 1	next_phase = 0	reward = -0.987493	array([[-3.8239908, -2.3731327]], dtype=float32)

time = 49748	action = 0	current_phase = 0	next_phase = 1	reward = -0.047672	array([[-1.7556778, -2.7947736]], dtype=float32)

time = 49753	action = 0	current_phase = 0	next_phase = 1	reward = 0.013302	array([[-2.1979907, -2.9133792]], dtype=float32)

time = 49758	action = 0	current_phase = 0	next_phase = 1	reward = 0.067391	array([[-2.7685807, -3.3707395]], dtype=float32)

time = 49763	action = 1	current_phase = 0	next_phase = 1	reward = -1.401544	array([[-6.9554095, -3.4535294]], dtype=float32)

time = 49771	action = 1	current_phase = 1	next_phase = 0	reward = -1.439395	array([[-3.8091125, -2.3779678]], dtype=float32)

time = 49779	action = 0	current_phase = 0	next_phase = 1	reward = 0.255928	array([[-1.388143 , -2.0140855]], dtype=float32)

time = 49784	action = 0	current_phase = 0	next_phase = 1	reward = 0.041017	array([[-1.8241267, -2.8168507]], dtype=float32)

time = 49789	action = 1	current_phase = 0	next_phase = 1	reward = -0.663674	array([[-2.406776 , -2.2909493]], dtype=float32)

time = 49797	action = 1	current_phase = 1	next_phase = 0	reward = -0.873693	array([[-3.3809981, -2.0805993]], dtype=float32)

time = 49805	action = 0	current_phase = 0	next_phase = 1	reward = 0.177667	array([[-1.4366953, -2.9793978]], dtype=float32)

time = 49810	action = 0	current_phase = 0	next_phase = 1	reward = -0.297176	array([[-2.0049834, -2.038907 ]], dtype=float32)

time = 49815	action = 0	current_phase = 0	next_phase = 1	reward = 0.332221	array([[-2.244966 , -2.9817748]], dtype=float32)

time = 49820	action = 1	current_phase = 0	next_phase = 1	reward = -1.314710	array([[-5.8732595, -2.946422 ]], dtype=float32)

time = 49828	action = 1	current_phase = 1	next_phase = 0	reward = -0.696322	array([[-3.4571536, -2.0779564]], dtype=float32)

time = 49836	action = 0	current_phase = 0	next_phase = 1	reward = -0.088064	array([[-1.7989733, -2.8234525]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0225 - val_loss: 0.0083

Epoch 2/50

 - 3s - loss: 0.0290 - val_loss: 0.0083

Epoch 3/50

 - 3s - loss: 0.0196 - val_loss: 0.0087

Epoch 4/50

 - 3s - loss: 0.0202 - val_loss: 0.0088

Epoch 5/50

 - 3s - loss: 0.0262 - val_loss: 0.0097

Epoch 6/50

 - 3s - loss: 0.0218 - val_loss: 0.0101

Epoch 7/50

 - 3s - loss: 0.0182 - val_loss: 0.0106

Epoch 8/50

 - 3s - loss: 0.0228 - val_loss: 0.0093

Epoch 9/50

 - 3s - loss: 0.0179 - val_loss: 0.0105

Epoch 10/50

 - 3s - loss: 0.0206 - val_loss: 0.0093

Epoch 11/50

 - 3s - loss: 0.0169 - val_loss: 0.0105

Epoch 12/50

 - 3s - loss: 0.0197 - val_loss: 0.0097

length of memory (state 0, action 0): 1025, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1011, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1011, before forget

length of memory (state 1, action 1): 1000, after forget

time = 49841	action = 0	current_phase = 0	next_phase = 1	reward = -0.008253	array([[-2.0726125, -2.9247415]], dtype=float32)

time = 49846	action = 0	current_phase = 0	next_phase = 1	reward = 0.062833	array([[-2.6395354, -3.2875206]], dtype=float32)

time = 49851	action = 1	current_phase = 0	next_phase = 1	reward = -1.424894	array([[-6.950144 , -3.3096004]], dtype=float32)

time = 49859	action = 1	current_phase = 1	next_phase = 0	reward = -0.895365	array([[-3.5928707, -2.2732668]], dtype=float32)

time = 49867	action = 0	current_phase = 0	next_phase = 1	reward = -0.086295	array([[-1.719255 , -2.8432512]], dtype=float32)

time = 49872	action = 0	current_phase = 0	next_phase = 1	reward = -0.006210	array([[-2.160885 , -2.9562488]], dtype=float32)

time = 49877	action = 0	current_phase = 0	next_phase = 1	reward = 0.062099	array([[-2.8110976, -3.3278885]], dtype=float32)

time = 49882	action = 1	current_phase = 0	next_phase = 1	reward = -1.532706	array([[-6.9259496, -3.5505016]], dtype=float32)

time = 49890	action = 1	current_phase = 1	next_phase = 0	reward = -1.002073	array([[-3.7605944, -2.4440713]], dtype=float32)

time = 49898	action = 0	current_phase = 0	next_phase = 1	reward = -0.052625	array([[-1.7501111, -2.8460479]], dtype=float32)

time = 49903	action = 0	current_phase = 0	next_phase = 1	reward = 0.008881	array([[-2.1744535, -2.9568496]], dtype=float32)

time = 49908	action = 0	current_phase = 0	next_phase = 1	reward = 0.083292	array([[-2.8098168, -3.3668947]], dtype=float32)

time = 49913	action = 1	current_phase = 0	next_phase = 1	reward = -1.911845	array([[-6.9093437, -3.6932912]], dtype=float32)

time = 49921	action = 1	current_phase = 1	next_phase = 0	reward = -1.134885	array([[-4.106702, -2.391382]], dtype=float32)

time = 49929	action = 0	current_phase = 0	next_phase = 1	reward = -0.026292	array([[-1.4242458, -2.0783012]], dtype=float32)

time = 49934	action = 0	current_phase = 0	next_phase = 1	reward = 0.044778	array([[-1.7623006, -2.8274302]], dtype=float32)

time = 49939	action = 1	current_phase = 0	next_phase = 1	reward = -0.652557	array([[-2.5540535, -2.2671204]], dtype=float32)

time = 49947	action = 1	current_phase = 1	next_phase = 0	reward = -0.925752	array([[-3.2768002, -2.015277 ]], dtype=float32)

time = 49955	action = 0	current_phase = 0	next_phase = 1	reward = -0.104018	array([[-1.4133984, -3.14436  ]], dtype=float32)

time = 49960	action = 0	current_phase = 0	next_phase = 1	reward = -0.027958	array([[-1.87535  , -2.0811784]], dtype=float32)

time = 49965	action = 0	current_phase = 0	next_phase = 1	reward = 0.325609	array([[-2.2495835, -3.031991 ]], dtype=float32)

time = 49970	action = 1	current_phase = 0	next_phase = 1	reward = -1.309735	array([[-5.87568  , -2.9884098]], dtype=float32)

time = 49978	action = 1	current_phase = 1	next_phase = 0	reward = -0.715521	array([[-3.4977415, -2.1344569]], dtype=float32)

time = 49986	action = 0	current_phase = 0	next_phase = 1	reward = -0.085751	array([[-1.7069528, -2.8446548]], dtype=float32)

time = 49991	action = 0	current_phase = 0	next_phase = 1	reward = -0.002740	array([[-2.1024666, -2.9373808]], dtype=float32)

time = 49996	action = 0	current_phase = 0	next_phase = 1	reward = 0.073869	array([[-2.6675448, -3.2784758]], dtype=float32)

time = 50001	action = 1	current_phase = 0	next_phase = 1	reward = -1.441548	array([[-6.9375205, -3.3374164]], dtype=float32)

time = 50009	action = 1	current_phase = 1	next_phase = 0	reward = -0.895307	array([[-3.6097553, -2.2848563]], dtype=float32)

time = 50017	action = 0	current_phase = 0	next_phase = 1	reward = -0.073629	array([[-1.789295, -2.844275]], dtype=float32)

time = 50022	action = 0	current_phase = 0	next_phase = 1	reward = 0.007697	array([[-2.1747048, -2.9618351]], dtype=float32)

time = 50027	action = 0	current_phase = 0	next_phase = 1	reward = 0.075765	array([[-2.7450488, -3.2863264]], dtype=float32)

time = 50032	action = 1	current_phase = 0	next_phase = 1	reward = -1.551575	array([[-6.9032826, -3.6436262]], dtype=float32)

time = 50040	action = 1	current_phase = 1	next_phase = 0	reward = -0.997634	array([[-3.7977757, -2.4632807]], dtype=float32)

time = 50048	action = 0	current_phase = 0	next_phase = 1	reward = -0.044288	array([[-1.6818583, -2.843154 ]], dtype=float32)

time = 50053	action = 0	current_phase = 0	next_phase = 1	reward = 0.025055	array([[-2.1822374, -2.9535513]], dtype=float32)

time = 50058	action = 0	current_phase = 0	next_phase = 1	reward = 0.081699	array([[-2.8069215, -3.3456779]], dtype=float32)

time = 50063	action = 1	current_phase = 0	next_phase = 1	reward = -1.877670	array([[-6.903697, -3.669437]], dtype=float32)

time = 50071	action = 1	current_phase = 1	next_phase = 0	reward = -1.330349	array([[-3.9908698, -2.3159413]], dtype=float32)

time = 50079	action = 0	current_phase = 0	next_phase = 1	reward = 0.259590	array([[-1.3197325, -2.0693603]], dtype=float32)

time = 50084	action = 0	current_phase = 0	next_phase = 1	reward = 0.017516	array([[-1.7768424, -2.8327317]], dtype=float32)

time = 50089	action = 1	current_phase = 0	next_phase = 1	reward = -0.666319	array([[-2.487916 , -2.2466996]], dtype=float32)

time = 50097	action = 1	current_phase = 1	next_phase = 0	reward = -0.650339	array([[-3.2230783, -1.9874313]], dtype=float32)

time = 50105	action = 0	current_phase = 0	next_phase = 1	reward = -0.109083	array([[-1.5515308, -3.057284 ]], dtype=float32)

time = 50110	action = 0	current_phase = 0	next_phase = 1	reward = -0.040593	array([[-1.9303601, -2.1613667]], dtype=float32)

time = 50115	action = 0	current_phase = 0	next_phase = 1	reward = 0.022776	array([[-2.3458936, -3.1076925]], dtype=float32)

time = 50120	action = 1	current_phase = 0	next_phase = 1	reward = -1.333573	array([[-5.8846717, -2.8500361]], dtype=float32)

time = 50128	action = 1	current_phase = 1	next_phase = 0	reward = -0.686408	array([[-3.4510589, -2.101358 ]], dtype=float32)

time = 50136	action = 0	current_phase = 0	next_phase = 1	reward = -0.077382	array([[-1.7421691, -2.861728 ]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0184 - val_loss: 0.0096

Epoch 2/50

 - 3s - loss: 0.0154 - val_loss: 0.0076

Epoch 3/50

 - 3s - loss: 0.0197 - val_loss: 0.0073

Epoch 4/50

 - 3s - loss: 0.0188 - val_loss: 0.0080

Epoch 5/50

 - 3s - loss: 0.0169 - val_loss: 0.0092

Epoch 6/50

 - 3s - loss: 0.0179 - val_loss: 0.0106

Epoch 7/50

 - 3s - loss: 0.0162 - val_loss: 0.0104

Epoch 8/50

 - 3s - loss: 0.0155 - val_loss: 0.0100

Epoch 9/50

 - 3s - loss: 0.0147 - val_loss: 0.0085

Epoch 10/50

 - 3s - loss: 0.0163 - val_loss: 0.0096

Epoch 11/50

 - 3s - loss: 0.0175 - val_loss: 0.0111

Epoch 12/50

 - 3s - loss: 0.0139 - val_loss: 0.0091

Epoch 13/50

 - 3s - loss: 0.0129 - val_loss: 0.0113

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 50141	action = 0	current_phase = 0	next_phase = 1	reward = 0.010364	array([[-2.0972676, -2.9407997]], dtype=float32)

time = 50146	action = 0	current_phase = 0	next_phase = 1	reward = 0.070677	array([[-2.5782466, -3.2368343]], dtype=float32)

time = 50151	action = 1	current_phase = 0	next_phase = 1	reward = -1.445484	array([[-6.940477 , -3.1962717]], dtype=float32)

time = 50159	action = 1	current_phase = 1	next_phase = 0	reward = -0.847487	array([[-3.6480765, -2.3667755]], dtype=float32)

time = 50167	action = 0	current_phase = 0	next_phase = 1	reward = -0.072603	array([[-1.7578682, -2.8402114]], dtype=float32)

time = 50172	action = 0	current_phase = 0	next_phase = 1	reward = -0.017374	array([[-2.1660433, -2.9655945]], dtype=float32)

time = 50177	action = 0	current_phase = 0	next_phase = 1	reward = 0.061693	array([[-2.7957537, -3.3375204]], dtype=float32)

time = 50182	action = 1	current_phase = 0	next_phase = 1	reward = -1.644174	array([[-6.9291544, -3.4512582]], dtype=float32)

time = 50190	action = 1	current_phase = 1	next_phase = 0	reward = -1.542317	array([[-3.900603 , -2.5575838]], dtype=float32)

time = 50198	action = 0	current_phase = 0	next_phase = 1	reward = 0.533902	array([[-1.6110699, -2.833798 ]], dtype=float32)

time = 50203	action = 0	current_phase = 0	next_phase = 1	reward = 0.019321	array([[-2.1844873, -2.9609063]], dtype=float32)

time = 50208	action = 0	current_phase = 0	next_phase = 1	reward = 0.073702	array([[-2.7823098, -3.3665085]], dtype=float32)

time = 50213	action = 1	current_phase = 0	next_phase = 1	reward = -1.315972	array([[-6.9455733, -3.30017  ]], dtype=float32)

time = 50221	action = 1	current_phase = 1	next_phase = 0	reward = -1.432678	array([[-3.830215 , -2.4922628]], dtype=float32)

time = 50229	action = 0	current_phase = 0	next_phase = 1	reward = 0.261578	array([[-1.2445171, -2.0759156]], dtype=float32)

time = 50234	action = 0	current_phase = 0	next_phase = 1	reward = 0.034627	array([[-1.759934 , -2.8523474]], dtype=float32)

time = 50239	action = 1	current_phase = 0	next_phase = 1	reward = -0.666526	array([[-2.3579867, -2.2514842]], dtype=float32)

time = 50247	action = 1	current_phase = 1	next_phase = 0	reward = -0.875132	array([[-3.3806748, -2.1869416]], dtype=float32)

time = 50255	action = 0	current_phase = 0	next_phase = 1	reward = -0.366698	array([[-1.4847765, -3.0603538]], dtype=float32)

time = 50260	action = 0	current_phase = 0	next_phase = 1	reward = 0.547975	array([[-1.7205889, -2.1126661]], dtype=float32)

time = 50265	action = 0	current_phase = 0	next_phase = 1	reward = 0.043845	array([[-2.2474022, -3.0451138]], dtype=float32)

time = 50270	action = 1	current_phase = 0	next_phase = 1	reward = -1.257143	array([[-5.8600397, -2.8847897]], dtype=float32)

time = 50278	action = 1	current_phase = 1	next_phase = 0	reward = -0.710189	array([[-3.440348 , -2.1785913]], dtype=float32)

time = 50286	action = 0	current_phase = 0	next_phase = 1	reward = -0.071258	array([[-1.7880161, -2.8301556]], dtype=float32)

time = 50291	action = 0	current_phase = 0	next_phase = 1	reward = 0.000375	array([[-2.0719404, -2.932938 ]], dtype=float32)

time = 50296	action = 0	current_phase = 0	next_phase = 1	reward = 0.057511	array([[-2.6252897, -3.2485838]], dtype=float32)

time = 50301	action = 1	current_phase = 0	next_phase = 1	reward = -1.379943	array([[-6.9534273, -3.3089077]], dtype=float32)

time = 50309	action = 1	current_phase = 1	next_phase = 0	reward = -0.769362	array([[-3.6372771, -2.3613663]], dtype=float32)

time = 50317	action = 0	current_phase = 0	next_phase = 1	reward = -0.081152	array([[-1.7581599, -2.840329 ]], dtype=float32)

time = 50322	action = 0	current_phase = 0	next_phase = 1	reward = -0.006056	array([[-2.1600807, -2.9543035]], dtype=float32)

time = 50327	action = 0	current_phase = 0	next_phase = 1	reward = 0.062406	array([[-2.809822 , -3.3256853]], dtype=float32)

time = 50332	action = 1	current_phase = 0	next_phase = 1	reward = -1.651850	array([[-6.9172616, -3.481824 ]], dtype=float32)

time = 50340	action = 1	current_phase = 1	next_phase = 0	reward = -1.191720	array([[-3.849036 , -2.5256486]], dtype=float32)

time = 50348	action = 0	current_phase = 0	next_phase = 1	reward = 0.242632	array([[-1.6709867, -2.834266 ]], dtype=float32)

time = 50353	action = 0	current_phase = 0	next_phase = 1	reward = 0.017606	array([[-2.2077463, -2.9712155]], dtype=float32)

time = 50358	action = 0	current_phase = 0	next_phase = 1	reward = 0.074731	array([[-2.83552  , -3.3866937]], dtype=float32)

time = 50363	action = 1	current_phase = 0	next_phase = 1	reward = -1.822031	array([[-6.8728247, -3.6745791]], dtype=float32)

time = 50371	action = 1	current_phase = 1	next_phase = 0	reward = -1.038599	array([[-4.049594, -2.438085]], dtype=float32)

time = 50379	action = 0	current_phase = 0	next_phase = 1	reward = -0.043998	array([[-1.3605764, -2.0328097]], dtype=float32)

time = 50384	action = 0	current_phase = 0	next_phase = 1	reward = 0.021661	array([[-1.7179618, -2.8377125]], dtype=float32)

time = 50389	action = 1	current_phase = 0	next_phase = 1	reward = -0.717543	array([[-2.511105, -2.232571]], dtype=float32)

time = 50397	action = 1	current_phase = 1	next_phase = 0	reward = -0.941387	array([[-3.2933962, -1.9644988]], dtype=float32)

time = 50405	action = 0	current_phase = 0	next_phase = 1	reward = -0.103767	array([[-1.4326298, -3.0616894]], dtype=float32)

time = 50410	action = 0	current_phase = 0	next_phase = 1	reward = -0.022182	array([[-1.7706642, -2.0812056]], dtype=float32)

time = 50415	action = 0	current_phase = 0	next_phase = 1	reward = 0.323136	array([[-2.1377249, -2.9919488]], dtype=float32)

time = 50420	action = 1	current_phase = 0	next_phase = 1	reward = -1.333867	array([[-5.869882 , -2.9184842]], dtype=float32)

time = 50428	action = 1	current_phase = 1	next_phase = 0	reward = -0.707461	array([[-3.4802713, -2.1733356]], dtype=float32)

time = 50436	action = 0	current_phase = 0	next_phase = 1	reward = -0.071801	array([[-1.7391841, -2.8626359]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0231 - val_loss: 0.0112

Epoch 2/50

 - 3s - loss: 0.0193 - val_loss: 0.0081

Epoch 3/50

 - 3s - loss: 0.0208 - val_loss: 0.0082

Epoch 4/50

 - 3s - loss: 0.0217 - val_loss: 0.0081

Epoch 5/50

 - 3s - loss: 0.0309 - val_loss: 0.0082

Epoch 6/50

 - 3s - loss: 0.0176 - val_loss: 0.0088

Epoch 7/50

 - 3s - loss: 0.0212 - val_loss: 0.0100

Epoch 8/50

 - 3s - loss: 0.0167 - val_loss: 0.0112

Epoch 9/50

 - 3s - loss: 0.0185 - val_loss: 0.0096

Epoch 10/50

 - 3s - loss: 0.0168 - val_loss: 0.0108

Epoch 11/50

 - 3s - loss: 0.0209 - val_loss: 0.0090

Epoch 12/50

 - 3s - loss: 0.0173 - val_loss: 0.0101

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 50441	action = 0	current_phase = 0	next_phase = 1	reward = -0.003947	array([[-2.0966384, -2.9601219]], dtype=float32)

time = 50446	action = 0	current_phase = 0	next_phase = 1	reward = 0.064959	array([[-2.6383858, -3.312077 ]], dtype=float32)

time = 50451	action = 1	current_phase = 0	next_phase = 1	reward = -1.447494	array([[-6.933546 , -3.2832174]], dtype=float32)

time = 50459	action = 1	current_phase = 1	next_phase = 0	reward = -0.779690	array([[-3.6215324, -2.2726994]], dtype=float32)

time = 50467	action = 0	current_phase = 0	next_phase = 1	reward = -0.080818	array([[-1.7628103, -2.8544679]], dtype=float32)

time = 50472	action = 0	current_phase = 0	next_phase = 1	reward = 0.000607	array([[-2.1225872, -2.9727771]], dtype=float32)

time = 50477	action = 0	current_phase = 0	next_phase = 1	reward = 0.071126	array([[-2.803824 , -3.4071665]], dtype=float32)

time = 50482	action = 1	current_phase = 0	next_phase = 1	reward = -1.540634	array([[-6.9137864, -3.4335754]], dtype=float32)

time = 50490	action = 1	current_phase = 1	next_phase = 0	reward = -0.905306	array([[-3.827383, -2.429803]], dtype=float32)

time = 50498	action = 0	current_phase = 0	next_phase = 1	reward = -0.057510	array([[-1.7420602, -2.863944 ]], dtype=float32)

time = 50503	action = 0	current_phase = 0	next_phase = 1	reward = 0.017818	array([[-2.1833587, -2.9725373]], dtype=float32)

time = 50508	action = 0	current_phase = 0	next_phase = 1	reward = 0.068688	array([[-2.8041596, -3.326564 ]], dtype=float32)

time = 50513	action = 1	current_phase = 0	next_phase = 1	reward = -1.334768	array([[-6.9187894, -3.3779202]], dtype=float32)

time = 50521	action = 1	current_phase = 1	next_phase = 0	reward = -1.369282	array([[-3.8950295, -2.3836725]], dtype=float32)

time = 50529	action = 0	current_phase = 0	next_phase = 1	reward = 0.252441	array([[-1.2975472, -2.1025984]], dtype=float32)

time = 50534	action = 0	current_phase = 0	next_phase = 1	reward = 0.031771	array([[-1.7494118, -2.870272 ]], dtype=float32)

time = 50539	action = 1	current_phase = 0	next_phase = 1	reward = -0.758612	array([[-2.409019 , -2.2580297]], dtype=float32)

time = 50547	action = 1	current_phase = 1	next_phase = 0	reward = -0.924659	array([[-3.3351593, -1.9413172]], dtype=float32)

time = 50555	action = 0	current_phase = 0	next_phase = 1	reward = 0.177194	array([[-1.4121848, -3.071006 ]], dtype=float32)

time = 50560	action = 0	current_phase = 0	next_phase = 1	reward = -0.037573	array([[-1.8626019, -2.108344 ]], dtype=float32)

time = 50565	action = 0	current_phase = 0	next_phase = 1	reward = 0.019717	array([[-2.4163022, -3.1626556]], dtype=float32)

time = 50570	action = 1	current_phase = 0	next_phase = 1	reward = -1.254294	array([[-5.8476677, -2.9563174]], dtype=float32)

time = 50578	action = 1	current_phase = 1	next_phase = 0	reward = -0.710934	array([[-3.4446025, -2.1473317]], dtype=float32)

time = 50586	action = 0	current_phase = 0	next_phase = 1	reward = -0.103930	array([[-1.7652378, -2.867203 ]], dtype=float32)

time = 50591	action = 0	current_phase = 0	next_phase = 1	reward = -0.008649	array([[-2.0595922, -2.9546049]], dtype=float32)

time = 50596	action = 0	current_phase = 0	next_phase = 1	reward = 0.072182	array([[-2.564959 , -3.2415476]], dtype=float32)

time = 50601	action = 1	current_phase = 0	next_phase = 1	reward = -1.501320	array([[-6.905672 , -3.2785976]], dtype=float32)

time = 50609	action = 1	current_phase = 1	next_phase = 0	reward = -0.786132	array([[-3.6488261, -2.2809324]], dtype=float32)

time = 50617	action = 0	current_phase = 0	next_phase = 1	reward = -0.045916	array([[-1.7869359, -2.8585079]], dtype=float32)

time = 50622	action = 0	current_phase = 0	next_phase = 1	reward = 0.032270	array([[-2.1701078, -2.973676 ]], dtype=float32)

time = 50627	action = 0	current_phase = 0	next_phase = 1	reward = 0.078214	array([[-2.8405168, -3.396841 ]], dtype=float32)

time = 50632	action = 1	current_phase = 0	next_phase = 1	reward = -1.633520	array([[-6.9019513, -3.5502145]], dtype=float32)

time = 50640	action = 1	current_phase = 1	next_phase = 0	reward = -1.050517	array([[-3.8885875, -2.480607 ]], dtype=float32)

time = 50648	action = 0	current_phase = 0	next_phase = 1	reward = -0.058514	array([[-1.7335671, -2.871845 ]], dtype=float32)

time = 50653	action = 0	current_phase = 0	next_phase = 1	reward = 0.013497	array([[-2.128232 , -2.9643939]], dtype=float32)

time = 50658	action = 0	current_phase = 0	next_phase = 1	reward = 0.079391	array([[-2.7050815, -3.1645598]], dtype=float32)

time = 50663	action = 1	current_phase = 0	next_phase = 1	reward = -1.406152	array([[-6.9639487, -3.285566 ]], dtype=float32)

time = 50671	action = 1	current_phase = 1	next_phase = 0	reward = -1.388094	array([[-3.8157935, -2.42529  ]], dtype=float32)

time = 50679	action = 0	current_phase = 0	next_phase = 1	reward = 0.255993	array([[-1.2492349, -2.0846376]], dtype=float32)

time = 50684	action = 0	current_phase = 0	next_phase = 1	reward = 0.026859	array([[-1.6420934, -2.838789 ]], dtype=float32)

time = 50689	action = 1	current_phase = 0	next_phase = 1	reward = -0.692643	array([[-2.431806 , -2.2623901]], dtype=float32)

time = 50697	action = 1	current_phase = 1	next_phase = 0	reward = -0.579742	array([[-3.4158735, -2.1116009]], dtype=float32)

time = 50705	action = 0	current_phase = 0	next_phase = 1	reward = -0.105402	array([[-1.5400796, -3.0520546]], dtype=float32)

time = 50710	action = 0	current_phase = 0	next_phase = 1	reward = -0.020941	array([[-1.925555 , -2.1044068]], dtype=float32)

time = 50715	action = 0	current_phase = 0	next_phase = 1	reward = 0.049161	array([[-2.3488188, -3.10863  ]], dtype=float32)

time = 50720	action = 1	current_phase = 0	next_phase = 1	reward = -1.264248	array([[-5.8397636, -2.9717486]], dtype=float32)

time = 50728	action = 1	current_phase = 1	next_phase = 0	reward = -0.693275	array([[-3.4592934, -2.1270914]], dtype=float32)

time = 50736	action = 0	current_phase = 0	next_phase = 1	reward = -0.084628	array([[-1.7430961, -2.8495228]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0161 - val_loss: 0.0088

Epoch 2/50

 - 3s - loss: 0.0181 - val_loss: 0.0085

Epoch 3/50

 - 3s - loss: 0.0145 - val_loss: 0.0111

Epoch 4/50

 - 3s - loss: 0.0173 - val_loss: 0.0096

Epoch 5/50

 - 3s - loss: 0.0136 - val_loss: 0.0143

Epoch 6/50

 - 3s - loss: 0.0150 - val_loss: 0.0181

Epoch 7/50

 - 3s - loss: 0.0187 - val_loss: 0.0078

Epoch 8/50

 - 3s - loss: 0.0154 - val_loss: 0.0119

Epoch 9/50

 - 3s - loss: 0.0155 - val_loss: 0.0078

Epoch 10/50

 - 3s - loss: 0.0176 - val_loss: 0.0075

Epoch 11/50

 - 3s - loss: 0.0126 - val_loss: 0.0126

Epoch 12/50

 - 3s - loss: 0.0129 - val_loss: 0.0095

Epoch 13/50

 - 3s - loss: 0.0147 - val_loss: 0.0122

Epoch 14/50

 - 3s - loss: 0.0163 - val_loss: 0.0087

Epoch 15/50

 - 3s - loss: 0.0130 - val_loss: 0.0121

Epoch 16/50

 - 3s - loss: 0.0143 - val_loss: 0.0078

Epoch 17/50

 - 3s - loss: 0.0155 - val_loss: 0.0089

Epoch 18/50

 - 3s - loss: 0.0161 - val_loss: 0.0085

Epoch 19/50

 - 3s - loss: 0.0211 - val_loss: 0.0101

Epoch 20/50

 - 3s - loss: 0.0117 - val_loss: 0.0153

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 50741	action = 0	current_phase = 0	next_phase = 1	reward = -0.002131	array([[-2.1214492, -2.9566178]], dtype=float32)

time = 50746	action = 0	current_phase = 0	next_phase = 1	reward = 0.074303	array([[-2.682663 , -3.3432295]], dtype=float32)

time = 50751	action = 1	current_phase = 0	next_phase = 1	reward = -1.491773	array([[-6.986152 , -3.2612205]], dtype=float32)

time = 50759	action = 1	current_phase = 1	next_phase = 0	reward = -0.837950	array([[-3.6018672, -2.2654943]], dtype=float32)

time = 50767	action = 0	current_phase = 0	next_phase = 1	reward = -0.094401	array([[-1.853405, -2.857388]], dtype=float32)

time = 50772	action = 0	current_phase = 0	next_phase = 1	reward = -0.007219	array([[-2.1651268, -2.9863706]], dtype=float32)

time = 50777	action = 0	current_phase = 0	next_phase = 1	reward = 0.057732	array([[-2.8074882, -3.3668833]], dtype=float32)

time = 50782	action = 1	current_phase = 0	next_phase = 1	reward = -1.532202	array([[-6.9195685, -3.5194845]], dtype=float32)

time = 50790	action = 1	current_phase = 1	next_phase = 0	reward = -0.940552	array([[-3.839252 , -2.5217614]], dtype=float32)

time = 50798	action = 0	current_phase = 0	next_phase = 1	reward = -0.045360	array([[-1.8217697, -2.8636813]], dtype=float32)

time = 50803	action = 0	current_phase = 0	next_phase = 1	reward = 0.018801	array([[-2.1715887, -2.9748573]], dtype=float32)

time = 50808	action = 0	current_phase = 0	next_phase = 1	reward = 0.071812	array([[-2.7909873, -3.3571048]], dtype=float32)

time = 50813	action = 1	current_phase = 0	next_phase = 1	reward = -1.376146	array([[-6.9796762, -3.1854262]], dtype=float32)

time = 50821	action = 1	current_phase = 1	next_phase = 0	reward = -1.481688	array([[-3.781612 , -2.4526825]], dtype=float32)

time = 50829	action = 0	current_phase = 0	next_phase = 1	reward = 0.256908	array([[-1.3230422, -2.0816197]], dtype=float32)

time = 50834	action = 0	current_phase = 0	next_phase = 1	reward = 0.034328	array([[-1.8460339, -2.8575957]], dtype=float32)

time = 50839	action = 1	current_phase = 0	next_phase = 1	reward = -0.641258	array([[-2.4511063, -2.2662973]], dtype=float32)

time = 50847	action = 1	current_phase = 1	next_phase = 0	reward = -0.910612	array([[-3.3359485, -2.117724 ]], dtype=float32)

time = 50855	action = 0	current_phase = 0	next_phase = 1	reward = 0.198793	array([[-1.5321932, -3.025743 ]], dtype=float32)

time = 50860	action = 0	current_phase = 0	next_phase = 1	reward = -0.003890	array([[-1.9661086, -2.0864387]], dtype=float32)

time = 50865	action = 0	current_phase = 0	next_phase = 1	reward = 0.065043	array([[-2.4308426, -3.1781554]], dtype=float32)

time = 50870	action = 1	current_phase = 0	next_phase = 1	reward = -1.382821	array([[-5.8690996, -2.9673932]], dtype=float32)

time = 50878	action = 1	current_phase = 1	next_phase = 0	reward = -0.712782	array([[-3.529819, -2.210679]], dtype=float32)

time = 50886	action = 0	current_phase = 0	next_phase = 1	reward = -0.087977	array([[-1.8474109, -2.8532794]], dtype=float32)

time = 50891	action = 0	current_phase = 0	next_phase = 1	reward = -0.002733	array([[-2.119551 , -2.9488144]], dtype=float32)

time = 50896	action = 0	current_phase = 0	next_phase = 1	reward = 0.065140	array([[-2.6951587, -3.3096142]], dtype=float32)

time = 50901	action = 1	current_phase = 0	next_phase = 1	reward = -1.385071	array([[-6.952928 , -3.2363882]], dtype=float32)

time = 50909	action = 1	current_phase = 1	next_phase = 0	reward = -1.127365	array([[-3.6082625, -2.2772715]], dtype=float32)

time = 50917	action = 0	current_phase = 0	next_phase = 1	reward = 0.231840	array([[-1.771213, -2.842285]], dtype=float32)

time = 50922	action = 0	current_phase = 0	next_phase = 1	reward = 0.021681	array([[-2.2046754, -2.9742267]], dtype=float32)

time = 50927	action = 0	current_phase = 0	next_phase = 1	reward = 0.072287	array([[-2.8344414, -3.355395 ]], dtype=float32)

time = 50932	action = 1	current_phase = 0	next_phase = 1	reward = -1.624347	array([[-6.931767 , -3.4808373]], dtype=float32)

time = 50940	action = 1	current_phase = 1	next_phase = 0	reward = -1.046551	array([[-3.9145756, -2.5445547]], dtype=float32)

time = 50948	action = 0	current_phase = 0	next_phase = 1	reward = -0.033006	array([[-1.8712013, -2.8842983]], dtype=float32)

time = 50953	action = 0	current_phase = 0	next_phase = 1	reward = 0.044231	array([[-2.2333882, -3.0058217]], dtype=float32)

time = 50958	action = 0	current_phase = 0	next_phase = 1	reward = 0.073802	array([[-2.8110933, -3.3464308]], dtype=float32)

time = 50963	action = 1	current_phase = 0	next_phase = 1	reward = -1.321602	array([[-6.926728 , -3.3505692]], dtype=float32)

time = 50971	action = 1	current_phase = 1	next_phase = 0	reward = -1.422222	array([[-3.8456864, -2.4656777]], dtype=float32)

time = 50979	action = 0	current_phase = 0	next_phase = 1	reward = 0.252177	array([[-1.3639445, -2.08338  ]], dtype=float32)

time = 50984	action = 0	current_phase = 0	next_phase = 1	reward = 0.017216	array([[-1.8286545, -2.901892 ]], dtype=float32)

time = 50989	action = 1	current_phase = 0	next_phase = 1	reward = -0.665064	array([[-2.424557 , -2.2636943]], dtype=float32)

time = 50997	action = 1	current_phase = 1	next_phase = 0	reward = -0.927531	array([[-3.2984414, -2.0089498]], dtype=float32)

time = 51005	action = 0	current_phase = 0	next_phase = 1	reward = 0.168644	array([[-1.3792498, -3.089274 ]], dtype=float32)

time = 51010	action = 0	current_phase = 0	next_phase = 1	reward = -0.312032	array([[-1.9152274, -2.1119022]], dtype=float32)

time = 51015	action = 0	current_phase = 0	next_phase = 1	reward = 0.326047	array([[-2.2216032, -3.0203564]], dtype=float32)

time = 51020	action = 1	current_phase = 0	next_phase = 1	reward = -1.305337	array([[-5.873997 , -2.8651273]], dtype=float32)

time = 51028	action = 1	current_phase = 1	next_phase = 0	reward = -0.709061	array([[-3.5342436, -2.1726966]], dtype=float32)

time = 51036	action = 0	current_phase = 0	next_phase = 1	reward = -0.076427	array([[-1.8171813, -2.877266 ]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0190 - val_loss: 0.0082

Epoch 2/50

 - 3s - loss: 0.0200 - val_loss: 0.0089

Epoch 3/50

 - 3s - loss: 0.0187 - val_loss: 0.0083

Epoch 4/50

 - 3s - loss: 0.0184 - val_loss: 0.0072

Epoch 5/50

 - 3s - loss: 0.0183 - val_loss: 0.0078

Epoch 6/50

 - 3s - loss: 0.0175 - val_loss: 0.0087

Epoch 7/50

 - 3s - loss: 0.0173 - val_loss: 0.0071

Epoch 8/50

 - 3s - loss: 0.0147 - val_loss: 0.0090

Epoch 9/50

 - 3s - loss: 0.0202 - val_loss: 0.0086

Epoch 10/50

 - 3s - loss: 0.0208 - val_loss: 0.0090

Epoch 11/50

 - 3s - loss: 0.0163 - val_loss: 0.0073

Epoch 12/50

 - 3s - loss: 0.0178 - val_loss: 0.0098

Epoch 13/50

 - 3s - loss: 0.0186 - val_loss: 0.0129

Epoch 14/50

 - 3s - loss: 0.0217 - val_loss: 0.0114

Epoch 15/50

 - 3s - loss: 0.0188 - val_loss: 0.0107

Epoch 16/50

 - 3s - loss: 0.0165 - val_loss: 0.0086

Epoch 17/50

 - 3s - loss: 0.0193 - val_loss: 0.0073

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 51041	action = 0	current_phase = 0	next_phase = 1	reward = 0.003345	array([[-2.1139832, -2.9662764]], dtype=float32)

time = 51046	action = 0	current_phase = 0	next_phase = 1	reward = 0.057845	array([[-2.6270294, -3.3238204]], dtype=float32)

time = 51051	action = 1	current_phase = 0	next_phase = 1	reward = -1.398130	array([[-6.954121 , -3.3040423]], dtype=float32)

time = 51059	action = 1	current_phase = 1	next_phase = 0	reward = -0.879698	array([[-3.6474395, -2.259799 ]], dtype=float32)

time = 51067	action = 0	current_phase = 0	next_phase = 1	reward = -0.078757	array([[-1.7838745, -2.8488085]], dtype=float32)

time = 51072	action = 0	current_phase = 0	next_phase = 1	reward = -0.006703	array([[-2.1697788, -2.9743214]], dtype=float32)

time = 51077	action = 0	current_phase = 0	next_phase = 1	reward = 0.062276	array([[-2.792708 , -3.3313348]], dtype=float32)

time = 51082	action = 1	current_phase = 0	next_phase = 1	reward = -1.533988	array([[-6.9485784, -3.4633033]], dtype=float32)

time = 51090	action = 1	current_phase = 1	next_phase = 0	reward = -0.987315	array([[-3.8956351, -2.4748769]], dtype=float32)

time = 51098	action = 0	current_phase = 0	next_phase = 1	reward = -0.050740	array([[-1.7803826, -2.8561845]], dtype=float32)

time = 51103	action = 0	current_phase = 0	next_phase = 1	reward = 0.022027	array([[-2.1565416, -2.9665987]], dtype=float32)

time = 51108	action = 0	current_phase = 0	next_phase = 1	reward = 0.074627	array([[-2.8283792, -3.383921 ]], dtype=float32)

time = 51113	action = 1	current_phase = 0	next_phase = 1	reward = -1.916455	array([[-6.925219, -3.623713]], dtype=float32)

time = 51121	action = 1	current_phase = 1	next_phase = 0	reward = -1.047875	array([[-4.0269446, -2.307076 ]], dtype=float32)

time = 51129	action = 0	current_phase = 0	next_phase = 1	reward = -0.030721	array([[-1.3137314, -2.0742035]], dtype=float32)

time = 51134	action = 0	current_phase = 0	next_phase = 1	reward = 0.042368	array([[-1.7544603, -2.8560057]], dtype=float32)

time = 51139	action = 1	current_phase = 0	next_phase = 1	reward = -0.735600	array([[-2.499723 , -2.2682014]], dtype=float32)

time = 51147	action = 1	current_phase = 1	next_phase = 0	reward = -0.652702	array([[-3.4832606, -2.0334704]], dtype=float32)

time = 51155	action = 0	current_phase = 0	next_phase = 1	reward = -0.094281	array([[-1.4593153, -3.0628855]], dtype=float32)

time = 51160	action = 0	current_phase = 0	next_phase = 1	reward = -0.290857	array([[-1.911072 , -2.1108732]], dtype=float32)

time = 51165	action = 0	current_phase = 0	next_phase = 1	reward = 0.331099	array([[-2.2614832, -3.0637217]], dtype=float32)

time = 51170	action = 1	current_phase = 0	next_phase = 1	reward = -1.296221	array([[-5.874013 , -2.8794982]], dtype=float32)

time = 51178	action = 1	current_phase = 1	next_phase = 0	reward = -0.716418	array([[-3.5492916, -2.1677952]], dtype=float32)

time = 51186	action = 0	current_phase = 0	next_phase = 1	reward = -0.082797	array([[-1.7889206, -2.845094 ]], dtype=float32)

time = 51191	action = 0	current_phase = 0	next_phase = 1	reward = -0.008287	array([[-2.1053827, -2.9417717]], dtype=float32)

time = 51196	action = 0	current_phase = 0	next_phase = 1	reward = 0.066229	array([[-2.6213424, -3.282626 ]], dtype=float32)

time = 51201	action = 1	current_phase = 0	next_phase = 1	reward = -1.486067	array([[-6.9596014, -3.3071659]], dtype=float32)

time = 51209	action = 1	current_phase = 1	next_phase = 0	reward = -0.840035	array([[-3.6409178, -2.2611144]], dtype=float32)

time = 51217	action = 0	current_phase = 0	next_phase = 1	reward = -0.073884	array([[-1.7779677, -2.8468094]], dtype=float32)

time = 51222	action = 0	current_phase = 0	next_phase = 1	reward = 0.001984	array([[-2.1976874, -2.9675553]], dtype=float32)

time = 51227	action = 0	current_phase = 0	next_phase = 1	reward = 0.061964	array([[-2.789108, -3.374676]], dtype=float32)

time = 51232	action = 1	current_phase = 0	next_phase = 1	reward = -1.542947	array([[-6.9385104, -3.4894059]], dtype=float32)

time = 51240	action = 1	current_phase = 1	next_phase = 0	reward = -0.894101	array([[-3.889194 , -2.4652123]], dtype=float32)

time = 51248	action = 0	current_phase = 0	next_phase = 1	reward = -0.063331	array([[-1.7726467, -2.872765 ]], dtype=float32)

time = 51253	action = 0	current_phase = 0	next_phase = 1	reward = 0.015377	array([[-2.1879709, -2.9704235]], dtype=float32)

time = 51258	action = 0	current_phase = 0	next_phase = 1	reward = 0.076716	array([[-2.8596704, -3.3792765]], dtype=float32)

time = 51263	action = 1	current_phase = 0	next_phase = 1	reward = -1.797842	array([[-6.902046 , -3.6670573]], dtype=float32)

time = 51271	action = 1	current_phase = 1	next_phase = 0	reward = -1.024439	array([[-4.072377 , -2.3194938]], dtype=float32)

time = 51279	action = 0	current_phase = 0	next_phase = 1	reward = -0.036090	array([[-1.3045797, -2.0656533]], dtype=float32)

time = 51284	action = 0	current_phase = 0	next_phase = 1	reward = 0.027680	array([[-1.7569871, -2.861527 ]], dtype=float32)

time = 51289	action = 1	current_phase = 0	next_phase = 1	reward = -0.764389	array([[-2.5260565, -2.2367072]], dtype=float32)

time = 51297	action = 1	current_phase = 1	next_phase = 0	reward = -1.206340	array([[-3.431727 , -1.9900935]], dtype=float32)

time = 51305	action = 0	current_phase = 0	next_phase = 1	reward = 0.468893	array([[-1.2243335, -3.0920098]], dtype=float32)

time = 51310	action = 0	current_phase = 0	next_phase = 1	reward = -0.026077	array([[-1.9576247, -2.1275723]], dtype=float32)

time = 51315	action = 0	current_phase = 0	next_phase = 1	reward = -0.226206	array([[-2.3854935, -3.1362123]], dtype=float32)

time = 51320	action = 1	current_phase = 0	next_phase = 1	reward = -1.037267	array([[-5.869185, -2.679789]], dtype=float32)

time = 51328	action = 1	current_phase = 1	next_phase = 0	reward = -0.705084	array([[-3.502418 , -2.1211538]], dtype=float32)

time = 51336	action = 0	current_phase = 0	next_phase = 1	reward = -0.099224	array([[-1.77527 , -2.859192]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0193 - val_loss: 0.0098

Epoch 2/50

 - 3s - loss: 0.0164 - val_loss: 0.0101

Epoch 3/50

 - 3s - loss: 0.0185 - val_loss: 0.0092

Epoch 4/50

 - 3s - loss: 0.0160 - val_loss: 0.0127

Epoch 5/50

 - 3s - loss: 0.0200 - val_loss: 0.0096

Epoch 6/50

 - 3s - loss: 0.0184 - val_loss: 0.0135

Epoch 7/50

 - 3s - loss: 0.0157 - val_loss: 0.0089

Epoch 8/50

 - 3s - loss: 0.0179 - val_loss: 0.0108

Epoch 9/50

 - 3s - loss: 0.0214 - val_loss: 0.0090

Epoch 10/50

 - 3s - loss: 0.0168 - val_loss: 0.0085

Epoch 11/50

 - 3s - loss: 0.0149 - val_loss: 0.0087

Epoch 12/50

 - 3s - loss: 0.0135 - val_loss: 0.0122

Epoch 13/50

 - 3s - loss: 0.0155 - val_loss: 0.0101

Epoch 14/50

 - 3s - loss: 0.0152 - val_loss: 0.0110

Epoch 15/50

 - 3s - loss: 0.0176 - val_loss: 0.0089

Epoch 16/50

 - 3s - loss: 0.0164 - val_loss: 0.0123

Epoch 17/50

 - 3s - loss: 0.0138 - val_loss: 0.0108

Epoch 18/50

 - 3s - loss: 0.0111 - val_loss: 0.0101

Epoch 19/50

 - 3s - loss: 0.0143 - val_loss: 0.0110

Epoch 20/50

 - 3s - loss: 0.0179 - val_loss: 0.0132

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 51341	action = 0	current_phase = 0	next_phase = 1	reward = -0.023196	array([[-2.070191, -2.938596]], dtype=float32)

time = 51346	action = 0	current_phase = 0	next_phase = 1	reward = 0.064002	array([[-2.448873, -3.224909]], dtype=float32)

time = 51351	action = 1	current_phase = 0	next_phase = 1	reward = -1.448040	array([[-6.9736533, -3.2888396]], dtype=float32)

time = 51359	action = 1	current_phase = 1	next_phase = 0	reward = -0.769322	array([[-3.6246278, -2.2629352]], dtype=float32)

time = 51367	action = 0	current_phase = 0	next_phase = 1	reward = -0.060532	array([[-1.7106752, -2.8513281]], dtype=float32)

time = 51372	action = 0	current_phase = 0	next_phase = 1	reward = -0.006448	array([[-2.1933432, -2.9653406]], dtype=float32)

time = 51377	action = 0	current_phase = 0	next_phase = 1	reward = 0.070579	array([[-2.6689363, -3.3251371]], dtype=float32)

time = 51382	action = 1	current_phase = 0	next_phase = 1	reward = -1.643189	array([[-6.939808, -3.548003]], dtype=float32)

time = 51390	action = 1	current_phase = 1	next_phase = 0	reward = -0.961944	array([[-3.8728092, -2.4562812]], dtype=float32)

time = 51398	action = 0	current_phase = 0	next_phase = 1	reward = -0.058236	array([[-1.7453264, -2.864539 ]], dtype=float32)

time = 51403	action = 0	current_phase = 0	next_phase = 1	reward = 0.030677	array([[-2.1831727, -2.9583888]], dtype=float32)

time = 51408	action = 0	current_phase = 0	next_phase = 1	reward = 0.082546	array([[-2.6575496, -3.3659086]], dtype=float32)

time = 51413	action = 1	current_phase = 0	next_phase = 1	reward = -1.331214	array([[-6.9348288, -3.4179525]], dtype=float32)

time = 51421	action = 1	current_phase = 1	next_phase = 0	reward = -1.511815	array([[-3.8493617, -2.3636308]], dtype=float32)

time = 51429	action = 0	current_phase = 0	next_phase = 1	reward = 0.258192	array([[-1.2695627, -2.1105123]], dtype=float32)

time = 51434	action = 0	current_phase = 0	next_phase = 1	reward = 0.022776	array([[-1.6964258, -2.842598 ]], dtype=float32)

time = 51439	action = 1	current_phase = 0	next_phase = 1	reward = -0.718071	array([[-2.3606682, -2.2886503]], dtype=float32)

time = 51447	action = 1	current_phase = 1	next_phase = 0	reward = -0.927340	array([[-3.4541204, -2.1059275]], dtype=float32)

time = 51455	action = 0	current_phase = 0	next_phase = 1	reward = 0.181840	array([[-1.2250198, -3.0626485]], dtype=float32)

time = 51460	action = 0	current_phase = 0	next_phase = 1	reward = -0.019095	array([[-1.7827696, -2.1244714]], dtype=float32)

time = 51465	action = 0	current_phase = 0	next_phase = 1	reward = 0.049792	array([[-2.2092335, -3.0838916]], dtype=float32)

time = 51470	action = 1	current_phase = 0	next_phase = 1	reward = -1.421662	array([[-5.8527536, -3.0134375]], dtype=float32)

time = 51478	action = 1	current_phase = 1	next_phase = 0	reward = -0.708320	array([[-3.5715015, -2.2082808]], dtype=float32)

time = 51486	action = 0	current_phase = 0	next_phase = 1	reward = -0.065830	array([[-1.7351047, -2.8444855]], dtype=float32)

time = 51491	action = 0	current_phase = 0	next_phase = 1	reward = 0.006350	array([[-2.0830445, -2.941984 ]], dtype=float32)

time = 51496	action = 0	current_phase = 0	next_phase = 1	reward = 0.068542	array([[-2.5079675, -3.3014014]], dtype=float32)

time = 51501	action = 1	current_phase = 0	next_phase = 1	reward = -1.392712	array([[-6.9434404, -3.2453392]], dtype=float32)

time = 51509	action = 1	current_phase = 1	next_phase = 0	reward = -0.832747	array([[-3.6500173, -2.2933576]], dtype=float32)

time = 51517	action = 0	current_phase = 0	next_phase = 1	reward = -0.063811	array([[-1.7053003, -2.8446634]], dtype=float32)

time = 51522	action = 0	current_phase = 0	next_phase = 1	reward = 0.002796	array([[-2.189495 , -2.9535077]], dtype=float32)

time = 51527	action = 0	current_phase = 0	next_phase = 1	reward = 0.074508	array([[-2.6712985, -3.3371677]], dtype=float32)

time = 51532	action = 1	current_phase = 0	next_phase = 1	reward = -1.590519	array([[-6.934654, -3.488837]], dtype=float32)

time = 51540	action = 1	current_phase = 1	next_phase = 0	reward = -0.960729	array([[-3.8772686, -2.4703996]], dtype=float32)

time = 51548	action = 0	current_phase = 0	next_phase = 1	reward = -0.040770	array([[-1.727131 , -2.8644505]], dtype=float32)

time = 51553	action = 0	current_phase = 0	next_phase = 1	reward = 0.033074	array([[-2.1923316, -2.9821842]], dtype=float32)

time = 51558	action = 0	current_phase = 0	next_phase = 1	reward = 0.081824	array([[-2.6946087, -3.3562608]], dtype=float32)

time = 51563	action = 1	current_phase = 0	next_phase = 1	reward = -1.897118	array([[-6.9291344, -3.5799892]], dtype=float32)

time = 51571	action = 1	current_phase = 1	next_phase = 0	reward = -1.088208	array([[-4.0813904, -2.2442799]], dtype=float32)

time = 51579	action = 0	current_phase = 0	next_phase = 1	reward = -0.025823	array([[-1.2461998, -2.0948997]], dtype=float32)

time = 51584	action = 0	current_phase = 0	next_phase = 1	reward = 0.043929	array([[-1.7296258, -2.8620782]], dtype=float32)

time = 51589	action = 1	current_phase = 0	next_phase = 1	reward = -0.680132	array([[-2.4908695, -2.3071196]], dtype=float32)

time = 51597	action = 1	current_phase = 1	next_phase = 0	reward = -0.592549	array([[-3.3921955, -2.0588593]], dtype=float32)

time = 51605	action = 0	current_phase = 0	next_phase = 1	reward = -0.112384	array([[-1.4859664, -3.01101  ]], dtype=float32)

time = 51610	action = 0	current_phase = 0	next_phase = 1	reward = -0.031978	array([[-1.8791273, -2.114762 ]], dtype=float32)

time = 51615	action = 0	current_phase = 0	next_phase = 1	reward = 0.042161	array([[-2.286512, -3.142956]], dtype=float32)

time = 51620	action = 1	current_phase = 0	next_phase = 1	reward = -1.243733	array([[-5.8581524, -2.9626796]], dtype=float32)

time = 51628	action = 1	current_phase = 1	next_phase = 0	reward = -0.714457	array([[-3.5149336, -2.1877563]], dtype=float32)

time = 51636	action = 0	current_phase = 0	next_phase = 1	reward = -0.088785	array([[-1.6901386, -2.8522458]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0161 - val_loss: 0.0107

Epoch 2/50

 - 3s - loss: 0.0148 - val_loss: 0.0101

Epoch 3/50

 - 3s - loss: 0.0139 - val_loss: 0.0099

Epoch 4/50

 - 3s - loss: 0.0137 - val_loss: 0.0088

Epoch 5/50

 - 3s - loss: 0.0170 - val_loss: 0.0093

Epoch 6/50

 - 3s - loss: 0.0139 - val_loss: 0.0085

Epoch 7/50

 - 3s - loss: 0.0147 - val_loss: 0.0082

Epoch 8/50

 - 3s - loss: 0.0134 - val_loss: 0.0113

Epoch 9/50

 - 3s - loss: 0.0126 - val_loss: 0.0098

Epoch 10/50

 - 3s - loss: 0.0140 - val_loss: 0.0100

Epoch 11/50

 - 3s - loss: 0.0145 - val_loss: 0.0106

Epoch 12/50

 - 3s - loss: 0.0131 - val_loss: 0.0105

Epoch 13/50

 - 3s - loss: 0.0129 - val_loss: 0.0095

Epoch 14/50

 - 3s - loss: 0.0180 - val_loss: 0.0104

Epoch 15/50

 - 3s - loss: 0.0123 - val_loss: 0.0115

Epoch 16/50

 - 3s - loss: 0.0130 - val_loss: 0.0100

Epoch 17/50

 - 3s - loss: 0.0114 - val_loss: 0.0096

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 51641	action = 0	current_phase = 0	next_phase = 1	reward = 0.015680	array([[-2.0576687, -2.9227333]], dtype=float32)

time = 51646	action = 0	current_phase = 0	next_phase = 1	reward = 0.058457	array([[-2.5153365, -3.2835531]], dtype=float32)

time = 51651	action = 1	current_phase = 0	next_phase = 1	reward = -1.494424	array([[-6.94839  , -3.2438598]], dtype=float32)

time = 51659	action = 1	current_phase = 1	next_phase = 0	reward = -0.773955	array([[-3.6530843, -2.298035 ]], dtype=float32)

time = 51667	action = 0	current_phase = 0	next_phase = 1	reward = -0.079475	array([[-1.8489661, -2.8683949]], dtype=float32)

time = 51672	action = 0	current_phase = 0	next_phase = 1	reward = -0.005112	array([[-2.1854155, -2.9769716]], dtype=float32)

time = 51677	action = 0	current_phase = 0	next_phase = 1	reward = 0.059511	array([[-2.638796, -3.312471]], dtype=float32)

time = 51682	action = 1	current_phase = 0	next_phase = 1	reward = -1.582788	array([[-6.919807 , -3.5188642]], dtype=float32)

time = 51690	action = 1	current_phase = 1	next_phase = 0	reward = -0.940291	array([[-3.7938137, -2.4537213]], dtype=float32)

time = 51698	action = 0	current_phase = 0	next_phase = 1	reward = -0.059923	array([[-1.7591739, -2.8613935]], dtype=float32)

time = 51703	action = 0	current_phase = 0	next_phase = 1	reward = 0.016971	array([[-2.243085, -2.967115]], dtype=float32)

time = 51708	action = 0	current_phase = 0	next_phase = 1	reward = 0.057377	array([[-2.7215538, -3.3932586]], dtype=float32)

time = 51713	action = 1	current_phase = 0	next_phase = 1	reward = -1.872742	array([[-6.911806 , -3.6268744]], dtype=float32)

time = 51721	action = 1	current_phase = 1	next_phase = 0	reward = -1.027958	array([[-3.989614 , -2.2062027]], dtype=float32)

time = 51729	action = 0	current_phase = 0	next_phase = 1	reward = -0.033632	array([[-1.3759451, -2.0956292]], dtype=float32)

time = 51734	action = 0	current_phase = 0	next_phase = 1	reward = 0.042916	array([[-1.739122 , -2.8647642]], dtype=float32)

time = 51739	action = 1	current_phase = 0	next_phase = 1	reward = -0.695932	array([[-2.4684234, -2.3471656]], dtype=float32)

time = 51747	action = 1	current_phase = 1	next_phase = 0	reward = -0.860327	array([[-3.3663387, -1.9864304]], dtype=float32)

time = 51755	action = 0	current_phase = 0	next_phase = 1	reward = 0.170463	array([[-1.1750546, -3.098672 ]], dtype=float32)

time = 51760	action = 0	current_phase = 0	next_phase = 1	reward = -0.027278	array([[-1.8850789, -2.0914316]], dtype=float32)

time = 51765	action = 0	current_phase = 0	next_phase = 1	reward = 0.043791	array([[-2.346405 , -3.1402812]], dtype=float32)

time = 51770	action = 1	current_phase = 0	next_phase = 1	reward = -1.351843	array([[-5.8441277, -2.9935603]], dtype=float32)

time = 51778	action = 1	current_phase = 1	next_phase = 0	reward = -0.993612	array([[-3.5320096, -2.155859 ]], dtype=float32)

time = 51786	action = 0	current_phase = 0	next_phase = 1	reward = 0.208683	array([[-1.7173731, -2.8892412]], dtype=float32)

time = 51791	action = 0	current_phase = 0	next_phase = 1	reward = -0.005392	array([[-2.1008415, -2.9473429]], dtype=float32)

time = 51796	action = 0	current_phase = 0	next_phase = 1	reward = 0.070123	array([[-2.6032567, -3.2953653]], dtype=float32)

time = 51801	action = 1	current_phase = 0	next_phase = 1	reward = -1.453241	array([[-6.9454803, -3.28862  ]], dtype=float32)

time = 51809	action = 1	current_phase = 1	next_phase = 0	reward = -0.826977	array([[-3.6489305, -2.2976115]], dtype=float32)

time = 51817	action = 0	current_phase = 0	next_phase = 1	reward = -0.065695	array([[-1.7825387, -2.8500795]], dtype=float32)

time = 51822	action = 0	current_phase = 0	next_phase = 1	reward = 0.009849	array([[-2.202543 , -2.9559374]], dtype=float32)

time = 51827	action = 0	current_phase = 0	next_phase = 1	reward = 0.069156	array([[-2.7323248, -3.3488336]], dtype=float32)

time = 51832	action = 1	current_phase = 0	next_phase = 1	reward = -1.714086	array([[-6.9215283, -3.5048203]], dtype=float32)

time = 51840	action = 1	current_phase = 1	next_phase = 0	reward = -1.018691	array([[-3.8297029, -2.4688601]], dtype=float32)

time = 51848	action = 0	current_phase = 0	next_phase = 1	reward = -0.050401	array([[-1.7520556, -2.8700104]], dtype=float32)

time = 51853	action = 0	current_phase = 0	next_phase = 1	reward = 0.018711	array([[-2.2069817, -2.949295 ]], dtype=float32)

time = 51858	action = 0	current_phase = 0	next_phase = 1	reward = 0.084864	array([[-2.7077823, -3.3714757]], dtype=float32)

time = 51863	action = 1	current_phase = 0	next_phase = 1	reward = -1.878401	array([[-6.910988 , -3.5950804]], dtype=float32)

time = 51871	action = 1	current_phase = 1	next_phase = 0	reward = -1.290742	array([[-4.026873, -2.14187 ]], dtype=float32)

time = 51879	action = 0	current_phase = 0	next_phase = 1	reward = 0.259004	array([[-1.2862759, -2.107273 ]], dtype=float32)

time = 51884	action = 0	current_phase = 0	next_phase = 1	reward = 0.023233	array([[-1.7784424, -2.8772507]], dtype=float32)

time = 51889	action = 1	current_phase = 0	next_phase = 1	reward = -0.626998	array([[-2.516438, -2.317009]], dtype=float32)

time = 51897	action = 1	current_phase = 1	next_phase = 0	reward = -0.539987	array([[-3.2984958, -1.9813638]], dtype=float32)

time = 51905	action = 0	current_phase = 0	next_phase = 1	reward = -0.105804	array([[-1.4443382, -3.0493321]], dtype=float32)

time = 51910	action = 0	current_phase = 0	next_phase = 1	reward = -0.024535	array([[-1.868913 , -2.1168861]], dtype=float32)

time = 51915	action = 0	current_phase = 0	next_phase = 1	reward = -0.238492	array([[-2.2519617, -3.09485  ]], dtype=float32)

time = 51920	action = 1	current_phase = 0	next_phase = 1	reward = -1.138896	array([[-5.8534374, -2.801725 ]], dtype=float32)

time = 51928	action = 1	current_phase = 1	next_phase = 0	reward = -1.011345	array([[-3.567412 , -2.1742232]], dtype=float32)

time = 51936	action = 0	current_phase = 0	next_phase = 1	reward = 0.197485	array([[-1.6826065, -2.8730066]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0162 - val_loss: 0.0118

Epoch 2/50

 - 3s - loss: 0.0162 - val_loss: 0.0144

Epoch 3/50

 - 3s - loss: 0.0167 - val_loss: 0.0154

Epoch 4/50

 - 3s - loss: 0.0192 - val_loss: 0.0130

Epoch 5/50

 - 3s - loss: 0.0161 - val_loss: 0.0144

Epoch 6/50

 - 3s - loss: 0.0128 - val_loss: 0.0157

Epoch 7/50

 - 3s - loss: 0.0151 - val_loss: 0.0110

Epoch 8/50

 - 3s - loss: 0.0156 - val_loss: 0.0132

Epoch 9/50

 - 3s - loss: 0.0131 - val_loss: 0.0138

Epoch 10/50

 - 3s - loss: 0.0137 - val_loss: 0.0130

Epoch 11/50

 - 3s - loss: 0.0131 - val_loss: 0.0120

Epoch 12/50

 - 3s - loss: 0.0118 - val_loss: 0.0163

Epoch 13/50

 - 3s - loss: 0.0128 - val_loss: 0.0119

Epoch 14/50

 - 3s - loss: 0.0140 - val_loss: 0.0119

Epoch 15/50

 - 3s - loss: 0.0124 - val_loss: 0.0131

Epoch 16/50

 - 3s - loss: 0.0120 - val_loss: 0.0168

Epoch 17/50

 - 3s - loss: 0.0139 - val_loss: 0.0126

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 51941	action = 0	current_phase = 0	next_phase = 1	reward = 0.004939	array([[-2.0844803, -2.9090781]], dtype=float32)

time = 51946	action = 0	current_phase = 0	next_phase = 1	reward = 0.080553	array([[-2.5328217, -3.2027202]], dtype=float32)

time = 51951	action = 1	current_phase = 0	next_phase = 1	reward = -1.389803	array([[-6.9458475, -3.258679 ]], dtype=float32)

time = 51959	action = 1	current_phase = 1	next_phase = 0	reward = -0.844132	array([[-3.6628327, -2.3225708]], dtype=float32)

time = 51967	action = 0	current_phase = 0	next_phase = 1	reward = -0.073881	array([[-1.756061 , -2.8389268]], dtype=float32)

time = 51972	action = 0	current_phase = 0	next_phase = 1	reward = 0.003107	array([[-2.232077 , -2.9386435]], dtype=float32)

time = 51977	action = 0	current_phase = 0	next_phase = 1	reward = 0.070016	array([[-2.6937168, -3.3084679]], dtype=float32)

time = 51982	action = 1	current_phase = 0	next_phase = 1	reward = -1.646644	array([[-6.924067 , -3.5028076]], dtype=float32)

time = 51990	action = 1	current_phase = 1	next_phase = 0	reward = -1.317398	array([[-3.839507, -2.476698]], dtype=float32)

time = 51998	action = 0	current_phase = 0	next_phase = 1	reward = 0.232644	array([[-1.6312062, -2.821744 ]], dtype=float32)

time = 52003	action = 0	current_phase = 0	next_phase = 1	reward = -0.002160	array([[-2.2019424, -2.9352384]], dtype=float32)

time = 52008	action = 0	current_phase = 0	next_phase = 1	reward = 0.059321	array([[-2.7065425, -3.3049498]], dtype=float32)

time = 52013	action = 1	current_phase = 0	next_phase = 1	reward = -1.846089	array([[-6.911833 , -3.6343951]], dtype=float32)

time = 52021	action = 1	current_phase = 1	next_phase = 0	reward = -0.982513	array([[-4.0262322, -2.3513746]], dtype=float32)

time = 52029	action = 0	current_phase = 0	next_phase = 1	reward = -0.036251	array([[-1.3951273, -2.045919 ]], dtype=float32)

time = 52034	action = 0	current_phase = 0	next_phase = 1	reward = 0.034135	array([[-1.8025408, -2.8612323]], dtype=float32)

time = 52039	action = 1	current_phase = 0	next_phase = 1	reward = -0.636010	array([[-2.503213 , -2.3027954]], dtype=float32)

time = 52047	action = 1	current_phase = 1	next_phase = 0	reward = -0.875832	array([[-3.3501725, -2.0449142]], dtype=float32)

time = 52055	action = 0	current_phase = 0	next_phase = 1	reward = -0.111631	array([[-1.3020239, -3.0043674]], dtype=float32)

time = 52060	action = 0	current_phase = 0	next_phase = 1	reward = 0.238892	array([[-1.6884782, -2.0883582]], dtype=float32)

time = 52065	action = 0	current_phase = 0	next_phase = 1	reward = -0.253945	array([[-2.262118 , -3.0608811]], dtype=float32)

time = 52070	action = 1	current_phase = 0	next_phase = 1	reward = -0.927847	array([[-5.832816 , -2.7597384]], dtype=float32)

time = 52078	action = 1	current_phase = 1	next_phase = 0	reward = -0.700915	array([[-3.4875226, -2.1836302]], dtype=float32)

time = 52086	action = 0	current_phase = 0	next_phase = 1	reward = -0.089023	array([[-1.7204146, -2.8458943]], dtype=float32)

time = 52091	action = 0	current_phase = 0	next_phase = 1	reward = -0.008930	array([[-2.0929759, -2.9022346]], dtype=float32)

time = 52096	action = 0	current_phase = 0	next_phase = 1	reward = 0.059620	array([[-2.51061  , -3.2610044]], dtype=float32)

time = 52101	action = 1	current_phase = 0	next_phase = 1	reward = -1.453594	array([[-6.943926 , -3.1626744]], dtype=float32)

time = 52109	action = 1	current_phase = 1	next_phase = 0	reward = -0.775102	array([[-3.6291862, -2.287208 ]], dtype=float32)

time = 52117	action = 0	current_phase = 0	next_phase = 1	reward = -0.063449	array([[-1.7648506, -2.8318396]], dtype=float32)

time = 52122	action = 0	current_phase = 0	next_phase = 1	reward = -0.002781	array([[-2.2134354, -2.9494839]], dtype=float32)

time = 52127	action = 0	current_phase = 0	next_phase = 1	reward = 0.060046	array([[-2.7579603, -3.3510633]], dtype=float32)

time = 52132	action = 1	current_phase = 0	next_phase = 1	reward = -1.660342	array([[-6.922305 , -3.5276241]], dtype=float32)

time = 52140	action = 1	current_phase = 1	next_phase = 0	reward = -0.993446	array([[-3.8112612, -2.445503 ]], dtype=float32)

time = 52148	action = 0	current_phase = 0	next_phase = 1	reward = -0.029706	array([[-1.7001765, -2.833748 ]], dtype=float32)

time = 52153	action = 0	current_phase = 0	next_phase = 1	reward = 0.029383	array([[-2.166848, -2.923459]], dtype=float32)

time = 52158	action = 0	current_phase = 0	next_phase = 1	reward = 0.079456	array([[-2.7318132, -3.3224535]], dtype=float32)

time = 52163	action = 1	current_phase = 0	next_phase = 1	reward = -1.367586	array([[-6.945324 , -3.3561563]], dtype=float32)

time = 52171	action = 1	current_phase = 1	next_phase = 0	reward = -1.493361	array([[-3.7951107, -2.40172  ]], dtype=float32)

time = 52179	action = 0	current_phase = 0	next_phase = 1	reward = 0.248986	array([[-1.2683194, -2.086523 ]], dtype=float32)

time = 52184	action = 0	current_phase = 0	next_phase = 1	reward = 0.039696	array([[-1.8140929, -2.8765597]], dtype=float32)

time = 52189	action = 1	current_phase = 0	next_phase = 1	reward = -1.297394	array([[-2.3179202, -2.2953358]], dtype=float32)

time = 52197	action = 1	current_phase = 1	next_phase = 0	reward = -0.775693	array([[-3.523374, -2.172389]], dtype=float32)

time = 52205	action = 0	current_phase = 0	next_phase = 1	reward = -0.098214	array([[-1.47824  , -2.9574604]], dtype=float32)

time = 52210	action = 0	current_phase = 0	next_phase = 1	reward = 0.259312	array([[-1.8138595, -2.0823944]], dtype=float32)

time = 52215	action = 0	current_phase = 0	next_phase = 1	reward = -0.238831	array([[-2.3864336, -3.1292462]], dtype=float32)

time = 52220	action = 1	current_phase = 0	next_phase = 1	reward = -0.986106	array([[-5.8473334, -2.7744331]], dtype=float32)

time = 52228	action = 1	current_phase = 1	next_phase = 0	reward = -0.693861	array([[-3.5053902, -2.1696427]], dtype=float32)

time = 52236	action = 0	current_phase = 0	next_phase = 1	reward = -0.095296	array([[-1.7118435, -2.822977 ]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0193 - val_loss: 0.0091

Epoch 2/50

 - 3s - loss: 0.0145 - val_loss: 0.0103

Epoch 3/50

 - 3s - loss: 0.0135 - val_loss: 0.0089

Epoch 4/50

 - 3s - loss: 0.0159 - val_loss: 0.0080

Epoch 5/50

 - 3s - loss: 0.0123 - val_loss: 0.0066

Epoch 6/50

 - 3s - loss: 0.0117 - val_loss: 0.0102

Epoch 7/50

 - 3s - loss: 0.0122 - val_loss: 0.0070

Epoch 8/50

 - 3s - loss: 0.0125 - val_loss: 0.0061

Epoch 9/50

 - 3s - loss: 0.0147 - val_loss: 0.0133

Epoch 10/50

 - 3s - loss: 0.0145 - val_loss: 0.0090

Epoch 11/50

 - 3s - loss: 0.0118 - val_loss: 0.0097

Epoch 12/50

 - 3s - loss: 0.0174 - val_loss: 0.0085

Epoch 13/50

 - 3s - loss: 0.0204 - val_loss: 0.0075

Epoch 14/50

 - 3s - loss: 0.0113 - val_loss: 0.0102

Epoch 15/50

 - 3s - loss: 0.0133 - val_loss: 0.0124

Epoch 16/50

 - 3s - loss: 0.0147 - val_loss: 0.0113

Epoch 17/50

 - 3s - loss: 0.0162 - val_loss: 0.0108

Epoch 18/50

 - 3s - loss: 0.0159 - val_loss: 0.0108

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 52241	action = 0	current_phase = 0	next_phase = 1	reward = -0.009568	array([[-2.1426888, -2.9059095]], dtype=float32)

time = 52246	action = 0	current_phase = 0	next_phase = 1	reward = 0.062729	array([[-2.6496847, -3.2707584]], dtype=float32)

time = 52251	action = 1	current_phase = 0	next_phase = 1	reward = -1.443643	array([[-7.0202675, -3.3505359]], dtype=float32)

time = 52259	action = 1	current_phase = 1	next_phase = 0	reward = -0.798170	array([[-3.674382 , -2.3165216]], dtype=float32)

time = 52267	action = 0	current_phase = 0	next_phase = 1	reward = -0.083814	array([[-1.8674492, -2.831518 ]], dtype=float32)

time = 52272	action = 0	current_phase = 0	next_phase = 1	reward = 0.022495	array([[-2.2823625, -2.937729 ]], dtype=float32)

time = 52277	action = 0	current_phase = 0	next_phase = 1	reward = 0.074195	array([[-2.8080592, -3.3561113]], dtype=float32)

time = 52282	action = 1	current_phase = 0	next_phase = 1	reward = -1.634838	array([[-6.994367 , -3.5346582]], dtype=float32)

time = 52290	action = 1	current_phase = 1	next_phase = 0	reward = -1.050773	array([[-3.856772 , -2.4934306]], dtype=float32)

time = 52298	action = 0	current_phase = 0	next_phase = 1	reward = -0.055964	array([[-1.7351091, -2.8078864]], dtype=float32)

time = 52303	action = 0	current_phase = 0	next_phase = 1	reward = 0.005931	array([[-2.2843425, -2.9190876]], dtype=float32)

time = 52308	action = 0	current_phase = 0	next_phase = 1	reward = 0.076043	array([[-2.7751107, -3.3604314]], dtype=float32)

time = 52313	action = 1	current_phase = 0	next_phase = 1	reward = -1.383032	array([[-7.022285 , -3.3015924]], dtype=float32)

time = 52321	action = 1	current_phase = 1	next_phase = 0	reward = -1.437430	array([[-3.7499695, -2.3335254]], dtype=float32)

time = 52329	action = 0	current_phase = 0	next_phase = 1	reward = 0.247718	array([[-1.346721 , -2.0844958]], dtype=float32)

time = 52334	action = 0	current_phase = 0	next_phase = 1	reward = 0.023939	array([[-1.845268 , -2.8500593]], dtype=float32)

time = 52339	action = 1	current_phase = 0	next_phase = 1	reward = -0.645730	array([[-2.464202 , -2.2878733]], dtype=float32)

time = 52347	action = 1	current_phase = 1	next_phase = 0	reward = -0.588977	array([[-3.3807025, -2.050426 ]], dtype=float32)

time = 52355	action = 0	current_phase = 0	next_phase = 1	reward = -0.383707	array([[-1.601568 , -3.0021966]], dtype=float32)

time = 52360	action = 0	current_phase = 0	next_phase = 1	reward = -0.033118	array([[-1.9118968, -2.0759099]], dtype=float32)

time = 52365	action = 0	current_phase = 0	next_phase = 1	reward = 0.324022	array([[-2.1992795, -2.9836729]], dtype=float32)

time = 52370	action = 1	current_phase = 0	next_phase = 1	reward = -1.312128	array([[-5.9353876, -3.1016266]], dtype=float32)

time = 52378	action = 1	current_phase = 1	next_phase = 0	reward = -0.702614	array([[-3.5452747, -2.1882818]], dtype=float32)

time = 52386	action = 0	current_phase = 0	next_phase = 1	reward = -0.072493	array([[-1.8186628, -2.8358893]], dtype=float32)

time = 52391	action = 0	current_phase = 0	next_phase = 1	reward = 0.008489	array([[-2.2038708, -2.9220684]], dtype=float32)

time = 52396	action = 0	current_phase = 0	next_phase = 1	reward = 0.062571	array([[-2.6273346, -3.2811735]], dtype=float32)

time = 52401	action = 1	current_phase = 0	next_phase = 1	reward = -1.429844	array([[-6.993979 , -3.3032892]], dtype=float32)

time = 52409	action = 1	current_phase = 1	next_phase = 0	reward = -0.762861	array([[-3.640595 , -2.2689817]], dtype=float32)

time = 52417	action = 0	current_phase = 0	next_phase = 1	reward = -0.061903	array([[-1.8521631, -2.836107 ]], dtype=float32)

time = 52422	action = 0	current_phase = 0	next_phase = 1	reward = 0.024195	array([[-2.2779212, -2.9399638]], dtype=float32)

time = 52427	action = 0	current_phase = 0	next_phase = 1	reward = 0.081777	array([[-2.8180022, -3.3597791]], dtype=float32)

time = 52432	action = 1	current_phase = 0	next_phase = 1	reward = -1.681594	array([[-6.979543, -3.58967 ]], dtype=float32)

time = 52440	action = 1	current_phase = 1	next_phase = 0	reward = -1.076514	array([[-3.8390164, -2.4679356]], dtype=float32)

time = 52448	action = 0	current_phase = 0	next_phase = 1	reward = -0.045865	array([[-1.7693297, -2.8351476]], dtype=float32)

time = 52453	action = 0	current_phase = 0	next_phase = 1	reward = 0.022597	array([[-2.257771 , -2.9252217]], dtype=float32)

time = 52458	action = 0	current_phase = 0	next_phase = 1	reward = 0.075578	array([[-2.7746825, -3.361285 ]], dtype=float32)

time = 52463	action = 1	current_phase = 0	next_phase = 1	reward = -1.891504	array([[-6.9877954, -3.5944862]], dtype=float32)

time = 52471	action = 1	current_phase = 1	next_phase = 0	reward = -1.134523	array([[-3.9200552, -2.2806969]], dtype=float32)

time = 52479	action = 0	current_phase = 0	next_phase = 1	reward = -0.035389	array([[-1.5909585, -2.0537927]], dtype=float32)

time = 52484	action = 0	current_phase = 0	next_phase = 1	reward = 0.037216	array([[-1.9534037, -2.8587003]], dtype=float32)

time = 52489	action = 1	current_phase = 0	next_phase = 1	reward = -0.702115	array([[-2.5262341, -2.280883 ]], dtype=float32)

time = 52497	action = 1	current_phase = 1	next_phase = 0	reward = -0.639664	array([[-3.3980272, -2.0538676]], dtype=float32)

time = 52505	action = 0	current_phase = 0	next_phase = 1	reward = -0.095588	array([[-1.5725701, -3.0312343]], dtype=float32)

time = 52510	action = 0	current_phase = 0	next_phase = 1	reward = -0.017566	array([[-1.9635986, -2.0892258]], dtype=float32)

time = 52515	action = 0	current_phase = 0	next_phase = 1	reward = 0.046980	array([[-2.355605, -3.051735]], dtype=float32)

time = 52520	action = 1	current_phase = 0	next_phase = 1	reward = -1.245868	array([[-5.926773 , -3.0698261]], dtype=float32)

time = 52528	action = 1	current_phase = 1	next_phase = 0	reward = -0.713838	array([[-3.525366, -2.171229]], dtype=float32)

time = 52536	action = 0	current_phase = 0	next_phase = 1	reward = -0.087792	array([[-1.7863734, -2.8333542]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0237 - val_loss: 0.0060

Epoch 2/50

 - 3s - loss: 0.0178 - val_loss: 0.0072

Epoch 3/50

 - 3s - loss: 0.0151 - val_loss: 0.0079

Epoch 4/50

 - 3s - loss: 0.0159 - val_loss: 0.0067

Epoch 5/50

 - 3s - loss: 0.0244 - val_loss: 0.0068

Epoch 6/50

 - 3s - loss: 0.0167 - val_loss: 0.0058

Epoch 7/50

 - 3s - loss: 0.0167 - val_loss: 0.0063

Epoch 8/50

 - 3s - loss: 0.0169 - val_loss: 0.0102

Epoch 9/50

 - 3s - loss: 0.0177 - val_loss: 0.0065

Epoch 10/50

 - 3s - loss: 0.0176 - val_loss: 0.0065

Epoch 11/50

 - 3s - loss: 0.0181 - val_loss: 0.0063

Epoch 12/50

 - 3s - loss: 0.0156 - val_loss: 0.0077

Epoch 13/50

 - 3s - loss: 0.0160 - val_loss: 0.0056

Epoch 14/50

 - 3s - loss: 0.0184 - val_loss: 0.0081

Epoch 15/50

 - 3s - loss: 0.0206 - val_loss: 0.0067

Epoch 16/50

 - 3s - loss: 0.0156 - val_loss: 0.0067

Epoch 17/50

 - 3s - loss: 0.0165 - val_loss: 0.0080

Epoch 18/50

 - 3s - loss: 0.0145 - val_loss: 0.0072

Epoch 19/50

 - 3s - loss: 0.0155 - val_loss: 0.0090

Epoch 20/50

 - 3s - loss: 0.0152 - val_loss: 0.0124

Epoch 21/50

 - 3s - loss: 0.0150 - val_loss: 0.0068

Epoch 22/50

 - 3s - loss: 0.0169 - val_loss: 0.0082

Epoch 23/50

 - 3s - loss: 0.0153 - val_loss: 0.0073

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 52541	action = 0	current_phase = 0	next_phase = 1	reward = -0.008462	array([[-2.1143837, -2.903942 ]], dtype=float32)

time = 52546	action = 0	current_phase = 0	next_phase = 1	reward = 0.066420	array([[-2.6249707, -3.3065424]], dtype=float32)

time = 52551	action = 1	current_phase = 0	next_phase = 1	reward = -1.381429	array([[-7.042316, -3.263657]], dtype=float32)

time = 52559	action = 1	current_phase = 1	next_phase = 0	reward = -0.769005	array([[-3.6777048, -2.3042998]], dtype=float32)

time = 52567	action = 0	current_phase = 0	next_phase = 1	reward = -0.082839	array([[-1.8086535, -2.821854 ]], dtype=float32)

time = 52572	action = 0	current_phase = 0	next_phase = 1	reward = -0.000250	array([[-2.1822634, -2.9333124]], dtype=float32)

time = 52577	action = 0	current_phase = 0	next_phase = 1	reward = 0.064998	array([[-2.6884336, -3.305841 ]], dtype=float32)

time = 52582	action = 1	current_phase = 0	next_phase = 1	reward = -1.583803	array([[-7.032637 , -3.5047836]], dtype=float32)

time = 52590	action = 1	current_phase = 1	next_phase = 0	reward = -0.948407	array([[-3.853585 , -2.4481616]], dtype=float32)

time = 52598	action = 0	current_phase = 0	next_phase = 1	reward = -0.044532	array([[-1.6661353, -2.8088598]], dtype=float32)

time = 52603	action = 0	current_phase = 0	next_phase = 1	reward = -0.004079	array([[-2.191893 , -2.9122343]], dtype=float32)

time = 52608	action = 0	current_phase = 0	next_phase = 1	reward = 0.075872	array([[-2.7502136, -3.3314772]], dtype=float32)

time = 52613	action = 1	current_phase = 0	next_phase = 1	reward = -1.938567	array([[-7.02446  , -3.6710358]], dtype=float32)

time = 52621	action = 1	current_phase = 1	next_phase = 0	reward = -1.042998	array([[-3.981387 , -2.2724733]], dtype=float32)

time = 52629	action = 0	current_phase = 0	next_phase = 1	reward = -0.033422	array([[-1.4371796, -2.056572 ]], dtype=float32)

time = 52634	action = 0	current_phase = 0	next_phase = 1	reward = 0.055076	array([[-1.8017595, -2.821814 ]], dtype=float32)

time = 52639	action = 1	current_phase = 0	next_phase = 1	reward = -0.757578	array([[-2.5957327, -2.3812246]], dtype=float32)

time = 52647	action = 1	current_phase = 1	next_phase = 0	reward = -0.931860	array([[-3.417764, -1.976571]], dtype=float32)

time = 52655	action = 0	current_phase = 0	next_phase = 1	reward = 0.189347	array([[-1.3325735, -3.0130243]], dtype=float32)

time = 52660	action = 0	current_phase = 0	next_phase = 1	reward = -0.008076	array([[-1.9636735, -2.096015 ]], dtype=float32)

time = 52665	action = 0	current_phase = 0	next_phase = 1	reward = -0.226657	array([[-2.338358 , -3.1018848]], dtype=float32)

time = 52670	action = 1	current_phase = 0	next_phase = 1	reward = -1.000639	array([[-5.954857 , -2.6865392]], dtype=float32)

time = 52678	action = 1	current_phase = 1	next_phase = 0	reward = -0.696026	array([[-3.527351 , -2.1453464]], dtype=float32)

time = 52686	action = 0	current_phase = 0	next_phase = 1	reward = -0.093563	array([[-1.7456598, -2.8151636]], dtype=float32)

time = 52691	action = 0	current_phase = 0	next_phase = 1	reward = -0.011597	array([[-2.1204362, -2.8957882]], dtype=float32)

time = 52696	action = 0	current_phase = 0	next_phase = 1	reward = 0.055282	array([[-2.519681 , -3.2346659]], dtype=float32)

time = 52701	action = 1	current_phase = 0	next_phase = 1	reward = -1.421854	array([[-7.0487733, -3.2521133]], dtype=float32)

time = 52709	action = 1	current_phase = 1	next_phase = 0	reward = -0.768871	array([[-3.6648521, -2.2819285]], dtype=float32)

time = 52717	action = 0	current_phase = 0	next_phase = 1	reward = -0.070566	array([[-1.7883644, -2.8187733]], dtype=float32)

time = 52722	action = 0	current_phase = 0	next_phase = 1	reward = -0.005598	array([[-2.215256 , -2.9341621]], dtype=float32)

time = 52727	action = 0	current_phase = 0	next_phase = 1	reward = 0.069276	array([[-2.774582 , -3.3407593]], dtype=float32)

time = 52732	action = 1	current_phase = 0	next_phase = 1	reward = -1.643540	array([[-7.0374217, -3.5528822]], dtype=float32)

time = 52740	action = 1	current_phase = 1	next_phase = 0	reward = -1.247335	array([[-3.8678753, -2.5014648]], dtype=float32)

time = 52748	action = 0	current_phase = 0	next_phase = 1	reward = 0.248615	array([[-1.5942942, -2.8018923]], dtype=float32)

time = 52753	action = 0	current_phase = 0	next_phase = 1	reward = 0.032079	array([[-2.2303085, -2.9268842]], dtype=float32)

time = 52758	action = 0	current_phase = 0	next_phase = 1	reward = 0.075826	array([[-2.7455726, -3.3670926]], dtype=float32)

time = 52763	action = 1	current_phase = 0	next_phase = 1	reward = -2.001328	array([[-7.041813 , -3.6403012]], dtype=float32)

time = 52771	action = 1	current_phase = 1	next_phase = 0	reward = -1.038932	array([[-4.0365286, -2.237201 ]], dtype=float32)

time = 52779	action = 0	current_phase = 0	next_phase = 1	reward = -0.040016	array([[-1.3569064, -2.0624647]], dtype=float32)

time = 52784	action = 0	current_phase = 0	next_phase = 1	reward = 0.313039	array([[-1.8150035, -2.8277867]], dtype=float32)

time = 52789	action = 1	current_phase = 0	next_phase = 1	reward = -0.911539	array([[-2.6226053, -2.5459027]], dtype=float32)

time = 52797	action = 1	current_phase = 1	next_phase = 0	reward = -0.642068	array([[-3.3039188, -1.947253 ]], dtype=float32)

time = 52805	action = 0	current_phase = 0	next_phase = 1	reward = -0.096884	array([[-1.592995 , -3.0103254]], dtype=float32)

time = 52810	action = 0	current_phase = 0	next_phase = 1	reward = -0.299741	array([[-1.977874, -2.06599 ]], dtype=float32)

time = 52815	action = 0	current_phase = 0	next_phase = 1	reward = 0.329231	array([[-2.1740117, -3.0213184]], dtype=float32)

time = 52820	action = 1	current_phase = 0	next_phase = 1	reward = -1.313166	array([[-5.9781413, -2.9757109]], dtype=float32)

time = 52828	action = 1	current_phase = 1	next_phase = 0	reward = -0.707413	array([[-3.5173306, -2.134429 ]], dtype=float32)

time = 52836	action = 0	current_phase = 0	next_phase = 1	reward = -0.096989	array([[-1.7369689, -2.8554535]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0183 - val_loss: 0.0078

Epoch 2/50

 - 3s - loss: 0.0177 - val_loss: 0.0075

Epoch 3/50

 - 3s - loss: 0.0159 - val_loss: 0.0065

Epoch 4/50

 - 3s - loss: 0.0193 - val_loss: 0.0061

Epoch 5/50

 - 3s - loss: 0.0177 - val_loss: 0.0075

Epoch 6/50

 - 3s - loss: 0.0165 - val_loss: 0.0071

Epoch 7/50

 - 3s - loss: 0.0146 - val_loss: 0.0083

Epoch 8/50

 - 3s - loss: 0.0123 - val_loss: 0.0106

Epoch 9/50

 - 3s - loss: 0.0176 - val_loss: 0.0073

Epoch 10/50

 - 3s - loss: 0.0163 - val_loss: 0.0083

Epoch 11/50

 - 3s - loss: 0.0146 - val_loss: 0.0070

Epoch 12/50

 - 3s - loss: 0.0115 - val_loss: 0.0094

Epoch 13/50

 - 3s - loss: 0.0186 - val_loss: 0.0063

Epoch 14/50

 - 3s - loss: 0.0142 - val_loss: 0.0080

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 52841	action = 0	current_phase = 0	next_phase = 1	reward = -0.020620	array([[-1.9530036, -2.873589 ]], dtype=float32)

time = 52846	action = 0	current_phase = 0	next_phase = 1	reward = 0.058262	array([[-2.5516343, -3.2459958]], dtype=float32)

time = 52851	action = 1	current_phase = 0	next_phase = 1	reward = -1.493891	array([[-7.050581 , -3.2985296]], dtype=float32)

time = 52859	action = 1	current_phase = 1	next_phase = 0	reward = -0.845626	array([[-3.7184482, -2.2893815]], dtype=float32)

time = 52867	action = 0	current_phase = 0	next_phase = 1	reward = -0.060456	array([[-1.7420106, -2.8285353]], dtype=float32)

time = 52872	action = 0	current_phase = 0	next_phase = 1	reward = 0.005351	array([[-2.1655555, -2.9607015]], dtype=float32)

time = 52877	action = 0	current_phase = 0	next_phase = 1	reward = 0.062755	array([[-2.7329488, -3.3494508]], dtype=float32)

time = 52882	action = 1	current_phase = 0	next_phase = 1	reward = -1.599087	array([[-7.0343075, -3.6086426]], dtype=float32)

time = 52890	action = 1	current_phase = 1	next_phase = 0	reward = -0.991988	array([[-3.9108338, -2.4381804]], dtype=float32)

time = 52898	action = 0	current_phase = 0	next_phase = 1	reward = -0.037430	array([[-1.7955971, -2.827745 ]], dtype=float32)

time = 52903	action = 0	current_phase = 0	next_phase = 1	reward = 0.029380	array([[-2.171999 , -2.9183648]], dtype=float32)

time = 52908	action = 0	current_phase = 0	next_phase = 1	reward = 0.065314	array([[-2.7536275, -3.4313712]], dtype=float32)

time = 52913	action = 1	current_phase = 0	next_phase = 1	reward = -1.992953	array([[-7.015831 , -3.6168437]], dtype=float32)

time = 52921	action = 1	current_phase = 1	next_phase = 0	reward = -1.031352	array([[-3.9954348, -2.2502077]], dtype=float32)

time = 52929	action = 0	current_phase = 0	next_phase = 1	reward = -0.029867	array([[-1.356389, -2.073563]], dtype=float32)

time = 52934	action = 0	current_phase = 0	next_phase = 1	reward = 0.045502	array([[-1.8933048, -2.872312 ]], dtype=float32)

time = 52939	action = 1	current_phase = 0	next_phase = 1	reward = -0.691952	array([[-2.4741664, -2.3319683]], dtype=float32)

time = 52947	action = 1	current_phase = 1	next_phase = 0	reward = -0.874617	array([[-3.406867, -1.984323]], dtype=float32)

time = 52955	action = 0	current_phase = 0	next_phase = 1	reward = 0.176060	array([[-1.390866, -3.04336 ]], dtype=float32)

time = 52960	action = 0	current_phase = 0	next_phase = 1	reward = -0.021178	array([[-1.8466022, -2.0819886]], dtype=float32)

time = 52965	action = 0	current_phase = 0	next_phase = 1	reward = -0.223213	array([[-2.3539565, -3.1384134]], dtype=float32)

time = 52970	action = 1	current_phase = 0	next_phase = 1	reward = -0.993997	array([[-5.9425335, -2.8034408]], dtype=float32)

time = 52978	action = 1	current_phase = 1	next_phase = 0	reward = -0.703743	array([[-3.4935412, -2.104883 ]], dtype=float32)

time = 52986	action = 0	current_phase = 0	next_phase = 1	reward = -0.077060	array([[-1.7722082, -2.8290849]], dtype=float32)

time = 52991	action = 0	current_phase = 0	next_phase = 1	reward = -0.011678	array([[-1.9668355, -2.890534 ]], dtype=float32)

time = 52996	action = 0	current_phase = 0	next_phase = 1	reward = 0.056098	array([[-2.4657905, -3.2259674]], dtype=float32)

time = 53001	action = 1	current_phase = 0	next_phase = 1	reward = -1.388248	array([[-7.033983 , -3.2734923]], dtype=float32)

time = 53009	action = 1	current_phase = 1	next_phase = 0	reward = -1.059217	array([[-3.665699 , -2.2148135]], dtype=float32)

time = 53017	action = 0	current_phase = 0	next_phase = 1	reward = 0.219045	array([[-1.6804497, -2.8143525]], dtype=float32)

time = 53022	action = 0	current_phase = 0	next_phase = 1	reward = -0.004632	array([[-2.1675177, -2.9315028]], dtype=float32)

time = 53027	action = 0	current_phase = 0	next_phase = 1	reward = 0.074555	array([[-2.760697 , -3.3698335]], dtype=float32)

time = 53032	action = 1	current_phase = 0	next_phase = 1	reward = -1.669566	array([[-7.016529 , -3.6344333]], dtype=float32)

time = 53040	action = 1	current_phase = 1	next_phase = 0	reward = -1.252810	array([[-3.9305706, -2.4028983]], dtype=float32)

time = 53048	action = 0	current_phase = 0	next_phase = 1	reward = 0.245520	array([[-1.7128  , -2.839477]], dtype=float32)

time = 53053	action = 0	current_phase = 0	next_phase = 1	reward = 0.025884	array([[-2.189935, -2.927016]], dtype=float32)

time = 53058	action = 0	current_phase = 0	next_phase = 1	reward = 0.083343	array([[-2.7798932, -3.4502153]], dtype=float32)

time = 53063	action = 1	current_phase = 0	next_phase = 1	reward = -1.339089	array([[-7.046843 , -3.3863406]], dtype=float32)

time = 53071	action = 1	current_phase = 1	next_phase = 0	reward = -1.497904	array([[-3.801148 , -2.3273196]], dtype=float32)

time = 53079	action = 0	current_phase = 0	next_phase = 1	reward = 0.233843	array([[-1.3711264, -2.1101727]], dtype=float32)

time = 53084	action = 0	current_phase = 0	next_phase = 1	reward = 0.013310	array([[-1.8389039, -2.8134265]], dtype=float32)

time = 53089	action = 1	current_phase = 0	next_phase = 1	reward = -0.672655	array([[-2.4277635, -2.3953414]], dtype=float32)

time = 53097	action = 1	current_phase = 1	next_phase = 0	reward = -1.196875	array([[-3.4693089, -2.1287086]], dtype=float32)

time = 53105	action = 0	current_phase = 0	next_phase = 1	reward = 0.465313	array([[-1.4067256, -3.0847046]], dtype=float32)

time = 53110	action = 0	current_phase = 0	next_phase = 1	reward = -0.028353	array([[-1.818525 , -2.0755231]], dtype=float32)

time = 53115	action = 0	current_phase = 0	next_phase = 1	reward = -0.233508	array([[-2.2666323, -3.061993 ]], dtype=float32)

time = 53120	action = 1	current_phase = 0	next_phase = 1	reward = -1.079901	array([[-5.941986 , -2.8422976]], dtype=float32)

time = 53128	action = 1	current_phase = 1	next_phase = 0	reward = -0.701410	array([[-3.5851297, -2.141788 ]], dtype=float32)

time = 53136	action = 0	current_phase = 0	next_phase = 1	reward = -0.095963	array([[-1.7569972, -2.8280873]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0133 - val_loss: 0.0072

Epoch 2/50

 - 3s - loss: 0.0167 - val_loss: 0.0070

Epoch 3/50

 - 3s - loss: 0.0216 - val_loss: 0.0098

Epoch 4/50

 - 3s - loss: 0.0136 - val_loss: 0.0085

Epoch 5/50

 - 3s - loss: 0.0136 - val_loss: 0.0077

Epoch 6/50

 - 3s - loss: 0.0117 - val_loss: 0.0081

Epoch 7/50

 - 3s - loss: 0.0152 - val_loss: 0.0074

Epoch 8/50

 - 3s - loss: 0.0118 - val_loss: 0.0073

Epoch 9/50

 - 3s - loss: 0.0138 - val_loss: 0.0068

Epoch 10/50

 - 3s - loss: 0.0126 - val_loss: 0.0077

Epoch 11/50

 - 3s - loss: 0.0127 - val_loss: 0.0074

Epoch 12/50

 - 3s - loss: 0.0138 - val_loss: 0.0074

Epoch 13/50

 - 3s - loss: 0.0119 - val_loss: 0.0089

Epoch 14/50

 - 3s - loss: 0.0133 - val_loss: 0.0090

Epoch 15/50

 - 3s - loss: 0.0200 - val_loss: 0.0076

Epoch 16/50

 - 3s - loss: 0.0108 - val_loss: 0.0084

Epoch 17/50

 - 3s - loss: 0.0130 - val_loss: 0.0087

Epoch 18/50

 - 3s - loss: 0.0143 - val_loss: 0.0077

Epoch 19/50

 - 3s - loss: 0.0151 - val_loss: 0.0078

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 53141	action = 0	current_phase = 0	next_phase = 1	reward = -0.016088	array([[-2.0558193, -2.8995752]], dtype=float32)

time = 53146	action = 0	current_phase = 0	next_phase = 1	reward = 0.058477	array([[-2.5146337, -3.2703028]], dtype=float32)

time = 53151	action = 1	current_phase = 0	next_phase = 1	reward = -1.395971	array([[-7.0258713, -3.2281613]], dtype=float32)

time = 53159	action = 1	current_phase = 1	next_phase = 0	reward = -0.839023	array([[-3.7167206, -2.2375987]], dtype=float32)

time = 53167	action = 0	current_phase = 0	next_phase = 1	reward = -0.091176	array([[-1.7654784, -2.8093987]], dtype=float32)

time = 53172	action = 0	current_phase = 0	next_phase = 1	reward = 0.005324	array([[-2.157202 , -2.9293475]], dtype=float32)

time = 53177	action = 0	current_phase = 0	next_phase = 1	reward = 0.063373	array([[-2.668976 , -3.3423405]], dtype=float32)

time = 53182	action = 1	current_phase = 0	next_phase = 1	reward = -1.700500	array([[-6.9950256, -3.6127648]], dtype=float32)

time = 53190	action = 1	current_phase = 1	next_phase = 0	reward = -0.925987	array([[-3.960383 , -2.4557452]], dtype=float32)

time = 53198	action = 0	current_phase = 0	next_phase = 1	reward = -0.044504	array([[-1.7985609, -2.805387 ]], dtype=float32)

time = 53203	action = 0	current_phase = 0	next_phase = 1	reward = 0.014894	array([[-2.1361372, -2.90806  ]], dtype=float32)

time = 53208	action = 0	current_phase = 0	next_phase = 1	reward = 0.074467	array([[-2.683382 , -3.3322482]], dtype=float32)

time = 53213	action = 1	current_phase = 0	next_phase = 1	reward = -0.802820	array([[-7.010619 , -2.9342294]], dtype=float32)

time = 53221	action = 1	current_phase = 1	next_phase = 0	reward = -1.291641	array([[-3.7836456, -2.3543236]], dtype=float32)

time = 53229	action = 0	current_phase = 0	next_phase = 1	reward = -0.051743	array([[-1.3247592, -2.0830255]], dtype=float32)

time = 53234	action = 0	current_phase = 0	next_phase = 1	reward = 0.014975	array([[-1.8242035, -2.821791 ]], dtype=float32)

time = 53239	action = 1	current_phase = 0	next_phase = 1	reward = -0.676674	array([[-2.345813 , -2.2981284]], dtype=float32)

time = 53247	action = 1	current_phase = 1	next_phase = 0	reward = -1.183996	array([[-3.4583588, -2.0497606]], dtype=float32)

time = 53255	action = 0	current_phase = 0	next_phase = 1	reward = 0.458971	array([[-1.4529644, -3.062677 ]], dtype=float32)

time = 53260	action = 0	current_phase = 0	next_phase = 1	reward = -0.032482	array([[-1.8240592, -2.0825453]], dtype=float32)

time = 53265	action = 0	current_phase = 0	next_phase = 1	reward = -0.241169	array([[-2.3732224, -3.141397 ]], dtype=float32)

time = 53270	action = 1	current_phase = 0	next_phase = 1	reward = -1.033937	array([[-5.933482, -2.725862]], dtype=float32)

time = 53278	action = 1	current_phase = 1	next_phase = 0	reward = -0.705779	array([[-3.6020594, -2.1270828]], dtype=float32)

time = 53286	action = 0	current_phase = 0	next_phase = 1	reward = -0.085797	array([[-1.7497787, -2.8140945]], dtype=float32)

time = 53291	action = 0	current_phase = 0	next_phase = 1	reward = -0.005279	array([[-2.04497  , -2.8921971]], dtype=float32)

time = 53296	action = 0	current_phase = 0	next_phase = 1	reward = 0.062042	array([[-2.560395, -3.267652]], dtype=float32)

time = 53301	action = 1	current_phase = 0	next_phase = 1	reward = -1.555674	array([[-7.043858 , -3.3040576]], dtype=float32)

time = 53309	action = 1	current_phase = 1	next_phase = 0	reward = -0.783329	array([[-3.7460766, -2.2517636]], dtype=float32)

time = 53317	action = 0	current_phase = 0	next_phase = 1	reward = -0.055501	array([[-1.759454 , -2.8163424]], dtype=float32)

time = 53322	action = 0	current_phase = 0	next_phase = 1	reward = 0.026168	array([[-2.1703994, -2.919067 ]], dtype=float32)

time = 53327	action = 0	current_phase = 0	next_phase = 1	reward = 0.074848	array([[-2.7360518, -3.415114 ]], dtype=float32)

time = 53332	action = 1	current_phase = 0	next_phase = 1	reward = -1.701813	array([[-7.0043983, -3.6130123]], dtype=float32)

time = 53340	action = 1	current_phase = 1	next_phase = 0	reward = -1.074403	array([[-3.97441  , -2.4560595]], dtype=float32)

time = 53348	action = 0	current_phase = 0	next_phase = 1	reward = -0.063420	array([[-1.8927495, -2.8694801]], dtype=float32)

time = 53353	action = 0	current_phase = 0	next_phase = 1	reward = 0.025238	array([[-2.113542 , -2.9051208]], dtype=float32)

time = 53358	action = 0	current_phase = 0	next_phase = 1	reward = 0.080180	array([[-2.6872005, -3.31773  ]], dtype=float32)

time = 53363	action = 1	current_phase = 0	next_phase = 1	reward = -1.266760	array([[-7.0254626, -3.4268656]], dtype=float32)

time = 53371	action = 1	current_phase = 1	next_phase = 0	reward = -1.419640	array([[-3.8396869, -2.3203764]], dtype=float32)

time = 53379	action = 0	current_phase = 0	next_phase = 1	reward = 0.254238	array([[-1.28982  , -2.0779757]], dtype=float32)

time = 53384	action = 0	current_phase = 0	next_phase = 1	reward = 0.039914	array([[-1.7899044, -2.7997055]], dtype=float32)

time = 53389	action = 1	current_phase = 0	next_phase = 1	reward = -0.626290	array([[-2.4112818, -2.3570948]], dtype=float32)

time = 53397	action = 1	current_phase = 1	next_phase = 0	reward = -1.198903	array([[-3.4499993, -2.0794592]], dtype=float32)

time = 53405	action = 0	current_phase = 0	next_phase = 1	reward = 0.462877	array([[-1.387634 , -3.0247712]], dtype=float32)

time = 53410	action = 0	current_phase = 0	next_phase = 1	reward = -0.037412	array([[-1.914547, -2.109629]], dtype=float32)

time = 53415	action = 0	current_phase = 0	next_phase = 1	reward = 0.028939	array([[-2.3547606, -3.133987 ]], dtype=float32)

time = 53420	action = 1	current_phase = 0	next_phase = 1	reward = -1.212386	array([[-5.94779  , -2.9487457]], dtype=float32)

time = 53428	action = 1	current_phase = 1	next_phase = 0	reward = -0.642224	array([[-3.524013 , -2.0979261]], dtype=float32)

time = 53436	action = 0	current_phase = 0	next_phase = 1	reward = -0.089956	array([[-1.7104025, -2.846696 ]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0184 - val_loss: 0.0054

Epoch 2/50

 - 3s - loss: 0.0178 - val_loss: 0.0054

Epoch 3/50

 - 3s - loss: 0.0135 - val_loss: 0.0080

Epoch 4/50

 - 3s - loss: 0.0175 - val_loss: 0.0052

Epoch 5/50

 - 3s - loss: 0.0141 - val_loss: 0.0052

Epoch 6/50

 - 3s - loss: 0.0167 - val_loss: 0.0065

Epoch 7/50

 - 3s - loss: 0.0151 - val_loss: 0.0058

Epoch 8/50

 - 3s - loss: 0.0128 - val_loss: 0.0069

Epoch 9/50

 - 3s - loss: 0.0159 - val_loss: 0.0074

Epoch 10/50

 - 3s - loss: 0.0147 - val_loss: 0.0076

Epoch 11/50

 - 3s - loss: 0.0139 - val_loss: 0.0071

Epoch 12/50

 - 3s - loss: 0.0121 - val_loss: 0.0096

Epoch 13/50

 - 3s - loss: 0.0153 - val_loss: 0.0069

Epoch 14/50

 - 3s - loss: 0.0116 - val_loss: 0.0072

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 53441	action = 0	current_phase = 0	next_phase = 1	reward = 0.001806	array([[-2.0780854, -2.8923357]], dtype=float32)

time = 53446	action = 0	current_phase = 0	next_phase = 1	reward = 0.074036	array([[-2.4910305, -3.2504847]], dtype=float32)

time = 53451	action = 1	current_phase = 0	next_phase = 1	reward = -1.448315	array([[-7.038949, -3.261057]], dtype=float32)

time = 53459	action = 1	current_phase = 1	next_phase = 0	reward = -0.782830	array([[-3.7343411, -2.2389684]], dtype=float32)

time = 53467	action = 0	current_phase = 0	next_phase = 1	reward = -0.057102	array([[-1.7630903, -2.819688 ]], dtype=float32)

time = 53472	action = 0	current_phase = 0	next_phase = 1	reward = 0.021017	array([[-2.1900897, -2.918985 ]], dtype=float32)

time = 53477	action = 0	current_phase = 0	next_phase = 1	reward = 0.068192	array([[-2.7554703, -3.3889928]], dtype=float32)

time = 53482	action = 1	current_phase = 0	next_phase = 1	reward = -1.695376	array([[-6.9942083, -3.6415606]], dtype=float32)

time = 53490	action = 1	current_phase = 1	next_phase = 0	reward = -1.010528	array([[-3.9625506, -2.4393244]], dtype=float32)

time = 53498	action = 0	current_phase = 0	next_phase = 1	reward = -0.066039	array([[-1.7745025, -2.8423226]], dtype=float32)

time = 53503	action = 0	current_phase = 0	next_phase = 1	reward = 0.013274	array([[-2.178535 , -2.9218025]], dtype=float32)

time = 53508	action = 0	current_phase = 0	next_phase = 1	reward = 0.067979	array([[-2.706279 , -3.3802733]], dtype=float32)

time = 53513	action = 1	current_phase = 0	next_phase = 1	reward = -1.918484	array([[-6.9946556, -3.6725585]], dtype=float32)

time = 53521	action = 1	current_phase = 1	next_phase = 0	reward = -1.382926	array([[-4.0763254, -2.3447537]], dtype=float32)

time = 53529	action = 0	current_phase = 0	next_phase = 1	reward = 0.263347	array([[-1.1892735, -2.0687633]], dtype=float32)

time = 53534	action = 0	current_phase = 0	next_phase = 1	reward = 0.030755	array([[-1.8366597, -2.830628 ]], dtype=float32)

time = 53539	action = 1	current_phase = 0	next_phase = 1	reward = -0.627144	array([[-2.509359 , -2.3348773]], dtype=float32)

time = 53547	action = 1	current_phase = 1	next_phase = 0	reward = -0.591652	array([[-3.37214  , -1.9485506]], dtype=float32)

time = 53555	action = 0	current_phase = 0	next_phase = 1	reward = -0.109352	array([[-1.4029646, -3.0966418]], dtype=float32)

time = 53560	action = 0	current_phase = 0	next_phase = 1	reward = -0.302941	array([[-1.7939136, -2.0599434]], dtype=float32)

time = 53565	action = 0	current_phase = 0	next_phase = 1	reward = 0.058907	array([[-2.1959624, -3.064002 ]], dtype=float32)

time = 53570	action = 1	current_phase = 0	next_phase = 1	reward = -1.076266	array([[-5.937145 , -2.8963213]], dtype=float32)

time = 53578	action = 1	current_phase = 1	next_phase = 0	reward = -0.683590	array([[-3.6612878, -2.146995 ]], dtype=float32)

time = 53586	action = 0	current_phase = 0	next_phase = 1	reward = -0.084330	array([[-1.7041417, -2.8437026]], dtype=float32)

time = 53591	action = 0	current_phase = 0	next_phase = 1	reward = -0.009853	array([[-2.0044746, -2.8933792]], dtype=float32)

time = 53596	action = 0	current_phase = 0	next_phase = 1	reward = 0.057267	array([[-2.539723 , -3.2776034]], dtype=float32)

time = 53601	action = 1	current_phase = 0	next_phase = 1	reward = -1.495417	array([[-7.028055 , -3.2997737]], dtype=float32)

time = 53609	action = 1	current_phase = 1	next_phase = 0	reward = -1.190197	array([[-3.7428737, -2.2349534]], dtype=float32)

time = 53617	action = 0	current_phase = 0	next_phase = 1	reward = 0.230670	array([[-1.6749113, -2.801107 ]], dtype=float32)

time = 53622	action = 0	current_phase = 0	next_phase = 1	reward = 0.008188	array([[-2.1508775, -2.937101 ]], dtype=float32)

time = 53627	action = 0	current_phase = 0	next_phase = 1	reward = 0.072592	array([[-2.7496548, -3.3807135]], dtype=float32)

time = 53632	action = 1	current_phase = 0	next_phase = 1	reward = -1.664160	array([[-7.0074005, -3.6221714]], dtype=float32)

time = 53640	action = 1	current_phase = 1	next_phase = 0	reward = -1.000421	array([[-3.961649 , -2.4194484]], dtype=float32)

time = 53648	action = 0	current_phase = 0	next_phase = 1	reward = -0.043156	array([[-1.7098002, -2.8143194]], dtype=float32)

time = 53653	action = 0	current_phase = 0	next_phase = 1	reward = 0.022337	array([[-2.1727257, -2.9132683]], dtype=float32)

time = 53658	action = 0	current_phase = 0	next_phase = 1	reward = 0.087390	array([[-2.6950681, -3.3638685]], dtype=float32)

time = 53663	action = 1	current_phase = 0	next_phase = 1	reward = -1.279041	array([[-7.0043926, -3.2644787]], dtype=float32)

time = 53671	action = 1	current_phase = 1	next_phase = 0	reward = -1.465651	array([[-3.9645748, -2.3929079]], dtype=float32)

time = 53679	action = 0	current_phase = 0	next_phase = 1	reward = 0.274744	array([[-1.3111236, -2.1099372]], dtype=float32)

time = 53684	action = 0	current_phase = 0	next_phase = 1	reward = 0.045532	array([[-1.9570943, -2.9210658]], dtype=float32)

time = 53689	action = 1	current_phase = 0	next_phase = 1	reward = -0.720824	array([[-2.398052, -2.305216]], dtype=float32)

time = 53697	action = 1	current_phase = 1	next_phase = 0	reward = -0.930481	array([[-3.5336866, -2.0777252]], dtype=float32)

time = 53705	action = 0	current_phase = 0	next_phase = 1	reward = -0.095466	array([[-1.3319557, -3.1121387]], dtype=float32)

time = 53710	action = 0	current_phase = 0	next_phase = 1	reward = -0.012788	array([[-1.7991755, -2.128617 ]], dtype=float32)

time = 53715	action = 0	current_phase = 0	next_phase = 1	reward = 0.323971	array([[-2.254582, -3.09564 ]], dtype=float32)

time = 53720	action = 1	current_phase = 0	next_phase = 1	reward = -1.383001	array([[-5.949237, -3.128774]], dtype=float32)

time = 53728	action = 1	current_phase = 1	next_phase = 0	reward = -0.707832	array([[-3.6464467, -2.153215 ]], dtype=float32)

time = 53736	action = 0	current_phase = 0	next_phase = 1	reward = -0.089086	array([[-1.7079914, -2.8063853]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0181 - val_loss: 0.0050

Epoch 2/50

 - 3s - loss: 0.0157 - val_loss: 0.0059

Epoch 3/50

 - 3s - loss: 0.0157 - val_loss: 0.0080

Epoch 4/50

 - 3s - loss: 0.0169 - val_loss: 0.0052

Epoch 5/50

 - 3s - loss: 0.0189 - val_loss: 0.0057

Epoch 6/50

 - 3s - loss: 0.0166 - val_loss: 0.0073

Epoch 7/50

 - 3s - loss: 0.0156 - val_loss: 0.0059

Epoch 8/50

 - 3s - loss: 0.0172 - val_loss: 0.0084

Epoch 9/50

 - 3s - loss: 0.0176 - val_loss: 0.0053

Epoch 10/50

 - 3s - loss: 0.0120 - val_loss: 0.0053

Epoch 11/50

 - 3s - loss: 0.0147 - val_loss: 0.0057

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 53741	action = 0	current_phase = 0	next_phase = 1	reward = 0.000009	array([[-2.047662 , -2.8657885]], dtype=float32)

time = 53746	action = 0	current_phase = 0	next_phase = 1	reward = 0.063216	array([[-2.4857025, -3.2357407]], dtype=float32)

time = 53751	action = 1	current_phase = 0	next_phase = 1	reward = -1.435178	array([[-7.0159993, -3.2134957]], dtype=float32)

time = 53759	action = 1	current_phase = 1	next_phase = 0	reward = -0.789736	array([[-3.7374191, -2.2190092]], dtype=float32)

time = 53767	action = 0	current_phase = 0	next_phase = 1	reward = -0.085875	array([[-1.7830715, -2.7918005]], dtype=float32)

time = 53772	action = 0	current_phase = 0	next_phase = 1	reward = 0.008373	array([[-2.141739 , -2.8836808]], dtype=float32)

time = 53777	action = 0	current_phase = 0	next_phase = 1	reward = 0.084780	array([[-2.710418 , -3.3415604]], dtype=float32)

time = 53782	action = 1	current_phase = 0	next_phase = 1	reward = -1.643532	array([[-7.004271 , -3.4883509]], dtype=float32)

time = 53790	action = 1	current_phase = 1	next_phase = 0	reward = -1.046151	array([[-3.9481127, -2.370802 ]], dtype=float32)

time = 53798	action = 0	current_phase = 0	next_phase = 1	reward = -0.059517	array([[-1.7085042, -2.7942219]], dtype=float32)

time = 53803	action = 0	current_phase = 0	next_phase = 1	reward = 0.012422	array([[-2.1033835, -2.8635252]], dtype=float32)

time = 53808	action = 0	current_phase = 0	next_phase = 1	reward = 0.075702	array([[-2.7176886, -3.3600893]], dtype=float32)

time = 53813	action = 1	current_phase = 0	next_phase = 1	reward = -1.802145	array([[-6.957865 , -3.5967731]], dtype=float32)

time = 53821	action = 1	current_phase = 1	next_phase = 0	reward = -1.294485	array([[-3.983119 , -2.2650328]], dtype=float32)

time = 53829	action = 0	current_phase = 0	next_phase = 1	reward = 0.264188	array([[-1.227521, -2.036003]], dtype=float32)

time = 53834	action = 0	current_phase = 0	next_phase = 1	reward = 0.034949	array([[-1.8517821, -2.8187537]], dtype=float32)

time = 53839	action = 1	current_phase = 0	next_phase = 1	reward = -0.643525	array([[-2.4752295, -2.3238785]], dtype=float32)

time = 53847	action = 1	current_phase = 1	next_phase = 0	reward = -0.608828	array([[-3.4710338, -2.0071871]], dtype=float32)

time = 53855	action = 0	current_phase = 0	next_phase = 1	reward = -0.393349	array([[-1.5658629, -3.0982118]], dtype=float32)

time = 53860	action = 0	current_phase = 0	next_phase = 1	reward = -0.292353	array([[-1.6404778, -2.053348 ]], dtype=float32)

time = 53865	action = 0	current_phase = 0	next_phase = 1	reward = 0.337253	array([[-2.1799781, -3.025086 ]], dtype=float32)

time = 53870	action = 1	current_phase = 0	next_phase = 1	reward = -1.066336	array([[-5.892001, -2.981042]], dtype=float32)

time = 53878	action = 1	current_phase = 1	next_phase = 0	reward = -0.701987	array([[-3.6650617, -2.1306882]], dtype=float32)

time = 53886	action = 0	current_phase = 0	next_phase = 1	reward = -0.078898	array([[-1.673428, -2.797285]], dtype=float32)

time = 53891	action = 0	current_phase = 0	next_phase = 1	reward = -0.007721	array([[-2.0031602, -2.8503928]], dtype=float32)

time = 53896	action = 0	current_phase = 0	next_phase = 1	reward = 0.053448	array([[-2.524204 , -3.2715907]], dtype=float32)

time = 53901	action = 1	current_phase = 0	next_phase = 1	reward = -1.439043	array([[-7.0265436, -3.254415 ]], dtype=float32)

time = 53909	action = 1	current_phase = 1	next_phase = 0	reward = -1.069350	array([[-3.7225666, -2.2039359]], dtype=float32)

time = 53917	action = 0	current_phase = 0	next_phase = 1	reward = 0.222255	array([[-1.7027898, -2.779255 ]], dtype=float32)

time = 53922	action = 0	current_phase = 0	next_phase = 1	reward = 0.006475	array([[-2.1037195, -2.8903084]], dtype=float32)

time = 53927	action = 0	current_phase = 0	next_phase = 1	reward = 0.061680	array([[-2.7089443, -3.328825 ]], dtype=float32)

time = 53932	action = 1	current_phase = 0	next_phase = 1	reward = -1.532222	array([[-6.9832287, -3.5471234]], dtype=float32)

time = 53940	action = 1	current_phase = 1	next_phase = 0	reward = -0.933583	array([[-3.8745615, -2.314629 ]], dtype=float32)

time = 53948	action = 0	current_phase = 0	next_phase = 1	reward = -0.033328	array([[-1.6827958, -2.790154 ]], dtype=float32)

time = 53953	action = 0	current_phase = 0	next_phase = 1	reward = 0.034844	array([[-2.1166394, -2.8750808]], dtype=float32)

time = 53958	action = 0	current_phase = 0	next_phase = 1	reward = 0.070213	array([[-2.746436 , -3.3546562]], dtype=float32)

time = 53963	action = 1	current_phase = 0	next_phase = 1	reward = -1.413453	array([[-6.9923787, -3.1924505]], dtype=float32)

time = 53971	action = 1	current_phase = 1	next_phase = 0	reward = -1.423869	array([[-3.8651915, -2.3371296]], dtype=float32)

time = 53979	action = 0	current_phase = 0	next_phase = 1	reward = 0.255955	array([[-1.2175802, -2.0623212]], dtype=float32)

time = 53984	action = 0	current_phase = 0	next_phase = 1	reward = 0.016403	array([[-1.7198689, -2.7760034]], dtype=float32)

time = 53989	action = 1	current_phase = 0	next_phase = 1	reward = -0.703295	array([[-2.39856  , -2.3135967]], dtype=float32)

time = 53997	action = 1	current_phase = 1	next_phase = 0	reward = -1.185646	array([[-3.476108 , -1.9880221]], dtype=float32)

time = 54005	action = 0	current_phase = 0	next_phase = 1	reward = 0.482922	array([[-1.1230334, -3.116578 ]], dtype=float32)

time = 54010	action = 0	current_phase = 0	next_phase = 1	reward = -0.008513	array([[-1.7806783, -2.0432386]], dtype=float32)

time = 54015	action = 0	current_phase = 0	next_phase = 1	reward = 0.060734	array([[-2.3103085, -3.0902324]], dtype=float32)

time = 54020	action = 1	current_phase = 0	next_phase = 1	reward = -1.365879	array([[-5.932404 , -2.9990091]], dtype=float32)

time = 54028	action = 1	current_phase = 1	next_phase = 0	reward = -0.712412	array([[-3.655663 , -2.1427076]], dtype=float32)

time = 54036	action = 0	current_phase = 0	next_phase = 1	reward = -0.091562	array([[-1.7252707, -2.8061023]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0134 - val_loss: 0.0057

Epoch 2/50

 - 3s - loss: 0.0174 - val_loss: 0.0089

Epoch 3/50

 - 3s - loss: 0.0128 - val_loss: 0.0083

Epoch 4/50

 - 3s - loss: 0.0141 - val_loss: 0.0100

Epoch 5/50

 - 3s - loss: 0.0152 - val_loss: 0.0084

Epoch 6/50

 - 3s - loss: 0.0121 - val_loss: 0.0061

Epoch 7/50

 - 3s - loss: 0.0189 - val_loss: 0.0059

Epoch 8/50

 - 3s - loss: 0.0118 - val_loss: 0.0076

Epoch 9/50

 - 3s - loss: 0.0137 - val_loss: 0.0067

Epoch 10/50

 - 3s - loss: 0.0130 - val_loss: 0.0057

Epoch 11/50

 - 3s - loss: 0.0137 - val_loss: 0.0070

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 54041	action = 0	current_phase = 0	next_phase = 1	reward = -0.012839	array([[-2.1305375, -2.857819 ]], dtype=float32)

time = 54046	action = 0	current_phase = 0	next_phase = 1	reward = 0.047892	array([[-2.614407 , -3.2463036]], dtype=float32)

time = 54051	action = 1	current_phase = 0	next_phase = 1	reward = -1.435474	array([[-7.0424585, -3.3175817]], dtype=float32)

time = 54059	action = 1	current_phase = 1	next_phase = 0	reward = -0.841537	array([[-3.748461 , -2.2716532]], dtype=float32)

time = 54067	action = 0	current_phase = 0	next_phase = 1	reward = -0.073072	array([[-1.820859 , -2.7888162]], dtype=float32)

time = 54072	action = 0	current_phase = 0	next_phase = 1	reward = 0.026122	array([[-2.2217822, -2.8897634]], dtype=float32)

time = 54077	action = 0	current_phase = 0	next_phase = 1	reward = 0.075152	array([[-2.7708054, -3.3545718]], dtype=float32)

time = 54082	action = 1	current_phase = 0	next_phase = 1	reward = -1.708093	array([[-7.0009384, -3.6294456]], dtype=float32)

time = 54090	action = 1	current_phase = 1	next_phase = 0	reward = -1.304054	array([[-3.9472713, -2.4339328]], dtype=float32)

time = 54098	action = 0	current_phase = 0	next_phase = 1	reward = 0.233474	array([[-1.70948 , -2.770664]], dtype=float32)

time = 54103	action = 0	current_phase = 0	next_phase = 1	reward = 0.014048	array([[-2.2154312, -2.8776197]], dtype=float32)

time = 54108	action = 0	current_phase = 0	next_phase = 1	reward = 0.072397	array([[-2.7735386, -3.3855052]], dtype=float32)

time = 54113	action = 1	current_phase = 0	next_phase = 1	reward = -1.778365	array([[-6.996269 , -3.6396365]], dtype=float32)

time = 54121	action = 1	current_phase = 1	next_phase = 0	reward = -1.336401	array([[-4.1095977, -2.3017845]], dtype=float32)

time = 54129	action = 0	current_phase = 0	next_phase = 1	reward = 0.266980	array([[-1.218916 , -2.0590863]], dtype=float32)

time = 54134	action = 0	current_phase = 0	next_phase = 1	reward = 0.032275	array([[-1.9412163, -2.8214488]], dtype=float32)

time = 54139	action = 1	current_phase = 0	next_phase = 1	reward = -0.691819	array([[-2.4103184, -2.3378024]], dtype=float32)

time = 54147	action = 1	current_phase = 1	next_phase = 0	reward = -0.637460	array([[-3.4848194, -1.9851189]], dtype=float32)

time = 54155	action = 0	current_phase = 0	next_phase = 1	reward = -0.095224	array([[-1.4631972, -3.043941 ]], dtype=float32)

time = 54160	action = 0	current_phase = 0	next_phase = 1	reward = -0.013566	array([[-1.8038663, -2.0507407]], dtype=float32)

time = 54165	action = 0	current_phase = 0	next_phase = 1	reward = 0.046162	array([[-2.2466745, -3.0504732]], dtype=float32)

time = 54170	action = 1	current_phase = 0	next_phase = 1	reward = -1.260506	array([[-5.936014, -3.043818]], dtype=float32)

time = 54178	action = 1	current_phase = 1	next_phase = 0	reward = -0.702369	array([[-3.6298184, -2.1692564]], dtype=float32)

time = 54186	action = 0	current_phase = 0	next_phase = 1	reward = -0.078504	array([[-1.7953875, -2.7946177]], dtype=float32)

time = 54191	action = 0	current_phase = 0	next_phase = 1	reward = 0.006714	array([[-2.1223154, -2.8827782]], dtype=float32)

time = 54196	action = 0	current_phase = 0	next_phase = 1	reward = 0.072898	array([[-2.6429086, -3.3229415]], dtype=float32)

time = 54201	action = 1	current_phase = 0	next_phase = 1	reward = -1.452776	array([[-7.0344515, -3.329249 ]], dtype=float32)

time = 54209	action = 1	current_phase = 1	next_phase = 0	reward = -0.775239	array([[-3.7150402, -2.221497 ]], dtype=float32)

time = 54217	action = 0	current_phase = 0	next_phase = 1	reward = -0.073116	array([[-1.8391563, -2.7838695]], dtype=float32)

time = 54222	action = 0	current_phase = 0	next_phase = 1	reward = 0.008335	array([[-2.2219665, -2.8911602]], dtype=float32)

time = 54227	action = 0	current_phase = 0	next_phase = 1	reward = 0.073945	array([[-2.7795978, -3.378476 ]], dtype=float32)

time = 54232	action = 1	current_phase = 0	next_phase = 1	reward = -1.654507	array([[-6.9940157, -3.6404777]], dtype=float32)

time = 54240	action = 1	current_phase = 1	next_phase = 0	reward = -0.948422	array([[-3.9252453, -2.4075534]], dtype=float32)

time = 54248	action = 0	current_phase = 0	next_phase = 1	reward = -0.056089	array([[-1.7254298, -2.791031 ]], dtype=float32)

time = 54253	action = 0	current_phase = 0	next_phase = 1	reward = 0.009766	array([[-2.2123737, -2.8855267]], dtype=float32)

time = 54258	action = 0	current_phase = 0	next_phase = 1	reward = 0.065974	array([[-2.7838612, -3.35034  ]], dtype=float32)

time = 54263	action = 1	current_phase = 0	next_phase = 1	reward = -1.837375	array([[-6.992523 , -3.6084979]], dtype=float32)

time = 54271	action = 1	current_phase = 1	next_phase = 0	reward = -1.037000	array([[-4.064762 , -2.3457398]], dtype=float32)

time = 54279	action = 0	current_phase = 0	next_phase = 1	reward = -0.038525	array([[-1.3083806, -2.0599794]], dtype=float32)

time = 54284	action = 0	current_phase = 0	next_phase = 1	reward = 0.047825	array([[-1.8431107, -2.816962 ]], dtype=float32)

time = 54289	action = 1	current_phase = 0	next_phase = 1	reward = -0.644672	array([[-2.4301965, -2.379935 ]], dtype=float32)

time = 54297	action = 1	current_phase = 1	next_phase = 0	reward = -0.633876	array([[-3.455021 , -1.9791182]], dtype=float32)

time = 54305	action = 0	current_phase = 0	next_phase = 1	reward = -0.372004	array([[-1.6399229, -3.022876 ]], dtype=float32)

time = 54310	action = 0	current_phase = 0	next_phase = 1	reward = 0.255491	array([[-1.710437 , -2.0700502]], dtype=float32)

time = 54315	action = 0	current_phase = 0	next_phase = 1	reward = 0.038563	array([[-2.457117 , -3.1763382]], dtype=float32)

time = 54320	action = 1	current_phase = 0	next_phase = 1	reward = -1.326312	array([[-5.92554  , -3.0385318]], dtype=float32)

time = 54328	action = 1	current_phase = 1	next_phase = 0	reward = -0.717676	array([[-3.6406853, -2.1555343]], dtype=float32)

time = 54336	action = 0	current_phase = 0	next_phase = 1	reward = -0.094333	array([[-1.7617147, -2.790406 ]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0186 - val_loss: 0.0075

Epoch 2/50

 - 3s - loss: 0.0169 - val_loss: 0.0071

Epoch 3/50

 - 3s - loss: 0.0147 - val_loss: 0.0086

Epoch 4/50

 - 3s - loss: 0.0149 - val_loss: 0.0070

Epoch 5/50

 - 3s - loss: 0.0204 - val_loss: 0.0070

Epoch 6/50

 - 3s - loss: 0.0157 - val_loss: 0.0108

Epoch 7/50

 - 3s - loss: 0.0161 - val_loss: 0.0076

Epoch 8/50

 - 3s - loss: 0.0165 - val_loss: 0.0060

Epoch 9/50

 - 3s - loss: 0.0158 - val_loss: 0.0068

Epoch 10/50

 - 3s - loss: 0.0155 - val_loss: 0.0071

Epoch 11/50

 - 3s - loss: 0.0167 - val_loss: 0.0072

Epoch 12/50

 - 3s - loss: 0.0182 - val_loss: 0.0064

Epoch 13/50

 - 3s - loss: 0.0135 - val_loss: 0.0064

Epoch 14/50

 - 3s - loss: 0.0150 - val_loss: 0.0066

Epoch 15/50

 - 3s - loss: 0.0166 - val_loss: 0.0086

Epoch 16/50

 - 3s - loss: 0.0143 - val_loss: 0.0076

Epoch 17/50

 - 3s - loss: 0.0138 - val_loss: 0.0079

Epoch 18/50

 - 3s - loss: 0.0138 - val_loss: 0.0116

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 54341	action = 0	current_phase = 0	next_phase = 1	reward = -0.025287	array([[-2.0965955, -2.8637218]], dtype=float32)

time = 54346	action = 0	current_phase = 0	next_phase = 1	reward = 0.046498	array([[-2.5642986, -3.2639759]], dtype=float32)

time = 54351	action = 1	current_phase = 0	next_phase = 1	reward = -1.404379	array([[-7.0515356, -3.2974796]], dtype=float32)

time = 54359	action = 1	current_phase = 1	next_phase = 0	reward = -0.754370	array([[-3.697004 , -2.2289586]], dtype=float32)

time = 54367	action = 0	current_phase = 0	next_phase = 1	reward = -0.061446	array([[-1.8062422, -2.804264 ]], dtype=float32)

time = 54372	action = 0	current_phase = 0	next_phase = 1	reward = -0.003612	array([[-2.2229228, -2.8994837]], dtype=float32)

time = 54377	action = 0	current_phase = 0	next_phase = 1	reward = 0.060546	array([[-2.769651 , -3.3691907]], dtype=float32)

time = 54382	action = 1	current_phase = 0	next_phase = 1	reward = -1.518715	array([[-7.02462  , -3.5646555]], dtype=float32)

time = 54390	action = 1	current_phase = 1	next_phase = 0	reward = -0.993834	array([[-3.9589467, -2.4955077]], dtype=float32)

time = 54398	action = 0	current_phase = 0	next_phase = 1	reward = -0.057014	array([[-1.8170656, -2.801445 ]], dtype=float32)

time = 54403	action = 0	current_phase = 0	next_phase = 1	reward = 0.019089	array([[-2.1994839, -2.8777635]], dtype=float32)

time = 54408	action = 0	current_phase = 0	next_phase = 1	reward = 0.078168	array([[-2.7845979, -3.4229944]], dtype=float32)

time = 54413	action = 1	current_phase = 0	next_phase = 1	reward = -1.924094	array([[-7.0084405, -3.6457639]], dtype=float32)

time = 54421	action = 1	current_phase = 1	next_phase = 0	reward = -1.005989	array([[-4.0183644, -2.1957371]], dtype=float32)

time = 54429	action = 0	current_phase = 0	next_phase = 1	reward = -0.044077	array([[-1.5300469, -2.0315912]], dtype=float32)

time = 54434	action = 0	current_phase = 0	next_phase = 1	reward = 0.038322	array([[-1.9660681, -2.8280897]], dtype=float32)

time = 54439	action = 1	current_phase = 0	next_phase = 1	reward = -0.714497	array([[-2.5131855, -2.3328116]], dtype=float32)

time = 54447	action = 1	current_phase = 1	next_phase = 0	reward = -0.882613	array([[-3.4986498, -2.0553238]], dtype=float32)

time = 54455	action = 0	current_phase = 0	next_phase = 1	reward = 0.177307	array([[-1.4649829, -3.00773  ]], dtype=float32)

time = 54460	action = 0	current_phase = 0	next_phase = 1	reward = -0.569654	array([[-1.9797773, -2.0629451]], dtype=float32)

time = 54465	action = 0	current_phase = 0	next_phase = 1	reward = 0.348106	array([[-2.1871967, -2.99689  ]], dtype=float32)

time = 54470	action = 1	current_phase = 0	next_phase = 1	reward = -1.106329	array([[-5.93401  , -2.8485465]], dtype=float32)

time = 54478	action = 1	current_phase = 1	next_phase = 0	reward = -0.701094	array([[-3.6383357, -2.1597471]], dtype=float32)

time = 54486	action = 0	current_phase = 0	next_phase = 1	reward = -0.089421	array([[-1.8442743, -2.8084333]], dtype=float32)

time = 54491	action = 0	current_phase = 0	next_phase = 1	reward = -0.009850	array([[-2.1419566, -2.882757 ]], dtype=float32)

time = 54496	action = 0	current_phase = 0	next_phase = 1	reward = 0.066286	array([[-2.6454499, -3.2976174]], dtype=float32)

time = 54501	action = 1	current_phase = 0	next_phase = 1	reward = -1.490587	array([[-7.047734 , -3.3542862]], dtype=float32)

time = 54509	action = 1	current_phase = 1	next_phase = 0	reward = -0.847613	array([[-3.782689 , -2.2936428]], dtype=float32)

time = 54517	action = 0	current_phase = 0	next_phase = 1	reward = -0.077638	array([[-1.8077712, -2.805313 ]], dtype=float32)

time = 54522	action = 0	current_phase = 0	next_phase = 1	reward = 0.013274	array([[-2.2388277, -2.8914661]], dtype=float32)

time = 54527	action = 0	current_phase = 0	next_phase = 1	reward = 0.063851	array([[-2.793149 , -3.3740668]], dtype=float32)

time = 54532	action = 1	current_phase = 0	next_phase = 1	reward = -1.654823	array([[-7.037281, -3.535365]], dtype=float32)

time = 54540	action = 1	current_phase = 1	next_phase = 0	reward = -0.993804	array([[-3.9020662, -2.409426 ]], dtype=float32)

time = 54548	action = 0	current_phase = 0	next_phase = 1	reward = -0.056358	array([[-1.8048284, -2.8273814]], dtype=float32)

time = 54553	action = 0	current_phase = 0	next_phase = 1	reward = 0.027432	array([[-2.1801288, -2.8797774]], dtype=float32)

time = 54558	action = 0	current_phase = 0	next_phase = 1	reward = 0.075825	array([[-2.7941782, -3.3891811]], dtype=float32)

time = 54563	action = 1	current_phase = 0	next_phase = 1	reward = -1.924498	array([[-7.001258, -3.719563]], dtype=float32)

time = 54571	action = 1	current_phase = 1	next_phase = 0	reward = -1.099352	array([[-4.0850086, -2.2520375]], dtype=float32)

time = 54579	action = 0	current_phase = 0	next_phase = 1	reward = -0.042842	array([[-1.4919186, -2.0596364]], dtype=float32)

time = 54584	action = 0	current_phase = 0	next_phase = 1	reward = 0.013427	array([[-1.8897675, -2.8166478]], dtype=float32)

time = 54589	action = 1	current_phase = 0	next_phase = 1	reward = -0.710831	array([[-2.4611835, -2.2879736]], dtype=float32)

time = 54597	action = 1	current_phase = 1	next_phase = 0	reward = -0.642198	array([[-3.4794042, -1.9863557]], dtype=float32)

time = 54605	action = 0	current_phase = 0	next_phase = 1	reward = -0.661762	array([[-1.5696112, -3.0251403]], dtype=float32)

time = 54610	action = 0	current_phase = 0	next_phase = 1	reward = 0.537866	array([[-1.7697188, -2.0693388]], dtype=float32)

time = 54615	action = 0	current_phase = 0	next_phase = 1	reward = -0.243548	array([[-2.4371588, -3.1579356]], dtype=float32)

time = 54620	action = 1	current_phase = 0	next_phase = 1	reward = -1.077816	array([[-5.956731 , -2.8092587]], dtype=float32)

time = 54628	action = 1	current_phase = 1	next_phase = 0	reward = -0.717092	array([[-3.6580257, -2.1604564]], dtype=float32)

time = 54636	action = 0	current_phase = 0	next_phase = 1	reward = -0.094220	array([[-1.7951393, -2.824014 ]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0163 - val_loss: 0.0092

Epoch 2/50

 - 3s - loss: 0.0135 - val_loss: 0.0073

Epoch 3/50

 - 3s - loss: 0.0140 - val_loss: 0.0071

Epoch 4/50

 - 3s - loss: 0.0130 - val_loss: 0.0055

Epoch 5/50

 - 3s - loss: 0.0112 - val_loss: 0.0104

Epoch 6/50

 - 3s - loss: 0.0128 - val_loss: 0.0082

Epoch 7/50

 - 3s - loss: 0.0143 - val_loss: 0.0071

Epoch 8/50

 - 3s - loss: 0.0109 - val_loss: 0.0066

Epoch 9/50

 - 3s - loss: 0.0153 - val_loss: 0.0067

Epoch 10/50

 - 3s - loss: 0.0142 - val_loss: 0.0079

Epoch 11/50

 - 3s - loss: 0.0119 - val_loss: 0.0093

Epoch 12/50

 - 3s - loss: 0.0105 - val_loss: 0.0069

Epoch 13/50

 - 3s - loss: 0.0142 - val_loss: 0.0077

Epoch 14/50

 - 3s - loss: 0.0128 - val_loss: 0.0081

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 54641	action = 0	current_phase = 0	next_phase = 1	reward = -0.012469	array([[-2.098063 , -2.8781965]], dtype=float32)

time = 54646	action = 0	current_phase = 0	next_phase = 1	reward = 0.049887	array([[-2.5586185, -3.3293893]], dtype=float32)

time = 54651	action = 1	current_phase = 0	next_phase = 1	reward = -1.334562	array([[-7.061378 , -3.2615228]], dtype=float32)

time = 54659	action = 1	current_phase = 1	next_phase = 0	reward = -1.101245	array([[-3.6868167, -2.246817 ]], dtype=float32)

time = 54667	action = 0	current_phase = 0	next_phase = 1	reward = 0.228650	array([[-1.6774905, -2.8144903]], dtype=float32)

time = 54672	action = 0	current_phase = 0	next_phase = 1	reward = 0.008938	array([[-2.2033908, -2.9081438]], dtype=float32)

time = 54677	action = 0	current_phase = 0	next_phase = 1	reward = 0.063978	array([[-2.7977467, -3.4125855]], dtype=float32)

time = 54682	action = 1	current_phase = 0	next_phase = 1	reward = -1.603927	array([[-7.0309525, -3.5762453]], dtype=float32)

time = 54690	action = 1	current_phase = 1	next_phase = 0	reward = -0.987564	array([[-3.8781223, -2.4387832]], dtype=float32)

time = 54698	action = 0	current_phase = 0	next_phase = 1	reward = -0.046099	array([[-1.839632 , -2.8341444]], dtype=float32)

time = 54703	action = 0	current_phase = 0	next_phase = 1	reward = 0.045335	array([[-2.1911266, -2.8944616]], dtype=float32)

time = 54708	action = 0	current_phase = 0	next_phase = 1	reward = 0.085619	array([[-2.791412 , -3.4418108]], dtype=float32)

time = 54713	action = 1	current_phase = 0	next_phase = 1	reward = -1.346809	array([[-7.0665536, -3.377105 ]], dtype=float32)

time = 54721	action = 1	current_phase = 1	next_phase = 0	reward = -1.500179	array([[-3.8093653, -2.2891083]], dtype=float32)

time = 54729	action = 0	current_phase = 0	next_phase = 1	reward = 0.212752	array([[-1.3151196, -2.102092 ]], dtype=float32)

time = 54734	action = 0	current_phase = 0	next_phase = 1	reward = 0.011503	array([[-1.8940576, -2.8355627]], dtype=float32)

time = 54739	action = 0	current_phase = 0	next_phase = 1	reward = 0.082986	array([[-2.3727531, -2.372835 ]], dtype=float32)

time = 54744	action = 1	current_phase = 0	next_phase = 1	reward = -0.917037	array([[-6.966599, -2.981674]], dtype=float32)

time = 54752	action = 1	current_phase = 1	next_phase = 0	reward = -0.713227	array([[-3.4215493, -2.241568 ]], dtype=float32)

time = 54760	action = 0	current_phase = 0	next_phase = 1	reward = -0.298267	array([[-1.9114723, -2.1316836]], dtype=float32)

time = 54765	action = 0	current_phase = 0	next_phase = 1	reward = 0.343017	array([[-2.0765948, -2.9730978]], dtype=float32)

time = 54770	action = 1	current_phase = 0	next_phase = 1	reward = -1.252236	array([[-5.9705334, -3.027972 ]], dtype=float32)

time = 54778	action = 1	current_phase = 1	next_phase = 0	reward = -0.708318	array([[-3.4633508, -2.078091 ]], dtype=float32)

time = 54786	action = 0	current_phase = 0	next_phase = 1	reward = -0.084027	array([[-1.8225275, -2.8234124]], dtype=float32)

time = 54791	action = 0	current_phase = 0	next_phase = 1	reward = -0.005586	array([[-2.1083245, -2.8899076]], dtype=float32)

time = 54796	action = 0	current_phase = 0	next_phase = 1	reward = 0.063291	array([[-2.674036 , -3.3674135]], dtype=float32)

time = 54801	action = 1	current_phase = 0	next_phase = 1	reward = -1.508661	array([[-7.0790424, -3.300629 ]], dtype=float32)

time = 54809	action = 1	current_phase = 1	next_phase = 0	reward = -0.796365	array([[-3.7308793, -2.3083906]], dtype=float32)

time = 54817	action = 0	current_phase = 0	next_phase = 1	reward = -0.078683	array([[-1.7415744, -2.8178055]], dtype=float32)

time = 54822	action = 0	current_phase = 0	next_phase = 1	reward = 0.009287	array([[-2.226399, -2.899313]], dtype=float32)

time = 54827	action = 0	current_phase = 0	next_phase = 1	reward = 0.074216	array([[-2.7929335, -3.4155576]], dtype=float32)

time = 54832	action = 1	current_phase = 0	next_phase = 1	reward = -1.644218	array([[-7.0457  , -3.478728]], dtype=float32)

time = 54840	action = 1	current_phase = 1	next_phase = 0	reward = -0.938962	array([[-3.8931785, -2.3848298]], dtype=float32)

time = 54848	action = 0	current_phase = 0	next_phase = 1	reward = -0.042563	array([[-1.7692213, -2.8298225]], dtype=float32)

time = 54853	action = 0	current_phase = 0	next_phase = 1	reward = 0.014229	array([[-2.1680946, -2.898507 ]], dtype=float32)

time = 54858	action = 0	current_phase = 0	next_phase = 1	reward = 0.073707	array([[-2.8470702, -3.393809 ]], dtype=float32)

time = 54863	action = 1	current_phase = 0	next_phase = 1	reward = -1.838986	array([[-7.0208683, -3.6501327]], dtype=float32)

time = 54871	action = 1	current_phase = 1	next_phase = 0	reward = -1.629518	array([[-4.067834 , -2.1681633]], dtype=float32)

time = 54879	action = 0	current_phase = 0	next_phase = 1	reward = 0.558783	array([[-1.3416231, -2.0969007]], dtype=float32)

time = 54884	action = 0	current_phase = 0	next_phase = 1	reward = 0.301978	array([[-1.8706166, -2.8688266]], dtype=float32)

time = 54889	action = 1	current_phase = 0	next_phase = 1	reward = -0.915800	array([[-2.5054092, -2.2874398]], dtype=float32)

time = 54897	action = 1	current_phase = 1	next_phase = 0	reward = -0.635933	array([[-3.371262 , -1.9843102]], dtype=float32)

time = 54905	action = 0	current_phase = 0	next_phase = 1	reward = -0.109433	array([[-1.3731923, -3.1373026]], dtype=float32)

time = 54910	action = 0	current_phase = 0	next_phase = 1	reward = -0.040962	array([[-1.9378155, -2.0425842]], dtype=float32)

time = 54915	action = 0	current_phase = 0	next_phase = 1	reward = 0.011379	array([[-2.478232, -3.183957]], dtype=float32)

time = 54920	action = 1	current_phase = 0	next_phase = 1	reward = -1.377265	array([[-5.97181  , -3.0337121]], dtype=float32)

time = 54928	action = 1	current_phase = 1	next_phase = 0	reward = -0.701832	array([[-3.6080623, -2.140338 ]], dtype=float32)

time = 54936	action = 0	current_phase = 0	next_phase = 1	reward = -0.093035	array([[-1.694763 , -2.8762133]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0153 - val_loss: 0.0057

Epoch 2/50

 - 3s - loss: 0.0139 - val_loss: 0.0065

Epoch 3/50

 - 3s - loss: 0.0175 - val_loss: 0.0073

Epoch 4/50

 - 3s - loss: 0.0187 - val_loss: 0.0060

Epoch 5/50

 - 3s - loss: 0.0166 - val_loss: 0.0066

Epoch 6/50

 - 3s - loss: 0.0146 - val_loss: 0.0103

Epoch 7/50

 - 3s - loss: 0.0125 - val_loss: 0.0059

Epoch 8/50

 - 3s - loss: 0.0174 - val_loss: 0.0079

Epoch 9/50

 - 3s - loss: 0.0146 - val_loss: 0.0067

Epoch 10/50

 - 3s - loss: 0.0148 - val_loss: 0.0067

Epoch 11/50

 - 3s - loss: 0.0176 - val_loss: 0.0076

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 54941	action = 0	current_phase = 0	next_phase = 1	reward = -0.008669	array([[-2.0852268, -2.8767743]], dtype=float32)

time = 54946	action = 0	current_phase = 0	next_phase = 1	reward = 0.063161	array([[-2.5211103, -3.2864206]], dtype=float32)

time = 54951	action = 1	current_phase = 0	next_phase = 1	reward = -1.383061	array([[-7.0704646, -3.2176821]], dtype=float32)

time = 54959	action = 1	current_phase = 1	next_phase = 0	reward = -0.780715	array([[-3.6910586, -2.2723114]], dtype=float32)

time = 54967	action = 0	current_phase = 0	next_phase = 1	reward = -0.074901	array([[-1.7183962, -2.8144932]], dtype=float32)

time = 54972	action = 0	current_phase = 0	next_phase = 1	reward = -0.003597	array([[-2.178491 , -2.8910804]], dtype=float32)

time = 54977	action = 0	current_phase = 0	next_phase = 1	reward = 0.062020	array([[-2.7579153, -3.378895 ]], dtype=float32)

time = 54982	action = 1	current_phase = 0	next_phase = 1	reward = -1.536786	array([[-7.0547476, -3.4816186]], dtype=float32)

time = 54990	action = 1	current_phase = 1	next_phase = 0	reward = -1.288620	array([[-3.926079, -2.458405]], dtype=float32)

time = 54998	action = 0	current_phase = 0	next_phase = 1	reward = 0.232690	array([[-1.578675 , -2.8157012]], dtype=float32)

time = 55003	action = 0	current_phase = 0	next_phase = 1	reward = 0.030339	array([[-2.1758854, -2.8904057]], dtype=float32)

time = 55008	action = 0	current_phase = 0	next_phase = 1	reward = 0.076940	array([[-2.7578394, -3.3980122]], dtype=float32)

time = 55013	action = 1	current_phase = 0	next_phase = 1	reward = -1.901536	array([[-7.024285 , -3.6452067]], dtype=float32)

time = 55021	action = 1	current_phase = 1	next_phase = 0	reward = -0.985391	array([[-4.086091 , -2.2268593]], dtype=float32)

time = 55029	action = 0	current_phase = 0	next_phase = 1	reward = -0.048529	array([[-1.3552306, -2.0507362]], dtype=float32)

time = 55034	action = 0	current_phase = 0	next_phase = 1	reward = 0.028042	array([[-1.7833498, -2.8420305]], dtype=float32)

time = 55039	action = 1	current_phase = 0	next_phase = 1	reward = -0.636787	array([[-2.519605 , -2.2912412]], dtype=float32)

time = 55047	action = 1	current_phase = 1	next_phase = 0	reward = -0.925830	array([[-3.3844612, -1.9836951]], dtype=float32)

time = 55055	action = 0	current_phase = 0	next_phase = 1	reward = 0.165053	array([[-1.2330807, -3.1300268]], dtype=float32)

time = 55060	action = 0	current_phase = 0	next_phase = 1	reward = -0.310063	array([[-1.8607483, -2.0296535]], dtype=float32)

time = 55065	action = 0	current_phase = 0	next_phase = 1	reward = -0.232297	array([[-2.2593622, -3.109323 ]], dtype=float32)

time = 55070	action = 1	current_phase = 0	next_phase = 1	reward = -0.698499	array([[-5.968995, -2.569367]], dtype=float32)

time = 55078	action = 1	current_phase = 1	next_phase = 0	reward = -0.690470	array([[-3.5337765, -2.1100307]], dtype=float32)

time = 55086	action = 0	current_phase = 0	next_phase = 1	reward = -0.091476	array([[-1.7563632, -2.8623202]], dtype=float32)

time = 55091	action = 0	current_phase = 0	next_phase = 1	reward = 0.003290	array([[-2.0626955, -2.8845296]], dtype=float32)

time = 55096	action = 0	current_phase = 0	next_phase = 1	reward = 0.067087	array([[-2.5504982, -3.3090208]], dtype=float32)

time = 55101	action = 1	current_phase = 0	next_phase = 1	reward = -1.489177	array([[-7.068869 , -3.2154238]], dtype=float32)

time = 55109	action = 1	current_phase = 1	next_phase = 0	reward = -0.797262	array([[-3.687612, -2.254373]], dtype=float32)

time = 55117	action = 0	current_phase = 0	next_phase = 1	reward = -0.080364	array([[-1.7679445, -2.8170495]], dtype=float32)

time = 55122	action = 0	current_phase = 0	next_phase = 1	reward = 0.010743	array([[-2.1836035, -2.9032588]], dtype=float32)

time = 55127	action = 0	current_phase = 0	next_phase = 1	reward = 0.072096	array([[-2.7654839, -3.411104 ]], dtype=float32)

time = 55132	action = 1	current_phase = 0	next_phase = 1	reward = -1.474708	array([[-7.051088 , -3.4912758]], dtype=float32)

time = 55140	action = 1	current_phase = 1	next_phase = 0	reward = -0.948873	array([[-3.8829293, -2.4712834]], dtype=float32)

time = 55148	action = 0	current_phase = 0	next_phase = 1	reward = -0.056242	array([[-1.7332463, -2.8119607]], dtype=float32)

time = 55153	action = 0	current_phase = 0	next_phase = 1	reward = 0.037394	array([[-2.1852436, -2.9017422]], dtype=float32)

time = 55158	action = 0	current_phase = 0	next_phase = 1	reward = 0.065393	array([[-2.7468185, -3.4171016]], dtype=float32)

time = 55163	action = 1	current_phase = 0	next_phase = 1	reward = -1.355195	array([[-7.08819  , -3.2543118]], dtype=float32)

time = 55171	action = 1	current_phase = 1	next_phase = 0	reward = -1.389850	array([[-3.833651 , -2.3505497]], dtype=float32)

time = 55179	action = 0	current_phase = 0	next_phase = 1	reward = 0.257851	array([[-1.2680817, -2.0674245]], dtype=float32)

time = 55184	action = 0	current_phase = 0	next_phase = 1	reward = 0.011636	array([[-1.8542948, -2.8368912]], dtype=float32)

time = 55189	action = 1	current_phase = 0	next_phase = 1	reward = -0.636762	array([[-2.4581537, -2.3431025]], dtype=float32)

time = 55197	action = 1	current_phase = 1	next_phase = 0	reward = -1.158868	array([[-3.413771, -2.035278]], dtype=float32)

time = 55205	action = 0	current_phase = 0	next_phase = 1	reward = 0.186597	array([[-1.10651  , -3.1447446]], dtype=float32)

time = 55210	action = 0	current_phase = 0	next_phase = 1	reward = -0.009792	array([[-1.776926 , -2.0539193]], dtype=float32)

time = 55215	action = 0	current_phase = 0	next_phase = 1	reward = 0.336981	array([[-2.2271397, -3.103213 ]], dtype=float32)

time = 55220	action = 1	current_phase = 0	next_phase = 1	reward = -1.375767	array([[-5.9787354, -3.0050457]], dtype=float32)

time = 55228	action = 1	current_phase = 1	next_phase = 0	reward = -0.699076	array([[-3.5976894, -2.1577845]], dtype=float32)

time = 55236	action = 0	current_phase = 0	next_phase = 1	reward = -0.074941	array([[-1.7418077, -2.826053 ]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0127 - val_loss: 0.0074

Epoch 2/50

 - 3s - loss: 0.0217 - val_loss: 0.0072

Epoch 3/50

 - 3s - loss: 0.0144 - val_loss: 0.0082

Epoch 4/50

 - 3s - loss: 0.0146 - val_loss: 0.0072

Epoch 5/50

 - 3s - loss: 0.0158 - val_loss: 0.0109

Epoch 6/50

 - 3s - loss: 0.0152 - val_loss: 0.0087

Epoch 7/50

 - 3s - loss: 0.0138 - val_loss: 0.0085

Epoch 8/50

 - 3s - loss: 0.0150 - val_loss: 0.0076

Epoch 9/50

 - 3s - loss: 0.0145 - val_loss: 0.0080

Epoch 10/50

 - 3s - loss: 0.0125 - val_loss: 0.0120

Epoch 11/50

 - 3s - loss: 0.0159 - val_loss: 0.0076

Epoch 12/50

 - 3s - loss: 0.0170 - val_loss: 0.0083

Epoch 13/50

 - 3s - loss: 0.0110 - val_loss: 0.0110

Epoch 14/50

 - 3s - loss: 0.0182 - val_loss: 0.0123

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 55241	action = 0	current_phase = 0	next_phase = 1	reward = -0.003935	array([[-2.0410962, -2.8795362]], dtype=float32)

time = 55246	action = 0	current_phase = 0	next_phase = 1	reward = 0.062933	array([[-2.557677, -3.313465]], dtype=float32)

time = 55251	action = 1	current_phase = 0	next_phase = 1	reward = -1.372658	array([[-7.0618377, -3.268787 ]], dtype=float32)

time = 55259	action = 1	current_phase = 1	next_phase = 0	reward = -0.844703	array([[-3.7978272, -2.336636 ]], dtype=float32)

time = 55267	action = 0	current_phase = 0	next_phase = 1	reward = -0.077054	array([[-1.7221396, -2.8284454]], dtype=float32)

time = 55272	action = 0	current_phase = 0	next_phase = 1	reward = 0.015245	array([[-2.1938224, -2.9034956]], dtype=float32)

time = 55277	action = 0	current_phase = 0	next_phase = 1	reward = 0.083853	array([[-2.73118  , -3.4071455]], dtype=float32)

time = 55282	action = 1	current_phase = 0	next_phase = 1	reward = -1.539258	array([[-7.042344 , -3.5392475]], dtype=float32)

time = 55290	action = 1	current_phase = 1	next_phase = 0	reward = -1.000182	array([[-4.000217 , -2.5116396]], dtype=float32)

time = 55298	action = 0	current_phase = 0	next_phase = 1	reward = -0.055108	array([[-1.7923212, -2.8517938]], dtype=float32)

time = 55303	action = 0	current_phase = 0	next_phase = 1	reward = 0.024269	array([[-2.1739016, -2.9013283]], dtype=float32)

time = 55308	action = 0	current_phase = 0	next_phase = 1	reward = 0.079151	array([[-2.7382348, -3.4385338]], dtype=float32)

time = 55313	action = 1	current_phase = 0	next_phase = 1	reward = -1.327136	array([[-7.090864, -3.199319]], dtype=float32)

time = 55321	action = 1	current_phase = 1	next_phase = 0	reward = -1.199420	array([[-3.8807702, -2.2715576]], dtype=float32)

time = 55329	action = 0	current_phase = 0	next_phase = 1	reward = -0.041620	array([[-1.353941 , -2.0479121]], dtype=float32)

time = 55334	action = 0	current_phase = 0	next_phase = 1	reward = 0.028307	array([[-1.7706606, -2.848143 ]], dtype=float32)

time = 55339	action = 1	current_phase = 0	next_phase = 1	reward = -0.681519	array([[-2.3077316, -2.2391076]], dtype=float32)

time = 55347	action = 1	current_phase = 1	next_phase = 0	reward = -0.586458	array([[-3.5180068, -2.0757926]], dtype=float32)

time = 55355	action = 0	current_phase = 0	next_phase = 1	reward = -0.098098	array([[-1.5498443, -3.088562 ]], dtype=float32)

time = 55360	action = 0	current_phase = 0	next_phase = 1	reward = -0.028673	array([[-1.850153 , -2.0951285]], dtype=float32)

time = 55365	action = 0	current_phase = 0	next_phase = 1	reward = -0.234825	array([[-2.3269143, -3.1923614]], dtype=float32)

time = 55370	action = 1	current_phase = 0	next_phase = 1	reward = -1.084204	array([[-5.9539895, -2.8247354]], dtype=float32)

time = 55378	action = 1	current_phase = 1	next_phase = 0	reward = -0.691074	array([[-3.7441468, -2.235417 ]], dtype=float32)

time = 55386	action = 0	current_phase = 0	next_phase = 1	reward = -0.085704	array([[-1.722755 , -2.8552723]], dtype=float32)

time = 55391	action = 0	current_phase = 0	next_phase = 1	reward = -0.019171	array([[-2.079074, -2.886868]], dtype=float32)

time = 55396	action = 0	current_phase = 0	next_phase = 1	reward = 0.055996	array([[-2.5463772, -3.300024 ]], dtype=float32)

time = 55401	action = 1	current_phase = 0	next_phase = 1	reward = -1.369381	array([[-7.047738, -3.263886]], dtype=float32)

time = 55409	action = 1	current_phase = 1	next_phase = 0	reward = -1.072697	array([[-3.7802458, -2.3179398]], dtype=float32)

time = 55417	action = 0	current_phase = 0	next_phase = 1	reward = 0.208499	array([[-1.7314258, -2.826206 ]], dtype=float32)

time = 55422	action = 0	current_phase = 0	next_phase = 1	reward = -0.009401	array([[-2.1593032, -2.9125013]], dtype=float32)

time = 55427	action = 0	current_phase = 0	next_phase = 1	reward = 0.063023	array([[-2.7628093, -3.3958435]], dtype=float32)

time = 55432	action = 1	current_phase = 0	next_phase = 1	reward = -1.699894	array([[-7.0428505, -3.5694082]], dtype=float32)

time = 55440	action = 1	current_phase = 1	next_phase = 0	reward = -1.003169	array([[-4.049387 , -2.5612998]], dtype=float32)

time = 55448	action = 0	current_phase = 0	next_phase = 1	reward = -0.043818	array([[-1.8288147, -2.853402 ]], dtype=float32)

time = 55453	action = 0	current_phase = 0	next_phase = 1	reward = 0.034501	array([[-2.207967 , -2.9129608]], dtype=float32)

time = 55458	action = 0	current_phase = 0	next_phase = 1	reward = 0.085282	array([[-2.7812924, -3.433866 ]], dtype=float32)

time = 55463	action = 1	current_phase = 0	next_phase = 1	reward = -1.818449	array([[-7.035009 , -3.7125962]], dtype=float32)

time = 55471	action = 1	current_phase = 1	next_phase = 0	reward = -1.284499	array([[-4.155489 , -2.2760236]], dtype=float32)

time = 55479	action = 0	current_phase = 0	next_phase = 1	reward = 0.253520	array([[-1.2605513, -2.0692744]], dtype=float32)

time = 55484	action = 0	current_phase = 0	next_phase = 1	reward = 0.030558	array([[-1.7767503, -2.8191884]], dtype=float32)

time = 55489	action = 1	current_phase = 0	next_phase = 1	reward = -0.710007	array([[-2.4934587, -2.3324616]], dtype=float32)

time = 55497	action = 1	current_phase = 1	next_phase = 0	reward = -1.193665	array([[-3.528007 , -2.0578427]], dtype=float32)

time = 55505	action = 0	current_phase = 0	next_phase = 1	reward = 0.467228	array([[-1.1462023, -3.1505263]], dtype=float32)

time = 55510	action = 0	current_phase = 0	next_phase = 1	reward = -0.285866	array([[-1.9955232, -2.066148 ]], dtype=float32)

time = 55515	action = 0	current_phase = 0	next_phase = 1	reward = 0.338128	array([[-2.2171168, -3.085928 ]], dtype=float32)

time = 55520	action = 1	current_phase = 0	next_phase = 1	reward = -1.320380	array([[-5.9776163, -3.0731404]], dtype=float32)

time = 55528	action = 1	current_phase = 1	next_phase = 0	reward = -0.714040	array([[-3.7269688, -2.235904 ]], dtype=float32)

time = 55536	action = 0	current_phase = 0	next_phase = 1	reward = -0.076660	array([[-1.7725959, -2.8683515]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0146 - val_loss: 0.0049

Epoch 2/50

 - 3s - loss: 0.0153 - val_loss: 0.0059

Epoch 3/50

 - 3s - loss: 0.0122 - val_loss: 0.0066

Epoch 4/50

 - 3s - loss: 0.0121 - val_loss: 0.0052

Epoch 5/50

 - 3s - loss: 0.0135 - val_loss: 0.0069

Epoch 6/50

 - 3s - loss: 0.0115 - val_loss: 0.0064

Epoch 7/50

 - 3s - loss: 0.0122 - val_loss: 0.0070

Epoch 8/50

 - 3s - loss: 0.0115 - val_loss: 0.0063

Epoch 9/50

 - 3s - loss: 0.0112 - val_loss: 0.0061

Epoch 10/50

 - 3s - loss: 0.0109 - val_loss: 0.0054

Epoch 11/50

 - 3s - loss: 0.0119 - val_loss: 0.0063

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 55541	action = 0	current_phase = 0	next_phase = 1	reward = -0.017101	array([[-2.0814283, -2.9001536]], dtype=float32)

time = 55546	action = 0	current_phase = 0	next_phase = 1	reward = 0.042913	array([[-2.6310868, -3.3731525]], dtype=float32)

time = 55551	action = 1	current_phase = 0	next_phase = 1	reward = -1.499033	array([[-7.0811796, -3.2911766]], dtype=float32)

time = 55559	action = 1	current_phase = 1	next_phase = 0	reward = -0.740366	array([[-3.7799945, -2.2342458]], dtype=float32)

time = 55567	action = 0	current_phase = 0	next_phase = 1	reward = -0.078461	array([[-1.8003045, -2.8452516]], dtype=float32)

time = 55572	action = 0	current_phase = 0	next_phase = 1	reward = -0.024313	array([[-2.1337538, -2.9214752]], dtype=float32)

time = 55577	action = 0	current_phase = 0	next_phase = 1	reward = 0.067858	array([[-2.7335978, -3.391694 ]], dtype=float32)

time = 55582	action = 1	current_phase = 0	next_phase = 1	reward = -1.582654	array([[-7.0603476, -3.5123966]], dtype=float32)

time = 55590	action = 1	current_phase = 1	next_phase = 0	reward = -1.244159	array([[-4.00354  , -2.4674497]], dtype=float32)

time = 55598	action = 0	current_phase = 0	next_phase = 1	reward = 0.247024	array([[-1.6503851, -2.830204 ]], dtype=float32)

time = 55603	action = 0	current_phase = 0	next_phase = 1	reward = 0.024273	array([[-2.1953533, -2.91706  ]], dtype=float32)

time = 55608	action = 0	current_phase = 0	next_phase = 1	reward = 0.072005	array([[-2.7761815, -3.4666991]], dtype=float32)

time = 55613	action = 1	current_phase = 0	next_phase = 1	reward = -1.773966	array([[-7.0356674, -3.736759 ]], dtype=float32)

time = 55621	action = 1	current_phase = 1	next_phase = 0	reward = -1.388103	array([[-4.1652126, -2.2874684]], dtype=float32)

time = 55629	action = 0	current_phase = 0	next_phase = 1	reward = 0.257597	array([[-1.3583075, -2.0829124]], dtype=float32)

time = 55634	action = 0	current_phase = 0	next_phase = 1	reward = 0.030668	array([[-1.8447161, -2.8365445]], dtype=float32)

time = 55639	action = 1	current_phase = 0	next_phase = 1	reward = -0.690527	array([[-2.5054405, -2.3319786]], dtype=float32)

time = 55647	action = 1	current_phase = 1	next_phase = 0	reward = -0.583393	array([[-3.5068016, -1.9680378]], dtype=float32)

time = 55655	action = 0	current_phase = 0	next_phase = 1	reward = -0.390254	array([[-1.604236, -3.076236]], dtype=float32)

time = 55660	action = 0	current_phase = 0	next_phase = 1	reward = 0.252631	array([[-1.881004 , -2.0845394]], dtype=float32)

time = 55665	action = 0	current_phase = 0	next_phase = 1	reward = 0.042944	array([[-2.4192572, -3.2119381]], dtype=float32)

time = 55670	action = 1	current_phase = 0	next_phase = 1	reward = -1.261881	array([[-5.98417 , -3.118639]], dtype=float32)

time = 55678	action = 1	current_phase = 1	next_phase = 0	reward = -0.721452	array([[-3.639059 , -2.1130238]], dtype=float32)

time = 55686	action = 0	current_phase = 0	next_phase = 1	reward = -0.100620	array([[-1.7878339, -2.8819246]], dtype=float32)

time = 55691	action = 0	current_phase = 0	next_phase = 1	reward = -0.006187	array([[-2.14646  , -2.9041753]], dtype=float32)

time = 55696	action = 0	current_phase = 0	next_phase = 1	reward = 0.053908	array([[-2.582772 , -3.3125613]], dtype=float32)

time = 55701	action = 1	current_phase = 0	next_phase = 1	reward = -1.430619	array([[-7.098667 , -3.2573195]], dtype=float32)

time = 55709	action = 1	current_phase = 1	next_phase = 0	reward = -0.788476	array([[-3.8002267, -2.2756047]], dtype=float32)

time = 55717	action = 0	current_phase = 0	next_phase = 1	reward = -0.067553	array([[-1.8082783, -2.844631 ]], dtype=float32)

time = 55722	action = 0	current_phase = 0	next_phase = 1	reward = 0.022414	array([[-2.1656616, -2.9217238]], dtype=float32)

time = 55727	action = 0	current_phase = 0	next_phase = 1	reward = 0.075779	array([[-2.790739 , -3.4518282]], dtype=float32)

time = 55732	action = 1	current_phase = 0	next_phase = 1	reward = -1.601863	array([[-7.055151 , -3.5902953]], dtype=float32)

time = 55740	action = 1	current_phase = 1	next_phase = 0	reward = -1.010445	array([[-3.9754496, -2.3965354]], dtype=float32)

time = 55748	action = 0	current_phase = 0	next_phase = 1	reward = -0.054965	array([[-1.7448058, -2.8410168]], dtype=float32)

time = 55753	action = 0	current_phase = 0	next_phase = 1	reward = 0.006592	array([[-2.1642616, -2.899305 ]], dtype=float32)

time = 55758	action = 0	current_phase = 0	next_phase = 1	reward = 0.076788	array([[-2.7010357, -3.37677  ]], dtype=float32)

time = 55763	action = 1	current_phase = 0	next_phase = 1	reward = -1.919076	array([[-7.0486317, -3.682662 ]], dtype=float32)

time = 55771	action = 1	current_phase = 1	next_phase = 0	reward = -1.030651	array([[-4.1585217, -2.2450094]], dtype=float32)

time = 55779	action = 0	current_phase = 0	next_phase = 1	reward = -0.041520	array([[-1.5213654, -2.0428023]], dtype=float32)

time = 55784	action = 0	current_phase = 0	next_phase = 1	reward = 0.028318	array([[-1.8513277, -2.8478746]], dtype=float32)

time = 55789	action = 1	current_phase = 0	next_phase = 1	reward = -0.645401	array([[-2.4938502, -2.3140922]], dtype=float32)

time = 55797	action = 1	current_phase = 1	next_phase = 0	reward = -1.205323	array([[-3.4759774, -1.9904437]], dtype=float32)

time = 55805	action = 0	current_phase = 0	next_phase = 1	reward = 0.469827	array([[-1.1861882, -3.0980167]], dtype=float32)

time = 55810	action = 0	current_phase = 0	next_phase = 1	reward = -0.020907	array([[-2.0123801, -2.074544 ]], dtype=float32)

time = 55815	action = 0	current_phase = 0	next_phase = 1	reward = -0.502603	array([[-2.3790581, -3.177948 ]], dtype=float32)

time = 55820	action = 1	current_phase = 0	next_phase = 1	reward = -0.775239	array([[-6.0086546, -2.6659918]], dtype=float32)

time = 55828	action = 1	current_phase = 1	next_phase = 0	reward = -0.717234	array([[-3.664435, -2.0945  ]], dtype=float32)

time = 55836	action = 0	current_phase = 0	next_phase = 1	reward = -0.078184	array([[-1.7687619, -2.8736527]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0181 - val_loss: 0.0094

Epoch 2/50

 - 3s - loss: 0.0156 - val_loss: 0.0062

Epoch 3/50

 - 3s - loss: 0.0209 - val_loss: 0.0071

Epoch 4/50

 - 3s - loss: 0.0204 - val_loss: 0.0065

Epoch 5/50

 - 3s - loss: 0.0170 - val_loss: 0.0069

Epoch 6/50

 - 3s - loss: 0.0144 - val_loss: 0.0065

Epoch 7/50

 - 3s - loss: 0.0156 - val_loss: 0.0070

Epoch 8/50

 - 3s - loss: 0.0194 - val_loss: 0.0062

Epoch 9/50

 - 3s - loss: 0.0137 - val_loss: 0.0079

Epoch 10/50

 - 3s - loss: 0.0150 - val_loss: 0.0061

Epoch 11/50

 - 3s - loss: 0.0157 - val_loss: 0.0088

Epoch 12/50

 - 3s - loss: 0.0159 - val_loss: 0.0092

Epoch 13/50

 - 3s - loss: 0.0141 - val_loss: 0.0081

Epoch 14/50

 - 3s - loss: 0.0147 - val_loss: 0.0071

Epoch 15/50

 - 3s - loss: 0.0124 - val_loss: 0.0071

Epoch 16/50

 - 3s - loss: 0.0166 - val_loss: 0.0074

Epoch 17/50

 - 3s - loss: 0.0191 - val_loss: 0.0074

Epoch 18/50

 - 3s - loss: 0.0137 - val_loss: 0.0059

Epoch 19/50

 - 3s - loss: 0.0158 - val_loss: 0.0077

Epoch 20/50

 - 3s - loss: 0.0151 - val_loss: 0.0103

Epoch 21/50

 - 3s - loss: 0.0140 - val_loss: 0.0059

Epoch 22/50

 - 3s - loss: 0.0151 - val_loss: 0.0073

Epoch 23/50

 - 3s - loss: 0.0112 - val_loss: 0.0071

Epoch 24/50

 - 3s - loss: 0.0143 - val_loss: 0.0071

Epoch 25/50

 - 3s - loss: 0.0129 - val_loss: 0.0061

Epoch 26/50

 - 3s - loss: 0.0140 - val_loss: 0.0083

Epoch 27/50

 - 3s - loss: 0.0140 - val_loss: 0.0083

Epoch 28/50

 - 3s - loss: 0.0137 - val_loss: 0.0079

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 55841	action = 0	current_phase = 0	next_phase = 1	reward = -0.000587	array([[-2.0626793, -2.8981323]], dtype=float32)

time = 55846	action = 0	current_phase = 0	next_phase = 1	reward = 0.064677	array([[-2.5495877, -3.3205338]], dtype=float32)

time = 55851	action = 1	current_phase = 0	next_phase = 1	reward = -1.461168	array([[-7.0822783, -3.228219 ]], dtype=float32)

time = 55859	action = 1	current_phase = 1	next_phase = 0	reward = -0.899155	array([[-3.8278124, -2.2736263]], dtype=float32)

time = 55867	action = 0	current_phase = 0	next_phase = 1	reward = -0.061044	array([[-1.6866244, -2.8339264]], dtype=float32)

time = 55872	action = 0	current_phase = 0	next_phase = 1	reward = 0.011360	array([[-2.1509507, -2.9338412]], dtype=float32)

time = 55877	action = 0	current_phase = 0	next_phase = 1	reward = 0.068508	array([[-2.7544224, -3.4121752]], dtype=float32)

time = 55882	action = 1	current_phase = 0	next_phase = 1	reward = -1.708780	array([[-7.0587068, -3.54507  ]], dtype=float32)

time = 55890	action = 1	current_phase = 1	next_phase = 0	reward = -0.950179	array([[-4.0643225, -2.4788013]], dtype=float32)

time = 55898	action = 0	current_phase = 0	next_phase = 1	reward = -0.074672	array([[-1.6897744, -2.8482022]], dtype=float32)

time = 55903	action = 0	current_phase = 0	next_phase = 1	reward = 0.004277	array([[-2.1754043, -2.917166 ]], dtype=float32)

time = 55908	action = 0	current_phase = 0	next_phase = 1	reward = 0.072490	array([[-2.7429204, -3.3867502]], dtype=float32)

time = 55913	action = 1	current_phase = 0	next_phase = 1	reward = -1.783509	array([[-7.051775 , -3.6040227]], dtype=float32)

time = 55921	action = 1	current_phase = 1	next_phase = 0	reward = -1.083638	array([[-4.1565247, -2.2992778]], dtype=float32)

time = 55929	action = 0	current_phase = 0	next_phase = 1	reward = -0.030527	array([[-1.3526161, -2.0607758]], dtype=float32)

time = 55934	action = 0	current_phase = 0	next_phase = 1	reward = 0.041222	array([[-1.7902267, -2.8330166]], dtype=float32)

time = 55939	action = 1	current_phase = 0	next_phase = 1	reward = -0.641723	array([[-2.4872937, -2.2908762]], dtype=float32)

time = 55947	action = 1	current_phase = 1	next_phase = 0	reward = -0.544179	array([[-3.4380403, -1.8873801]], dtype=float32)

time = 55955	action = 0	current_phase = 0	next_phase = 1	reward = -0.375469	array([[-1.5903714, -3.101984 ]], dtype=float32)

time = 55960	action = 0	current_phase = 0	next_phase = 1	reward = 0.252724	array([[-1.7293732, -2.0702968]], dtype=float32)

time = 55965	action = 0	current_phase = 0	next_phase = 1	reward = -0.238701	array([[-2.4386873, -3.2302277]], dtype=float32)

time = 55970	action = 1	current_phase = 0	next_phase = 1	reward = -1.079521	array([[-6.002604 , -2.7368286]], dtype=float32)

time = 55978	action = 1	current_phase = 1	next_phase = 0	reward = -0.720065	array([[-3.7349038, -2.161809 ]], dtype=float32)

time = 55986	action = 0	current_phase = 0	next_phase = 1	reward = -0.078954	array([[-1.7195963, -2.8820815]], dtype=float32)

time = 55991	action = 0	current_phase = 0	next_phase = 1	reward = -0.019051	array([[-2.06499  , -2.9087327]], dtype=float32)

time = 55996	action = 0	current_phase = 0	next_phase = 1	reward = 0.044732	array([[-2.5652518, -3.3519437]], dtype=float32)

time = 56001	action = 1	current_phase = 0	next_phase = 1	reward = -1.500481	array([[-7.0896387, -3.2270074]], dtype=float32)

time = 56009	action = 1	current_phase = 1	next_phase = 0	reward = -1.119638	array([[-3.7833064, -2.2150893]], dtype=float32)

time = 56017	action = 0	current_phase = 0	next_phase = 1	reward = 0.212134	array([[-1.6347786, -2.8286462]], dtype=float32)

time = 56022	action = 0	current_phase = 0	next_phase = 1	reward = 0.000525	array([[-2.1948495, -2.9175775]], dtype=float32)

time = 56027	action = 0	current_phase = 0	next_phase = 1	reward = 0.057397	array([[-2.7404265, -3.4185512]], dtype=float32)

time = 56032	action = 1	current_phase = 0	next_phase = 1	reward = -1.543937	array([[-7.050275 , -3.5131187]], dtype=float32)

time = 56040	action = 1	current_phase = 1	next_phase = 0	reward = -0.866732	array([[-4.0003104, -2.4363155]], dtype=float32)

time = 56048	action = 0	current_phase = 0	next_phase = 1	reward = -0.052767	array([[-1.7029371, -2.8437972]], dtype=float32)

time = 56053	action = 0	current_phase = 0	next_phase = 1	reward = 0.025280	array([[-2.158267, -2.920177]], dtype=float32)

time = 56058	action = 0	current_phase = 0	next_phase = 1	reward = 0.078564	array([[-2.7498088, -3.428942 ]], dtype=float32)

time = 56063	action = 1	current_phase = 0	next_phase = 1	reward = -1.260043	array([[-7.0994015, -3.3057985]], dtype=float32)

time = 56071	action = 1	current_phase = 1	next_phase = 0	reward = -1.501660	array([[-3.9807441, -2.3754585]], dtype=float32)

time = 56079	action = 0	current_phase = 0	next_phase = 1	reward = 0.253555	array([[-1.3276699, -2.0805874]], dtype=float32)

time = 56084	action = 0	current_phase = 0	next_phase = 1	reward = 0.056819	array([[-1.7826741, -2.8295105]], dtype=float32)

time = 56089	action = 1	current_phase = 0	next_phase = 1	reward = -0.643344	array([[-2.3791518, -2.3271847]], dtype=float32)

time = 56097	action = 1	current_phase = 1	next_phase = 0	reward = -0.907947	array([[-3.5050108, -1.9700372]], dtype=float32)

time = 56105	action = 0	current_phase = 0	next_phase = 1	reward = 0.179813	array([[-1.4351603, -3.0931158]], dtype=float32)

time = 56110	action = 0	current_phase = 0	next_phase = 1	reward = -0.036966	array([[-1.9674451, -2.0708723]], dtype=float32)

time = 56115	action = 0	current_phase = 0	next_phase = 1	reward = 0.025098	array([[-2.260758 , -3.0705616]], dtype=float32)

time = 56120	action = 1	current_phase = 0	next_phase = 1	reward = -1.365739	array([[-5.9943395, -3.0066066]], dtype=float32)

time = 56128	action = 1	current_phase = 1	next_phase = 0	reward = -0.701866	array([[-3.7239943, -2.1373408]], dtype=float32)

time = 56136	action = 0	current_phase = 0	next_phase = 1	reward = -0.085881	array([[-1.6740886, -2.8663838]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0143 - val_loss: 0.0082

Epoch 2/50

 - 3s - loss: 0.0179 - val_loss: 0.0059

Epoch 3/50

 - 3s - loss: 0.0144 - val_loss: 0.0063

Epoch 4/50

 - 3s - loss: 0.0138 - val_loss: 0.0074

Epoch 5/50

 - 3s - loss: 0.0141 - val_loss: 0.0061

Epoch 6/50

 - 3s - loss: 0.0150 - val_loss: 0.0061

Epoch 7/50

 - 3s - loss: 0.0121 - val_loss: 0.0073

Epoch 8/50

 - 3s - loss: 0.0140 - val_loss: 0.0081

Epoch 9/50

 - 3s - loss: 0.0144 - val_loss: 0.0080

Epoch 10/50

 - 3s - loss: 0.0187 - val_loss: 0.0062

Epoch 11/50

 - 3s - loss: 0.0155 - val_loss: 0.0060

Epoch 12/50

 - 3s - loss: 0.0108 - val_loss: 0.0055

Epoch 13/50

 - 3s - loss: 0.0141 - val_loss: 0.0098

Epoch 14/50

 - 3s - loss: 0.0121 - val_loss: 0.0077

Epoch 15/50

 - 3s - loss: 0.0102 - val_loss: 0.0060

Epoch 16/50

 - 3s - loss: 0.0175 - val_loss: 0.0072

Epoch 17/50

 - 3s - loss: 0.0114 - val_loss: 0.0087

Epoch 18/50

 - 3s - loss: 0.0107 - val_loss: 0.0077

Epoch 19/50

 - 3s - loss: 0.0150 - val_loss: 0.0062

Epoch 20/50

 - 3s - loss: 0.0108 - val_loss: 0.0070

Epoch 21/50

 - 3s - loss: 0.0148 - val_loss: 0.0057

Epoch 22/50

 - 3s - loss: 0.0105 - val_loss: 0.0088

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 56141	action = 0	current_phase = 0	next_phase = 1	reward = -0.005439	array([[-1.988656 , -2.8452568]], dtype=float32)

time = 56146	action = 0	current_phase = 0	next_phase = 1	reward = 0.048301	array([[-2.5314999, -3.2901914]], dtype=float32)

time = 56151	action = 1	current_phase = 0	next_phase = 1	reward = -1.329550	array([[-7.065736 , -3.1853135]], dtype=float32)

time = 56159	action = 1	current_phase = 1	next_phase = 0	reward = -0.761148	array([[-3.7859097, -2.2402759]], dtype=float32)

time = 56167	action = 0	current_phase = 0	next_phase = 1	reward = -0.072785	array([[-1.7149847, -2.7988856]], dtype=float32)

time = 56172	action = 0	current_phase = 0	next_phase = 1	reward = 0.010024	array([[-2.1256065, -2.8883324]], dtype=float32)

time = 56177	action = 0	current_phase = 0	next_phase = 1	reward = 0.053776	array([[-2.697982 , -3.3196197]], dtype=float32)

time = 56182	action = 1	current_phase = 0	next_phase = 1	reward = -1.618637	array([[-7.0154796, -3.5653412]], dtype=float32)

time = 56190	action = 1	current_phase = 1	next_phase = 0	reward = -0.951195	array([[-3.9674191, -2.4090552]], dtype=float32)

time = 56198	action = 0	current_phase = 0	next_phase = 1	reward = -0.035834	array([[-1.6765882, -2.7982876]], dtype=float32)

time = 56203	action = 0	current_phase = 0	next_phase = 1	reward = 0.037337	array([[-2.1656122, -2.8828518]], dtype=float32)

time = 56208	action = 0	current_phase = 0	next_phase = 1	reward = 0.091757	array([[-2.6930618, -3.3597262]], dtype=float32)

time = 56213	action = 1	current_phase = 0	next_phase = 1	reward = -1.347509	array([[-7.0694556, -3.143037 ]], dtype=float32)

time = 56221	action = 1	current_phase = 1	next_phase = 0	reward = -1.449753	array([[-4.0091314, -2.3685544]], dtype=float32)

time = 56229	action = 0	current_phase = 0	next_phase = 1	reward = 0.246166	array([[-1.1585078, -2.0580673]], dtype=float32)

time = 56234	action = 0	current_phase = 0	next_phase = 1	reward = 0.038896	array([[-1.760839 , -2.8135312]], dtype=float32)

time = 56239	action = 1	current_phase = 0	next_phase = 1	reward = -0.752069	array([[-2.2989774, -2.2321029]], dtype=float32)

time = 56247	action = 1	current_phase = 1	next_phase = 0	reward = -0.639675	array([[-3.6263127, -2.0479383]], dtype=float32)

time = 56255	action = 0	current_phase = 0	next_phase = 1	reward = -0.100072	array([[-1.5801722, -2.9781601]], dtype=float32)

time = 56260	action = 0	current_phase = 0	next_phase = 1	reward = -0.023509	array([[-1.95385  , -2.0356202]], dtype=float32)

time = 56265	action = 0	current_phase = 0	next_phase = 1	reward = 0.046323	array([[-2.3760805, -3.1591175]], dtype=float32)

time = 56270	action = 1	current_phase = 0	next_phase = 1	reward = -1.315004	array([[-5.9759192, -3.0672877]], dtype=float32)

time = 56278	action = 1	current_phase = 1	next_phase = 0	reward = -0.726266	array([[-3.727469 , -2.1567698]], dtype=float32)

time = 56286	action = 0	current_phase = 0	next_phase = 1	reward = -0.091297	array([[-1.6644598, -2.8560615]], dtype=float32)

time = 56291	action = 0	current_phase = 0	next_phase = 1	reward = -0.016010	array([[-1.9674287, -2.8513181]], dtype=float32)

time = 56296	action = 0	current_phase = 0	next_phase = 1	reward = 0.051702	array([[-2.494682 , -3.2454715]], dtype=float32)

time = 56301	action = 1	current_phase = 0	next_phase = 1	reward = -1.435418	array([[-7.092835 , -3.2006269]], dtype=float32)

time = 56309	action = 1	current_phase = 1	next_phase = 0	reward = -0.740276	array([[-3.8291965, -2.2758777]], dtype=float32)

time = 56317	action = 0	current_phase = 0	next_phase = 1	reward = -0.059139	array([[-1.7102277, -2.7985058]], dtype=float32)

time = 56322	action = 0	current_phase = 0	next_phase = 1	reward = -0.009849	array([[-2.1189823, -2.8880396]], dtype=float32)

time = 56327	action = 0	current_phase = 0	next_phase = 1	reward = 0.068304	array([[-2.7191775, -3.3323653]], dtype=float32)

time = 56332	action = 1	current_phase = 0	next_phase = 1	reward = -1.655959	array([[-7.0376534, -3.5034554]], dtype=float32)

time = 56340	action = 1	current_phase = 1	next_phase = 0	reward = -0.945486	array([[-4.0287485, -2.4290247]], dtype=float32)

time = 56348	action = 0	current_phase = 0	next_phase = 1	reward = -0.049548	array([[-1.662457 , -2.8047414]], dtype=float32)

time = 56353	action = 0	current_phase = 0	next_phase = 1	reward = 0.033733	array([[-2.1350975, -2.8617065]], dtype=float32)

time = 56358	action = 0	current_phase = 0	next_phase = 1	reward = 0.081265	array([[-2.7290068, -3.3771472]], dtype=float32)

time = 56363	action = 1	current_phase = 0	next_phase = 1	reward = -1.348786	array([[-7.0771894, -3.1827517]], dtype=float32)

time = 56371	action = 1	current_phase = 1	next_phase = 0	reward = -1.538309	array([[-3.9602876, -2.3410134]], dtype=float32)

time = 56379	action = 0	current_phase = 0	next_phase = 1	reward = 0.252793	array([[-1.1266639, -2.052263 ]], dtype=float32)

time = 56384	action = 0	current_phase = 0	next_phase = 1	reward = 0.011282	array([[-1.8107648, -2.8123538]], dtype=float32)

time = 56389	action = 1	current_phase = 0	next_phase = 1	reward = -0.737557	array([[-2.3908458, -2.2301593]], dtype=float32)

time = 56397	action = 1	current_phase = 1	next_phase = 0	reward = -0.901547	array([[-3.559945 , -2.0515852]], dtype=float32)

time = 56405	action = 0	current_phase = 0	next_phase = 1	reward = 0.179184	array([[-1.4581026, -2.9658866]], dtype=float32)

time = 56410	action = 0	current_phase = 0	next_phase = 1	reward = -0.016819	array([[-1.8511559, -2.0205235]], dtype=float32)

time = 56415	action = 0	current_phase = 0	next_phase = 1	reward = 0.054381	array([[-2.3392096, -3.1026752]], dtype=float32)

time = 56420	action = 1	current_phase = 0	next_phase = 1	reward = -1.289385	array([[-5.991426 , -2.9903486]], dtype=float32)

time = 56428	action = 1	current_phase = 1	next_phase = 0	reward = -0.704590	array([[-3.6794424, -2.1400442]], dtype=float32)

time = 56436	action = 0	current_phase = 0	next_phase = 1	reward = -0.105127	array([[-1.6953256, -2.855336 ]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0143 - val_loss: 0.0142

Epoch 2/50

 - 3s - loss: 0.0127 - val_loss: 0.0131

Epoch 3/50

 - 3s - loss: 0.0110 - val_loss: 0.0146

Epoch 4/50

 - 3s - loss: 0.0098 - val_loss: 0.0122

Epoch 5/50

 - 3s - loss: 0.0112 - val_loss: 0.0132

Epoch 6/50

 - 3s - loss: 0.0115 - val_loss: 0.0153

Epoch 7/50

 - 3s - loss: 0.0103 - val_loss: 0.0138

Epoch 8/50

 - 3s - loss: 0.0166 - val_loss: 0.0149

Epoch 9/50

 - 3s - loss: 0.0127 - val_loss: 0.0135

Epoch 10/50

 - 3s - loss: 0.0151 - val_loss: 0.0136

Epoch 11/50

 - 3s - loss: 0.0117 - val_loss: 0.0128

Epoch 12/50

 - 3s - loss: 0.0108 - val_loss: 0.0123

Epoch 13/50

 - 3s - loss: 0.0113 - val_loss: 0.0131

Epoch 14/50

 - 3s - loss: 0.0105 - val_loss: 0.0135

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 56441	action = 0	current_phase = 0	next_phase = 1	reward = 0.001392	array([[-1.9935921, -2.8258157]], dtype=float32)

time = 56446	action = 0	current_phase = 0	next_phase = 1	reward = 0.056201	array([[-2.5630941, -3.209578 ]], dtype=float32)

time = 56451	action = 1	current_phase = 0	next_phase = 1	reward = -1.486238	array([[-7.121673 , -3.2466373]], dtype=float32)

time = 56459	action = 1	current_phase = 1	next_phase = 0	reward = -0.799931	array([[-3.7856874, -2.246926 ]], dtype=float32)

time = 56467	action = 0	current_phase = 0	next_phase = 1	reward = -0.075489	array([[-1.8178403, -2.7927775]], dtype=float32)

time = 56472	action = 0	current_phase = 0	next_phase = 1	reward = 0.013969	array([[-2.2108607, -2.872122 ]], dtype=float32)

time = 56477	action = 0	current_phase = 0	next_phase = 1	reward = 0.078282	array([[-2.7769976, -3.3445334]], dtype=float32)

time = 56482	action = 1	current_phase = 0	next_phase = 1	reward = -1.539770	array([[-7.0736513, -3.5513752]], dtype=float32)

time = 56490	action = 1	current_phase = 1	next_phase = 0	reward = -1.063509	array([[-3.9825697, -2.4218216]], dtype=float32)

time = 56498	action = 0	current_phase = 0	next_phase = 1	reward = -0.053340	array([[-1.7976373, -2.8062084]], dtype=float32)

time = 56503	action = 0	current_phase = 0	next_phase = 1	reward = 0.041170	array([[-2.2036145, -2.8671575]], dtype=float32)

time = 56508	action = 0	current_phase = 0	next_phase = 1	reward = 0.082486	array([[-2.7788792, -3.3593364]], dtype=float32)

time = 56513	action = 1	current_phase = 0	next_phase = 1	reward = -1.327637	array([[-7.100855 , -3.2635362]], dtype=float32)

time = 56521	action = 1	current_phase = 1	next_phase = 0	reward = -1.485696	array([[-3.9886355, -2.3890772]], dtype=float32)

time = 56529	action = 0	current_phase = 0	next_phase = 1	reward = 0.277408	array([[-1.3399731, -2.0343027]], dtype=float32)

time = 56534	action = 0	current_phase = 0	next_phase = 1	reward = 0.057749	array([[-1.8429596, -2.7999258]], dtype=float32)

time = 56539	action = 1	current_phase = 0	next_phase = 1	reward = -0.634853	array([[-2.3907166, -2.2903504]], dtype=float32)

time = 56547	action = 1	current_phase = 1	next_phase = 0	reward = -0.925638	array([[-3.4697876, -1.9741704]], dtype=float32)

time = 56555	action = 0	current_phase = 0	next_phase = 1	reward = 0.181215	array([[-1.4222907, -2.995187 ]], dtype=float32)

time = 56560	action = 0	current_phase = 0	next_phase = 1	reward = -0.018080	array([[-1.9891071, -2.0139332]], dtype=float32)

time = 56565	action = 0	current_phase = 0	next_phase = 1	reward = 0.050550	array([[-2.4869199, -3.176445 ]], dtype=float32)

time = 56570	action = 1	current_phase = 0	next_phase = 1	reward = -1.257997	array([[-6.0182424, -2.976617 ]], dtype=float32)

time = 56578	action = 1	current_phase = 1	next_phase = 0	reward = -1.003402	array([[-3.602168 , -2.0981088]], dtype=float32)

time = 56586	action = 0	current_phase = 0	next_phase = 1	reward = 0.196454	array([[-1.6074193, -2.8361692]], dtype=float32)

time = 56591	action = 0	current_phase = 0	next_phase = 1	reward = -0.013516	array([[-2.0980172, -2.8529532]], dtype=float32)

time = 56596	action = 0	current_phase = 0	next_phase = 1	reward = 0.046930	array([[-2.577909, -3.207626]], dtype=float32)

time = 56601	action = 1	current_phase = 0	next_phase = 1	reward = -1.509616	array([[-7.113996, -3.258138]], dtype=float32)

time = 56609	action = 1	current_phase = 1	next_phase = 0	reward = -0.761667	array([[-3.7410955, -2.2010698]], dtype=float32)

time = 56617	action = 0	current_phase = 0	next_phase = 1	reward = -0.055664	array([[-1.7534106, -2.7883718]], dtype=float32)

time = 56622	action = 0	current_phase = 0	next_phase = 1	reward = 0.007971	array([[-2.204146, -2.868482]], dtype=float32)

time = 56627	action = 0	current_phase = 0	next_phase = 1	reward = 0.065164	array([[-2.807293 , -3.3690217]], dtype=float32)

time = 56632	action = 1	current_phase = 0	next_phase = 1	reward = -1.617108	array([[-7.066913 , -3.5647137]], dtype=float32)

time = 56640	action = 1	current_phase = 1	next_phase = 0	reward = -0.951840	array([[-4.0008106, -2.4312987]], dtype=float32)

time = 56648	action = 0	current_phase = 0	next_phase = 1	reward = -0.046324	array([[-1.7893258, -2.80112  ]], dtype=float32)

time = 56653	action = 0	current_phase = 0	next_phase = 1	reward = 0.009746	array([[-2.1875129, -2.8613844]], dtype=float32)

time = 56658	action = 0	current_phase = 0	next_phase = 1	reward = 0.077759	array([[-2.773017 , -3.3693562]], dtype=float32)

time = 56663	action = 1	current_phase = 0	next_phase = 1	reward = -1.915611	array([[-7.0806313, -3.5463572]], dtype=float32)

time = 56671	action = 1	current_phase = 1	next_phase = 0	reward = -1.340818	array([[-4.0555696, -2.323936 ]], dtype=float32)

time = 56679	action = 0	current_phase = 0	next_phase = 1	reward = 0.257329	array([[-1.2824639, -2.0417564]], dtype=float32)

time = 56684	action = 0	current_phase = 0	next_phase = 1	reward = 0.028946	array([[-1.8890848, -2.795893 ]], dtype=float32)

time = 56689	action = 1	current_phase = 0	next_phase = 1	reward = -0.704111	array([[-2.547604, -2.266695]], dtype=float32)

time = 56697	action = 1	current_phase = 1	next_phase = 0	reward = -1.208504	array([[-3.4981256, -1.9886937]], dtype=float32)

time = 56705	action = 0	current_phase = 0	next_phase = 1	reward = 0.463653	array([[-1.2999846, -3.0065508]], dtype=float32)

time = 56710	action = 0	current_phase = 0	next_phase = 1	reward = -0.027176	array([[-1.9729599, -2.017067 ]], dtype=float32)

time = 56715	action = 0	current_phase = 0	next_phase = 1	reward = -0.235726	array([[-2.4103603, -3.1451743]], dtype=float32)

time = 56720	action = 1	current_phase = 0	next_phase = 1	reward = -1.035513	array([[-6.006634 , -2.7760832]], dtype=float32)

time = 56728	action = 1	current_phase = 1	next_phase = 0	reward = -0.715083	array([[-3.7073536, -2.1550333]], dtype=float32)

time = 56736	action = 0	current_phase = 0	next_phase = 1	reward = -0.083358	array([[-1.7565875, -2.8189301]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0184 - val_loss: 0.0079

Epoch 2/50

 - 3s - loss: 0.0186 - val_loss: 0.0064

Epoch 3/50

 - 3s - loss: 0.0168 - val_loss: 0.0075

Epoch 4/50

 - 3s - loss: 0.0165 - val_loss: 0.0073

Epoch 5/50

 - 3s - loss: 0.0164 - val_loss: 0.0064

Epoch 6/50

 - 3s - loss: 0.0169 - val_loss: 0.0067

Epoch 7/50

 - 3s - loss: 0.0156 - val_loss: 0.0088

Epoch 8/50

 - 3s - loss: 0.0168 - val_loss: 0.0068

Epoch 9/50

 - 3s - loss: 0.0169 - val_loss: 0.0086

Epoch 10/50

 - 3s - loss: 0.0146 - val_loss: 0.0068

Epoch 11/50

 - 3s - loss: 0.0140 - val_loss: 0.0062

Epoch 12/50

 - 3s - loss: 0.0145 - val_loss: 0.0091

Epoch 13/50

 - 3s - loss: 0.0169 - val_loss: 0.0060

Epoch 14/50

 - 3s - loss: 0.0129 - val_loss: 0.0068

Epoch 15/50

 - 3s - loss: 0.0137 - val_loss: 0.0065

Epoch 16/50

 - 3s - loss: 0.0145 - val_loss: 0.0060

Epoch 17/50

 - 3s - loss: 0.0162 - val_loss: 0.0062

Epoch 18/50

 - 3s - loss: 0.0166 - val_loss: 0.0080

Epoch 19/50

 - 3s - loss: 0.0145 - val_loss: 0.0074

Epoch 20/50

 - 3s - loss: 0.0143 - val_loss: 0.0098

Epoch 21/50

 - 3s - loss: 0.0147 - val_loss: 0.0072

Epoch 22/50

 - 3s - loss: 0.0160 - val_loss: 0.0073

Epoch 23/50

 - 3s - loss: 0.0155 - val_loss: 0.0073

Epoch 24/50

 - 3s - loss: 0.0144 - val_loss: 0.0077

Epoch 25/50

 - 3s - loss: 0.0137 - val_loss: 0.0068

Epoch 26/50

 - 3s - loss: 0.0157 - val_loss: 0.0084

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 56741	action = 0	current_phase = 0	next_phase = 1	reward = -0.006237	array([[-2.0696888, -2.8536367]], dtype=float32)

time = 56746	action = 0	current_phase = 0	next_phase = 1	reward = 0.057598	array([[-2.5902717, -3.2921224]], dtype=float32)

time = 56751	action = 1	current_phase = 0	next_phase = 1	reward = -1.380137	array([[-7.130541 , -3.2712984]], dtype=float32)

time = 56759	action = 1	current_phase = 1	next_phase = 0	reward = -0.781223	array([[-3.7810583, -2.2319758]], dtype=float32)

time = 56767	action = 0	current_phase = 0	next_phase = 1	reward = -0.079098	array([[-1.7858752, -2.8020158]], dtype=float32)

time = 56772	action = 0	current_phase = 0	next_phase = 1	reward = 0.014162	array([[-2.194896 , -2.8858328]], dtype=float32)

time = 56777	action = 0	current_phase = 0	next_phase = 1	reward = 0.071897	array([[-2.790655, -3.361679]], dtype=float32)

time = 56782	action = 1	current_phase = 0	next_phase = 1	reward = -1.529059	array([[-7.0945206, -3.6008673]], dtype=float32)

time = 56790	action = 1	current_phase = 1	next_phase = 0	reward = -1.291746	array([[-3.9123673, -2.344158 ]], dtype=float32)

time = 56798	action = 0	current_phase = 0	next_phase = 1	reward = 0.239564	array([[-1.6109556, -2.8015494]], dtype=float32)

time = 56803	action = 0	current_phase = 0	next_phase = 1	reward = 0.036613	array([[-2.1930146, -2.889651 ]], dtype=float32)

time = 56808	action = 0	current_phase = 0	next_phase = 1	reward = 0.071887	array([[-2.8080206, -3.399528 ]], dtype=float32)

time = 56813	action = 1	current_phase = 0	next_phase = 1	reward = -1.295740	array([[-7.1649323, -3.228541 ]], dtype=float32)

time = 56821	action = 1	current_phase = 1	next_phase = 0	reward = -1.543174	array([[-3.8970933, -2.3172064]], dtype=float32)

time = 56829	action = 0	current_phase = 0	next_phase = 1	reward = 0.242555	array([[-1.2276438, -2.0489826]], dtype=float32)

time = 56834	action = 0	current_phase = 0	next_phase = 1	reward = 0.035727	array([[-1.8525684, -2.836985 ]], dtype=float32)

time = 56839	action = 0	current_phase = 0	next_phase = 1	reward = 0.054380	array([[-2.3217943, -2.3567982]], dtype=float32)

time = 56844	action = 1	current_phase = 0	next_phase = 1	reward = -1.626385	array([[-7.0241747, -2.9588366]], dtype=float32)

time = 56852	action = 1	current_phase = 1	next_phase = 0	reward = -0.930294	array([[-3.8018503, -2.2777233]], dtype=float32)

time = 56860	action = 0	current_phase = 0	next_phase = 1	reward = -0.013961	array([[-1.7031727, -2.0428128]], dtype=float32)

time = 56865	action = 0	current_phase = 0	next_phase = 1	reward = 0.334170	array([[-2.2138715, -3.0557756]], dtype=float32)

time = 56870	action = 1	current_phase = 0	next_phase = 1	reward = -1.358321	array([[-6.025816 , -2.9922972]], dtype=float32)

time = 56878	action = 1	current_phase = 1	next_phase = 0	reward = -0.729442	array([[-3.654128 , -2.0922956]], dtype=float32)

time = 56886	action = 0	current_phase = 0	next_phase = 1	reward = -0.090724	array([[-1.744425, -2.821241]], dtype=float32)

time = 56891	action = 0	current_phase = 0	next_phase = 1	reward = -0.003780	array([[-2.094481 , -2.8631363]], dtype=float32)

time = 56896	action = 0	current_phase = 0	next_phase = 1	reward = 0.058954	array([[-2.6041543, -3.2769132]], dtype=float32)

time = 56901	action = 1	current_phase = 0	next_phase = 1	reward = -1.441797	array([[-7.150488 , -3.2621746]], dtype=float32)

time = 56909	action = 1	current_phase = 1	next_phase = 0	reward = -0.843978	array([[-3.8128667, -2.2647777]], dtype=float32)

time = 56917	action = 0	current_phase = 0	next_phase = 1	reward = -0.065675	array([[-1.7867875, -2.7984285]], dtype=float32)

time = 56922	action = 0	current_phase = 0	next_phase = 1	reward = 0.005093	array([[-2.1978948, -2.8825488]], dtype=float32)

time = 56927	action = 0	current_phase = 0	next_phase = 1	reward = 0.066379	array([[-2.7820942, -3.3547783]], dtype=float32)

time = 56932	action = 1	current_phase = 0	next_phase = 1	reward = -1.640943	array([[-7.0846777, -3.57762  ]], dtype=float32)

time = 56940	action = 1	current_phase = 1	next_phase = 0	reward = -1.363561	array([[-3.9409723, -2.373999 ]], dtype=float32)

time = 56948	action = 0	current_phase = 0	next_phase = 1	reward = 0.248362	array([[-1.6372894, -2.8002272]], dtype=float32)

time = 56953	action = 0	current_phase = 0	next_phase = 1	reward = 0.022238	array([[-2.1667483, -2.874349 ]], dtype=float32)

time = 56958	action = 0	current_phase = 0	next_phase = 1	reward = 0.077688	array([[-2.7656188, -3.3489475]], dtype=float32)

time = 56963	action = 1	current_phase = 0	next_phase = 1	reward = -0.799893	array([[-7.1618996, -3.1264243]], dtype=float32)

time = 56971	action = 1	current_phase = 1	next_phase = 0	reward = -1.251627	array([[-3.8585863, -2.357937 ]], dtype=float32)

time = 56979	action = 0	current_phase = 0	next_phase = 1	reward = -0.045142	array([[-1.3786988, -2.0419545]], dtype=float32)

time = 56984	action = 0	current_phase = 0	next_phase = 1	reward = 0.037857	array([[-1.8313037, -2.8512454]], dtype=float32)

time = 56989	action = 1	current_phase = 0	next_phase = 1	reward = -0.700771	array([[-2.3616824, -2.2934594]], dtype=float32)

time = 56997	action = 1	current_phase = 1	next_phase = 0	reward = -0.871009	array([[-3.5163608, -2.0478914]], dtype=float32)

time = 57005	action = 0	current_phase = 0	next_phase = 1	reward = 0.183965	array([[-1.3796442, -2.9872632]], dtype=float32)

time = 57010	action = 0	current_phase = 0	next_phase = 1	reward = -0.012998	array([[-1.9195485, -2.0207539]], dtype=float32)

time = 57015	action = 0	current_phase = 0	next_phase = 1	reward = 0.053060	array([[-2.4105544, -3.1587253]], dtype=float32)

time = 57020	action = 1	current_phase = 0	next_phase = 1	reward = -1.245675	array([[-6.0212603, -2.9471645]], dtype=float32)

time = 57028	action = 1	current_phase = 1	next_phase = 0	reward = -0.724133	array([[-3.624919, -2.093974]], dtype=float32)

time = 57036	action = 0	current_phase = 0	next_phase = 1	reward = -0.093817	array([[-1.7523465, -2.825162 ]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0176 - val_loss: 0.0118

Epoch 2/50

 - 3s - loss: 0.0154 - val_loss: 0.0119

Epoch 3/50

 - 3s - loss: 0.0197 - val_loss: 0.0125

Epoch 4/50

 - 3s - loss: 0.0131 - val_loss: 0.0153

Epoch 5/50

 - 3s - loss: 0.0116 - val_loss: 0.0125

Epoch 6/50

 - 3s - loss: 0.0133 - val_loss: 0.0151

Epoch 7/50

 - 3s - loss: 0.0127 - val_loss: 0.0114

Epoch 8/50

 - 3s - loss: 0.0133 - val_loss: 0.0145

Epoch 9/50

 - 3s - loss: 0.0113 - val_loss: 0.0128

Epoch 10/50

 - 3s - loss: 0.0159 - val_loss: 0.0138

Epoch 11/50

 - 3s - loss: 0.0124 - val_loss: 0.0146

Epoch 12/50

 - 3s - loss: 0.0160 - val_loss: 0.0119

Epoch 13/50

 - 3s - loss: 0.0123 - val_loss: 0.0133

Epoch 14/50

 - 3s - loss: 0.0133 - val_loss: 0.0133

Epoch 15/50

 - 3s - loss: 0.0108 - val_loss: 0.0126

Epoch 16/50

 - 3s - loss: 0.0129 - val_loss: 0.0117

Epoch 17/50

 - 3s - loss: 0.0127 - val_loss: 0.0170

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 57041	action = 0	current_phase = 0	next_phase = 1	reward = 0.003292	array([[-2.1731663, -2.9162831]], dtype=float32)

time = 57046	action = 0	current_phase = 0	next_phase = 1	reward = 0.066975	array([[-2.748063 , -3.4232292]], dtype=float32)

time = 57051	action = 1	current_phase = 0	next_phase = 1	reward = -1.565944	array([[-7.2193737, -3.453622 ]], dtype=float32)

time = 57059	action = 1	current_phase = 1	next_phase = 0	reward = -0.848320	array([[-3.772509 , -2.2552588]], dtype=float32)

time = 57067	action = 0	current_phase = 0	next_phase = 1	reward = -0.069108	array([[-1.9850115, -2.8623793]], dtype=float32)

time = 57072	action = 0	current_phase = 0	next_phase = 1	reward = 0.004344	array([[-2.2598934, -2.9349685]], dtype=float32)

time = 57077	action = 0	current_phase = 0	next_phase = 1	reward = 0.061918	array([[-2.8212233, -3.4371135]], dtype=float32)

time = 57082	action = 1	current_phase = 0	next_phase = 1	reward = -1.588784	array([[-7.1834135, -3.6134636]], dtype=float32)

time = 57090	action = 1	current_phase = 1	next_phase = 0	reward = -0.905910	array([[-3.9807029, -2.4325576]], dtype=float32)

time = 57098	action = 0	current_phase = 0	next_phase = 1	reward = -0.042658	array([[-1.9161471, -2.872902 ]], dtype=float32)

time = 57103	action = 0	current_phase = 0	next_phase = 1	reward = 0.042656	array([[-2.2105308, -2.9092011]], dtype=float32)

time = 57108	action = 0	current_phase = 0	next_phase = 1	reward = 0.085356	array([[-2.8399692, -3.391822 ]], dtype=float32)

time = 57113	action = 1	current_phase = 0	next_phase = 1	reward = -1.382599	array([[-7.18435  , -3.5904982]], dtype=float32)

time = 57121	action = 1	current_phase = 1	next_phase = 0	reward = -1.251089	array([[-3.912003 , -2.3502374]], dtype=float32)

time = 57129	action = 0	current_phase = 0	next_phase = 1	reward = -0.027360	array([[-1.5881575, -2.0664406]], dtype=float32)

time = 57134	action = 0	current_phase = 0	next_phase = 1	reward = 0.039423	array([[-2.0204496, -2.8421493]], dtype=float32)

time = 57139	action = 1	current_phase = 0	next_phase = 1	reward = -0.725278	array([[-2.6140814, -2.3729239]], dtype=float32)

time = 57147	action = 1	current_phase = 1	next_phase = 0	reward = -0.943986	array([[-3.4059052, -1.9240414]], dtype=float32)

time = 57155	action = 0	current_phase = 0	next_phase = 1	reward = -0.104450	array([[-1.6422877, -3.0385   ]], dtype=float32)

time = 57160	action = 0	current_phase = 0	next_phase = 1	reward = -0.020082	array([[-1.9226364, -2.0770473]], dtype=float32)

time = 57165	action = 0	current_phase = 0	next_phase = 1	reward = 0.055636	array([[-2.439162 , -3.1592426]], dtype=float32)

time = 57170	action = 1	current_phase = 0	next_phase = 1	reward = -1.068066	array([[-6.0969424, -2.9201171]], dtype=float32)

time = 57178	action = 1	current_phase = 1	next_phase = 0	reward = -0.724641	array([[-3.6497598, -2.1062858]], dtype=float32)

time = 57186	action = 0	current_phase = 0	next_phase = 1	reward = -0.087928	array([[-1.871934 , -2.8870401]], dtype=float32)

time = 57191	action = 0	current_phase = 0	next_phase = 1	reward = -0.012909	array([[-2.1606393, -2.9055321]], dtype=float32)

time = 57196	action = 0	current_phase = 0	next_phase = 1	reward = 0.042861	array([[-2.6849198, -3.3770614]], dtype=float32)

time = 57201	action = 1	current_phase = 0	next_phase = 1	reward = -1.339253	array([[-7.219366 , -3.4071338]], dtype=float32)

time = 57209	action = 1	current_phase = 1	next_phase = 0	reward = -1.076355	array([[-3.7499738, -2.2423377]], dtype=float32)

time = 57217	action = 0	current_phase = 0	next_phase = 1	reward = 0.212841	array([[-1.8305258, -2.8514445]], dtype=float32)

time = 57222	action = 0	current_phase = 0	next_phase = 1	reward = 0.002739	array([[-2.2908154, -2.9130957]], dtype=float32)

time = 57227	action = 0	current_phase = 0	next_phase = 1	reward = 0.072219	array([[-2.821333, -3.419374]], dtype=float32)

time = 57232	action = 1	current_phase = 0	next_phase = 1	reward = -1.530570	array([[-7.14047, -3.71255]], dtype=float32)

time = 57240	action = 1	current_phase = 1	next_phase = 0	reward = -1.247190	array([[-3.955133 , -2.4472585]], dtype=float32)

time = 57248	action = 0	current_phase = 0	next_phase = 1	reward = 0.223769	array([[-1.807273 , -2.8608637]], dtype=float32)

time = 57253	action = 0	current_phase = 0	next_phase = 1	reward = 0.020573	array([[-2.2544842, -2.9218423]], dtype=float32)

time = 57258	action = 0	current_phase = 0	next_phase = 1	reward = 0.082985	array([[-2.8112426, -3.4272625]], dtype=float32)

time = 57263	action = 1	current_phase = 0	next_phase = 1	reward = -1.950877	array([[-7.1461525, -3.744939 ]], dtype=float32)

time = 57271	action = 1	current_phase = 1	next_phase = 0	reward = -1.094465	array([[-4.1035805, -2.3259203]], dtype=float32)

time = 57279	action = 0	current_phase = 0	next_phase = 1	reward = -0.033022	array([[-1.5940201, -2.0350883]], dtype=float32)

time = 57284	action = 0	current_phase = 0	next_phase = 1	reward = 0.033378	array([[-2.0217946, -2.8679938]], dtype=float32)

time = 57289	action = 1	current_phase = 0	next_phase = 1	reward = -0.708025	array([[-2.6149974, -2.3239226]], dtype=float32)

time = 57297	action = 1	current_phase = 1	next_phase = 0	reward = -0.865179	array([[-3.4535766, -1.9150198]], dtype=float32)

time = 57305	action = 0	current_phase = 0	next_phase = 1	reward = -0.088377	array([[-1.6331068, -3.1024978]], dtype=float32)

time = 57310	action = 0	current_phase = 0	next_phase = 1	reward = 0.264978	array([[-2.0251296, -2.069322 ]], dtype=float32)

time = 57315	action = 0	current_phase = 0	next_phase = 1	reward = -0.228995	array([[-2.6165743, -3.3011532]], dtype=float32)

time = 57320	action = 1	current_phase = 0	next_phase = 1	reward = -1.135318	array([[-6.0884523, -2.882956 ]], dtype=float32)

time = 57328	action = 1	current_phase = 1	next_phase = 0	reward = -0.703374	array([[-3.6572738, -2.1340938]], dtype=float32)

time = 57336	action = 0	current_phase = 0	next_phase = 1	reward = -0.092496	array([[-1.9154704, -2.8584168]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0157 - val_loss: 0.0075

Epoch 2/50

 - 3s - loss: 0.0139 - val_loss: 0.0083

Epoch 3/50

 - 3s - loss: 0.0151 - val_loss: 0.0077

Epoch 4/50

 - 3s - loss: 0.0134 - val_loss: 0.0069

Epoch 5/50

 - 3s - loss: 0.0152 - val_loss: 0.0069

Epoch 6/50

 - 3s - loss: 0.0142 - val_loss: 0.0072

Epoch 7/50

 - 3s - loss: 0.0123 - val_loss: 0.0097

Epoch 8/50

 - 3s - loss: 0.0127 - val_loss: 0.0078

Epoch 9/50

 - 3s - loss: 0.0153 - val_loss: 0.0070

Epoch 10/50

 - 3s - loss: 0.0118 - val_loss: 0.0081

Epoch 11/50

 - 3s - loss: 0.0115 - val_loss: 0.0074

Epoch 12/50

 - 3s - loss: 0.0140 - val_loss: 0.0078

Epoch 13/50

 - 3s - loss: 0.0123 - val_loss: 0.0071

Epoch 14/50

 - 3s - loss: 0.0132 - val_loss: 0.0082

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 57341	action = 0	current_phase = 0	next_phase = 1	reward = 0.005536	array([[-2.1567116, -2.893778 ]], dtype=float32)

time = 57346	action = 0	current_phase = 0	next_phase = 1	reward = 0.053574	array([[-2.6103165, -3.3691912]], dtype=float32)

time = 57351	action = 1	current_phase = 0	next_phase = 1	reward = -1.446000	array([[-7.202152 , -3.2781303]], dtype=float32)

time = 57359	action = 1	current_phase = 1	next_phase = 0	reward = -0.774974	array([[-3.7983203, -2.4105163]], dtype=float32)

time = 57367	action = 0	current_phase = 0	next_phase = 1	reward = -0.075583	array([[-1.859894, -2.855537]], dtype=float32)

time = 57372	action = 0	current_phase = 0	next_phase = 1	reward = -0.002938	array([[-2.263134, -2.914434]], dtype=float32)

time = 57377	action = 0	current_phase = 0	next_phase = 1	reward = 0.058613	array([[-2.8068712, -3.4503918]], dtype=float32)

time = 57382	action = 1	current_phase = 0	next_phase = 1	reward = -1.581524	array([[-7.134821 , -3.5770166]], dtype=float32)

time = 57390	action = 1	current_phase = 1	next_phase = 0	reward = -0.954098	array([[-3.9476278, -2.5509381]], dtype=float32)

time = 57398	action = 0	current_phase = 0	next_phase = 1	reward = -0.059193	array([[-1.8689208, -2.8517888]], dtype=float32)

time = 57403	action = 0	current_phase = 0	next_phase = 1	reward = 0.045811	array([[-2.2628164, -2.920539 ]], dtype=float32)

time = 57408	action = 0	current_phase = 0	next_phase = 1	reward = 0.078450	array([[-2.8029044, -3.460856 ]], dtype=float32)

time = 57413	action = 1	current_phase = 0	next_phase = 1	reward = -1.333485	array([[-7.179865 , -3.3972096]], dtype=float32)

time = 57421	action = 1	current_phase = 1	next_phase = 0	reward = -1.390261	array([[-3.9703834, -2.5138135]], dtype=float32)

time = 57429	action = 0	current_phase = 0	next_phase = 1	reward = 0.252122	array([[-1.3060559, -2.0701609]], dtype=float32)

time = 57434	action = 0	current_phase = 0	next_phase = 1	reward = 0.034511	array([[-1.813705, -2.883415]], dtype=float32)

time = 57439	action = 1	current_phase = 0	next_phase = 1	reward = -0.644481	array([[-2.5311592, -2.2372828]], dtype=float32)

time = 57447	action = 1	current_phase = 1	next_phase = 0	reward = -0.636913	array([[-3.488178 , -2.1742635]], dtype=float32)

time = 57455	action = 0	current_phase = 0	next_phase = 1	reward = -0.100307	array([[-1.7788618, -3.0580602]], dtype=float32)

time = 57460	action = 1	current_phase = 0	next_phase = 1	reward = -1.951353	array([[-2.1979601, -2.0245337]], dtype=float32)

time = 57468	action = 1	current_phase = 1	next_phase = 0	reward = -0.572427	array([[-2.5199986, -1.4913212]], dtype=float32)

time = 57476	action = 0	current_phase = 0	next_phase = 1	reward = -0.931041	array([[-3.2272758, -3.4072506]], dtype=float32)

time = 57481	action = 1	current_phase = 0	next_phase = 1	reward = -2.512500	array([[-6.7540717, -3.4596994]], dtype=float32)

time = 57489	action = 1	current_phase = 1	next_phase = 0	reward = -0.652753	array([[-3.4464781, -2.1895292]], dtype=float32)

time = 57497	action = 0	current_phase = 0	next_phase = 1	reward = 0.067854	array([[-2.4040341, -3.3398461]], dtype=float32)

time = 57502	action = 0	current_phase = 0	next_phase = 1	reward = -0.243101	array([[-2.4238956, -3.2490287]], dtype=float32)

time = 57507	action = 1	current_phase = 0	next_phase = 1	reward = -2.497877	array([[-6.159626, -3.2655  ]], dtype=float32)

time = 57515	action = 1	current_phase = 1	next_phase = 0	reward = -0.740125	array([[-3.9412956, -2.2370615]], dtype=float32)

time = 57523	action = 0	current_phase = 0	next_phase = 1	reward = 0.047632	array([[-2.0949125, -2.9005623]], dtype=float32)

time = 57528	action = 0	current_phase = 0	next_phase = 1	reward = 0.079283	array([[-2.416793 , -3.1906147]], dtype=float32)

time = 57533	action = 1	current_phase = 0	next_phase = 1	reward = -0.878910	array([[-7.063969 , -3.2181537]], dtype=float32)

time = 57541	action = 1	current_phase = 1	next_phase = 0	reward = -1.303536	array([[-3.589381, -2.388982]], dtype=float32)

time = 57549	action = 0	current_phase = 0	next_phase = 1	reward = -0.031958	array([[-1.4504826, -2.0621104]], dtype=float32)

time = 57554	action = 0	current_phase = 0	next_phase = 1	reward = 0.026912	array([[-1.9244418, -2.8844295]], dtype=float32)

time = 57559	action = 1	current_phase = 0	next_phase = 1	reward = -0.756309	array([[-2.3595915, -2.2106216]], dtype=float32)

time = 57567	action = 1	current_phase = 1	next_phase = 0	reward = -0.633564	array([[-3.559586 , -2.1676035]], dtype=float32)

time = 57575	action = 0	current_phase = 0	next_phase = 1	reward = -0.359881	array([[-1.6542125, -3.077183 ]], dtype=float32)

time = 57580	action = 0	current_phase = 0	next_phase = 1	reward = 0.264943	array([[-2.0084682, -2.023337 ]], dtype=float32)

time = 57585	action = 0	current_phase = 0	next_phase = 1	reward = 0.050278	array([[-2.4384289, -3.267865 ]], dtype=float32)

time = 57590	action = 1	current_phase = 0	next_phase = 1	reward = -1.378662	array([[-6.088216 , -2.9699183]], dtype=float32)

time = 57598	action = 1	current_phase = 1	next_phase = 0	reward = -0.700103	array([[-3.7054083, -2.3229856]], dtype=float32)

time = 57606	action = 0	current_phase = 0	next_phase = 1	reward = -0.086385	array([[-1.793333 , -2.8756988]], dtype=float32)

time = 57611	action = 0	current_phase = 0	next_phase = 1	reward = -0.019148	array([[-2.1677728, -2.8966637]], dtype=float32)

time = 57616	action = 0	current_phase = 0	next_phase = 1	reward = 0.051311	array([[-2.6545234, -3.3761418]], dtype=float32)

time = 57621	action = 1	current_phase = 0	next_phase = 1	reward = -1.461283	array([[-7.1981072, -3.3231378]], dtype=float32)

time = 57629	action = 1	current_phase = 1	next_phase = 0	reward = -0.768146	array([[-3.7906172, -2.4078712]], dtype=float32)

time = 57637	action = 0	current_phase = 0	next_phase = 1	reward = -0.072964	array([[-1.8174489, -2.8585231]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0162 - val_loss: 0.0184

Epoch 2/50

 - 3s - loss: 0.0172 - val_loss: 0.0169

Epoch 3/50

 - 3s - loss: 0.0141 - val_loss: 0.0147

Epoch 4/50

 - 3s - loss: 0.0191 - val_loss: 0.0176

Epoch 5/50

 - 3s - loss: 0.0179 - val_loss: 0.0169

Epoch 6/50

 - 3s - loss: 0.0196 - val_loss: 0.0154

Epoch 7/50

 - 3s - loss: 0.0164 - val_loss: 0.0165

Epoch 8/50

 - 3s - loss: 0.0146 - val_loss: 0.0188

Epoch 9/50

 - 3s - loss: 0.0160 - val_loss: 0.0164

Epoch 10/50

 - 3s - loss: 0.0150 - val_loss: 0.0163

Epoch 11/50

 - 3s - loss: 0.0142 - val_loss: 0.0177

Epoch 12/50

 - 3s - loss: 0.0151 - val_loss: 0.0191

Epoch 13/50

 - 3s - loss: 0.0186 - val_loss: 0.0170

length of memory (state 0, action 0): 1025, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1011, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1011, before forget

length of memory (state 1, action 1): 1000, after forget

time = 57642	action = 0	current_phase = 0	next_phase = 1	reward = 0.009385	array([[-2.2584467, -2.9289975]], dtype=float32)

time = 57647	action = 0	current_phase = 0	next_phase = 1	reward = 0.063273	array([[-2.8622875, -3.4676223]], dtype=float32)

time = 57652	action = 1	current_phase = 0	next_phase = 1	reward = -1.701463	array([[-7.14992  , -3.5400794]], dtype=float32)

time = 57660	action = 1	current_phase = 1	next_phase = 0	reward = -0.952458	array([[-3.9866138, -2.5330153]], dtype=float32)

time = 57668	action = 0	current_phase = 0	next_phase = 1	reward = -0.050740	array([[-1.813625 , -2.8613267]], dtype=float32)

time = 57673	action = 0	current_phase = 0	next_phase = 1	reward = 0.021048	array([[-2.259122 , -2.9228673]], dtype=float32)

time = 57678	action = 0	current_phase = 0	next_phase = 1	reward = 0.081748	array([[-2.836397, -3.472758]], dtype=float32)

time = 57683	action = 1	current_phase = 0	next_phase = 1	reward = -1.368409	array([[-7.1827426, -3.2465837]], dtype=float32)

time = 57691	action = 1	current_phase = 1	next_phase = 0	reward = -1.443400	array([[-4.02903  , -2.4978385]], dtype=float32)

time = 57699	action = 0	current_phase = 0	next_phase = 1	reward = 0.260754	array([[-1.3708134, -2.0525455]], dtype=float32)

time = 57704	action = 0	current_phase = 0	next_phase = 1	reward = 0.069187	array([[-1.8171166, -2.8748915]], dtype=float32)

time = 57709	action = 1	current_phase = 0	next_phase = 1	reward = -0.688324	array([[-2.5632505, -2.3580413]], dtype=float32)

time = 57717	action = 1	current_phase = 1	next_phase = 0	reward = -0.649336	array([[-3.5210712, -2.0945983]], dtype=float32)

time = 57725	action = 0	current_phase = 0	next_phase = 1	reward = -0.115090	array([[-1.6789453, -3.0580168]], dtype=float32)

time = 57730	action = 1	current_phase = 0	next_phase = 1	reward = -2.117328	array([[-2.153535 , -2.0807822]], dtype=float32)

time = 57738	action = 1	current_phase = 1	next_phase = 0	reward = -0.561243	array([[-2.7448049, -1.4438658]], dtype=float32)

time = 57746	action = 0	current_phase = 0	next_phase = 1	reward = -0.875456	array([[-3.0205276, -3.4373503]], dtype=float32)

time = 57751	action = 1	current_phase = 0	next_phase = 1	reward = -2.502692	array([[-6.7516165, -3.3903103]], dtype=float32)

time = 57759	action = 1	current_phase = 1	next_phase = 0	reward = -0.655657	array([[-3.3739314, -2.1754403]], dtype=float32)

time = 57767	action = 0	current_phase = 0	next_phase = 1	reward = 0.079621	array([[-2.451034 , -3.3268347]], dtype=float32)

time = 57772	action = 0	current_phase = 0	next_phase = 1	reward = -0.204931	array([[-2.561411, -3.323014]], dtype=float32)

time = 57777	action = 0	current_phase = 0	next_phase = 1	reward = -1.019378	array([[-2.4749615, -3.325995 ]], dtype=float32)

time = 57782	action = 1	current_phase = 0	next_phase = 1	reward = -2.656635	array([[-6.7921953, -3.3272243]], dtype=float32)

time = 57790	action = 1	current_phase = 1	next_phase = 0	reward = -0.991335	array([[-3.9379787, -2.1702065]], dtype=float32)

time = 57798	action = 0	current_phase = 0	next_phase = 1	reward = 0.383210	array([[-2.3722672, -3.3466358]], dtype=float32)

time = 57803	action = 0	current_phase = 0	next_phase = 1	reward = -0.368512	array([[-2.6388843, -3.2116723]], dtype=float32)

time = 57808	action = 0	current_phase = 0	next_phase = 1	reward = -1.090881	array([[-2.6882563, -3.191767 ]], dtype=float32)

time = 57813	action = 1	current_phase = 0	next_phase = 1	reward = -2.712908	array([[-6.786906 , -3.3240185]], dtype=float32)

time = 57821	action = 1	current_phase = 1	next_phase = 0	reward = -0.773150	array([[-3.7336936, -2.2009935]], dtype=float32)

time = 57829	action = 0	current_phase = 0	next_phase = 1	reward = 0.064828	array([[-2.1686728, -2.3307273]], dtype=float32)

time = 57834	action = 1	current_phase = 0	next_phase = 1	reward = -1.125334	array([[-3.3888085, -2.969235 ]], dtype=float32)

time = 57842	action = 1	current_phase = 1	next_phase = 0	reward = -1.034024	array([[-3.1411586, -2.2656069]], dtype=float32)

time = 57850	action = 0	current_phase = 0	next_phase = 1	reward = 0.254774	array([[-1.6844256, -2.213679 ]], dtype=float32)

time = 57855	action = 0	current_phase = 0	next_phase = 1	reward = 0.042681	array([[-2.3557665, -3.1872582]], dtype=float32)

time = 57860	action = 1	current_phase = 0	next_phase = 1	reward = -1.308091	array([[-6.0027575, -2.921388 ]], dtype=float32)

time = 57868	action = 1	current_phase = 1	next_phase = 0	reward = -0.702287	array([[-3.5652668, -2.1912622]], dtype=float32)

time = 57876	action = 0	current_phase = 0	next_phase = 1	reward = -0.094726	array([[-1.8230441, -2.922127 ]], dtype=float32)

time = 57881	action = 0	current_phase = 0	next_phase = 1	reward = -0.000379	array([[-2.1359801, -2.9143357]], dtype=float32)

time = 57886	action = 0	current_phase = 0	next_phase = 1	reward = 0.073213	array([[-2.5700698, -3.3495831]], dtype=float32)

time = 57891	action = 1	current_phase = 0	next_phase = 1	reward = -1.556827	array([[-7.1876645, -3.3376458]], dtype=float32)

time = 57899	action = 1	current_phase = 1	next_phase = 0	reward = -0.902050	array([[-3.8346686, -2.389923 ]], dtype=float32)

time = 57907	action = 0	current_phase = 0	next_phase = 1	reward = -0.075104	array([[-1.8847022, -2.861061 ]], dtype=float32)

time = 57912	action = 0	current_phase = 0	next_phase = 1	reward = -0.000540	array([[-2.2496026, -2.9396174]], dtype=float32)

time = 57917	action = 0	current_phase = 0	next_phase = 1	reward = 0.075347	array([[-2.835847 , -3.4420846]], dtype=float32)

time = 57922	action = 1	current_phase = 0	next_phase = 1	reward = -1.657279	array([[-7.1377006, -3.5957835]], dtype=float32)

time = 57930	action = 1	current_phase = 1	next_phase = 0	reward = -0.853114	array([[-4.0140066, -2.5307698]], dtype=float32)

time = 57938	action = 0	current_phase = 0	next_phase = 1	reward = -0.069915	array([[-1.7731159, -2.8822703]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0147 - val_loss: 0.0096

Epoch 2/50

 - 3s - loss: 0.0142 - val_loss: 0.0108

Epoch 3/50

 - 3s - loss: 0.0138 - val_loss: 0.0139

Epoch 4/50

 - 3s - loss: 0.0153 - val_loss: 0.0084

Epoch 5/50

 - 3s - loss: 0.0205 - val_loss: 0.0099

Epoch 6/50

 - 3s - loss: 0.0149 - val_loss: 0.0105

Epoch 7/50

 - 3s - loss: 0.0150 - val_loss: 0.0097

Epoch 8/50

 - 3s - loss: 0.0199 - val_loss: 0.0108

Epoch 9/50

 - 3s - loss: 0.0127 - val_loss: 0.0092

Epoch 10/50

 - 3s - loss: 0.0176 - val_loss: 0.0097

Epoch 11/50

 - 3s - loss: 0.0124 - val_loss: 0.0088

Epoch 12/50

 - 3s - loss: 0.0146 - val_loss: 0.0142

Epoch 13/50

 - 3s - loss: 0.0168 - val_loss: 0.0101

Epoch 14/50

 - 3s - loss: 0.0131 - val_loss: 0.0075

Epoch 15/50

 - 3s - loss: 0.0149 - val_loss: 0.0088

Epoch 16/50

 - 3s - loss: 0.0130 - val_loss: 0.0067

Epoch 17/50

 - 3s - loss: 0.0108 - val_loss: 0.0088

Epoch 18/50

 - 3s - loss: 0.0144 - val_loss: 0.0126

Epoch 19/50

 - 3s - loss: 0.0129 - val_loss: 0.0229

Epoch 20/50

 - 3s - loss: 0.0165 - val_loss: 0.0141

Epoch 21/50

 - 3s - loss: 0.0149 - val_loss: 0.0119

Epoch 22/50

 - 3s - loss: 0.0119 - val_loss: 0.0093

Epoch 23/50

 - 3s - loss: 0.0134 - val_loss: 0.0101

Epoch 24/50

 - 3s - loss: 0.0154 - val_loss: 0.0118

Epoch 25/50

 - 3s - loss: 0.0171 - val_loss: 0.0106

Epoch 26/50

 - 3s - loss: 0.0137 - val_loss: 0.0113

length of memory (state 0, action 0): 1025, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1011, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1011, before forget

length of memory (state 1, action 1): 1000, after forget

time = 57943	action = 0	current_phase = 0	next_phase = 1	reward = -0.009152	array([[-2.2881806, -2.9248006]], dtype=float32)

time = 57948	action = 0	current_phase = 0	next_phase = 1	reward = 0.064745	array([[-2.8813572, -3.4489279]], dtype=float32)

time = 57953	action = 1	current_phase = 0	next_phase = 1	reward = -1.837870	array([[-7.175152 , -3.5263805]], dtype=float32)

time = 57961	action = 1	current_phase = 1	next_phase = 0	reward = -0.981703	array([[-4.0354886, -2.3684862]], dtype=float32)

time = 57969	action = 0	current_phase = 0	next_phase = 1	reward = -0.037311	array([[-1.5419362, -2.0515842]], dtype=float32)

time = 57974	action = 0	current_phase = 0	next_phase = 1	reward = 0.024476	array([[-1.9211681, -2.8897343]], dtype=float32)

time = 57979	action = 1	current_phase = 0	next_phase = 1	reward = -0.723674	array([[-2.703377 , -2.2584033]], dtype=float32)

time = 57987	action = 1	current_phase = 1	next_phase = 0	reward = -0.928933	array([[-3.5483248, -2.1448927]], dtype=float32)

time = 57995	action = 0	current_phase = 0	next_phase = 1	reward = -0.104483	array([[-1.3880501, -3.1125298]], dtype=float32)

time = 58000	action = 1	current_phase = 0	next_phase = 1	reward = -1.798424	array([[-2.0458498, -2.0335588]], dtype=float32)

time = 58008	action = 1	current_phase = 1	next_phase = 0	reward = -0.566018	array([[-2.4295833, -1.4050357]], dtype=float32)

time = 58016	action = 1	current_phase = 0	next_phase = 1	reward = -2.220478	array([[-3.4091465, -3.3113303]], dtype=float32)

time = 58024	action = 1	current_phase = 1	next_phase = 0	reward = -0.744377	array([[-3.899581, -2.414405]], dtype=float32)

time = 58032	action = 0	current_phase = 0	next_phase = 1	reward = -0.003583	array([[-1.9503481, -2.9046917]], dtype=float32)

time = 58037	action = 0	current_phase = 0	next_phase = 1	reward = 0.073712	array([[-2.4649336, -3.2997928]], dtype=float32)

time = 58042	action = 1	current_phase = 0	next_phase = 1	reward = -1.593109	array([[-6.87132  , -3.3180647]], dtype=float32)

time = 58050	action = 1	current_phase = 1	next_phase = 0	reward = -0.970127	array([[-3.7746584, -2.3382106]], dtype=float32)

time = 58058	action = 0	current_phase = 0	next_phase = 1	reward = -0.065493	array([[-1.8781817, -2.8727942]], dtype=float32)

time = 58063	action = 0	current_phase = 0	next_phase = 1	reward = 0.025346	array([[-2.2675726, -2.9191031]], dtype=float32)

time = 58068	action = 0	current_phase = 0	next_phase = 1	reward = 0.079218	array([[-2.8788528, -3.4816737]], dtype=float32)

time = 58073	action = 1	current_phase = 0	next_phase = 1	reward = -1.812341	array([[-7.1278987, -3.64775  ]], dtype=float32)

time = 58081	action = 1	current_phase = 1	next_phase = 0	reward = -1.326582	array([[-4.063735 , -2.3494785]], dtype=float32)

time = 58089	action = 0	current_phase = 0	next_phase = 1	reward = 0.263107	array([[-1.4062624, -2.0460258]], dtype=float32)

time = 58094	action = 0	current_phase = 0	next_phase = 1	reward = 0.032874	array([[-1.8792214, -2.8809743]], dtype=float32)

time = 58099	action = 1	current_phase = 0	next_phase = 1	reward = -0.656272	array([[-2.7182524, -2.2563424]], dtype=float32)

time = 58107	action = 1	current_phase = 1	next_phase = 0	reward = -0.910479	array([[-3.4618201, -2.074096 ]], dtype=float32)

time = 58115	action = 0	current_phase = 0	next_phase = 1	reward = 0.189234	array([[-1.5413578, -3.0854788]], dtype=float32)

time = 58120	action = 1	current_phase = 0	next_phase = 1	reward = -2.126977	array([[-2.1658921, -2.025613 ]], dtype=float32)

time = 58128	action = 1	current_phase = 1	next_phase = 0	reward = -0.570615	array([[-2.5698965, -1.3918362]], dtype=float32)

time = 58136	action = 1	current_phase = 0	next_phase = 1	reward = -2.242364	array([[-3.5488706, -3.4202561]], dtype=float32)

time = 58144	action = 1	current_phase = 1	next_phase = 0	reward = -0.738610	array([[-3.9028366, -2.457303 ]], dtype=float32)

time = 58152	action = 0	current_phase = 0	next_phase = 1	reward = 0.000489	array([[-1.9823568, -2.9165945]], dtype=float32)

time = 58157	action = 0	current_phase = 0	next_phase = 1	reward = 0.067057	array([[-2.4528975, -3.2864175]], dtype=float32)

time = 58162	action = 1	current_phase = 0	next_phase = 1	reward = -1.610551	array([[-5.1651983, -3.2121253]], dtype=float32)

time = 58170	action = 1	current_phase = 1	next_phase = 0	reward = -0.959938	array([[-3.7745798, -2.305241 ]], dtype=float32)

time = 58178	action = 0	current_phase = 0	next_phase = 1	reward = -0.062552	array([[-1.8446746, -2.8763118]], dtype=float32)

time = 58183	action = 0	current_phase = 0	next_phase = 1	reward = -0.001527	array([[-2.2710173, -2.9233856]], dtype=float32)

time = 58188	action = 0	current_phase = 0	next_phase = 1	reward = 0.083109	array([[-2.8625772, -3.4542174]], dtype=float32)

time = 58193	action = 1	current_phase = 0	next_phase = 1	reward = -1.363079	array([[-7.214882 , -3.3745198]], dtype=float32)

time = 58201	action = 1	current_phase = 1	next_phase = 0	reward = -1.494013	array([[-3.904936 , -2.4187012]], dtype=float32)

time = 58209	action = 0	current_phase = 0	next_phase = 1	reward = 0.242255	array([[-1.4044101, -2.0922456]], dtype=float32)

time = 58214	action = 0	current_phase = 0	next_phase = 1	reward = 0.004949	array([[-1.8605435, -2.881205 ]], dtype=float32)

time = 58219	action = 1	current_phase = 0	next_phase = 1	reward = -0.667894	array([[-2.640757 , -2.3038106]], dtype=float32)

time = 58227	action = 1	current_phase = 1	next_phase = 0	reward = -0.639918	array([[-3.488671 , -2.1204553]], dtype=float32)

time = 58235	action = 0	current_phase = 0	next_phase = 1	reward = -0.644100	array([[-1.6792874, -3.0894728]], dtype=float32)

time = 58240	action = 0	current_phase = 0	next_phase = 1	reward = 0.553511	array([[-1.8432446, -2.0585837]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0167 - val_loss: 0.0093

Epoch 2/50

 - 3s - loss: 0.0189 - val_loss: 0.0097

Epoch 3/50

 - 3s - loss: 0.0150 - val_loss: 0.0112

Epoch 4/50

 - 3s - loss: 0.0177 - val_loss: 0.0086

Epoch 5/50

 - 3s - loss: 0.0166 - val_loss: 0.0076

Epoch 6/50

 - 3s - loss: 0.0157 - val_loss: 0.0101

Epoch 7/50

 - 3s - loss: 0.0141 - val_loss: 0.0105

Epoch 8/50

 - 3s - loss: 0.0192 - val_loss: 0.0086

Epoch 9/50

 - 3s - loss: 0.0188 - val_loss: 0.0090

Epoch 10/50

 - 3s - loss: 0.0144 - val_loss: 0.0101

Epoch 11/50

 - 3s - loss: 0.0176 - val_loss: 0.0117

Epoch 12/50

 - 3s - loss: 0.0201 - val_loss: 0.0094

Epoch 13/50

 - 3s - loss: 0.0146 - val_loss: 0.0103

Epoch 14/50

 - 3s - loss: 0.0163 - val_loss: 0.0101

Epoch 15/50

 - 3s - loss: 0.0153 - val_loss: 0.0108

length of memory (state 0, action 0): 1022, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1012, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1012, before forget

length of memory (state 1, action 1): 1000, after forget

time = 58245	action = 0	current_phase = 0	next_phase = 1	reward = -0.215943	array([[-2.509934 , -3.2704809]], dtype=float32)

time = 58250	action = 1	current_phase = 0	next_phase = 1	reward = -1.107516	array([[-6.156743, -2.789202]], dtype=float32)

time = 58258	action = 1	current_phase = 1	next_phase = 0	reward = -0.722490	array([[-3.6166997, -2.2560496]], dtype=float32)

time = 58266	action = 0	current_phase = 0	next_phase = 1	reward = -0.077829	array([[-1.9332364, -2.8837886]], dtype=float32)

time = 58271	action = 0	current_phase = 0	next_phase = 1	reward = -0.005454	array([[-2.2401798, -2.9111476]], dtype=float32)

time = 58276	action = 0	current_phase = 0	next_phase = 1	reward = 0.065470	array([[-2.7138052, -3.409404 ]], dtype=float32)

time = 58281	action = 1	current_phase = 0	next_phase = 1	reward = -1.502178	array([[-7.2231174, -3.2616026]], dtype=float32)

time = 58289	action = 1	current_phase = 1	next_phase = 0	reward = -0.838316	array([[-3.7641468, -2.4028497]], dtype=float32)

time = 58297	action = 0	current_phase = 0	next_phase = 1	reward = -0.069545	array([[-1.9381441, -2.8703363]], dtype=float32)

time = 58302	action = 0	current_phase = 0	next_phase = 1	reward = 0.009602	array([[-2.2742505, -2.9477148]], dtype=float32)

time = 58307	action = 0	current_phase = 0	next_phase = 1	reward = 0.072343	array([[-2.878478, -3.487981]], dtype=float32)

time = 58312	action = 1	current_phase = 0	next_phase = 1	reward = -1.584387	array([[-7.1809516, -3.5755384]], dtype=float32)

time = 58320	action = 1	current_phase = 1	next_phase = 0	reward = -0.856188	array([[-3.903429, -2.500041]], dtype=float32)

time = 58328	action = 0	current_phase = 0	next_phase = 1	reward = -0.077646	array([[-1.8022738, -2.88466  ]], dtype=float32)

time = 58333	action = 0	current_phase = 0	next_phase = 1	reward = -0.000624	array([[-2.282301, -2.94046 ]], dtype=float32)

time = 58338	action = 0	current_phase = 0	next_phase = 1	reward = 0.074946	array([[-2.845036 , -3.4739246]], dtype=float32)

time = 58343	action = 1	current_phase = 0	next_phase = 1	reward = -1.900852	array([[-7.156852 , -3.7142355]], dtype=float32)

time = 58351	action = 1	current_phase = 1	next_phase = 0	reward = -1.093504	array([[-4.0504255, -2.3948278]], dtype=float32)

time = 58359	action = 0	current_phase = 0	next_phase = 1	reward = -0.031871	array([[-1.5712054, -2.0264888]], dtype=float32)

time = 58364	action = 0	current_phase = 0	next_phase = 1	reward = 0.049950	array([[-2.0062203, -2.9186697]], dtype=float32)

time = 58369	action = 1	current_phase = 0	next_phase = 1	reward = -0.685450	array([[-2.7424688, -2.2896266]], dtype=float32)

time = 58377	action = 1	current_phase = 1	next_phase = 0	reward = -0.895149	array([[-3.4846778, -2.1683304]], dtype=float32)

time = 58385	action = 0	current_phase = 0	next_phase = 1	reward = 0.186226	array([[-1.6384146, -3.0861032]], dtype=float32)

time = 58390	action = 1	current_phase = 0	next_phase = 1	reward = -2.005883	array([[-2.1485705, -2.0201018]], dtype=float32)

time = 58398	action = 1	current_phase = 1	next_phase = 0	reward = -0.562044	array([[-2.5712957, -1.4628972]], dtype=float32)

time = 58406	action = 1	current_phase = 0	next_phase = 1	reward = -2.377922	array([[-4.612467 , -3.3962972]], dtype=float32)

time = 58414	action = 1	current_phase = 1	next_phase = 0	reward = -0.750115	array([[-4.0668836, -2.504315 ]], dtype=float32)

time = 58422	action = 0	current_phase = 0	next_phase = 1	reward = 0.015805	array([[-1.9848955, -2.908744 ]], dtype=float32)

time = 58427	action = 0	current_phase = 0	next_phase = 1	reward = 0.079855	array([[-2.4496038, -3.400453 ]], dtype=float32)

time = 58432	action = 1	current_phase = 0	next_phase = 1	reward = -1.486019	array([[-4.546975 , -3.2716677]], dtype=float32)

time = 58440	action = 1	current_phase = 1	next_phase = 0	reward = -1.317877	array([[-3.826428 , -2.3899271]], dtype=float32)

time = 58448	action = 0	current_phase = 0	next_phase = 1	reward = 0.219772	array([[-1.7575742, -2.8793588]], dtype=float32)

time = 58453	action = 0	current_phase = 0	next_phase = 1	reward = 0.017306	array([[-2.2790587, -2.9447677]], dtype=float32)

time = 58458	action = 0	current_phase = 0	next_phase = 1	reward = 0.066253	array([[-2.8472862, -3.4768653]], dtype=float32)

time = 58463	action = 1	current_phase = 0	next_phase = 1	reward = -1.947486	array([[-7.178018 , -3.6375887]], dtype=float32)

time = 58471	action = 1	current_phase = 1	next_phase = 0	reward = -0.921748	array([[-4.1139083, -2.3994832]], dtype=float32)

time = 58479	action = 0	current_phase = 0	next_phase = 1	reward = -0.027174	array([[-1.5510497, -2.022986 ]], dtype=float32)

time = 58484	action = 0	current_phase = 0	next_phase = 1	reward = 0.032919	array([[-1.9410952, -2.9158342]], dtype=float32)

time = 58489	action = 1	current_phase = 0	next_phase = 1	reward = -0.703372	array([[-2.7619162, -2.2987106]], dtype=float32)

time = 58497	action = 1	current_phase = 1	next_phase = 0	reward = -1.213778	array([[-3.5352411, -2.1776485]], dtype=float32)

time = 58505	action = 0	current_phase = 0	next_phase = 1	reward = 0.182088	array([[-1.4471486, -3.144439 ]], dtype=float32)

time = 58510	action = 1	current_phase = 0	next_phase = 1	reward = -1.718248	array([[-2.0557547, -2.0141761]], dtype=float32)

time = 58518	action = 1	current_phase = 1	next_phase = 0	reward = -0.554644	array([[-2.6677723, -1.4740467]], dtype=float32)

time = 58526	action = 1	current_phase = 0	next_phase = 1	reward = -2.265363	array([[-4.78558  , -3.4153054]], dtype=float32)

time = 58534	action = 1	current_phase = 1	next_phase = 0	reward = -0.746525	array([[-3.9473796, -2.4737654]], dtype=float32)

time = 58542	action = 0	current_phase = 0	next_phase = 1	reward = 0.002941	array([[-2.035702 , -2.9288177]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0159 - val_loss: 0.0192

Epoch 2/50

 - 3s - loss: 0.0278 - val_loss: 0.0202

Epoch 3/50

 - 3s - loss: 0.0155 - val_loss: 0.0222

Epoch 4/50

 - 3s - loss: 0.0160 - val_loss: 0.0210

Epoch 5/50

 - 3s - loss: 0.0175 - val_loss: 0.0186

Epoch 6/50

 - 3s - loss: 0.0273 - val_loss: 0.0196

Epoch 7/50

 - 3s - loss: 0.0217 - val_loss: 0.0196

Epoch 8/50

 - 3s - loss: 0.0139 - val_loss: 0.0177

Epoch 9/50

 - 3s - loss: 0.0157 - val_loss: 0.0207

Epoch 10/50

 - 3s - loss: 0.0223 - val_loss: 0.0197

Epoch 11/50

 - 3s - loss: 0.0158 - val_loss: 0.0240

Epoch 12/50

 - 3s - loss: 0.0139 - val_loss: 0.0202

Epoch 13/50

 - 3s - loss: 0.0196 - val_loss: 0.0195

Epoch 14/50

 - 3s - loss: 0.0165 - val_loss: 0.0200

Epoch 15/50

 - 3s - loss: 0.0183 - val_loss: 0.0194

Epoch 16/50

 - 3s - loss: 0.0168 - val_loss: 0.0220

Epoch 17/50

 - 3s - loss: 0.0158 - val_loss: 0.0181

Epoch 18/50

 - 3s - loss: 0.0226 - val_loss: 0.0192

length of memory (state 0, action 0): 1022, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1012, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1012, before forget

length of memory (state 1, action 1): 1000, after forget

time = 58547	action = 0	current_phase = 0	next_phase = 1	reward = 0.065679	array([[-2.4090302, -3.3921356]], dtype=float32)

time = 58552	action = 1	current_phase = 0	next_phase = 1	reward = -1.690830	array([[-6.134041, -3.323287]], dtype=float32)

time = 58560	action = 1	current_phase = 1	next_phase = 0	reward = -0.960033	array([[-3.7680497, -2.408216 ]], dtype=float32)

time = 58568	action = 0	current_phase = 0	next_phase = 1	reward = -0.050383	array([[-1.7071877, -2.8625002]], dtype=float32)

time = 58573	action = 0	current_phase = 0	next_phase = 1	reward = 0.020835	array([[-2.2221577, -2.9447503]], dtype=float32)

time = 58578	action = 0	current_phase = 0	next_phase = 1	reward = 0.076097	array([[-2.789402 , -3.4747138]], dtype=float32)

time = 58583	action = 1	current_phase = 0	next_phase = 1	reward = -1.307951	array([[-7.1558723, -3.2950916]], dtype=float32)

time = 58591	action = 1	current_phase = 1	next_phase = 0	reward = -1.450321	array([[-3.9235854, -2.460011 ]], dtype=float32)

time = 58599	action = 0	current_phase = 0	next_phase = 1	reward = 0.253248	array([[-1.514245 , -2.0314531]], dtype=float32)

time = 58604	action = 0	current_phase = 0	next_phase = 1	reward = 0.047451	array([[-1.8012352, -2.8948057]], dtype=float32)

time = 58609	action = 1	current_phase = 0	next_phase = 1	reward = -0.671428	array([[-2.6436095, -2.2931871]], dtype=float32)

time = 58617	action = 1	current_phase = 1	next_phase = 0	reward = -1.203075	array([[-3.4716725, -2.1914592]], dtype=float32)

time = 58625	action = 0	current_phase = 0	next_phase = 1	reward = 0.200903	array([[-1.3112371, -3.1219187]], dtype=float32)

time = 58630	action = 0	current_phase = 0	next_phase = 1	reward = 0.271043	array([[-1.8547943, -1.9904746]], dtype=float32)

time = 58635	action = 0	current_phase = 0	next_phase = 1	reward = 0.046768	array([[-2.4649205, -3.3171349]], dtype=float32)

time = 58640	action = 1	current_phase = 0	next_phase = 1	reward = -1.272646	array([[-6.080068 , -2.9309082]], dtype=float32)

time = 58648	action = 1	current_phase = 1	next_phase = 0	reward = -0.995818	array([[-3.5497308, -2.2329962]], dtype=float32)

time = 58656	action = 0	current_phase = 0	next_phase = 1	reward = 0.213002	array([[-1.7384874, -2.8815622]], dtype=float32)

time = 58661	action = 0	current_phase = 0	next_phase = 1	reward = -0.004941	array([[-2.199825 , -2.9477816]], dtype=float32)

time = 58666	action = 0	current_phase = 0	next_phase = 1	reward = 0.062523	array([[-2.5386806, -3.371231 ]], dtype=float32)

time = 58671	action = 1	current_phase = 0	next_phase = 1	reward = -1.424441	array([[-7.182707 , -3.2848186]], dtype=float32)

time = 58679	action = 1	current_phase = 1	next_phase = 0	reward = -1.499203	array([[-3.7602205, -2.4093838]], dtype=float32)

time = 58687	action = 0	current_phase = 0	next_phase = 1	reward = 0.512377	array([[-1.6408012, -2.8722873]], dtype=float32)

time = 58692	action = 0	current_phase = 0	next_phase = 1	reward = -0.001226	array([[-2.2343657, -2.9496613]], dtype=float32)

time = 58697	action = 0	current_phase = 0	next_phase = 1	reward = 0.062610	array([[-2.8398457, -3.4982514]], dtype=float32)

time = 58702	action = 1	current_phase = 0	next_phase = 1	reward = -1.587916	array([[-7.1339273, -3.5803466]], dtype=float32)

time = 58710	action = 1	current_phase = 1	next_phase = 0	reward = -1.245944	array([[-3.8939219, -2.5115151]], dtype=float32)

time = 58718	action = 0	current_phase = 0	next_phase = 1	reward = 0.243609	array([[-1.6273404, -2.863842 ]], dtype=float32)

time = 58723	action = 0	current_phase = 0	next_phase = 1	reward = 0.012144	array([[-2.223226 , -2.9539676]], dtype=float32)

time = 58728	action = 0	current_phase = 0	next_phase = 1	reward = 0.064343	array([[-2.7768636, -3.4687457]], dtype=float32)

time = 58733	action = 1	current_phase = 0	next_phase = 1	reward = -1.769118	array([[-7.113649, -3.719606]], dtype=float32)

time = 58741	action = 1	current_phase = 1	next_phase = 0	reward = -0.900506	array([[-4.132016 , -2.4497623]], dtype=float32)

time = 58749	action = 0	current_phase = 0	next_phase = 1	reward = -0.025626	array([[-1.5332754, -2.0565252]], dtype=float32)

time = 58754	action = 0	current_phase = 0	next_phase = 1	reward = 0.047883	array([[-1.7800033, -2.9132218]], dtype=float32)

time = 58759	action = 1	current_phase = 0	next_phase = 1	reward = -0.631767	array([[-2.6689236, -2.277761 ]], dtype=float32)

time = 58767	action = 1	current_phase = 1	next_phase = 0	reward = -0.633373	array([[-3.4392142, -2.1456943]], dtype=float32)

time = 58775	action = 0	current_phase = 0	next_phase = 1	reward = -0.096465	array([[-1.5957832, -3.1280608]], dtype=float32)

time = 58780	action = 1	current_phase = 0	next_phase = 1	reward = -2.192694	array([[-2.070858, -2.007646]], dtype=float32)

time = 58788	action = 1	current_phase = 1	next_phase = 0	reward = -0.555899	array([[-2.6974065, -1.5025904]], dtype=float32)

time = 58796	action = 0	current_phase = 0	next_phase = 1	reward = -0.774663	array([[-3.1404464, -3.5555654]], dtype=float32)

time = 58801	action = 1	current_phase = 0	next_phase = 1	reward = -2.496128	array([[-6.635759, -3.748599]], dtype=float32)

time = 58809	action = 1	current_phase = 1	next_phase = 0	reward = -0.651451	array([[-3.3511405, -2.168859 ]], dtype=float32)

time = 58817	action = 0	current_phase = 0	next_phase = 1	reward = 0.075317	array([[-2.4985764, -3.30903  ]], dtype=float32)

time = 58822	action = 0	current_phase = 0	next_phase = 1	reward = -0.141839	array([[-2.6137521, -3.4319882]], dtype=float32)

time = 58827	action = 0	current_phase = 0	next_phase = 1	reward = -0.918985	array([[-2.659137 , -3.4744244]], dtype=float32)

time = 58832	action = 1	current_phase = 0	next_phase = 1	reward = -2.528746	array([[-6.762877 , -3.8487864]], dtype=float32)

time = 58840	action = 1	current_phase = 1	next_phase = 0	reward = -0.556974	array([[-3.4093132, -2.1986456]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0206 - val_loss: 0.0221

Epoch 2/50

 - 3s - loss: 0.0189 - val_loss: 0.0210

Epoch 3/50

 - 3s - loss: 0.0170 - val_loss: 0.0210

Epoch 4/50

 - 3s - loss: 0.0166 - val_loss: 0.0213

Epoch 5/50

 - 3s - loss: 0.0172 - val_loss: 0.0209

Epoch 6/50

 - 3s - loss: 0.0194 - val_loss: 0.0215

Epoch 7/50

 - 3s - loss: 0.0172 - val_loss: 0.0218

Epoch 8/50

 - 3s - loss: 0.0182 - val_loss: 0.0215

Epoch 9/50

 - 3s - loss: 0.0146 - val_loss: 0.0229

Epoch 10/50

 - 3s - loss: 0.0187 - val_loss: 0.0249

Epoch 11/50

 - 3s - loss: 0.0172 - val_loss: 0.0225

Epoch 12/50

 - 3s - loss: 0.0180 - val_loss: 0.0237

Epoch 13/50

 - 3s - loss: 0.0147 - val_loss: 0.0235

Epoch 14/50

 - 3s - loss: 0.0188 - val_loss: 0.0213

Epoch 15/50

 - 3s - loss: 0.0157 - val_loss: 0.0211

length of memory (state 0, action 0): 1025, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1011, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1011, before forget

length of memory (state 1, action 1): 1000, after forget

time = 58848	action = 0	current_phase = 0	next_phase = 1	reward = 0.071765	array([[-2.3945005, -3.3955302]], dtype=float32)

time = 58853	action = 1	current_phase = 0	next_phase = 1	reward = -1.384722	array([[-4.7223196, -3.2529898]], dtype=float32)

time = 58861	action = 1	current_phase = 1	next_phase = 0	reward = -1.181748	array([[-3.653286, -2.313959]], dtype=float32)

time = 58869	action = 0	current_phase = 0	next_phase = 1	reward = -0.040139	array([[-1.6035028, -2.0939162]], dtype=float32)

time = 58874	action = 0	current_phase = 0	next_phase = 1	reward = 0.043058	array([[-1.886713 , -2.8979697]], dtype=float32)

time = 58879	action = 1	current_phase = 0	next_phase = 1	reward = -0.694671	array([[-2.5517197, -2.483959 ]], dtype=float32)

time = 58887	action = 1	current_phase = 1	next_phase = 0	reward = -0.889548	array([[-3.5036864, -2.10482  ]], dtype=float32)

time = 58895	action = 0	current_phase = 0	next_phase = 1	reward = -0.383642	array([[-1.5017672, -3.0973976]], dtype=float32)

time = 58900	action = 0	current_phase = 0	next_phase = 1	reward = 0.543524	array([[-1.7113719, -2.0851882]], dtype=float32)

time = 58905	action = 0	current_phase = 0	next_phase = 1	reward = 0.038913	array([[-2.3317587, -3.3058562]], dtype=float32)

time = 58910	action = 1	current_phase = 0	next_phase = 1	reward = -1.418336	array([[-6.082914 , -3.1640582]], dtype=float32)

time = 58918	action = 1	current_phase = 1	next_phase = 0	reward = -0.717262	array([[-3.6808958, -2.222178 ]], dtype=float32)

time = 58926	action = 0	current_phase = 0	next_phase = 1	reward = -0.101948	array([[-1.8078401, -2.927002 ]], dtype=float32)

time = 58931	action = 0	current_phase = 0	next_phase = 1	reward = -0.018936	array([[-2.1535802, -2.9448252]], dtype=float32)

time = 58936	action = 0	current_phase = 0	next_phase = 1	reward = 0.042864	array([[-2.5662074, -3.4275386]], dtype=float32)

time = 58941	action = 1	current_phase = 0	next_phase = 1	reward = -1.498011	array([[-7.1788583, -3.3586082]], dtype=float32)

time = 58949	action = 1	current_phase = 1	next_phase = 0	reward = -1.112640	array([[-3.7012978, -2.2630687]], dtype=float32)

time = 58957	action = 0	current_phase = 0	next_phase = 1	reward = 0.230154	array([[-1.817402 , -2.8811495]], dtype=float32)

time = 58962	action = 0	current_phase = 0	next_phase = 1	reward = 0.017201	array([[-2.2331517, -2.9521396]], dtype=float32)

time = 58967	action = 0	current_phase = 0	next_phase = 1	reward = 0.071411	array([[-2.7607632, -3.5209754]], dtype=float32)

time = 58972	action = 1	current_phase = 0	next_phase = 1	reward = -1.558563	array([[-7.16058  , -3.6600153]], dtype=float32)

time = 58980	action = 1	current_phase = 1	next_phase = 0	reward = -0.999280	array([[-3.8680267, -2.4126625]], dtype=float32)

time = 58988	action = 0	current_phase = 0	next_phase = 1	reward = -0.048078	array([[-1.7201595, -2.8686056]], dtype=float32)

time = 58993	action = 0	current_phase = 0	next_phase = 1	reward = 0.007471	array([[-2.2591507, -2.9610808]], dtype=float32)

time = 58998	action = 0	current_phase = 0	next_phase = 1	reward = 0.064535	array([[-2.7921062, -3.529508 ]], dtype=float32)

time = 59003	action = 1	current_phase = 0	next_phase = 1	reward = -1.821103	array([[-7.1564646, -3.7509305]], dtype=float32)

time = 59011	action = 1	current_phase = 1	next_phase = 0	reward = -1.333304	array([[-4.0832467, -2.412617 ]], dtype=float32)

time = 59019	action = 0	current_phase = 0	next_phase = 1	reward = 0.248225	array([[-1.3209313, -2.0791483]], dtype=float32)

time = 59024	action = 0	current_phase = 0	next_phase = 1	reward = 0.001671	array([[-1.804946 , -2.9005063]], dtype=float32)

time = 59029	action = 1	current_phase = 0	next_phase = 1	reward = -0.677023	array([[-2.6726208, -2.3729548]], dtype=float32)

time = 59037	action = 1	current_phase = 1	next_phase = 0	reward = -0.872697	array([[-3.424406 , -2.0442357]], dtype=float32)

time = 59045	action = 0	current_phase = 0	next_phase = 1	reward = 0.175121	array([[-1.4720824, -3.0981855]], dtype=float32)

time = 59050	action = 0	current_phase = 0	next_phase = 1	reward = -0.038161	array([[-2.0009038, -2.0403664]], dtype=float32)

time = 59055	action = 0	current_phase = 0	next_phase = 1	reward = -0.251625	array([[-2.3158963, -3.2457082]], dtype=float32)

time = 59060	action = 1	current_phase = 0	next_phase = 1	reward = -0.971300	array([[-6.089612 , -2.9258206]], dtype=float32)

time = 59068	action = 1	current_phase = 1	next_phase = 0	reward = -0.705409	array([[-3.554132 , -2.1583219]], dtype=float32)

time = 59076	action = 0	current_phase = 0	next_phase = 1	reward = -0.086787	array([[-1.850182 , -2.9271493]], dtype=float32)

time = 59081	action = 0	current_phase = 0	next_phase = 1	reward = -0.008495	array([[-2.1057098, -2.9246757]], dtype=float32)

time = 59086	action = 0	current_phase = 0	next_phase = 1	reward = 0.070498	array([[-2.5492494, -3.4431884]], dtype=float32)

time = 59091	action = 1	current_phase = 0	next_phase = 1	reward = -1.492204	array([[-7.210387 , -3.3985615]], dtype=float32)

time = 59099	action = 1	current_phase = 1	next_phase = 0	reward = -0.847968	array([[-3.7389717, -2.3057334]], dtype=float32)

time = 59107	action = 0	current_phase = 0	next_phase = 1	reward = -0.073004	array([[-1.8401867, -2.8813324]], dtype=float32)

time = 59112	action = 0	current_phase = 0	next_phase = 1	reward = 0.001194	array([[-2.2568474, -2.9525456]], dtype=float32)

time = 59117	action = 0	current_phase = 0	next_phase = 1	reward = 0.075740	array([[-2.8050601, -3.5150182]], dtype=float32)

time = 59122	action = 1	current_phase = 0	next_phase = 1	reward = -1.609255	array([[-7.1735454, -3.6453297]], dtype=float32)

time = 59130	action = 1	current_phase = 1	next_phase = 0	reward = -0.952850	array([[-3.8987412, -2.4654348]], dtype=float32)

time = 59138	action = 0	current_phase = 0	next_phase = 1	reward = -0.045662	array([[-1.8248718, -2.8843923]], dtype=float32)

time = 59143	action = 0	current_phase = 0	next_phase = 1	reward = 0.038294	array([[-2.2310207, -2.9433055]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0214 - val_loss: 0.0146

Epoch 2/50

 - 3s - loss: 0.0219 - val_loss: 0.0125

Epoch 3/50

 - 3s - loss: 0.0203 - val_loss: 0.0151

Epoch 4/50

 - 3s - loss: 0.0226 - val_loss: 0.0127

Epoch 5/50

 - 3s - loss: 0.0207 - val_loss: 0.0114

Epoch 6/50

 - 3s - loss: 0.0201 - val_loss: 0.0113

Epoch 7/50

 - 3s - loss: 0.0232 - val_loss: 0.0116

Epoch 8/50

 - 3s - loss: 0.0219 - val_loss: 0.0121

Epoch 9/50

 - 3s - loss: 0.0200 - val_loss: 0.0115

Epoch 10/50

 - 3s - loss: 0.0185 - val_loss: 0.0138

Epoch 11/50

 - 3s - loss: 0.0218 - val_loss: 0.0133

Epoch 12/50

 - 3s - loss: 0.0290 - val_loss: 0.0110

Epoch 13/50

 - 3s - loss: 0.0186 - val_loss: 0.0175

Epoch 14/50

 - 3s - loss: 0.0191 - val_loss: 0.0128

Epoch 15/50

 - 3s - loss: 0.0185 - val_loss: 0.0102

Epoch 16/50

 - 3s - loss: 0.0245 - val_loss: 0.0116

Epoch 17/50

 - 3s - loss: 0.0184 - val_loss: 0.0116

Epoch 18/50

 - 3s - loss: 0.0193 - val_loss: 0.0105

Epoch 19/50

 - 3s - loss: 0.0210 - val_loss: 0.0103

Epoch 20/50

 - 3s - loss: 0.0165 - val_loss: 0.0110

Epoch 21/50

 - 3s - loss: 0.0168 - val_loss: 0.0114

Epoch 22/50

 - 3s - loss: 0.0181 - val_loss: 0.0113

Epoch 23/50

 - 3s - loss: 0.0182 - val_loss: 0.0118

Epoch 24/50

 - 3s - loss: 0.0192 - val_loss: 0.0110

Epoch 25/50

 - 3s - loss: 0.0185 - val_loss: 0.0112

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 59148	action = 0	current_phase = 0	next_phase = 1	reward = 0.078642	array([[-2.7688742, -3.6110365]], dtype=float32)

time = 59153	action = 1	current_phase = 0	next_phase = 1	reward = -1.355232	array([[-7.2317047, -3.320031 ]], dtype=float32)

time = 59161	action = 1	current_phase = 1	next_phase = 0	reward = -1.435705	array([[-3.8749323, -2.4146705]], dtype=float32)

time = 59169	action = 0	current_phase = 0	next_phase = 1	reward = 0.244362	array([[-1.2649286, -2.0866127]], dtype=float32)

time = 59174	action = 0	current_phase = 0	next_phase = 1	reward = 0.039108	array([[-1.9274299, -2.946484 ]], dtype=float32)

time = 59179	action = 1	current_phase = 0	next_phase = 1	reward = -0.700700	array([[-2.5650923, -2.4661922]], dtype=float32)

time = 59187	action = 1	current_phase = 1	next_phase = 0	reward = -0.647478	array([[-3.4173398, -2.1017325]], dtype=float32)

time = 59195	action = 0	current_phase = 0	next_phase = 1	reward = -0.107628	array([[-1.5824866, -3.0817482]], dtype=float32)

time = 59200	action = 1	current_phase = 0	next_phase = 1	reward = -2.037582	array([[-2.0587032, -2.0520217]], dtype=float32)

time = 59208	action = 1	current_phase = 1	next_phase = 0	reward = -0.562125	array([[-2.552997 , -1.7357652]], dtype=float32)

time = 59216	action = 0	current_phase = 0	next_phase = 1	reward = -0.885063	array([[-2.6918976, -3.543349 ]], dtype=float32)

time = 59221	action = 1	current_phase = 0	next_phase = 1	reward = -2.488336	array([[-6.7135873, -4.1969824]], dtype=float32)

time = 59229	action = 1	current_phase = 1	next_phase = 0	reward = -0.606976	array([[-3.4204402, -2.2269402]], dtype=float32)

time = 59237	action = 0	current_phase = 0	next_phase = 1	reward = 0.067273	array([[-2.3752503, -3.498055 ]], dtype=float32)

time = 59242	action = 0	current_phase = 0	next_phase = 1	reward = -0.185263	array([[-2.3733678, -3.3111932]], dtype=float32)

time = 59247	action = 0	current_phase = 0	next_phase = 1	reward = -0.963414	array([[-2.9551413, -3.5437899]], dtype=float32)

time = 59252	action = 1	current_phase = 0	next_phase = 1	reward = -2.593815	array([[-6.8808565, -4.2769976]], dtype=float32)

time = 59260	action = 1	current_phase = 1	next_phase = 0	reward = -0.751348	array([[-3.9602184, -2.178004 ]], dtype=float32)

time = 59268	action = 0	current_phase = 0	next_phase = 1	reward = 0.077737	array([[-2.4076126, -3.402891 ]], dtype=float32)

time = 59273	action = 0	current_phase = 0	next_phase = 1	reward = -0.379354	array([[-2.5700915, -3.3147895]], dtype=float32)

time = 59278	action = 0	current_phase = 0	next_phase = 1	reward = -1.092570	array([[-1.9824784, -3.4166157]], dtype=float32)

time = 59283	action = 1	current_phase = 0	next_phase = 1	reward = -2.177150	array([[-6.879651, -3.668633]], dtype=float32)

time = 59291	action = 1	current_phase = 1	next_phase = 0	reward = -0.912233	array([[-3.4658527, -2.2520766]], dtype=float32)

time = 59299	action = 0	current_phase = 0	next_phase = 1	reward = 0.059518	array([[-2.2131214, -2.5996633]], dtype=float32)

time = 59304	action = 0	current_phase = 0	next_phase = 1	reward = -0.565382	array([[-2.1149678, -3.1290455]], dtype=float32)

time = 59309	action = 0	current_phase = 0	next_phase = 1	reward = -1.164934	array([[-2.4784603, -2.4886534]], dtype=float32)

time = 59314	action = 1	current_phase = 0	next_phase = 1	reward = -1.682521	array([[-6.919003, -3.289522]], dtype=float32)

time = 59322	action = 1	current_phase = 1	next_phase = 0	reward = -0.750239	array([[-3.1680858, -2.2070599]], dtype=float32)

time = 59330	action = 0	current_phase = 0	next_phase = 1	reward = 0.296835	array([[-2.6722858, -2.7846174]], dtype=float32)

time = 59335	action = 0	current_phase = 0	next_phase = 1	reward = -0.928447	array([[-2.4058821, -3.39382  ]], dtype=float32)

time = 59340	action = 1	current_phase = 0	next_phase = 1	reward = -2.120156	array([[-6.1284485, -3.4198923]], dtype=float32)

time = 59348	action = 1	current_phase = 1	next_phase = 0	reward = -0.623450	array([[-3.033657 , -2.2038784]], dtype=float32)

time = 59356	action = 0	current_phase = 0	next_phase = 1	reward = 0.055348	array([[-2.3415596, -3.3284428]], dtype=float32)

time = 59361	action = 0	current_phase = 0	next_phase = 1	reward = -0.052360	array([[-3.0684745, -3.319115 ]], dtype=float32)

time = 59366	action = 0	current_phase = 0	next_phase = 1	reward = -0.840825	array([[-2.443049, -3.320576]], dtype=float32)

time = 59371	action = 1	current_phase = 0	next_phase = 1	reward = -2.370242	array([[-6.6906505, -4.0283084]], dtype=float32)

time = 59379	action = 1	current_phase = 1	next_phase = 0	reward = -0.637783	array([[-3.4230366, -2.1278102]], dtype=float32)

time = 59387	action = 0	current_phase = 0	next_phase = 1	reward = 0.082053	array([[-2.41126 , -3.311928]], dtype=float32)

time = 59392	action = 0	current_phase = 0	next_phase = 1	reward = -0.248055	array([[-3.191023 , -3.3051946]], dtype=float32)

time = 59397	action = 0	current_phase = 0	next_phase = 1	reward = -1.020492	array([[-2.9668276, -3.4015768]], dtype=float32)

time = 59402	action = 1	current_phase = 0	next_phase = 1	reward = -2.543577	array([[-6.8672047, -4.291499 ]], dtype=float32)

time = 59410	action = 1	current_phase = 1	next_phase = 0	reward = -0.734359	array([[-3.625605, -2.236589]], dtype=float32)

time = 59418	action = 0	current_phase = 0	next_phase = 1	reward = 0.078020	array([[-2.4124496, -3.4338183]], dtype=float32)

time = 59423	action = 0	current_phase = 0	next_phase = 1	reward = -0.368819	array([[-2.5146792, -3.324756 ]], dtype=float32)

time = 59428	action = 0	current_phase = 0	next_phase = 1	reward = -1.087750	array([[-2.0170236, -3.5262046]], dtype=float32)

time = 59433	action = 1	current_phase = 0	next_phase = 1	reward = -2.122045	array([[-6.9109416, -3.733599 ]], dtype=float32)

time = 59441	action = 1	current_phase = 1	next_phase = 0	reward = -1.177775	array([[-3.1015358, -2.2231717]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0240 - val_loss: 0.0094

Epoch 2/50

 - 3s - loss: 0.0322 - val_loss: 0.0112

Epoch 3/50

 - 3s - loss: 0.0254 - val_loss: 0.0101

Epoch 4/50

 - 3s - loss: 0.0202 - val_loss: 0.0085

Epoch 5/50

 - 3s - loss: 0.0239 - val_loss: 0.0096

Epoch 6/50

 - 3s - loss: 0.0286 - val_loss: 0.0093

Epoch 7/50

 - 3s - loss: 0.0209 - val_loss: 0.0094

Epoch 8/50

 - 3s - loss: 0.0220 - val_loss: 0.0132

Epoch 9/50

 - 3s - loss: 0.0249 - val_loss: 0.0147

Epoch 10/50

 - 3s - loss: 0.0207 - val_loss: 0.0160

Epoch 11/50

 - 3s - loss: 0.0193 - val_loss: 0.0108

Epoch 12/50

 - 3s - loss: 0.0184 - val_loss: 0.0114

Epoch 13/50

 - 3s - loss: 0.0208 - val_loss: 0.0124

Epoch 14/50

 - 3s - loss: 0.0225 - val_loss: 0.0122

length of memory (state 0, action 0): 1025, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1011, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1011, before forget

length of memory (state 1, action 1): 1000, after forget

time = 59449	action = 0	current_phase = 0	next_phase = 1	reward = 0.352663	array([[-1.9375576, -2.4340305]], dtype=float32)

time = 59454	action = 1	current_phase = 0	next_phase = 1	reward = -1.189190	array([[-3.5789022, -3.2162762]], dtype=float32)

time = 59462	action = 1	current_phase = 1	next_phase = 0	reward = -0.771176	array([[-3.450379 , -2.3191879]], dtype=float32)

time = 59470	action = 0	current_phase = 0	next_phase = 1	reward = -0.307047	array([[-1.8198813, -2.1528246]], dtype=float32)

time = 59475	action = 0	current_phase = 0	next_phase = 1	reward = 0.325148	array([[-2.2047288, -3.2896395]], dtype=float32)

time = 59480	action = 1	current_phase = 0	next_phase = 1	reward = -1.362620	array([[-6.101062 , -3.1612158]], dtype=float32)

time = 59488	action = 1	current_phase = 1	next_phase = 0	reward = -0.702928	array([[-3.5564916, -2.1765823]], dtype=float32)

time = 59496	action = 0	current_phase = 0	next_phase = 1	reward = -0.080908	array([[-1.8271272, -2.9214628]], dtype=float32)

time = 59501	action = 0	current_phase = 0	next_phase = 1	reward = 0.004571	array([[-2.2339153, -2.96739  ]], dtype=float32)

time = 59506	action = 0	current_phase = 0	next_phase = 1	reward = 0.062746	array([[-2.5424266, -3.4177155]], dtype=float32)

time = 59511	action = 1	current_phase = 0	next_phase = 1	reward = -1.433003	array([[-7.2171345, -3.3382735]], dtype=float32)

time = 59519	action = 1	current_phase = 1	next_phase = 0	reward = -0.841067	array([[-3.742278, -2.329848]], dtype=float32)

time = 59527	action = 0	current_phase = 0	next_phase = 1	reward = -0.080066	array([[-1.8719987, -2.8738022]], dtype=float32)

time = 59532	action = 0	current_phase = 0	next_phase = 1	reward = 0.001638	array([[-2.2870555, -2.9747968]], dtype=float32)

time = 59537	action = 0	current_phase = 0	next_phase = 1	reward = 0.072132	array([[-2.8384676, -3.5367446]], dtype=float32)

time = 59542	action = 1	current_phase = 0	next_phase = 1	reward = -1.649088	array([[-7.1889825, -3.6212707]], dtype=float32)

time = 59550	action = 1	current_phase = 1	next_phase = 0	reward = -1.005629	array([[-3.8672056, -2.402621 ]], dtype=float32)

time = 59558	action = 0	current_phase = 0	next_phase = 1	reward = -0.054396	array([[-1.9070599, -2.9013033]], dtype=float32)

time = 59563	action = 0	current_phase = 0	next_phase = 1	reward = 0.022060	array([[-2.2836924, -2.9612408]], dtype=float32)

time = 59568	action = 0	current_phase = 0	next_phase = 1	reward = 0.073415	array([[-2.812838 , -3.5324588]], dtype=float32)

time = 59573	action = 1	current_phase = 0	next_phase = 1	reward = -1.861946	array([[-7.169284 , -3.7279162]], dtype=float32)

time = 59581	action = 1	current_phase = 1	next_phase = 0	reward = -1.076546	array([[-4.0648975, -2.3915312]], dtype=float32)

time = 59589	action = 0	current_phase = 0	next_phase = 1	reward = -0.031940	array([[-1.391014, -2.095017]], dtype=float32)

time = 59594	action = 0	current_phase = 0	next_phase = 1	reward = 0.036327	array([[-1.8454924, -2.9090166]], dtype=float32)

time = 59599	action = 1	current_phase = 0	next_phase = 1	reward = -0.652421	array([[-2.731655 , -2.4279263]], dtype=float32)

time = 59607	action = 1	current_phase = 1	next_phase = 0	reward = -0.650332	array([[-3.4086585, -2.0262852]], dtype=float32)

time = 59615	action = 0	current_phase = 0	next_phase = 1	reward = -0.389678	array([[-1.916285 , -3.0567923]], dtype=float32)

time = 59620	action = 0	current_phase = 0	next_phase = 1	reward = -0.028682	array([[-1.7985399, -2.15177  ]], dtype=float32)

time = 59625	action = 0	current_phase = 0	next_phase = 1	reward = 0.044770	array([[-2.2924538, -3.255258 ]], dtype=float32)

time = 59630	action = 1	current_phase = 0	next_phase = 1	reward = -0.985303	array([[-6.078417 , -2.8660784]], dtype=float32)

time = 59638	action = 1	current_phase = 1	next_phase = 0	reward = -0.694565	array([[-3.6106622, -2.2205882]], dtype=float32)

time = 59646	action = 0	current_phase = 0	next_phase = 1	reward = -0.079821	array([[-1.8721788, -2.8931665]], dtype=float32)

time = 59651	action = 0	current_phase = 0	next_phase = 1	reward = 0.017967	array([[-2.202002 , -2.9708142]], dtype=float32)

time = 59656	action = 0	current_phase = 0	next_phase = 1	reward = 0.067015	array([[-2.5916877, -3.45848  ]], dtype=float32)

time = 59661	action = 1	current_phase = 0	next_phase = 1	reward = -1.493579	array([[-7.2266936, -3.328804 ]], dtype=float32)

time = 59669	action = 1	current_phase = 1	next_phase = 0	reward = -0.836378	array([[-3.7439141, -2.3231955]], dtype=float32)

time = 59677	action = 0	current_phase = 0	next_phase = 1	reward = -0.078169	array([[-1.8812903, -2.868219 ]], dtype=float32)

time = 59682	action = 0	current_phase = 0	next_phase = 1	reward = 0.006811	array([[-2.295113 , -2.9938579]], dtype=float32)

time = 59687	action = 0	current_phase = 0	next_phase = 1	reward = 0.066883	array([[-2.8218791, -3.5328832]], dtype=float32)

time = 59692	action = 1	current_phase = 0	next_phase = 1	reward = -1.704560	array([[-7.182237 , -3.6420088]], dtype=float32)

time = 59700	action = 1	current_phase = 1	next_phase = 0	reward = -0.875381	array([[-3.8724415, -2.3975391]], dtype=float32)

time = 59708	action = 0	current_phase = 0	next_phase = 1	reward = -0.047807	array([[-1.893387, -2.892342]], dtype=float32)

time = 59713	action = 0	current_phase = 0	next_phase = 1	reward = 0.034122	array([[-2.291048 , -2.9650435]], dtype=float32)

time = 59718	action = 0	current_phase = 0	next_phase = 1	reward = 0.082851	array([[-2.7962904, -3.564454 ]], dtype=float32)

time = 59723	action = 1	current_phase = 0	next_phase = 1	reward = -1.925943	array([[-7.185612 , -3.6494303]], dtype=float32)

time = 59731	action = 1	current_phase = 1	next_phase = 0	reward = -1.390309	array([[-4.089511 , -2.3079112]], dtype=float32)

time = 59739	action = 0	current_phase = 0	next_phase = 1	reward = 0.252575	array([[-1.191392 , -2.1089604]], dtype=float32)

time = 59744	action = 0	current_phase = 0	next_phase = 1	reward = 0.030444	array([[-1.9337366, -2.914822 ]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0266 - val_loss: 0.0133

Epoch 2/50

 - 3s - loss: 0.0319 - val_loss: 0.0111

Epoch 3/50

 - 3s - loss: 0.0276 - val_loss: 0.0140

Epoch 4/50

 - 3s - loss: 0.0265 - val_loss: 0.0108

Epoch 5/50

 - 3s - loss: 0.0316 - val_loss: 0.0117

Epoch 6/50

 - 3s - loss: 0.0242 - val_loss: 0.0132

Epoch 7/50

 - 3s - loss: 0.0268 - val_loss: 0.0142

Epoch 8/50

 - 3s - loss: 0.0280 - val_loss: 0.0149

Epoch 9/50

 - 3s - loss: 0.0290 - val_loss: 0.0123

Epoch 10/50

 - 3s - loss: 0.0318 - val_loss: 0.0128

Epoch 11/50

 - 3s - loss: 0.0197 - val_loss: 0.0150

Epoch 12/50

 - 3s - loss: 0.0218 - val_loss: 0.0117

Epoch 13/50

 - 3s - loss: 0.0236 - val_loss: 0.0125

Epoch 14/50

 - 3s - loss: 0.0282 - val_loss: 0.0131

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 59749	action = 1	current_phase = 0	next_phase = 1	reward = -0.640992	array([[-2.7138906, -2.4483478]], dtype=float32)

time = 59757	action = 1	current_phase = 1	next_phase = 0	reward = -0.857390	array([[-3.426525 , -2.0439694]], dtype=float32)

time = 59765	action = 0	current_phase = 0	next_phase = 1	reward = 0.205706	array([[-1.6272691, -3.0745924]], dtype=float32)

time = 59770	action = 0	current_phase = 0	next_phase = 1	reward = -0.016818	array([[-1.9565065, -2.214563 ]], dtype=float32)

time = 59775	action = 0	current_phase = 0	next_phase = 1	reward = -0.226751	array([[-2.3751845, -3.2970624]], dtype=float32)

time = 59780	action = 1	current_phase = 0	next_phase = 1	reward = -1.157120	array([[-6.099924 , -2.9376419]], dtype=float32)

time = 59788	action = 1	current_phase = 1	next_phase = 0	reward = -0.715804	array([[-3.6738381, -2.2477803]], dtype=float32)

time = 59796	action = 0	current_phase = 0	next_phase = 1	reward = -0.081068	array([[-1.8468287, -2.9482324]], dtype=float32)

time = 59801	action = 0	current_phase = 0	next_phase = 1	reward = -0.005348	array([[-2.1323357, -2.9773262]], dtype=float32)

time = 59806	action = 0	current_phase = 0	next_phase = 1	reward = 0.064584	array([[-2.5718904, -3.4366941]], dtype=float32)

time = 59811	action = 1	current_phase = 0	next_phase = 1	reward = -1.505882	array([[-7.234269 , -3.4692245]], dtype=float32)

time = 59819	action = 1	current_phase = 1	next_phase = 0	reward = -0.808779	array([[-3.7754624, -2.3431935]], dtype=float32)

time = 59827	action = 0	current_phase = 0	next_phase = 1	reward = -0.063993	array([[-1.827797, -2.907024]], dtype=float32)

time = 59832	action = 0	current_phase = 0	next_phase = 1	reward = 0.020208	array([[-2.2412806, -2.9755921]], dtype=float32)

time = 59837	action = 0	current_phase = 0	next_phase = 1	reward = 0.074875	array([[-2.8246055, -3.5249007]], dtype=float32)

time = 59842	action = 1	current_phase = 0	next_phase = 1	reward = -1.679263	array([[-7.204522 , -3.6259167]], dtype=float32)

time = 59850	action = 1	current_phase = 1	next_phase = 0	reward = -1.262733	array([[-3.8893626, -2.4180489]], dtype=float32)

time = 59858	action = 0	current_phase = 0	next_phase = 1	reward = 0.254024	array([[-1.6498808, -2.8561625]], dtype=float32)

time = 59863	action = 0	current_phase = 0	next_phase = 1	reward = 0.028086	array([[-2.2311285, -2.9481852]], dtype=float32)

time = 59868	action = 0	current_phase = 0	next_phase = 1	reward = 0.076657	array([[-2.7306743, -3.4851294]], dtype=float32)

time = 59873	action = 1	current_phase = 0	next_phase = 1	reward = -0.817348	array([[-7.2270584, -3.3358955]], dtype=float32)

time = 59881	action = 1	current_phase = 1	next_phase = 0	reward = -1.308482	array([[-3.7562532, -2.40523  ]], dtype=float32)

time = 59889	action = 0	current_phase = 0	next_phase = 1	reward = -0.035928	array([[-1.3966016, -2.1611958]], dtype=float32)

time = 59894	action = 0	current_phase = 0	next_phase = 1	reward = 0.041408	array([[-1.8438388, -2.918026 ]], dtype=float32)

time = 59899	action = 1	current_phase = 0	next_phase = 1	reward = -0.639374	array([[-2.6721363, -2.5540414]], dtype=float32)

time = 59907	action = 1	current_phase = 1	next_phase = 0	reward = -0.828225	array([[-3.4628706, -2.0989895]], dtype=float32)

time = 59915	action = 0	current_phase = 0	next_phase = 1	reward = 0.178204	array([[-1.5484576, -3.061043 ]], dtype=float32)

time = 59920	action = 0	current_phase = 0	next_phase = 1	reward = -0.293080	array([[-1.9597934, -2.2226038]], dtype=float32)

time = 59925	action = 0	current_phase = 0	next_phase = 1	reward = 0.064877	array([[-2.2955363, -3.2895951]], dtype=float32)

time = 59930	action = 1	current_phase = 0	next_phase = 1	reward = -1.030167	array([[-6.114224 , -2.8941467]], dtype=float32)

time = 59938	action = 1	current_phase = 1	next_phase = 0	reward = -0.706342	array([[-3.5646458, -2.1895072]], dtype=float32)

time = 59946	action = 0	current_phase = 0	next_phase = 1	reward = -0.092890	array([[-1.8059399, -2.9068296]], dtype=float32)

time = 59951	action = 0	current_phase = 0	next_phase = 1	reward = -0.018894	array([[-2.1576986, -2.968008 ]], dtype=float32)

time = 59956	action = 0	current_phase = 0	next_phase = 1	reward = 0.049396	array([[-2.562991 , -3.4394777]], dtype=float32)

time = 59961	action = 1	current_phase = 0	next_phase = 1	reward = -1.494224	array([[-7.2490735, -3.4250922]], dtype=float32)

time = 59969	action = 1	current_phase = 1	next_phase = 0	reward = -1.186781	array([[-3.7570112, -2.330153 ]], dtype=float32)

time = 59977	action = 0	current_phase = 0	next_phase = 1	reward = 0.248444	array([[-1.7466116, -2.8735583]], dtype=float32)

time = 59982	action = 0	current_phase = 0	next_phase = 1	reward = 0.012852	array([[-2.2436833, -2.997896 ]], dtype=float32)

time = 59987	action = 0	current_phase = 0	next_phase = 1	reward = 0.062044	array([[-2.7923436, -3.5134425]], dtype=float32)

time = 59992	action = 1	current_phase = 0	next_phase = 1	reward = -1.628855	array([[-7.2025766, -3.6352518]], dtype=float32)

time = 60000	action = 1	current_phase = 1	next_phase = 0	reward = -1.250737	array([[-3.9146461, -2.412191 ]], dtype=float32)

time = 60008	action = 0	current_phase = 0	next_phase = 1	reward = 0.242038	array([[-1.6504725, -2.8953643]], dtype=float32)

time = 60013	action = 0	current_phase = 0	next_phase = 1	reward = 0.009181	array([[-2.2135105, -2.982876 ]], dtype=float32)

time = 60018	action = 0	current_phase = 0	next_phase = 1	reward = 0.064326	array([[-2.79472 , -3.519836]], dtype=float32)

time = 60023	action = 1	current_phase = 0	next_phase = 1	reward = -1.813360	array([[-7.1985574, -3.783181 ]], dtype=float32)

time = 60031	action = 1	current_phase = 1	next_phase = 0	reward = -1.315096	array([[-4.0465317, -2.3990638]], dtype=float32)

time = 60039	action = 0	current_phase = 0	next_phase = 1	reward = 0.284889	array([[-1.3336793, -2.175379 ]], dtype=float32)

time = 60044	action = 0	current_phase = 0	next_phase = 1	reward = 0.044656	array([[-1.8738197, -2.9732246]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0361 - val_loss: 0.0115

Epoch 2/50

 - 3s - loss: 0.0191 - val_loss: 0.0117

Epoch 3/50

 - 3s - loss: 0.0363 - val_loss: 0.0098

Epoch 4/50

 - 3s - loss: 0.0168 - val_loss: 0.0107

Epoch 5/50

 - 3s - loss: 0.0238 - val_loss: 0.0121

Epoch 6/50

 - 3s - loss: 0.0218 - val_loss: 0.0138

Epoch 7/50

 - 3s - loss: 0.0199 - val_loss: 0.0154

Epoch 8/50

 - 3s - loss: 0.0197 - val_loss: 0.0164

Epoch 9/50

 - 3s - loss: 0.0187 - val_loss: 0.0134

Epoch 10/50

 - 3s - loss: 0.0208 - val_loss: 0.0113

Epoch 11/50

 - 3s - loss: 0.0187 - val_loss: 0.0169

Epoch 12/50

 - 3s - loss: 0.0195 - val_loss: 0.0121

Epoch 13/50

 - 3s - loss: 0.0242 - val_loss: 0.0134

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 60049	action = 1	current_phase = 0	next_phase = 1	reward = -0.711704	array([[-2.7366047, -2.4701948]], dtype=float32)

time = 60057	action = 1	current_phase = 1	next_phase = 0	reward = -1.200522	array([[-3.4637942, -2.1595407]], dtype=float32)

time = 60065	action = 0	current_phase = 0	next_phase = 1	reward = 0.185994	array([[-1.3842809, -3.0607004]], dtype=float32)

time = 60070	action = 0	current_phase = 0	next_phase = 1	reward = -0.024676	array([[-1.8958061, -2.242541 ]], dtype=float32)

time = 60075	action = 0	current_phase = 0	next_phase = 1	reward = 0.323053	array([[-2.2062178, -3.2041664]], dtype=float32)

time = 60080	action = 1	current_phase = 0	next_phase = 1	reward = -1.254055	array([[-6.1328826, -3.0886054]], dtype=float32)

time = 60088	action = 1	current_phase = 1	next_phase = 0	reward = -0.698419	array([[-3.6206074, -2.2451825]], dtype=float32)

time = 60096	action = 0	current_phase = 0	next_phase = 1	reward = -0.080028	array([[-1.8417604, -2.9392257]], dtype=float32)

time = 60101	action = 0	current_phase = 0	next_phase = 1	reward = -0.013379	array([[-2.1413548, -2.9718597]], dtype=float32)

time = 60106	action = 0	current_phase = 0	next_phase = 1	reward = 0.048967	array([[-2.5951238, -3.4682064]], dtype=float32)

time = 60111	action = 1	current_phase = 0	next_phase = 1	reward = -1.458103	array([[-7.239535 , -3.3231485]], dtype=float32)

time = 60119	action = 1	current_phase = 1	next_phase = 0	reward = -0.779352	array([[-3.688034, -2.289938]], dtype=float32)

time = 60127	action = 0	current_phase = 0	next_phase = 1	reward = -0.081671	array([[-1.8488549, -2.8752198]], dtype=float32)

time = 60132	action = 0	current_phase = 0	next_phase = 1	reward = -0.002374	array([[-2.2728615, -2.97882  ]], dtype=float32)

time = 60137	action = 0	current_phase = 0	next_phase = 1	reward = 0.054527	array([[-2.8087006, -3.5165608]], dtype=float32)

time = 60142	action = 1	current_phase = 0	next_phase = 1	reward = -1.569276	array([[-7.206333 , -3.6255062]], dtype=float32)

time = 60150	action = 1	current_phase = 1	next_phase = 0	reward = -0.843263	array([[-3.7836013, -2.345549 ]], dtype=float32)

time = 60158	action = 0	current_phase = 0	next_phase = 1	reward = -0.051389	array([[-1.8007635, -2.8899288]], dtype=float32)

time = 60163	action = 0	current_phase = 0	next_phase = 1	reward = 0.016008	array([[-2.247737, -2.955756]], dtype=float32)

time = 60168	action = 0	current_phase = 0	next_phase = 1	reward = 0.079843	array([[-2.8149924, -3.5502129]], dtype=float32)

time = 60173	action = 1	current_phase = 0	next_phase = 1	reward = -1.351113	array([[-7.218621 , -3.3632088]], dtype=float32)

time = 60181	action = 1	current_phase = 1	next_phase = 0	reward = -1.494509	array([[-3.8691826, -2.41089  ]], dtype=float32)

time = 60189	action = 0	current_phase = 0	next_phase = 1	reward = 0.266119	array([[-1.3148077, -2.1907175]], dtype=float32)

time = 60194	action = 0	current_phase = 0	next_phase = 1	reward = 0.063134	array([[-1.7732354, -2.9316819]], dtype=float32)

time = 60199	action = 1	current_phase = 0	next_phase = 1	reward = -0.630724	array([[-2.7583337, -2.4785314]], dtype=float32)

time = 60207	action = 1	current_phase = 1	next_phase = 0	reward = -0.652961	array([[-3.4221692, -2.0919683]], dtype=float32)

time = 60215	action = 0	current_phase = 0	next_phase = 1	reward = -0.385595	array([[-1.6024721, -3.0467842]], dtype=float32)

time = 60220	action = 0	current_phase = 0	next_phase = 1	reward = 0.267430	array([[-1.8172865, -2.2380462]], dtype=float32)

time = 60225	action = 0	current_phase = 0	next_phase = 1	reward = 0.055882	array([[-2.302712 , -3.2899814]], dtype=float32)

time = 60230	action = 1	current_phase = 0	next_phase = 1	reward = -1.363423	array([[-6.126075 , -3.0351777]], dtype=float32)

time = 60238	action = 1	current_phase = 1	next_phase = 0	reward = -0.714531	array([[-3.6334934, -2.2423718]], dtype=float32)

time = 60246	action = 0	current_phase = 0	next_phase = 1	reward = -0.088597	array([[-1.9134878, -2.9688559]], dtype=float32)

time = 60251	action = 0	current_phase = 0	next_phase = 1	reward = -0.007642	array([[-2.1462326, -2.9625225]], dtype=float32)

time = 60256	action = 0	current_phase = 0	next_phase = 1	reward = 0.058875	array([[-2.5389209, -3.4317513]], dtype=float32)

time = 60261	action = 1	current_phase = 0	next_phase = 1	reward = -1.570745	array([[-7.2486362, -3.3900692]], dtype=float32)

time = 60269	action = 1	current_phase = 1	next_phase = 0	reward = -0.843803	array([[-3.7232041, -2.3045695]], dtype=float32)

time = 60277	action = 0	current_phase = 0	next_phase = 1	reward = -0.066304	array([[-1.8299496, -2.8702545]], dtype=float32)

time = 60282	action = 0	current_phase = 0	next_phase = 1	reward = 0.007927	array([[-2.2657351, -2.9644947]], dtype=float32)

time = 60287	action = 0	current_phase = 0	next_phase = 1	reward = 0.074720	array([[-2.7727156, -3.4833977]], dtype=float32)

time = 60292	action = 1	current_phase = 0	next_phase = 1	reward = -1.568159	array([[-7.192305 , -3.6493685]], dtype=float32)

time = 60300	action = 1	current_phase = 1	next_phase = 0	reward = -0.890125	array([[-3.8298593, -2.3769588]], dtype=float32)

time = 60308	action = 0	current_phase = 0	next_phase = 1	reward = -0.060784	array([[-1.7789283, -2.9073834]], dtype=float32)

time = 60313	action = 0	current_phase = 0	next_phase = 1	reward = 0.006951	array([[-2.1943483, -2.947374 ]], dtype=float32)

time = 60318	action = 0	current_phase = 0	next_phase = 1	reward = 0.086502	array([[-2.6904979, -3.4364614]], dtype=float32)

time = 60323	action = 1	current_phase = 0	next_phase = 1	reward = -1.815026	array([[-7.197838 , -3.6318386]], dtype=float32)

time = 60331	action = 1	current_phase = 1	next_phase = 0	reward = -1.341256	array([[-3.9536557, -2.3302207]], dtype=float32)

time = 60339	action = 0	current_phase = 0	next_phase = 1	reward = 0.265106	array([[-1.2651403, -2.1785264]], dtype=float32)

time = 60344	action = 0	current_phase = 0	next_phase = 1	reward = 0.064299	array([[-1.8576291, -2.923836 ]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0266 - val_loss: 0.0204

Epoch 2/50

 - 3s - loss: 0.0252 - val_loss: 0.0227

Epoch 3/50

 - 3s - loss: 0.0403 - val_loss: 0.0219

Epoch 4/50

 - 3s - loss: 0.0298 - val_loss: 0.0184

Epoch 5/50

 - 3s - loss: 0.0235 - val_loss: 0.0254

Epoch 6/50

 - 3s - loss: 0.0245 - val_loss: 0.0229

Epoch 7/50

 - 3s - loss: 0.0210 - val_loss: 0.0200

Epoch 8/50

 - 3s - loss: 0.0275 - val_loss: 0.0217

Epoch 9/50

 - 3s - loss: 0.0247 - val_loss: 0.0217

Epoch 10/50

 - 3s - loss: 0.0196 - val_loss: 0.0209

Epoch 11/50

 - 3s - loss: 0.0240 - val_loss: 0.0233

Epoch 12/50

 - 3s - loss: 0.0256 - val_loss: 0.0203

Epoch 13/50

 - 3s - loss: 0.0200 - val_loss: 0.0187

Epoch 14/50

 - 3s - loss: 0.0194 - val_loss: 0.0206

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 60349	action = 1	current_phase = 0	next_phase = 1	reward = -0.652630	array([[-2.7238529, -2.4603686]], dtype=float32)

time = 60357	action = 1	current_phase = 1	next_phase = 0	reward = -1.203528	array([[-3.4438887, -2.0630665]], dtype=float32)

time = 60365	action = 0	current_phase = 0	next_phase = 1	reward = 0.464451	array([[-1.3978753, -3.1898224]], dtype=float32)

time = 60370	action = 0	current_phase = 0	next_phase = 1	reward = -0.028401	array([[-1.8309603, -2.3012848]], dtype=float32)

time = 60375	action = 0	current_phase = 0	next_phase = 1	reward = -0.234060	array([[-2.3428357, -3.288747 ]], dtype=float32)

time = 60380	action = 1	current_phase = 0	next_phase = 1	reward = -1.024917	array([[-6.0736403, -2.8985927]], dtype=float32)

time = 60388	action = 1	current_phase = 1	next_phase = 0	reward = -0.712126	array([[-3.6516845, -2.2334003]], dtype=float32)

time = 60396	action = 0	current_phase = 0	next_phase = 1	reward = -0.063401	array([[-1.8177664, -2.9610002]], dtype=float32)

time = 60401	action = 0	current_phase = 0	next_phase = 1	reward = 0.014891	array([[-2.0456784, -2.9880724]], dtype=float32)

time = 60406	action = 0	current_phase = 0	next_phase = 1	reward = 0.068889	array([[-2.626234 , -3.4476037]], dtype=float32)

time = 60411	action = 1	current_phase = 0	next_phase = 1	reward = -1.445134	array([[-7.2226067, -3.364591 ]], dtype=float32)

time = 60419	action = 1	current_phase = 1	next_phase = 0	reward = -1.123235	array([[-3.7235155, -2.2980564]], dtype=float32)

time = 60427	action = 0	current_phase = 0	next_phase = 1	reward = 0.226955	array([[-1.7439454, -2.910001 ]], dtype=float32)

time = 60432	action = 0	current_phase = 0	next_phase = 1	reward = 0.019744	array([[-2.216446 , -2.9973223]], dtype=float32)

time = 60437	action = 0	current_phase = 0	next_phase = 1	reward = 0.074280	array([[-2.8419716, -3.556671 ]], dtype=float32)

time = 60442	action = 1	current_phase = 0	next_phase = 1	reward = -1.518692	array([[-7.203785 , -3.5725682]], dtype=float32)

time = 60450	action = 1	current_phase = 1	next_phase = 0	reward = -0.949509	array([[-3.8243124, -2.3597815]], dtype=float32)

time = 60458	action = 0	current_phase = 0	next_phase = 1	reward = -0.058774	array([[-1.6656616, -2.902629 ]], dtype=float32)

time = 60463	action = 0	current_phase = 0	next_phase = 1	reward = 0.009817	array([[-2.1135395, -2.9595542]], dtype=float32)

time = 60468	action = 0	current_phase = 0	next_phase = 1	reward = 0.071158	array([[-2.67797  , -3.4778159]], dtype=float32)

time = 60473	action = 1	current_phase = 0	next_phase = 1	reward = -0.770247	array([[-7.1431575, -3.002833 ]], dtype=float32)

time = 60481	action = 1	current_phase = 1	next_phase = 0	reward = -1.183628	array([[-3.7565958, -2.3858056]], dtype=float32)

time = 60489	action = 0	current_phase = 0	next_phase = 1	reward = -0.028537	array([[-1.3276149, -2.2231152]], dtype=float32)

time = 60494	action = 0	current_phase = 0	next_phase = 1	reward = 0.021412	array([[-1.8919666, -3.0005443]], dtype=float32)

time = 60499	action = 1	current_phase = 0	next_phase = 1	reward = -0.650189	array([[-2.5472348, -2.3750873]], dtype=float32)

time = 60507	action = 1	current_phase = 1	next_phase = 0	reward = -0.864444	array([[-3.430026, -2.085495]], dtype=float32)

time = 60515	action = 0	current_phase = 0	next_phase = 1	reward = 0.174734	array([[-1.4667244, -3.126715 ]], dtype=float32)

time = 60520	action = 0	current_phase = 0	next_phase = 1	reward = -0.030478	array([[-1.8291744, -2.2657213]], dtype=float32)

time = 60525	action = 0	current_phase = 0	next_phase = 1	reward = -0.225992	array([[-2.2943282, -3.2397175]], dtype=float32)

time = 60530	action = 1	current_phase = 0	next_phase = 1	reward = -0.956845	array([[-6.0702643, -2.8707843]], dtype=float32)

time = 60538	action = 1	current_phase = 1	next_phase = 0	reward = -0.711192	array([[-3.6364894, -2.2245977]], dtype=float32)

time = 60546	action = 0	current_phase = 0	next_phase = 1	reward = -0.079498	array([[-1.7656271, -2.965585 ]], dtype=float32)

time = 60551	action = 0	current_phase = 0	next_phase = 1	reward = 0.000625	array([[-2.102396, -2.984422]], dtype=float32)

time = 60556	action = 0	current_phase = 0	next_phase = 1	reward = 0.062535	array([[-2.6385279, -3.4665003]], dtype=float32)

time = 60561	action = 1	current_phase = 0	next_phase = 1	reward = -1.509807	array([[-7.225734 , -3.3467534]], dtype=float32)

time = 60569	action = 1	current_phase = 1	next_phase = 0	reward = -0.835104	array([[-3.7208195, -2.284801 ]], dtype=float32)

time = 60577	action = 0	current_phase = 0	next_phase = 1	reward = -0.070309	array([[-1.8244696, -2.9101105]], dtype=float32)

time = 60582	action = 0	current_phase = 0	next_phase = 1	reward = 0.016327	array([[-2.224291, -3.026663]], dtype=float32)

time = 60587	action = 0	current_phase = 0	next_phase = 1	reward = 0.067724	array([[-2.854944, -3.548958]], dtype=float32)

time = 60592	action = 1	current_phase = 0	next_phase = 1	reward = -1.492110	array([[-7.197057 , -3.5783548]], dtype=float32)

time = 60600	action = 1	current_phase = 1	next_phase = 0	reward = -0.940491	array([[-3.8261516, -2.3559804]], dtype=float32)

time = 60608	action = 0	current_phase = 0	next_phase = 1	reward = -0.054671	array([[-1.7178583, -2.9193933]], dtype=float32)

time = 60613	action = 0	current_phase = 0	next_phase = 1	reward = 0.019852	array([[-2.19219  , -2.9798763]], dtype=float32)

time = 60618	action = 0	current_phase = 0	next_phase = 1	reward = 0.066990	array([[-2.760836, -3.540232]], dtype=float32)

time = 60623	action = 1	current_phase = 0	next_phase = 1	reward = -1.801374	array([[-7.186475 , -3.7058513]], dtype=float32)

time = 60631	action = 1	current_phase = 1	next_phase = 0	reward = -1.337735	array([[-3.9878893, -2.2537167]], dtype=float32)

time = 60639	action = 0	current_phase = 0	next_phase = 1	reward = 0.261193	array([[-1.1733224, -2.2074208]], dtype=float32)

time = 60644	action = 0	current_phase = 0	next_phase = 1	reward = 0.031093	array([[-1.9659495, -2.9919538]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0292 - val_loss: 0.0089

Epoch 2/50

 - 3s - loss: 0.0288 - val_loss: 0.0125

Epoch 3/50

 - 3s - loss: 0.0342 - val_loss: 0.0130

Epoch 4/50

 - 3s - loss: 0.0278 - val_loss: 0.0083

Epoch 5/50

 - 3s - loss: 0.0251 - val_loss: 0.0101

Epoch 6/50

 - 3s - loss: 0.0288 - val_loss: 0.0101

Epoch 7/50

 - 3s - loss: 0.0214 - val_loss: 0.0132

Epoch 8/50

 - 3s - loss: 0.0271 - val_loss: 0.0100

Epoch 9/50

 - 3s - loss: 0.0249 - val_loss: 0.0109

Epoch 10/50

 - 3s - loss: 0.0296 - val_loss: 0.0102

Epoch 11/50

 - 3s - loss: 0.0225 - val_loss: 0.0092

Epoch 12/50

 - 3s - loss: 0.0248 - val_loss: 0.0105

Epoch 13/50

 - 3s - loss: 0.0263 - val_loss: 0.0147

Epoch 14/50

 - 3s - loss: 0.0223 - val_loss: 0.0109

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 60649	action = 1	current_phase = 0	next_phase = 1	reward = -0.640772	array([[-2.7850966, -2.429367 ]], dtype=float32)

time = 60657	action = 1	current_phase = 1	next_phase = 0	reward = -1.163772	array([[-3.4301388, -2.0976562]], dtype=float32)

time = 60665	action = 0	current_phase = 0	next_phase = 1	reward = 0.460364	array([[-1.268479, -3.176673]], dtype=float32)

time = 60670	action = 0	current_phase = 0	next_phase = 1	reward = -0.573644	array([[-2.0064332, -2.363832 ]], dtype=float32)

time = 60675	action = 0	current_phase = 0	next_phase = 1	reward = 0.608449	array([[-2.2419534, -3.2051315]], dtype=float32)

time = 60680	action = 1	current_phase = 0	next_phase = 1	reward = -1.303572	array([[-6.122054 , -3.1174974]], dtype=float32)

time = 60688	action = 1	current_phase = 1	next_phase = 0	reward = -0.704999	array([[-3.610564 , -2.2355778]], dtype=float32)

time = 60696	action = 0	current_phase = 0	next_phase = 1	reward = -0.091408	array([[-1.7797803, -2.9733658]], dtype=float32)

time = 60701	action = 0	current_phase = 0	next_phase = 1	reward = -0.006008	array([[-2.1035428, -3.0120163]], dtype=float32)

time = 60706	action = 0	current_phase = 0	next_phase = 1	reward = 0.063947	array([[-2.6763272, -3.4656992]], dtype=float32)

time = 60711	action = 1	current_phase = 0	next_phase = 1	reward = -1.388780	array([[-7.2244954, -3.3048716]], dtype=float32)

time = 60719	action = 1	current_phase = 1	next_phase = 0	reward = -0.816345	array([[-3.6794405, -2.2774057]], dtype=float32)

time = 60727	action = 0	current_phase = 0	next_phase = 1	reward = -0.058409	array([[-1.7912217, -2.9042687]], dtype=float32)

time = 60732	action = 0	current_phase = 0	next_phase = 1	reward = 0.008672	array([[-2.2173457, -2.996221 ]], dtype=float32)

time = 60737	action = 0	current_phase = 0	next_phase = 1	reward = 0.082932	array([[-2.853856, -3.488306]], dtype=float32)

time = 60742	action = 1	current_phase = 0	next_phase = 1	reward = -1.620679	array([[-7.2270937, -3.6215792]], dtype=float32)

time = 60750	action = 1	current_phase = 1	next_phase = 0	reward = -0.902674	array([[-3.825432 , -2.4268513]], dtype=float32)

time = 60758	action = 0	current_phase = 0	next_phase = 1	reward = -0.064614	array([[-1.798571 , -2.9364321]], dtype=float32)

time = 60763	action = 0	current_phase = 0	next_phase = 1	reward = 0.013131	array([[-2.1918027, -2.980577 ]], dtype=float32)

time = 60768	action = 0	current_phase = 0	next_phase = 1	reward = 0.077068	array([[-2.7050934, -3.431233 ]], dtype=float32)

time = 60773	action = 1	current_phase = 0	next_phase = 1	reward = -1.378872	array([[-7.236774 , -3.2687593]], dtype=float32)

time = 60781	action = 1	current_phase = 1	next_phase = 0	reward = -1.496948	array([[-3.8478804, -2.4117336]], dtype=float32)

time = 60789	action = 0	current_phase = 0	next_phase = 1	reward = 0.243343	array([[-1.2643619, -2.1955447]], dtype=float32)

time = 60794	action = 0	current_phase = 0	next_phase = 1	reward = 0.041432	array([[-1.9108391, -2.9964967]], dtype=float32)

time = 60799	action = 1	current_phase = 0	next_phase = 1	reward = -0.697918	array([[-2.6879244, -2.42549  ]], dtype=float32)

time = 60807	action = 1	current_phase = 1	next_phase = 0	reward = -0.929167	array([[-3.498804, -2.186252]], dtype=float32)

time = 60815	action = 0	current_phase = 0	next_phase = 1	reward = 0.167207	array([[-1.414603 , -3.0988545]], dtype=float32)

time = 60820	action = 0	current_phase = 0	next_phase = 1	reward = -0.314189	array([[-1.9886655, -2.3733454]], dtype=float32)

time = 60825	action = 0	current_phase = 0	next_phase = 1	reward = 0.315662	array([[-2.300057 , -3.2252269]], dtype=float32)

time = 60830	action = 1	current_phase = 0	next_phase = 1	reward = -1.261105	array([[-6.1511188, -3.1466079]], dtype=float32)

time = 60838	action = 1	current_phase = 1	next_phase = 0	reward = -0.709962	array([[-3.6091776, -2.227706 ]], dtype=float32)

time = 60846	action = 0	current_phase = 0	next_phase = 1	reward = -0.080826	array([[-1.7710091, -3.0201578]], dtype=float32)

time = 60851	action = 0	current_phase = 0	next_phase = 1	reward = -0.015875	array([[-2.064793 , -3.0015998]], dtype=float32)

time = 60856	action = 0	current_phase = 0	next_phase = 1	reward = 0.065319	array([[-2.600017 , -3.4295416]], dtype=float32)

time = 60861	action = 1	current_phase = 0	next_phase = 1	reward = -1.494683	array([[-7.2338457, -3.3419552]], dtype=float32)

time = 60869	action = 1	current_phase = 1	next_phase = 0	reward = -0.755500	array([[-3.675434 , -2.2722058]], dtype=float32)

time = 60877	action = 0	current_phase = 0	next_phase = 1	reward = -0.083111	array([[-1.8123043, -2.906733 ]], dtype=float32)

time = 60882	action = 0	current_phase = 0	next_phase = 1	reward = -0.001944	array([[-2.2062237, -3.0274148]], dtype=float32)

time = 60887	action = 0	current_phase = 0	next_phase = 1	reward = 0.056151	array([[-2.8132453, -3.4726906]], dtype=float32)

time = 60892	action = 1	current_phase = 0	next_phase = 1	reward = -1.587374	array([[-7.2182503, -3.5625792]], dtype=float32)

time = 60900	action = 1	current_phase = 1	next_phase = 0	reward = -0.955350	array([[-3.84306  , -2.4474096]], dtype=float32)

time = 60908	action = 0	current_phase = 0	next_phase = 1	reward = -0.064997	array([[-1.7679787, -2.9219518]], dtype=float32)

time = 60913	action = 0	current_phase = 0	next_phase = 1	reward = 0.006164	array([[-2.2147713, -2.9865184]], dtype=float32)

time = 60918	action = 0	current_phase = 0	next_phase = 1	reward = 0.078202	array([[-2.7842937, -3.4794865]], dtype=float32)

time = 60923	action = 1	current_phase = 0	next_phase = 1	reward = -1.896151	array([[-7.2074523, -3.7224236]], dtype=float32)

time = 60931	action = 1	current_phase = 1	next_phase = 0	reward = -1.086552	array([[-4.0723476, -2.2572079]], dtype=float32)

time = 60939	action = 0	current_phase = 0	next_phase = 1	reward = -0.031299	array([[-1.4000882, -2.1947522]], dtype=float32)

time = 60944	action = 0	current_phase = 0	next_phase = 1	reward = 0.037287	array([[-1.9912167, -3.0013208]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0248 - val_loss: 0.0114

Epoch 2/50

 - 3s - loss: 0.0254 - val_loss: 0.0112

Epoch 3/50

 - 3s - loss: 0.0211 - val_loss: 0.0129

Epoch 4/50

 - 3s - loss: 0.0207 - val_loss: 0.0141

Epoch 5/50

 - 3s - loss: 0.0230 - val_loss: 0.0109

Epoch 6/50

 - 3s - loss: 0.0222 - val_loss: 0.0165

Epoch 7/50

 - 3s - loss: 0.0210 - val_loss: 0.0132

Epoch 8/50

 - 3s - loss: 0.0190 - val_loss: 0.0145

Epoch 9/50

 - 3s - loss: 0.0219 - val_loss: 0.0149

Epoch 10/50

 - 3s - loss: 0.0171 - val_loss: 0.0148

Epoch 11/50

 - 3s - loss: 0.0238 - val_loss: 0.0120

Epoch 12/50

 - 3s - loss: 0.0227 - val_loss: 0.0160

Epoch 13/50

 - 3s - loss: 0.0206 - val_loss: 0.0143

Epoch 14/50

 - 3s - loss: 0.0171 - val_loss: 0.0144

Epoch 15/50

 - 3s - loss: 0.0187 - val_loss: 0.0186

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 60949	action = 1	current_phase = 0	next_phase = 1	reward = -0.651380	array([[-2.822622 , -2.4771287]], dtype=float32)

time = 60957	action = 1	current_phase = 1	next_phase = 0	reward = -0.583012	array([[-3.2685418, -1.9643731]], dtype=float32)

time = 60965	action = 0	current_phase = 0	next_phase = 1	reward = -0.362543	array([[-1.6269178, -3.115386 ]], dtype=float32)

time = 60970	action = 0	current_phase = 0	next_phase = 1	reward = 0.261040	array([[-1.76441, -2.33486]], dtype=float32)

time = 60975	action = 0	current_phase = 0	next_phase = 1	reward = -0.519113	array([[-2.493516 , -3.3412313]], dtype=float32)

time = 60980	action = 1	current_phase = 0	next_phase = 1	reward = -0.765509	array([[-6.089175 , -2.7382243]], dtype=float32)

time = 60988	action = 1	current_phase = 1	next_phase = 0	reward = -0.696702	array([[-3.5044088, -2.1403303]], dtype=float32)

time = 60996	action = 0	current_phase = 0	next_phase = 1	reward = -0.086904	array([[-1.8166006, -3.0024095]], dtype=float32)

time = 61001	action = 0	current_phase = 0	next_phase = 1	reward = 0.001055	array([[-2.0888412, -3.0029378]], dtype=float32)

time = 61006	action = 0	current_phase = 0	next_phase = 1	reward = 0.062108	array([[-2.660608 , -3.4794753]], dtype=float32)

time = 61011	action = 1	current_phase = 0	next_phase = 1	reward = -1.443398	array([[-7.2399187, -3.316553 ]], dtype=float32)

time = 61019	action = 1	current_phase = 1	next_phase = 0	reward = -1.115974	array([[-3.6856074, -2.2896957]], dtype=float32)

time = 61027	action = 0	current_phase = 0	next_phase = 1	reward = 0.220293	array([[-1.837152 , -2.9108028]], dtype=float32)

time = 61032	action = 0	current_phase = 0	next_phase = 1	reward = -0.026228	array([[-2.1802783, -3.0251896]], dtype=float32)

time = 61037	action = 0	current_phase = 0	next_phase = 1	reward = 0.052413	array([[-2.7967138, -3.459758 ]], dtype=float32)

time = 61042	action = 1	current_phase = 0	next_phase = 1	reward = -1.485670	array([[-7.2278285, -3.5316052]], dtype=float32)

time = 61050	action = 1	current_phase = 1	next_phase = 0	reward = -0.895835	array([[-3.7555008, -2.39855  ]], dtype=float32)

time = 61058	action = 0	current_phase = 0	next_phase = 1	reward = -0.055048	array([[-1.781771 , -2.9606142]], dtype=float32)

time = 61063	action = 0	current_phase = 0	next_phase = 1	reward = 0.032735	array([[-2.235115 , -3.0038753]], dtype=float32)

time = 61068	action = 0	current_phase = 0	next_phase = 1	reward = 0.077935	array([[-2.8398037, -3.6611195]], dtype=float32)

time = 61073	action = 1	current_phase = 0	next_phase = 1	reward = -1.828656	array([[-7.219855, -3.383121]], dtype=float32)

time = 61081	action = 1	current_phase = 1	next_phase = 0	reward = -1.118957	array([[-3.9772408, -2.211766 ]], dtype=float32)

time = 61089	action = 0	current_phase = 0	next_phase = 1	reward = -0.034718	array([[-1.3641357, -2.2444654]], dtype=float32)

time = 61094	action = 0	current_phase = 0	next_phase = 1	reward = 0.023421	array([[-1.9787743, -2.9721775]], dtype=float32)

time = 61099	action = 1	current_phase = 0	next_phase = 1	reward = -0.654359	array([[-2.7361076, -2.3884974]], dtype=float32)

time = 61107	action = 1	current_phase = 1	next_phase = 0	reward = -0.804596	array([[-3.3047771, -1.9676719]], dtype=float32)

time = 61115	action = 0	current_phase = 0	next_phase = 1	reward = 0.194648	array([[-1.4948874, -3.1809103]], dtype=float32)

time = 61120	action = 0	current_phase = 0	next_phase = 1	reward = -0.016349	array([[-1.8698914, -2.3179297]], dtype=float32)

time = 61125	action = 0	current_phase = 0	next_phase = 1	reward = 0.046576	array([[-2.5069666, -3.340455 ]], dtype=float32)

time = 61130	action = 1	current_phase = 0	next_phase = 1	reward = -1.327815	array([[-6.174442 , -3.2051811]], dtype=float32)

time = 61138	action = 1	current_phase = 1	next_phase = 0	reward = -0.703368	array([[-3.5597396, -2.206109 ]], dtype=float32)

time = 61146	action = 0	current_phase = 0	next_phase = 1	reward = -0.084948	array([[-1.8176584, -2.9563396]], dtype=float32)

time = 61151	action = 0	current_phase = 0	next_phase = 1	reward = 0.012114	array([[-2.058244 , -3.0204446]], dtype=float32)

time = 61156	action = 0	current_phase = 0	next_phase = 1	reward = 0.071373	array([[-2.6434665, -3.4825847]], dtype=float32)

time = 61161	action = 1	current_phase = 0	next_phase = 1	reward = -1.517802	array([[-7.238852 , -3.3957124]], dtype=float32)

time = 61169	action = 1	current_phase = 1	next_phase = 0	reward = -0.816436	array([[-3.6542487, -2.2733538]], dtype=float32)

time = 61177	action = 0	current_phase = 0	next_phase = 1	reward = -0.065732	array([[-1.9209478, -2.9425087]], dtype=float32)

time = 61182	action = 0	current_phase = 0	next_phase = 1	reward = 0.012554	array([[-2.24272  , -3.0285358]], dtype=float32)

time = 61187	action = 0	current_phase = 0	next_phase = 1	reward = 0.065649	array([[-2.8968108, -3.5499415]], dtype=float32)

time = 61192	action = 1	current_phase = 0	next_phase = 1	reward = -1.547347	array([[-7.22803, -3.52586]], dtype=float32)

time = 61200	action = 1	current_phase = 1	next_phase = 0	reward = -0.957969	array([[-3.7770905, -2.3998563]], dtype=float32)

time = 61208	action = 0	current_phase = 0	next_phase = 1	reward = -0.069845	array([[-1.7655904, -2.9325776]], dtype=float32)

time = 61213	action = 0	current_phase = 0	next_phase = 1	reward = 0.024372	array([[-2.1986265, -3.0084631]], dtype=float32)

time = 61218	action = 0	current_phase = 0	next_phase = 1	reward = 0.089025	array([[-2.8840034, -3.6307101]], dtype=float32)

time = 61223	action = 1	current_phase = 0	next_phase = 1	reward = -1.851335	array([[-7.192663 , -3.7108014]], dtype=float32)

time = 61231	action = 1	current_phase = 1	next_phase = 0	reward = -1.033029	array([[-3.9434507, -2.2289524]], dtype=float32)

time = 61239	action = 0	current_phase = 0	next_phase = 1	reward = -0.043909	array([[-1.4029379, -2.2204127]], dtype=float32)

time = 61244	action = 0	current_phase = 0	next_phase = 1	reward = 0.013158	array([[-1.9507525, -2.9781728]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0216 - val_loss: 0.0107

Epoch 2/50

 - 3s - loss: 0.0177 - val_loss: 0.0142

Epoch 3/50

 - 3s - loss: 0.0257 - val_loss: 0.0120

Epoch 4/50

 - 3s - loss: 0.0192 - val_loss: 0.0113

Epoch 5/50

 - 3s - loss: 0.0194 - val_loss: 0.0112

Epoch 6/50

 - 3s - loss: 0.0205 - val_loss: 0.0126

Epoch 7/50

 - 3s - loss: 0.0243 - val_loss: 0.0085

Epoch 8/50

 - 3s - loss: 0.0161 - val_loss: 0.0137

Epoch 9/50

 - 3s - loss: 0.0184 - val_loss: 0.0119

Epoch 10/50

 - 3s - loss: 0.0186 - val_loss: 0.0103

Epoch 11/50

 - 3s - loss: 0.0192 - val_loss: 0.0209

Epoch 12/50

 - 3s - loss: 0.0174 - val_loss: 0.0142

Epoch 13/50

 - 3s - loss: 0.0210 - val_loss: 0.0102

Epoch 14/50

 - 3s - loss: 0.0175 - val_loss: 0.0096

Epoch 15/50

 - 3s - loss: 0.0194 - val_loss: 0.0113

Epoch 16/50

 - 3s - loss: 0.0174 - val_loss: 0.0129

Epoch 17/50

 - 3s - loss: 0.0213 - val_loss: 0.0133

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 61249	action = 1	current_phase = 0	next_phase = 1	reward = -0.716020	array([[-2.7808027, -2.4023924]], dtype=float32)

time = 61257	action = 1	current_phase = 1	next_phase = 0	reward = -0.646769	array([[-3.3998265, -2.0930073]], dtype=float32)

time = 61265	action = 0	current_phase = 0	next_phase = 1	reward = -0.668414	array([[-1.5880773, -3.156571 ]], dtype=float32)

time = 61270	action = 0	current_phase = 0	next_phase = 1	reward = 0.529057	array([[-1.4462217, -2.2726233]], dtype=float32)

time = 61275	action = 0	current_phase = 0	next_phase = 1	reward = -0.255514	array([[-2.4499917, -3.3078403]], dtype=float32)

time = 61280	action = 1	current_phase = 0	next_phase = 1	reward = -0.927560	array([[-6.1466565, -2.8386266]], dtype=float32)

time = 61288	action = 1	current_phase = 1	next_phase = 0	reward = -0.687014	array([[-3.474321, -2.220294]], dtype=float32)

time = 61296	action = 0	current_phase = 0	next_phase = 1	reward = -0.074632	array([[-1.7988653, -2.9772007]], dtype=float32)

time = 61301	action = 0	current_phase = 0	next_phase = 1	reward = -0.010978	array([[-2.082212 , -3.0249488]], dtype=float32)

time = 61306	action = 0	current_phase = 0	next_phase = 1	reward = 0.042145	array([[-2.6419723, -3.5082455]], dtype=float32)

time = 61311	action = 1	current_phase = 0	next_phase = 1	reward = -1.486216	array([[-7.2857428, -3.3139937]], dtype=float32)

time = 61319	action = 1	current_phase = 1	next_phase = 0	reward = -0.768432	array([[-3.6757383, -2.3625565]], dtype=float32)

time = 61327	action = 0	current_phase = 0	next_phase = 1	reward = -0.074249	array([[-1.8018134, -2.9269674]], dtype=float32)

time = 61332	action = 0	current_phase = 0	next_phase = 1	reward = 0.002689	array([[-2.2235518, -3.0127404]], dtype=float32)

time = 61337	action = 0	current_phase = 0	next_phase = 1	reward = 0.086314	array([[-2.8438835, -3.5270643]], dtype=float32)

time = 61342	action = 1	current_phase = 0	next_phase = 1	reward = -1.663232	array([[-7.2509093, -3.5263724]], dtype=float32)

time = 61350	action = 1	current_phase = 1	next_phase = 0	reward = -1.248193	array([[-3.783061 , -2.4570868]], dtype=float32)

time = 61358	action = 0	current_phase = 0	next_phase = 1	reward = 0.249606	array([[-1.6981406, -2.9814713]], dtype=float32)

time = 61363	action = 0	current_phase = 0	next_phase = 1	reward = 0.027340	array([[-2.1688619, -2.992914 ]], dtype=float32)

time = 61368	action = 0	current_phase = 0	next_phase = 1	reward = 0.081046	array([[-2.7602997, -3.5080254]], dtype=float32)

time = 61373	action = 1	current_phase = 0	next_phase = 1	reward = -1.999004	array([[-7.2231975, -3.706309 ]], dtype=float32)

time = 61381	action = 1	current_phase = 1	next_phase = 0	reward = -1.106524	array([[-3.979456 , -2.3355567]], dtype=float32)

time = 61389	action = 0	current_phase = 0	next_phase = 1	reward = -0.047817	array([[-1.3480557, -2.2772634]], dtype=float32)

time = 61394	action = 0	current_phase = 0	next_phase = 1	reward = 0.018005	array([[-1.8605474, -2.9777071]], dtype=float32)

time = 61399	action = 1	current_phase = 0	next_phase = 1	reward = -0.786877	array([[-2.7829428, -2.4172192]], dtype=float32)

time = 61407	action = 1	current_phase = 1	next_phase = 0	reward = -1.193006	array([[-3.528143 , -2.1656895]], dtype=float32)

time = 61415	action = 0	current_phase = 0	next_phase = 1	reward = 0.199614	array([[-1.2819862, -3.1723335]], dtype=float32)

time = 61420	action = 0	current_phase = 0	next_phase = 1	reward = 0.261679	array([[-1.6800036, -2.4229302]], dtype=float32)

time = 61425	action = 0	current_phase = 0	next_phase = 1	reward = 0.038172	array([[-2.461104 , -3.3249297]], dtype=float32)

time = 61430	action = 1	current_phase = 0	next_phase = 1	reward = -1.426072	array([[-6.172713 , -3.1092634]], dtype=float32)

time = 61438	action = 1	current_phase = 1	next_phase = 0	reward = -0.710963	array([[-3.5859709, -2.2820191]], dtype=float32)

time = 61446	action = 0	current_phase = 0	next_phase = 1	reward = -0.081524	array([[-1.7961757, -2.9692357]], dtype=float32)

time = 61451	action = 0	current_phase = 0	next_phase = 1	reward = -0.002643	array([[-2.1260455, -3.0250187]], dtype=float32)

time = 61456	action = 0	current_phase = 0	next_phase = 1	reward = 0.061995	array([[-2.6076765, -3.4928763]], dtype=float32)

time = 61461	action = 1	current_phase = 0	next_phase = 1	reward = -1.545345	array([[-7.2625   , -3.3349216]], dtype=float32)

time = 61469	action = 1	current_phase = 1	next_phase = 0	reward = -0.846059	array([[-3.6860209, -2.373098 ]], dtype=float32)

time = 61477	action = 0	current_phase = 0	next_phase = 1	reward = -0.079862	array([[-1.8229275, -2.9360273]], dtype=float32)

time = 61482	action = 0	current_phase = 0	next_phase = 1	reward = -0.003111	array([[-2.2118723, -3.0441792]], dtype=float32)

time = 61487	action = 0	current_phase = 0	next_phase = 1	reward = 0.071961	array([[-2.81421  , -3.4970338]], dtype=float32)

time = 61492	action = 1	current_phase = 0	next_phase = 1	reward = -1.632376	array([[-7.25214  , -3.5015404]], dtype=float32)

time = 61500	action = 1	current_phase = 1	next_phase = 0	reward = -1.013088	array([[-3.8184977, -2.4314704]], dtype=float32)

time = 61508	action = 0	current_phase = 0	next_phase = 1	reward = -0.065900	array([[-1.7585771, -2.9482217]], dtype=float32)

time = 61513	action = 0	current_phase = 0	next_phase = 1	reward = 0.003908	array([[-2.225278 , -2.9965272]], dtype=float32)

time = 61518	action = 0	current_phase = 0	next_phase = 1	reward = 0.075317	array([[-2.7407188, -3.544992 ]], dtype=float32)

time = 61523	action = 1	current_phase = 0	next_phase = 1	reward = -1.738336	array([[-7.2454004, -3.5561771]], dtype=float32)

time = 61531	action = 1	current_phase = 1	next_phase = 0	reward = -1.342218	array([[-3.882988 , -2.2831376]], dtype=float32)

time = 61539	action = 0	current_phase = 0	next_phase = 1	reward = 0.258555	array([[-1.1758051, -2.2410305]], dtype=float32)

time = 61544	action = 0	current_phase = 0	next_phase = 1	reward = 0.036639	array([[-1.9115224, -2.9847162]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0190 - val_loss: 0.0210

Epoch 2/50

 - 3s - loss: 0.0162 - val_loss: 0.0233

Epoch 3/50

 - 3s - loss: 0.0183 - val_loss: 0.0232

Epoch 4/50

 - 3s - loss: 0.0197 - val_loss: 0.0231

Epoch 5/50

 - 3s - loss: 0.0171 - val_loss: 0.0208

Epoch 6/50

 - 3s - loss: 0.0151 - val_loss: 0.0205

Epoch 7/50

 - 3s - loss: 0.0184 - val_loss: 0.0205

Epoch 8/50

 - 3s - loss: 0.0177 - val_loss: 0.0249

Epoch 9/50

 - 3s - loss: 0.0207 - val_loss: 0.0231

Epoch 10/50

 - 3s - loss: 0.0174 - val_loss: 0.0248

Epoch 11/50

 - 3s - loss: 0.0153 - val_loss: 0.0230

Epoch 12/50

 - 3s - loss: 0.0161 - val_loss: 0.0239

Epoch 13/50

 - 3s - loss: 0.0150 - val_loss: 0.0292

Epoch 14/50

 - 3s - loss: 0.0177 - val_loss: 0.0251

Epoch 15/50

 - 3s - loss: 0.0146 - val_loss: 0.0241

Epoch 16/50

 - 3s - loss: 0.0156 - val_loss: 0.0240

Epoch 17/50

 - 3s - loss: 0.0144 - val_loss: 0.0241

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 61549	action = 1	current_phase = 0	next_phase = 1	reward = -0.644580	array([[-2.8502822, -2.4539866]], dtype=float32)

time = 61557	action = 1	current_phase = 1	next_phase = 0	reward = -0.922273	array([[-3.3271084, -2.0086432]], dtype=float32)

time = 61565	action = 0	current_phase = 0	next_phase = 1	reward = 0.178157	array([[-1.4543737, -3.1158931]], dtype=float32)

time = 61570	action = 0	current_phase = 0	next_phase = 1	reward = -0.303256	array([[-1.8628231, -2.3680303]], dtype=float32)

time = 61575	action = 0	current_phase = 0	next_phase = 1	reward = 0.045267	array([[-2.3148053, -3.2678776]], dtype=float32)

time = 61580	action = 1	current_phase = 0	next_phase = 1	reward = -1.075813	array([[-6.1035757, -2.834397 ]], dtype=float32)

time = 61588	action = 1	current_phase = 1	next_phase = 0	reward = -0.712752	array([[-3.6210594, -2.226795 ]], dtype=float32)

time = 61596	action = 0	current_phase = 0	next_phase = 1	reward = -0.100767	array([[-1.7136264, -2.988418 ]], dtype=float32)

time = 61601	action = 0	current_phase = 0	next_phase = 1	reward = -0.019141	array([[-2.053874 , -3.0252197]], dtype=float32)

time = 61606	action = 0	current_phase = 0	next_phase = 1	reward = 0.057479	array([[-2.6038678, -3.4670367]], dtype=float32)

time = 61611	action = 1	current_phase = 0	next_phase = 1	reward = -1.371815	array([[-7.2754097, -3.3491726]], dtype=float32)

time = 61619	action = 1	current_phase = 1	next_phase = 0	reward = -0.825114	array([[-3.7035456, -2.3421228]], dtype=float32)

time = 61627	action = 0	current_phase = 0	next_phase = 1	reward = -0.071611	array([[-1.8181808, -2.9604652]], dtype=float32)

time = 61632	action = 0	current_phase = 0	next_phase = 1	reward = 0.022081	array([[-2.2053523, -3.0475225]], dtype=float32)

time = 61637	action = 0	current_phase = 0	next_phase = 1	reward = 0.076487	array([[-2.7854004, -3.5113964]], dtype=float32)

time = 61642	action = 1	current_phase = 0	next_phase = 1	reward = -1.669945	array([[-7.259124 , -3.6294246]], dtype=float32)

time = 61650	action = 1	current_phase = 1	next_phase = 0	reward = -1.356650	array([[-3.8287916, -2.445846 ]], dtype=float32)

time = 61658	action = 0	current_phase = 0	next_phase = 1	reward = 0.249936	array([[-1.5950565, -2.9709344]], dtype=float32)

time = 61663	action = 0	current_phase = 0	next_phase = 1	reward = 0.020012	array([[-2.2066274, -3.0370188]], dtype=float32)

time = 61668	action = 0	current_phase = 0	next_phase = 1	reward = 0.079357	array([[-2.7926722, -3.6171076]], dtype=float32)

time = 61673	action = 1	current_phase = 0	next_phase = 1	reward = -1.864498	array([[-7.233325 , -3.6836808]], dtype=float32)

time = 61681	action = 1	current_phase = 1	next_phase = 0	reward = -1.047792	array([[-3.9839563, -2.2471232]], dtype=float32)

time = 61689	action = 0	current_phase = 0	next_phase = 1	reward = -0.042383	array([[-1.3734723, -2.2451782]], dtype=float32)

time = 61694	action = 0	current_phase = 0	next_phase = 1	reward = 0.024263	array([[-1.877909 , -3.0099444]], dtype=float32)

time = 61699	action = 1	current_phase = 0	next_phase = 1	reward = -0.641964	array([[-2.8155859, -2.4629858]], dtype=float32)

time = 61707	action = 1	current_phase = 1	next_phase = 0	reward = -0.819277	array([[-3.4113693, -2.0881467]], dtype=float32)

time = 61715	action = 0	current_phase = 0	next_phase = 1	reward = -0.107847	array([[-1.408409 , -3.1422205]], dtype=float32)

time = 61720	action = 0	current_phase = 0	next_phase = 1	reward = 0.257176	array([[-1.7251029, -2.3526056]], dtype=float32)

time = 61725	action = 0	current_phase = 0	next_phase = 1	reward = 0.039206	array([[-2.4142804, -3.372629 ]], dtype=float32)

time = 61730	action = 1	current_phase = 0	next_phase = 1	reward = -1.200744	array([[-6.2022867, -3.1818206]], dtype=float32)

time = 61738	action = 1	current_phase = 1	next_phase = 0	reward = -0.712582	array([[-3.5166883, -2.1958005]], dtype=float32)

time = 61746	action = 0	current_phase = 0	next_phase = 1	reward = -0.089466	array([[-1.7247958, -3.0269568]], dtype=float32)

time = 61751	action = 0	current_phase = 0	next_phase = 1	reward = -0.019182	array([[-2.0667624, -3.0286093]], dtype=float32)

time = 61756	action = 0	current_phase = 0	next_phase = 1	reward = 0.056883	array([[-2.5301654, -3.439212 ]], dtype=float32)

time = 61761	action = 1	current_phase = 0	next_phase = 1	reward = -1.323313	array([[-7.3012586, -3.3412712]], dtype=float32)

time = 61769	action = 1	current_phase = 1	next_phase = 0	reward = -0.846942	array([[-3.7298136, -2.3604984]], dtype=float32)

time = 61777	action = 0	current_phase = 0	next_phase = 1	reward = -0.078981	array([[-1.8295407, -2.9609447]], dtype=float32)

time = 61782	action = 0	current_phase = 0	next_phase = 1	reward = -0.002580	array([[-2.1965463, -3.0352576]], dtype=float32)

time = 61787	action = 0	current_phase = 0	next_phase = 1	reward = 0.060121	array([[-2.8536866, -3.5772676]], dtype=float32)

time = 61792	action = 1	current_phase = 0	next_phase = 1	reward = -1.583679	array([[-7.2713795, -3.540104 ]], dtype=float32)

time = 61800	action = 1	current_phase = 1	next_phase = 0	reward = -0.996166	array([[-3.7961307, -2.426014 ]], dtype=float32)

time = 61808	action = 0	current_phase = 0	next_phase = 1	reward = -0.055948	array([[-1.7704372, -2.9600961]], dtype=float32)

time = 61813	action = 0	current_phase = 0	next_phase = 1	reward = 0.018958	array([[-2.1810372, -3.0124137]], dtype=float32)

time = 61818	action = 0	current_phase = 0	next_phase = 1	reward = 0.082147	array([[-2.7827044, -3.5352583]], dtype=float32)

time = 61823	action = 1	current_phase = 0	next_phase = 1	reward = -1.339911	array([[-7.265245 , -3.4755983]], dtype=float32)

time = 61831	action = 1	current_phase = 1	next_phase = 0	reward = -1.122653	array([[-3.8493028, -2.3670151]], dtype=float32)

time = 61839	action = 0	current_phase = 0	next_phase = 1	reward = -0.023409	array([[-1.3540449, -2.2631097]], dtype=float32)

time = 61844	action = 0	current_phase = 0	next_phase = 1	reward = 0.038376	array([[-1.9395075, -3.0057302]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0191 - val_loss: 0.0205

Epoch 2/50

 - 3s - loss: 0.0206 - val_loss: 0.0170

Epoch 3/50

 - 3s - loss: 0.0212 - val_loss: 0.0156

Epoch 4/50

 - 3s - loss: 0.0208 - val_loss: 0.0159

Epoch 5/50

 - 3s - loss: 0.0207 - val_loss: 0.0174

Epoch 6/50

 - 3s - loss: 0.0157 - val_loss: 0.0192

Epoch 7/50

 - 3s - loss: 0.0160 - val_loss: 0.0161

Epoch 8/50

 - 3s - loss: 0.0194 - val_loss: 0.0171

Epoch 9/50

 - 3s - loss: 0.0172 - val_loss: 0.0157

Epoch 10/50

 - 3s - loss: 0.0171 - val_loss: 0.0193

Epoch 11/50

 - 3s - loss: 0.0231 - val_loss: 0.0216

Epoch 12/50

 - 3s - loss: 0.0195 - val_loss: 0.0156

Epoch 13/50

 - 3s - loss: 0.0201 - val_loss: 0.0167

Epoch 14/50

 - 3s - loss: 0.0173 - val_loss: 0.0181

Epoch 15/50

 - 3s - loss: 0.0185 - val_loss: 0.0175

Epoch 16/50

 - 3s - loss: 0.0153 - val_loss: 0.0180

Epoch 17/50

 - 3s - loss: 0.0137 - val_loss: 0.0150

Epoch 18/50

 - 3s - loss: 0.0155 - val_loss: 0.0164

Epoch 19/50

 - 3s - loss: 0.0158 - val_loss: 0.0184

Epoch 20/50

 - 3s - loss: 0.0182 - val_loss: 0.0167

Epoch 21/50

 - 3s - loss: 0.0146 - val_loss: 0.0182

Epoch 22/50

 - 3s - loss: 0.0161 - val_loss: 0.0189

Epoch 23/50

 - 3s - loss: 0.0175 - val_loss: 0.0184

Epoch 24/50

 - 3s - loss: 0.0159 - val_loss: 0.0202

Epoch 25/50

 - 3s - loss: 0.0181 - val_loss: 0.0181

Epoch 26/50

 - 3s - loss: 0.0155 - val_loss: 0.0199

Epoch 27/50

 - 3s - loss: 0.0128 - val_loss: 0.0168

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 61849	action = 1	current_phase = 0	next_phase = 1	reward = -0.627249	array([[-2.8430479, -2.3751447]], dtype=float32)

time = 61857	action = 1	current_phase = 1	next_phase = 0	reward = -0.931477	array([[-3.4607322, -2.0589457]], dtype=float32)

time = 61865	action = 0	current_phase = 0	next_phase = 1	reward = 0.162085	array([[-1.3870881, -3.1565027]], dtype=float32)

time = 61870	action = 0	current_phase = 0	next_phase = 1	reward = -0.311366	array([[-1.7823305, -2.4046438]], dtype=float32)

time = 61875	action = 0	current_phase = 0	next_phase = 1	reward = 0.036197	array([[-2.3061578, -3.2375796]], dtype=float32)

time = 61880	action = 1	current_phase = 0	next_phase = 1	reward = -1.080450	array([[-6.1403666, -2.7610695]], dtype=float32)

time = 61888	action = 1	current_phase = 1	next_phase = 0	reward = -0.707081	array([[-3.6090925, -2.1352222]], dtype=float32)

time = 61896	action = 0	current_phase = 0	next_phase = 1	reward = -0.081808	array([[-1.7037313, -2.986753 ]], dtype=float32)

time = 61901	action = 0	current_phase = 0	next_phase = 1	reward = -0.016327	array([[-2.099291 , -3.0557816]], dtype=float32)

time = 61906	action = 0	current_phase = 0	next_phase = 1	reward = 0.049331	array([[-2.6384053, -3.4614773]], dtype=float32)

time = 61911	action = 1	current_phase = 0	next_phase = 1	reward = -1.444354	array([[-7.2967777, -3.2705061]], dtype=float32)

time = 61919	action = 1	current_phase = 1	next_phase = 0	reward = -0.762780	array([[-3.6884165, -2.2360127]], dtype=float32)

time = 61927	action = 0	current_phase = 0	next_phase = 1	reward = -0.071582	array([[-1.8005145, -2.9478812]], dtype=float32)

time = 61932	action = 0	current_phase = 0	next_phase = 1	reward = 0.001785	array([[-2.2595956, -3.0759692]], dtype=float32)

time = 61937	action = 0	current_phase = 0	next_phase = 1	reward = 0.065575	array([[-2.8168046, -3.5242176]], dtype=float32)

time = 61942	action = 1	current_phase = 0	next_phase = 1	reward = -1.583810	array([[-7.2585263, -3.4993207]], dtype=float32)

time = 61950	action = 1	current_phase = 1	next_phase = 0	reward = -0.884934	array([[-3.8152914, -2.3715484]], dtype=float32)

time = 61958	action = 0	current_phase = 0	next_phase = 1	reward = -0.044453	array([[-1.6827476, -2.9626908]], dtype=float32)

time = 61963	action = 0	current_phase = 0	next_phase = 1	reward = 0.032485	array([[-2.2588832, -3.022473 ]], dtype=float32)

time = 61968	action = 0	current_phase = 0	next_phase = 1	reward = 0.068295	array([[-2.8282135, -3.603138 ]], dtype=float32)

time = 61973	action = 1	current_phase = 0	next_phase = 1	reward = -1.950313	array([[-7.2587276, -3.5761847]], dtype=float32)

time = 61981	action = 1	current_phase = 1	next_phase = 0	reward = -1.026877	array([[-4.0305476, -2.2121968]], dtype=float32)

time = 61989	action = 0	current_phase = 0	next_phase = 1	reward = -0.044124	array([[-1.4128686, -2.281092 ]], dtype=float32)

time = 61994	action = 0	current_phase = 0	next_phase = 1	reward = 0.012084	array([[-1.8592134, -2.968452 ]], dtype=float32)

time = 61999	action = 1	current_phase = 0	next_phase = 1	reward = -0.737834	array([[-2.8145473, -2.402888 ]], dtype=float32)

time = 62007	action = 1	current_phase = 1	next_phase = 0	reward = -1.205954	array([[-3.4761477, -2.0471463]], dtype=float32)

time = 62015	action = 0	current_phase = 0	next_phase = 1	reward = 0.192486	array([[-1.079279 , -3.2033906]], dtype=float32)

time = 62020	action = 0	current_phase = 0	next_phase = 1	reward = -0.012869	array([[-1.5944791, -2.4329157]], dtype=float32)

time = 62025	action = 0	current_phase = 0	next_phase = 1	reward = 0.061188	array([[-2.2843173, -3.2126691]], dtype=float32)

time = 62030	action = 1	current_phase = 0	next_phase = 1	reward = -1.136154	array([[-6.1602883, -2.9836733]], dtype=float32)

time = 62038	action = 1	current_phase = 1	next_phase = 0	reward = -0.702701	array([[-3.6278522, -2.1506436]], dtype=float32)

time = 62046	action = 0	current_phase = 0	next_phase = 1	reward = -0.081144	array([[-1.757936 , -2.9889388]], dtype=float32)

time = 62051	action = 0	current_phase = 0	next_phase = 1	reward = -0.004348	array([[-2.0928106, -3.0228128]], dtype=float32)

time = 62056	action = 0	current_phase = 0	next_phase = 1	reward = 0.064527	array([[-2.6404727, -3.464808 ]], dtype=float32)

time = 62061	action = 1	current_phase = 0	next_phase = 1	reward = -1.496762	array([[-7.280131 , -3.3417006]], dtype=float32)

time = 62069	action = 1	current_phase = 1	next_phase = 0	reward = -0.841699	array([[-3.7508345, -2.305394 ]], dtype=float32)

time = 62077	action = 0	current_phase = 0	next_phase = 1	reward = -0.065627	array([[-1.7730637, -2.9451218]], dtype=float32)

time = 62082	action = 0	current_phase = 0	next_phase = 1	reward = -0.005274	array([[-2.2605119, -3.0386992]], dtype=float32)

time = 62087	action = 0	current_phase = 0	next_phase = 1	reward = 0.064179	array([[-2.8136127, -3.4810128]], dtype=float32)

time = 62092	action = 1	current_phase = 0	next_phase = 1	reward = -1.588234	array([[-7.2833385, -3.5015144]], dtype=float32)

time = 62100	action = 1	current_phase = 1	next_phase = 0	reward = -0.957220	array([[-3.8485637, -2.3932366]], dtype=float32)

time = 62108	action = 0	current_phase = 0	next_phase = 1	reward = -0.069528	array([[-1.7641277, -2.961845 ]], dtype=float32)

time = 62113	action = 0	current_phase = 0	next_phase = 1	reward = 0.024277	array([[-2.2258384, -3.0119646]], dtype=float32)

time = 62118	action = 0	current_phase = 0	next_phase = 1	reward = 0.079696	array([[-2.8135347, -3.5393195]], dtype=float32)

time = 62123	action = 1	current_phase = 0	next_phase = 1	reward = -1.884106	array([[-7.268535 , -3.5774298]], dtype=float32)

time = 62131	action = 1	current_phase = 1	next_phase = 0	reward = -1.348530	array([[-4.036358, -2.216336]], dtype=float32)

time = 62139	action = 0	current_phase = 0	next_phase = 1	reward = 0.248622	array([[-1.2936894, -2.2624016]], dtype=float32)

time = 62144	action = 0	current_phase = 0	next_phase = 1	reward = 0.022457	array([[-1.9172149, -2.9720912]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0232 - val_loss: 0.0215

Epoch 2/50

 - 3s - loss: 0.0218 - val_loss: 0.0210

Epoch 3/50

 - 3s - loss: 0.0251 - val_loss: 0.0189

Epoch 4/50

 - 3s - loss: 0.0219 - val_loss: 0.0207

Epoch 5/50

 - 3s - loss: 0.0179 - val_loss: 0.0237

Epoch 6/50

 - 3s - loss: 0.0216 - val_loss: 0.0210

Epoch 7/50

 - 3s - loss: 0.0194 - val_loss: 0.0211

Epoch 8/50

 - 3s - loss: 0.0235 - val_loss: 0.0217

Epoch 9/50

 - 3s - loss: 0.0211 - val_loss: 0.0210

Epoch 10/50

 - 3s - loss: 0.0209 - val_loss: 0.0183

Epoch 11/50

 - 3s - loss: 0.0205 - val_loss: 0.0202

Epoch 12/50

 - 3s - loss: 0.0302 - val_loss: 0.0194

Epoch 13/50

 - 3s - loss: 0.0179 - val_loss: 0.0204

Epoch 14/50

 - 3s - loss: 0.0211 - val_loss: 0.0214

Epoch 15/50

 - 3s - loss: 0.0216 - val_loss: 0.0206

Epoch 16/50

 - 3s - loss: 0.0196 - val_loss: 0.0204

Epoch 17/50

 - 3s - loss: 0.0237 - val_loss: 0.0210

Epoch 18/50

 - 3s - loss: 0.0174 - val_loss: 0.0187

Epoch 19/50

 - 3s - loss: 0.0164 - val_loss: 0.0181

Epoch 20/50

 - 3s - loss: 0.0237 - val_loss: 0.0204

Epoch 21/50

 - 3s - loss: 0.0174 - val_loss: 0.0190

Epoch 22/50

 - 3s - loss: 0.0204 - val_loss: 0.0199

Epoch 23/50

 - 3s - loss: 0.0149 - val_loss: 0.0184

Epoch 24/50

 - 3s - loss: 0.0221 - val_loss: 0.0198

Epoch 25/50

 - 3s - loss: 0.0167 - val_loss: 0.0221

Epoch 26/50

 - 3s - loss: 0.0168 - val_loss: 0.0213

Epoch 27/50

 - 3s - loss: 0.0199 - val_loss: 0.0191

Epoch 28/50

 - 3s - loss: 0.0198 - val_loss: 0.0208

Epoch 29/50

 - 3s - loss: 0.0163 - val_loss: 0.0226

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 62149	action = 1	current_phase = 0	next_phase = 1	reward = -0.683189	array([[-2.8134859, -2.3470562]], dtype=float32)

time = 62157	action = 1	current_phase = 1	next_phase = 0	reward = -0.639128	array([[-3.385667 , -2.0173762]], dtype=float32)

time = 62165	action = 0	current_phase = 0	next_phase = 1	reward = -0.368653	array([[-1.5799925, -3.0881689]], dtype=float32)

time = 62170	action = 0	current_phase = 0	next_phase = 1	reward = 0.264527	array([[-1.6037123, -2.436859 ]], dtype=float32)

time = 62175	action = 0	current_phase = 0	next_phase = 1	reward = 0.047714	array([[-2.3977044, -3.2779233]], dtype=float32)

time = 62180	action = 1	current_phase = 0	next_phase = 1	reward = -1.427865	array([[-6.19861, -3.02745]], dtype=float32)

time = 62188	action = 1	current_phase = 1	next_phase = 0	reward = -0.703236	array([[-3.5423014, -2.1167567]], dtype=float32)

time = 62196	action = 0	current_phase = 0	next_phase = 1	reward = -0.082712	array([[-1.6861157, -2.9981859]], dtype=float32)

time = 62201	action = 0	current_phase = 0	next_phase = 1	reward = -0.004849	array([[-2.0734477, -3.0244014]], dtype=float32)

time = 62206	action = 0	current_phase = 0	next_phase = 1	reward = 0.064503	array([[-2.6062472, -3.4395664]], dtype=float32)

time = 62211	action = 1	current_phase = 0	next_phase = 1	reward = -1.450201	array([[-7.309977, -3.27038 ]], dtype=float32)

time = 62219	action = 1	current_phase = 1	next_phase = 0	reward = -0.843020	array([[-3.6873817, -2.2580242]], dtype=float32)

time = 62227	action = 0	current_phase = 0	next_phase = 1	reward = -0.080730	array([[-1.729996, -2.928957]], dtype=float32)

time = 62232	action = 0	current_phase = 0	next_phase = 1	reward = 0.012840	array([[-2.1724813, -2.996351 ]], dtype=float32)

time = 62237	action = 0	current_phase = 0	next_phase = 1	reward = 0.068346	array([[-2.7572088, -3.481656 ]], dtype=float32)

time = 62242	action = 1	current_phase = 0	next_phase = 1	reward = -1.594132	array([[-7.2999625, -3.5302608]], dtype=float32)

time = 62250	action = 1	current_phase = 1	next_phase = 0	reward = -0.920895	array([[-3.8076594, -2.3852153]], dtype=float32)

time = 62258	action = 0	current_phase = 0	next_phase = 1	reward = -0.057788	array([[-1.7181349, -2.9389572]], dtype=float32)

time = 62263	action = 0	current_phase = 0	next_phase = 1	reward = 0.033644	array([[-2.1991527, -2.9896452]], dtype=float32)

time = 62268	action = 0	current_phase = 0	next_phase = 1	reward = 0.067168	array([[-2.8028126, -3.5312006]], dtype=float32)

time = 62273	action = 1	current_phase = 0	next_phase = 1	reward = -1.916277	array([[-7.2727246, -3.5337167]], dtype=float32)

time = 62281	action = 1	current_phase = 1	next_phase = 0	reward = -1.423767	array([[-3.9522877, -2.26725  ]], dtype=float32)

time = 62289	action = 0	current_phase = 0	next_phase = 1	reward = 0.266571	array([[-1.3104401, -2.2856684]], dtype=float32)

time = 62294	action = 0	current_phase = 0	next_phase = 1	reward = 0.031980	array([[-1.9161966, -2.9942358]], dtype=float32)

time = 62299	action = 1	current_phase = 0	next_phase = 1	reward = -0.713577	array([[-2.8490407, -2.3689077]], dtype=float32)

time = 62307	action = 1	current_phase = 1	next_phase = 0	reward = -1.215882	array([[-3.3779035, -1.9859862]], dtype=float32)

time = 62315	action = 0	current_phase = 0	next_phase = 1	reward = 0.454400	array([[-1.2235711, -3.1284957]], dtype=float32)

time = 62320	action = 0	current_phase = 0	next_phase = 1	reward = -0.579493	array([[-1.9300032, -2.5351791]], dtype=float32)

time = 62325	action = 0	current_phase = 0	next_phase = 1	reward = 0.612086	array([[-2.2053308, -3.1793714]], dtype=float32)

time = 62330	action = 1	current_phase = 0	next_phase = 1	reward = -1.358336	array([[-6.2339926, -3.0265212]], dtype=float32)

time = 62338	action = 1	current_phase = 1	next_phase = 0	reward = -0.716018	array([[-3.5730147, -2.1352687]], dtype=float32)

time = 62346	action = 0	current_phase = 0	next_phase = 1	reward = -0.085870	array([[-1.6481647, -3.0146563]], dtype=float32)

time = 62351	action = 0	current_phase = 0	next_phase = 1	reward = -0.007543	array([[-1.9934087, -3.0164094]], dtype=float32)

time = 62356	action = 0	current_phase = 0	next_phase = 1	reward = 0.061518	array([[-2.633501 , -3.4791532]], dtype=float32)

time = 62361	action = 1	current_phase = 0	next_phase = 1	reward = -1.453507	array([[-7.3025026, -3.2676404]], dtype=float32)

time = 62369	action = 1	current_phase = 1	next_phase = 0	reward = -0.859516	array([[-3.6987834, -2.2689161]], dtype=float32)

time = 62377	action = 0	current_phase = 0	next_phase = 1	reward = -0.073817	array([[-1.6841216, -2.9319637]], dtype=float32)

time = 62382	action = 0	current_phase = 0	next_phase = 1	reward = 0.001878	array([[-2.2035801, -2.9989884]], dtype=float32)

time = 62387	action = 0	current_phase = 0	next_phase = 1	reward = 0.070207	array([[-2.7679424, -3.4811609]], dtype=float32)

time = 62392	action = 1	current_phase = 0	next_phase = 1	reward = -1.583325	array([[-7.2882843, -3.5496547]], dtype=float32)

time = 62400	action = 1	current_phase = 1	next_phase = 0	reward = -1.320313	array([[-3.80319  , -2.3628476]], dtype=float32)

time = 62408	action = 0	current_phase = 0	next_phase = 1	reward = 0.231505	array([[-1.6560584, -2.949459 ]], dtype=float32)

time = 62413	action = 0	current_phase = 0	next_phase = 1	reward = 0.029671	array([[-2.2326589, -2.9930842]], dtype=float32)

time = 62418	action = 0	current_phase = 0	next_phase = 1	reward = 0.093641	array([[-2.7970462, -3.5576103]], dtype=float32)

time = 62423	action = 1	current_phase = 0	next_phase = 1	reward = -1.808887	array([[-7.2659435, -3.6286678]], dtype=float32)

time = 62431	action = 1	current_phase = 1	next_phase = 0	reward = -1.029582	array([[-3.9319937, -2.2603962]], dtype=float32)

time = 62439	action = 0	current_phase = 0	next_phase = 1	reward = -0.031783	array([[-1.3902168, -2.2772646]], dtype=float32)

time = 62444	action = 0	current_phase = 0	next_phase = 1	reward = 0.030292	array([[-1.8757635, -2.9881804]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0248 - val_loss: 0.0180

Epoch 2/50

 - 3s - loss: 0.0196 - val_loss: 0.0184

Epoch 3/50

 - 3s - loss: 0.0235 - val_loss: 0.0135

Epoch 4/50

 - 3s - loss: 0.0188 - val_loss: 0.0144

Epoch 5/50

 - 3s - loss: 0.0221 - val_loss: 0.0142

Epoch 6/50

 - 3s - loss: 0.0187 - val_loss: 0.0152

Epoch 7/50

 - 3s - loss: 0.0242 - val_loss: 0.0159

Epoch 8/50

 - 3s - loss: 0.0170 - val_loss: 0.0153

Epoch 9/50

 - 3s - loss: 0.0189 - val_loss: 0.0151

Epoch 10/50

 - 3s - loss: 0.0269 - val_loss: 0.0140

Epoch 11/50

 - 3s - loss: 0.0226 - val_loss: 0.0154

Epoch 12/50

 - 3s - loss: 0.0209 - val_loss: 0.0158

Epoch 13/50

 - 3s - loss: 0.0194 - val_loss: 0.0144

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 62449	action = 1	current_phase = 0	next_phase = 1	reward = -0.640152	array([[-2.8046155, -2.3309443]], dtype=float32)

time = 62457	action = 1	current_phase = 1	next_phase = 0	reward = -0.878637	array([[-3.3429942, -2.0340195]], dtype=float32)

time = 62465	action = 0	current_phase = 0	next_phase = 1	reward = 0.168620	array([[-1.2706804, -3.1225607]], dtype=float32)

time = 62470	action = 0	current_phase = 0	next_phase = 1	reward = -0.026999	array([[-1.8247144, -2.382781 ]], dtype=float32)

time = 62475	action = 0	current_phase = 0	next_phase = 1	reward = 0.043859	array([[-2.394578, -3.295795]], dtype=float32)

time = 62480	action = 1	current_phase = 0	next_phase = 1	reward = -1.372540	array([[-6.2222314, -3.1519432]], dtype=float32)

time = 62488	action = 1	current_phase = 1	next_phase = 0	reward = -0.683933	array([[-3.5636334, -2.1960073]], dtype=float32)

time = 62496	action = 0	current_phase = 0	next_phase = 1	reward = -0.085055	array([[-1.6797159, -3.007507 ]], dtype=float32)

time = 62501	action = 0	current_phase = 0	next_phase = 1	reward = 0.002771	array([[-2.1345053, -3.0229049]], dtype=float32)

time = 62506	action = 0	current_phase = 0	next_phase = 1	reward = 0.058051	array([[-2.6722157, -3.482665 ]], dtype=float32)

time = 62511	action = 1	current_phase = 0	next_phase = 1	reward = -1.401387	array([[-7.273261 , -3.3822699]], dtype=float32)

time = 62519	action = 1	current_phase = 1	next_phase = 0	reward = -1.117251	array([[-3.7282567, -2.33524  ]], dtype=float32)

time = 62527	action = 0	current_phase = 0	next_phase = 1	reward = 0.223647	array([[-1.6953815, -2.9204302]], dtype=float32)

time = 62532	action = 0	current_phase = 0	next_phase = 1	reward = 0.006184	array([[-2.2043622, -3.0271573]], dtype=float32)

time = 62537	action = 0	current_phase = 0	next_phase = 1	reward = 0.062249	array([[-2.7791805, -3.5314472]], dtype=float32)

time = 62542	action = 1	current_phase = 0	next_phase = 1	reward = -1.543016	array([[-7.3021765, -3.563607 ]], dtype=float32)

time = 62550	action = 1	current_phase = 1	next_phase = 0	reward = -0.955235	array([[-3.8288145, -2.427141 ]], dtype=float32)

time = 62558	action = 0	current_phase = 0	next_phase = 1	reward = -0.062627	array([[-1.7147732, -2.9475362]], dtype=float32)

time = 62563	action = 0	current_phase = 0	next_phase = 1	reward = 0.002513	array([[-2.1732116, -2.9896803]], dtype=float32)

time = 62568	action = 0	current_phase = 0	next_phase = 1	reward = 0.066331	array([[-2.7800715, -3.4852903]], dtype=float32)

time = 62573	action = 1	current_phase = 0	next_phase = 1	reward = -1.783689	array([[-7.2851944, -3.6601567]], dtype=float32)

time = 62581	action = 1	current_phase = 1	next_phase = 0	reward = -1.342407	array([[-3.988511, -2.265934]], dtype=float32)

time = 62589	action = 0	current_phase = 0	next_phase = 1	reward = 0.248455	array([[-1.252311 , -2.2495914]], dtype=float32)

time = 62594	action = 0	current_phase = 0	next_phase = 1	reward = 0.021869	array([[-1.843318 , -2.9841228]], dtype=float32)

time = 62599	action = 1	current_phase = 0	next_phase = 1	reward = -0.725176	array([[-2.814371 , -2.3570106]], dtype=float32)

time = 62607	action = 1	current_phase = 1	next_phase = 0	reward = -0.913829	array([[-3.4130678, -2.0558462]], dtype=float32)

time = 62615	action = 0	current_phase = 0	next_phase = 1	reward = 0.183110	array([[-1.2399325, -3.1241143]], dtype=float32)

time = 62620	action = 0	current_phase = 0	next_phase = 1	reward = -0.018037	array([[-1.862509 , -2.5416322]], dtype=float32)

time = 62625	action = 0	current_phase = 0	next_phase = 1	reward = 0.054504	array([[-2.4471264, -3.3101122]], dtype=float32)

time = 62630	action = 1	current_phase = 0	next_phase = 1	reward = -1.269103	array([[-6.1886744, -3.0697513]], dtype=float32)

time = 62638	action = 1	current_phase = 1	next_phase = 0	reward = -1.013767	array([[-3.489058 , -2.1232615]], dtype=float32)

time = 62646	action = 0	current_phase = 0	next_phase = 1	reward = 0.211962	array([[-1.590398, -3.00478 ]], dtype=float32)

time = 62651	action = 0	current_phase = 0	next_phase = 1	reward = -0.007346	array([[-2.1207767, -3.0140438]], dtype=float32)

time = 62656	action = 0	current_phase = 0	next_phase = 1	reward = 0.069496	array([[-2.7293081, -3.494697 ]], dtype=float32)

time = 62661	action = 1	current_phase = 0	next_phase = 1	reward = -1.561874	array([[-7.318913 , -3.4038904]], dtype=float32)

time = 62669	action = 1	current_phase = 1	next_phase = 0	reward = -0.785129	array([[-3.6687117, -2.287109 ]], dtype=float32)

time = 62677	action = 0	current_phase = 0	next_phase = 1	reward = -0.079504	array([[-1.7250292, -2.9281113]], dtype=float32)

time = 62682	action = 0	current_phase = 0	next_phase = 1	reward = 0.000886	array([[-2.2252667, -3.024114 ]], dtype=float32)

time = 62687	action = 0	current_phase = 0	next_phase = 1	reward = 0.075326	array([[-2.7618973, -3.4825945]], dtype=float32)

time = 62692	action = 1	current_phase = 0	next_phase = 1	reward = -1.596472	array([[-7.292165, -3.594016]], dtype=float32)

time = 62700	action = 1	current_phase = 1	next_phase = 0	reward = -0.948606	array([[-3.7892833, -2.407911 ]], dtype=float32)

time = 62708	action = 0	current_phase = 0	next_phase = 1	reward = -0.041964	array([[-1.744556, -2.931476]], dtype=float32)

time = 62713	action = 0	current_phase = 0	next_phase = 1	reward = 0.029131	array([[-2.2148888, -3.00087  ]], dtype=float32)

time = 62718	action = 0	current_phase = 0	next_phase = 1	reward = 0.078974	array([[-2.7001388, -3.4438171]], dtype=float32)

time = 62723	action = 1	current_phase = 0	next_phase = 1	reward = -1.356581	array([[-7.292421 , -3.3063798]], dtype=float32)

time = 62731	action = 1	current_phase = 1	next_phase = 0	reward = -1.498132	array([[-3.7618122, -2.3418083]], dtype=float32)

time = 62739	action = 0	current_phase = 0	next_phase = 1	reward = 0.254324	array([[-1.2071869, -2.244444 ]], dtype=float32)

time = 62744	action = 0	current_phase = 0	next_phase = 1	reward = 0.036082	array([[-1.9458373, -3.0271137]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0228 - val_loss: 0.0111

Epoch 2/50

 - 3s - loss: 0.0270 - val_loss: 0.0120

Epoch 3/50

 - 3s - loss: 0.0209 - val_loss: 0.0124

Epoch 4/50

 - 3s - loss: 0.0201 - val_loss: 0.0120

Epoch 5/50

 - 3s - loss: 0.0286 - val_loss: 0.0156

Epoch 6/50

 - 3s - loss: 0.0243 - val_loss: 0.0146

Epoch 7/50

 - 3s - loss: 0.0180 - val_loss: 0.0126

Epoch 8/50

 - 3s - loss: 0.0218 - val_loss: 0.0132

Epoch 9/50

 - 3s - loss: 0.0269 - val_loss: 0.0147

Epoch 10/50

 - 3s - loss: 0.0229 - val_loss: 0.0139

Epoch 11/50

 - 3s - loss: 0.0230 - val_loss: 0.0159

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 62749	action = 1	current_phase = 0	next_phase = 1	reward = -0.708888	array([[-2.6887112, -2.3657928]], dtype=float32)

time = 62757	action = 1	current_phase = 1	next_phase = 0	reward = -0.653437	array([[-3.344139 , -2.1140513]], dtype=float32)

time = 62765	action = 0	current_phase = 0	next_phase = 1	reward = -0.113148	array([[-1.5510217, -3.0902762]], dtype=float32)

time = 62770	action = 0	current_phase = 0	next_phase = 1	reward = -0.301096	array([[-1.8423433, -2.568057 ]], dtype=float32)

time = 62775	action = 0	current_phase = 0	next_phase = 1	reward = 0.324486	array([[-2.208622, -3.189475]], dtype=float32)

time = 62780	action = 1	current_phase = 0	next_phase = 1	reward = -1.300715	array([[-6.240094 , -3.1783576]], dtype=float32)

time = 62788	action = 1	current_phase = 1	next_phase = 0	reward = -0.703886	array([[-3.5218332, -2.1947503]], dtype=float32)

time = 62796	action = 0	current_phase = 0	next_phase = 1	reward = -0.097040	array([[-1.6797512, -3.0183506]], dtype=float32)

time = 62801	action = 0	current_phase = 0	next_phase = 1	reward = -0.019599	array([[-2.068029, -3.039896]], dtype=float32)

time = 62806	action = 0	current_phase = 0	next_phase = 1	reward = 0.057702	array([[-2.5647945, -3.4314814]], dtype=float32)

time = 62811	action = 1	current_phase = 0	next_phase = 1	reward = -1.509273	array([[-7.35226  , -3.3543072]], dtype=float32)

time = 62819	action = 1	current_phase = 1	next_phase = 0	reward = -0.769893	array([[-3.6421437, -2.3103123]], dtype=float32)

time = 62827	action = 0	current_phase = 0	next_phase = 1	reward = -0.050904	array([[-1.7655884, -2.9287522]], dtype=float32)

time = 62832	action = 0	current_phase = 0	next_phase = 1	reward = 0.020477	array([[-2.2485187, -3.0240917]], dtype=float32)

time = 62837	action = 0	current_phase = 0	next_phase = 1	reward = 0.073350	array([[-2.853138, -3.574204]], dtype=float32)

time = 62842	action = 1	current_phase = 0	next_phase = 1	reward = -1.618153	array([[-7.315645 , -3.5549088]], dtype=float32)

time = 62850	action = 1	current_phase = 1	next_phase = 0	reward = -1.013433	array([[-3.7788093, -2.450457 ]], dtype=float32)

time = 62858	action = 0	current_phase = 0	next_phase = 1	reward = -0.057296	array([[-1.7860903, -2.9656181]], dtype=float32)

time = 62863	action = 0	current_phase = 0	next_phase = 1	reward = 0.018283	array([[-2.2189727, -3.0183187]], dtype=float32)

time = 62868	action = 0	current_phase = 0	next_phase = 1	reward = 0.080525	array([[-2.7965772, -3.5478745]], dtype=float32)

time = 62873	action = 1	current_phase = 0	next_phase = 1	reward = -1.948431	array([[-7.294862, -3.625033]], dtype=float32)

time = 62881	action = 1	current_phase = 1	next_phase = 0	reward = -1.384588	array([[-4.0143604, -2.3011374]], dtype=float32)

time = 62889	action = 0	current_phase = 0	next_phase = 1	reward = 0.271897	array([[-1.2668407, -2.2819533]], dtype=float32)

time = 62894	action = 0	current_phase = 0	next_phase = 1	reward = 0.303791	array([[-1.8820673, -2.9880624]], dtype=float32)

time = 62899	action = 1	current_phase = 0	next_phase = 1	reward = -1.046057	array([[-2.8398623, -2.364441 ]], dtype=float32)

time = 62907	action = 1	current_phase = 1	next_phase = 0	reward = -0.929144	array([[-3.459031 , -2.1293833]], dtype=float32)

time = 62915	action = 0	current_phase = 0	next_phase = 1	reward = 0.188815	array([[-1.2583456, -3.1134653]], dtype=float32)

time = 62920	action = 0	current_phase = 0	next_phase = 1	reward = -0.020220	array([[-1.8493534, -2.612386 ]], dtype=float32)

time = 62925	action = 0	current_phase = 0	next_phase = 1	reward = -0.504827	array([[-2.464155 , -3.3815002]], dtype=float32)

time = 62930	action = 1	current_phase = 0	next_phase = 1	reward = -0.804700	array([[-6.1478214, -2.7332087]], dtype=float32)

time = 62938	action = 1	current_phase = 1	next_phase = 0	reward = -1.011466	array([[-3.5262687, -2.2067943]], dtype=float32)

time = 62946	action = 0	current_phase = 0	next_phase = 1	reward = 0.195039	array([[-1.6609097, -2.966835 ]], dtype=float32)

time = 62951	action = 0	current_phase = 0	next_phase = 1	reward = 0.000202	array([[-2.0228238, -3.0180125]], dtype=float32)

time = 62956	action = 0	current_phase = 0	next_phase = 1	reward = 0.066337	array([[-2.5361156, -3.3941855]], dtype=float32)

time = 62961	action = 1	current_phase = 0	next_phase = 1	reward = -1.433261	array([[-7.3292537, -3.3899221]], dtype=float32)

time = 62969	action = 1	current_phase = 1	next_phase = 0	reward = -0.889499	array([[-3.68427  , -2.3519783]], dtype=float32)

time = 62977	action = 0	current_phase = 0	next_phase = 1	reward = -0.065577	array([[-1.757614 , -2.9464064]], dtype=float32)

time = 62982	action = 0	current_phase = 0	next_phase = 1	reward = -0.000401	array([[-2.2445045, -3.0413795]], dtype=float32)

time = 62987	action = 0	current_phase = 0	next_phase = 1	reward = 0.055556	array([[-2.8272562, -3.5529604]], dtype=float32)

time = 62992	action = 1	current_phase = 0	next_phase = 1	reward = -1.709480	array([[-7.3206472, -3.5750504]], dtype=float32)

time = 63000	action = 1	current_phase = 1	next_phase = 0	reward = -0.986386	array([[-3.7796469, -2.4528131]], dtype=float32)

time = 63008	action = 0	current_phase = 0	next_phase = 1	reward = -0.042583	array([[-1.7773165, -2.949577 ]], dtype=float32)

time = 63013	action = 0	current_phase = 0	next_phase = 1	reward = -0.001592	array([[-2.2331688, -3.0076556]], dtype=float32)

time = 63018	action = 0	current_phase = 0	next_phase = 1	reward = 0.071820	array([[-2.8370476, -3.572195 ]], dtype=float32)

time = 63023	action = 1	current_phase = 0	next_phase = 1	reward = -1.929078	array([[-7.2928896, -3.687902 ]], dtype=float32)

time = 63031	action = 1	current_phase = 1	next_phase = 0	reward = -1.337814	array([[-3.8491893, -2.2601254]], dtype=float32)

time = 63039	action = 0	current_phase = 0	next_phase = 1	reward = 0.256142	array([[-1.3060396, -2.2883668]], dtype=float32)

time = 63044	action = 0	current_phase = 0	next_phase = 1	reward = 0.013407	array([[-1.8319657, -2.9770055]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0231 - val_loss: 0.0074

Epoch 2/50

 - 3s - loss: 0.0198 - val_loss: 0.0090

Epoch 3/50

 - 3s - loss: 0.0180 - val_loss: 0.0072

Epoch 4/50

 - 3s - loss: 0.0198 - val_loss: 0.0070

Epoch 5/50

 - 3s - loss: 0.0193 - val_loss: 0.0077

Epoch 6/50

 - 3s - loss: 0.0243 - val_loss: 0.0079

Epoch 7/50

 - 3s - loss: 0.0182 - val_loss: 0.0077

Epoch 8/50

 - 3s - loss: 0.0160 - val_loss: 0.0077

Epoch 9/50

 - 3s - loss: 0.0194 - val_loss: 0.0073

Epoch 10/50

 - 3s - loss: 0.0218 - val_loss: 0.0076

Epoch 11/50

 - 3s - loss: 0.0161 - val_loss: 0.0101

Epoch 12/50

 - 3s - loss: 0.0161 - val_loss: 0.0071

Epoch 13/50

 - 3s - loss: 0.0166 - val_loss: 0.0073

Epoch 14/50

 - 3s - loss: 0.0172 - val_loss: 0.0077

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 63049	action = 1	current_phase = 0	next_phase = 1	reward = -0.649139	array([[-2.8108218, -2.360961 ]], dtype=float32)

time = 63057	action = 1	current_phase = 1	next_phase = 0	reward = -0.582935	array([[-3.3276892, -2.0670602]], dtype=float32)

time = 63065	action = 0	current_phase = 0	next_phase = 1	reward = -0.104502	array([[-1.5207756, -3.1427996]], dtype=float32)

time = 63070	action = 0	current_phase = 0	next_phase = 1	reward = -0.024740	array([[-1.8719327, -2.6442604]], dtype=float32)

time = 63075	action = 0	current_phase = 0	next_phase = 1	reward = -0.234508	array([[-2.4794705, -3.3030462]], dtype=float32)

time = 63080	action = 1	current_phase = 0	next_phase = 1	reward = -1.094673	array([[-6.216096, -2.927993]], dtype=float32)

time = 63088	action = 1	current_phase = 1	next_phase = 0	reward = -0.704741	array([[-3.5696402, -2.2174027]], dtype=float32)

time = 63096	action = 0	current_phase = 0	next_phase = 1	reward = -0.068563	array([[-1.7071422, -3.0091994]], dtype=float32)

time = 63101	action = 0	current_phase = 0	next_phase = 1	reward = 0.000312	array([[-2.135799 , -3.0645838]], dtype=float32)

time = 63106	action = 0	current_phase = 0	next_phase = 1	reward = 0.059230	array([[-2.614211 , -3.4906082]], dtype=float32)

time = 63111	action = 1	current_phase = 0	next_phase = 1	reward = -1.456539	array([[-7.3052816, -3.3882854]], dtype=float32)

time = 63119	action = 1	current_phase = 1	next_phase = 0	reward = -0.832470	array([[-3.6848736, -2.341764 ]], dtype=float32)

time = 63127	action = 0	current_phase = 0	next_phase = 1	reward = -0.071513	array([[-1.736051 , -2.9396906]], dtype=float32)

time = 63132	action = 0	current_phase = 0	next_phase = 1	reward = 0.006370	array([[-2.2420027, -3.0498228]], dtype=float32)

time = 63137	action = 0	current_phase = 0	next_phase = 1	reward = 0.071671	array([[-2.7766953, -3.4964423]], dtype=float32)

time = 63142	action = 1	current_phase = 0	next_phase = 1	reward = -1.696110	array([[-7.293055 , -3.5952177]], dtype=float32)

time = 63150	action = 1	current_phase = 1	next_phase = 0	reward = -0.946135	array([[-3.8286738, -2.461069 ]], dtype=float32)

time = 63158	action = 0	current_phase = 0	next_phase = 1	reward = -0.053885	array([[-1.6904144, -2.9645598]], dtype=float32)

time = 63163	action = 0	current_phase = 0	next_phase = 1	reward = 0.034726	array([[-2.2208295, -3.018138 ]], dtype=float32)

time = 63168	action = 0	current_phase = 0	next_phase = 1	reward = 0.074469	array([[-2.8678684, -3.7048965]], dtype=float32)

time = 63173	action = 1	current_phase = 0	next_phase = 1	reward = -1.338420	array([[-7.274645 , -3.6846552]], dtype=float32)

time = 63181	action = 1	current_phase = 1	next_phase = 0	reward = -1.396196	array([[-3.8057551, -2.3680022]], dtype=float32)

time = 63189	action = 0	current_phase = 0	next_phase = 1	reward = 0.229101	array([[-1.3162583, -2.2932787]], dtype=float32)

time = 63194	action = 0	current_phase = 0	next_phase = 1	reward = 0.020342	array([[-1.9065117, -2.9986339]], dtype=float32)

time = 63199	action = 1	current_phase = 0	next_phase = 1	reward = -0.634955	array([[-2.837671, -2.452644]], dtype=float32)

time = 63207	action = 1	current_phase = 1	next_phase = 0	reward = -0.593293	array([[-3.4011035, -2.12121  ]], dtype=float32)

time = 63215	action = 0	current_phase = 0	next_phase = 1	reward = -0.119151	array([[-1.4575912, -3.1115232]], dtype=float32)

time = 63220	action = 0	current_phase = 0	next_phase = 1	reward = -0.016893	array([[-1.8974359, -2.6506057]], dtype=float32)

time = 63225	action = 0	current_phase = 0	next_phase = 1	reward = -0.224602	array([[-2.4525642, -3.3257666]], dtype=float32)

time = 63230	action = 1	current_phase = 0	next_phase = 1	reward = -1.037364	array([[-6.2150736, -2.9125974]], dtype=float32)

time = 63238	action = 1	current_phase = 1	next_phase = 0	reward = -0.714483	array([[-3.518519 , -2.1987543]], dtype=float32)

time = 63246	action = 0	current_phase = 0	next_phase = 1	reward = -0.091361	array([[-1.6803557, -2.9855855]], dtype=float32)

time = 63251	action = 0	current_phase = 0	next_phase = 1	reward = -0.002815	array([[-2.052137, -3.038251]], dtype=float32)

time = 63256	action = 0	current_phase = 0	next_phase = 1	reward = 0.059608	array([[-2.6040328, -3.4245477]], dtype=float32)

time = 63261	action = 1	current_phase = 0	next_phase = 1	reward = -1.439729	array([[-7.3357267, -3.3741565]], dtype=float32)

time = 63269	action = 1	current_phase = 1	next_phase = 0	reward = -0.832948	array([[-3.6606216, -2.3242278]], dtype=float32)

time = 63277	action = 0	current_phase = 0	next_phase = 1	reward = -0.059825	array([[-1.6766942, -2.9486077]], dtype=float32)

time = 63282	action = 0	current_phase = 0	next_phase = 1	reward = 0.005223	array([[-2.271559 , -3.0738502]], dtype=float32)

time = 63287	action = 0	current_phase = 0	next_phase = 1	reward = 0.074495	array([[-2.814005, -3.522775]], dtype=float32)

time = 63292	action = 1	current_phase = 0	next_phase = 1	reward = -1.660909	array([[-7.2816267, -3.6160743]], dtype=float32)

time = 63300	action = 1	current_phase = 1	next_phase = 0	reward = -0.951085	array([[-3.8128352, -2.4284368]], dtype=float32)

time = 63308	action = 0	current_phase = 0	next_phase = 1	reward = -0.050822	array([[-1.7955142, -2.9672606]], dtype=float32)

time = 63313	action = 0	current_phase = 0	next_phase = 1	reward = 0.026900	array([[-2.1784675, -3.0356627]], dtype=float32)

time = 63318	action = 0	current_phase = 0	next_phase = 1	reward = 0.069585	array([[-2.8090765, -3.5582685]], dtype=float32)

time = 63323	action = 1	current_phase = 0	next_phase = 1	reward = -0.802336	array([[-7.309414 , -3.1916966]], dtype=float32)

time = 63331	action = 1	current_phase = 1	next_phase = 0	reward = -1.113077	array([[-3.6442103, -2.4461236]], dtype=float32)

time = 63339	action = 0	current_phase = 0	next_phase = 1	reward = -0.027939	array([[-1.3954735, -2.3000994]], dtype=float32)

time = 63344	action = 0	current_phase = 0	next_phase = 1	reward = 0.027293	array([[-1.9135984, -3.083876 ]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0180 - val_loss: 0.0129

Epoch 2/50

 - 3s - loss: 0.0202 - val_loss: 0.0133

Epoch 3/50

 - 3s - loss: 0.0170 - val_loss: 0.0114

Epoch 4/50

 - 3s - loss: 0.0182 - val_loss: 0.0113

Epoch 5/50

 - 3s - loss: 0.0205 - val_loss: 0.0145

Epoch 6/50

 - 3s - loss: 0.0195 - val_loss: 0.0121

Epoch 7/50

 - 3s - loss: 0.0182 - val_loss: 0.0157

Epoch 8/50

 - 3s - loss: 0.0198 - val_loss: 0.0133

Epoch 9/50

 - 3s - loss: 0.0192 - val_loss: 0.0140

Epoch 10/50

 - 3s - loss: 0.0219 - val_loss: 0.0135

Epoch 11/50

 - 3s - loss: 0.0155 - val_loss: 0.0129

Epoch 12/50

 - 3s - loss: 0.0212 - val_loss: 0.0129

Epoch 13/50

 - 3s - loss: 0.0171 - val_loss: 0.0176

Epoch 14/50

 - 3s - loss: 0.0199 - val_loss: 0.0145

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 63349	action = 1	current_phase = 0	next_phase = 1	reward = -0.652012	array([[-2.8499553, -2.4545507]], dtype=float32)

time = 63357	action = 1	current_phase = 1	next_phase = 0	reward = -0.638838	array([[-3.343275 , -1.9599793]], dtype=float32)

time = 63365	action = 0	current_phase = 0	next_phase = 1	reward = -0.106643	array([[-1.6294389, -3.1372879]], dtype=float32)

time = 63370	action = 0	current_phase = 0	next_phase = 1	reward = -0.041864	array([[-1.9869416, -2.7885633]], dtype=float32)

time = 63375	action = 0	current_phase = 0	next_phase = 1	reward = -0.255344	array([[-2.501218, -3.384783]], dtype=float32)

time = 63380	action = 1	current_phase = 0	next_phase = 1	reward = -1.106402	array([[-6.2027674, -2.7870505]], dtype=float32)

time = 63388	action = 1	current_phase = 1	next_phase = 0	reward = -0.703031	array([[-3.5710163, -2.1105375]], dtype=float32)

time = 63396	action = 0	current_phase = 0	next_phase = 1	reward = -0.078164	array([[-1.7056837, -3.0502174]], dtype=float32)

time = 63401	action = 0	current_phase = 0	next_phase = 1	reward = -0.014092	array([[-2.1625528, -3.0641134]], dtype=float32)

time = 63406	action = 0	current_phase = 0	next_phase = 1	reward = 0.040238	array([[-2.6242692, -3.491412 ]], dtype=float32)

time = 63411	action = 1	current_phase = 0	next_phase = 1	reward = -1.484771	array([[-7.3413415, -3.348043 ]], dtype=float32)

time = 63419	action = 1	current_phase = 1	next_phase = 0	reward = -0.776751	array([[-3.6743808, -2.2026   ]], dtype=float32)

time = 63427	action = 0	current_phase = 0	next_phase = 1	reward = -0.069556	array([[-1.8031197, -2.9492912]], dtype=float32)

time = 63432	action = 0	current_phase = 0	next_phase = 1	reward = -0.004849	array([[-2.2315047, -3.0674493]], dtype=float32)

time = 63437	action = 0	current_phase = 0	next_phase = 1	reward = 0.071192	array([[-2.8100085, -3.538191 ]], dtype=float32)

time = 63442	action = 1	current_phase = 0	next_phase = 1	reward = -1.551659	array([[-7.3194604, -3.5842838]], dtype=float32)

time = 63450	action = 1	current_phase = 1	next_phase = 0	reward = -1.012757	array([[-3.8332205, -2.3618066]], dtype=float32)

time = 63458	action = 0	current_phase = 0	next_phase = 1	reward = -0.053581	array([[-1.7660414, -2.9668396]], dtype=float32)

time = 63463	action = 0	current_phase = 0	next_phase = 1	reward = 0.012559	array([[-2.2589202, -3.0413616]], dtype=float32)

time = 63468	action = 0	current_phase = 0	next_phase = 1	reward = 0.064965	array([[-2.8363645, -3.580093 ]], dtype=float32)

time = 63473	action = 1	current_phase = 0	next_phase = 1	reward = -1.319736	array([[-7.2945137, -3.5033154]], dtype=float32)

time = 63481	action = 1	current_phase = 1	next_phase = 0	reward = -1.375166	array([[-3.8052406, -2.320715 ]], dtype=float32)

time = 63489	action = 0	current_phase = 0	next_phase = 1	reward = 0.246158	array([[-1.2956557, -2.345998 ]], dtype=float32)

time = 63494	action = 0	current_phase = 0	next_phase = 1	reward = 0.016846	array([[-1.8275833, -3.0247836]], dtype=float32)

time = 63499	action = 1	current_phase = 0	next_phase = 1	reward = -0.650791	array([[-2.7822742, -2.4165096]], dtype=float32)

time = 63507	action = 1	current_phase = 1	next_phase = 0	reward = -1.184098	array([[-3.4324598, -2.0180993]], dtype=float32)

time = 63515	action = 0	current_phase = 0	next_phase = 1	reward = 0.463877	array([[-1.2713296, -3.129347 ]], dtype=float32)

time = 63520	action = 0	current_phase = 0	next_phase = 1	reward = -0.035812	array([[-1.8793676, -2.700302 ]], dtype=float32)

time = 63525	action = 0	current_phase = 0	next_phase = 1	reward = -0.244476	array([[-2.4064436, -3.3385832]], dtype=float32)

time = 63530	action = 1	current_phase = 0	next_phase = 1	reward = -1.079529	array([[-6.2088823, -2.796893 ]], dtype=float32)

time = 63538	action = 1	current_phase = 1	next_phase = 0	reward = -0.698528	array([[-3.5593987, -2.100567 ]], dtype=float32)

time = 63546	action = 0	current_phase = 0	next_phase = 1	reward = -0.087971	array([[-1.7564437, -3.0151622]], dtype=float32)

time = 63551	action = 0	current_phase = 0	next_phase = 1	reward = -0.014138	array([[-2.1635804, -3.0739837]], dtype=float32)

time = 63556	action = 0	current_phase = 0	next_phase = 1	reward = 0.055505	array([[-2.6656818, -3.5122883]], dtype=float32)

time = 63561	action = 1	current_phase = 0	next_phase = 1	reward = -1.487389	array([[-7.3242507, -3.3039262]], dtype=float32)

time = 63569	action = 1	current_phase = 1	next_phase = 0	reward = -0.779564	array([[-3.6805987, -2.2023044]], dtype=float32)

time = 63577	action = 0	current_phase = 0	next_phase = 1	reward = -0.057711	array([[-1.8063972, -2.9501274]], dtype=float32)

time = 63582	action = 0	current_phase = 0	next_phase = 1	reward = 0.017212	array([[-2.2612834, -3.062432 ]], dtype=float32)

time = 63587	action = 0	current_phase = 0	next_phase = 1	reward = 0.077764	array([[-2.809793 , -3.5988514]], dtype=float32)

time = 63592	action = 1	current_phase = 0	next_phase = 1	reward = -1.645208	array([[-7.317249, -3.583984]], dtype=float32)

time = 63600	action = 1	current_phase = 1	next_phase = 0	reward = -1.312542	array([[-3.7977395, -2.318441 ]], dtype=float32)

time = 63608	action = 0	current_phase = 0	next_phase = 1	reward = 0.236287	array([[-1.6871375, -2.945694 ]], dtype=float32)

time = 63613	action = 0	current_phase = 0	next_phase = 1	reward = 0.003834	array([[-2.1702092, -3.0238144]], dtype=float32)

time = 63618	action = 0	current_phase = 0	next_phase = 1	reward = 0.061322	array([[-2.8151462, -3.5298016]], dtype=float32)

time = 63623	action = 1	current_phase = 0	next_phase = 1	reward = -1.317656	array([[-7.331016 , -3.3444517]], dtype=float32)

time = 63631	action = 1	current_phase = 1	next_phase = 0	reward = -1.425773	array([[-3.720337 , -2.3547308]], dtype=float32)

time = 63639	action = 0	current_phase = 0	next_phase = 1	reward = 0.262725	array([[-1.3616515, -2.3566818]], dtype=float32)

time = 63644	action = 0	current_phase = 0	next_phase = 1	reward = 0.034637	array([[-1.9258788, -3.0558877]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0271 - val_loss: 0.0130

Epoch 2/50

 - 3s - loss: 0.0205 - val_loss: 0.0165

Epoch 3/50

 - 3s - loss: 0.0192 - val_loss: 0.0116

Epoch 4/50

 - 3s - loss: 0.0227 - val_loss: 0.0103

Epoch 5/50

 - 3s - loss: 0.0200 - val_loss: 0.0111

Epoch 6/50

 - 3s - loss: 0.0182 - val_loss: 0.0133

Epoch 7/50

 - 3s - loss: 0.0203 - val_loss: 0.0115

Epoch 8/50

 - 3s - loss: 0.0202 - val_loss: 0.0112

Epoch 9/50

 - 3s - loss: 0.0182 - val_loss: 0.0126

Epoch 10/50

 - 3s - loss: 0.0186 - val_loss: 0.0111

Epoch 11/50

 - 3s - loss: 0.0158 - val_loss: 0.0155

Epoch 12/50

 - 3s - loss: 0.0165 - val_loss: 0.0142

Epoch 13/50

 - 3s - loss: 0.0179 - val_loss: 0.0148

Epoch 14/50

 - 3s - loss: 0.0206 - val_loss: 0.0106

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 63649	action = 1	current_phase = 0	next_phase = 1	reward = -0.669941	array([[-2.8793886, -2.3634605]], dtype=float32)

time = 63657	action = 1	current_phase = 1	next_phase = 0	reward = -0.649916	array([[-3.4088926, -2.0144422]], dtype=float32)

time = 63665	action = 0	current_phase = 0	next_phase = 1	reward = -0.111837	array([[-1.487628 , -3.1772573]], dtype=float32)

time = 63670	action = 0	current_phase = 0	next_phase = 1	reward = -0.031975	array([[-2.0210981, -2.788217 ]], dtype=float32)

time = 63675	action = 0	current_phase = 0	next_phase = 1	reward = 0.034715	array([[-2.486332 , -3.3700314]], dtype=float32)

time = 63680	action = 1	current_phase = 0	next_phase = 1	reward = -1.250797	array([[-6.207423 , -2.9927192]], dtype=float32)

time = 63688	action = 1	current_phase = 1	next_phase = 0	reward = -0.705627	array([[-3.5466483, -2.0946658]], dtype=float32)

time = 63696	action = 0	current_phase = 0	next_phase = 1	reward = -0.089632	array([[-1.7609732, -3.038048 ]], dtype=float32)

time = 63701	action = 0	current_phase = 0	next_phase = 1	reward = -0.000802	array([[-2.1567116, -3.0650048]], dtype=float32)

time = 63706	action = 0	current_phase = 0	next_phase = 1	reward = 0.065369	array([[-2.684201 , -3.5322626]], dtype=float32)

time = 63711	action = 1	current_phase = 0	next_phase = 1	reward = -1.539792	array([[-7.3503566, -3.319177 ]], dtype=float32)

time = 63719	action = 1	current_phase = 1	next_phase = 0	reward = -0.835372	array([[-3.709664 , -2.2078466]], dtype=float32)

time = 63727	action = 0	current_phase = 0	next_phase = 1	reward = -0.083033	array([[-1.8210747, -2.9336147]], dtype=float32)

time = 63732	action = 0	current_phase = 0	next_phase = 1	reward = 0.021054	array([[-2.2643507, -3.0701945]], dtype=float32)

time = 63737	action = 0	current_phase = 0	next_phase = 1	reward = 0.087313	array([[-2.810471 , -3.5912833]], dtype=float32)

time = 63742	action = 1	current_phase = 0	next_phase = 1	reward = -1.663502	array([[-7.3229365, -3.540616 ]], dtype=float32)

time = 63750	action = 1	current_phase = 1	next_phase = 0	reward = -0.943020	array([[-3.9111037, -2.3919044]], dtype=float32)

time = 63758	action = 0	current_phase = 0	next_phase = 1	reward = -0.062528	array([[-1.7507637, -2.9648483]], dtype=float32)

time = 63763	action = 0	current_phase = 0	next_phase = 1	reward = 0.014724	array([[-2.192117 , -3.0232074]], dtype=float32)

time = 63768	action = 0	current_phase = 0	next_phase = 1	reward = 0.082695	array([[-2.7644193, -3.5234168]], dtype=float32)

time = 63773	action = 1	current_phase = 0	next_phase = 1	reward = -1.879245	array([[-7.3038588, -3.6648393]], dtype=float32)

time = 63781	action = 1	current_phase = 1	next_phase = 0	reward = -1.331656	array([[-4.0026894, -2.2712202]], dtype=float32)

time = 63789	action = 0	current_phase = 0	next_phase = 1	reward = 0.274222	array([[-1.309655 , -2.3107762]], dtype=float32)

time = 63794	action = 0	current_phase = 0	next_phase = 1	reward = 0.049562	array([[-1.8987565, -3.0022902]], dtype=float32)

time = 63799	action = 1	current_phase = 0	next_phase = 1	reward = -0.700638	array([[-2.837528, -2.357772]], dtype=float32)

time = 63807	action = 1	current_phase = 1	next_phase = 0	reward = -0.665347	array([[-3.4022303, -1.9231541]], dtype=float32)

time = 63815	action = 0	current_phase = 0	next_phase = 1	reward = -0.386469	array([[-1.4147497, -3.1656623]], dtype=float32)

time = 63820	action = 0	current_phase = 0	next_phase = 1	reward = -0.301154	array([[-1.9111993, -2.843951 ]], dtype=float32)

time = 63825	action = 0	current_phase = 0	next_phase = 1	reward = 0.326084	array([[-2.1156738, -3.1788917]], dtype=float32)

time = 63830	action = 1	current_phase = 0	next_phase = 1	reward = -1.090242	array([[-6.1836977, -2.9904983]], dtype=float32)

time = 63838	action = 1	current_phase = 1	next_phase = 0	reward = -0.711082	array([[-3.5992312, -2.0767398]], dtype=float32)

time = 63846	action = 0	current_phase = 0	next_phase = 1	reward = -0.094938	array([[-1.7567747, -3.0803225]], dtype=float32)

time = 63851	action = 0	current_phase = 0	next_phase = 1	reward = -0.019282	array([[-2.0806446, -3.0624635]], dtype=float32)

time = 63856	action = 0	current_phase = 0	next_phase = 1	reward = 0.035440	array([[-2.5212278, -3.4580379]], dtype=float32)

time = 63861	action = 1	current_phase = 0	next_phase = 1	reward = -1.279011	array([[-7.357093 , -3.2892504]], dtype=float32)

time = 63869	action = 1	current_phase = 1	next_phase = 0	reward = -0.834677	array([[-3.6796834, -2.2031298]], dtype=float32)

time = 63877	action = 0	current_phase = 0	next_phase = 1	reward = -0.082386	array([[-1.8120675, -2.948003 ]], dtype=float32)

time = 63882	action = 0	current_phase = 0	next_phase = 1	reward = 0.001612	array([[-2.2534957, -3.0492752]], dtype=float32)

time = 63887	action = 0	current_phase = 0	next_phase = 1	reward = 0.060561	array([[-2.775029 , -3.5384774]], dtype=float32)

time = 63892	action = 1	current_phase = 0	next_phase = 1	reward = -1.534229	array([[-7.328343 , -3.5565803]], dtype=float32)

time = 63900	action = 1	current_phase = 1	next_phase = 0	reward = -0.904822	array([[-3.864769 , -2.3632214]], dtype=float32)

time = 63908	action = 0	current_phase = 0	next_phase = 1	reward = -0.054366	array([[-1.7627251, -2.972329 ]], dtype=float32)

time = 63913	action = 0	current_phase = 0	next_phase = 1	reward = 0.023416	array([[-2.2743795, -3.0497727]], dtype=float32)

time = 63918	action = 0	current_phase = 0	next_phase = 1	reward = 0.066797	array([[-2.7763155, -3.5425963]], dtype=float32)

time = 63923	action = 1	current_phase = 0	next_phase = 1	reward = -1.875488	array([[-7.3092976, -3.6835563]], dtype=float32)

time = 63931	action = 1	current_phase = 1	next_phase = 0	reward = -1.087503	array([[-4.018996 , -2.3129148]], dtype=float32)

time = 63939	action = 0	current_phase = 0	next_phase = 1	reward = -0.051988	array([[-1.3552076, -2.357909 ]], dtype=float32)

time = 63944	action = 0	current_phase = 0	next_phase = 1	reward = 0.008574	array([[-1.9308584, -3.0434706]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0245 - val_loss: 0.0143

Epoch 2/50

 - 3s - loss: 0.0238 - val_loss: 0.0128

Epoch 3/50

 - 3s - loss: 0.0202 - val_loss: 0.0131

Epoch 4/50

 - 3s - loss: 0.0218 - val_loss: 0.0125

Epoch 5/50

 - 3s - loss: 0.0218 - val_loss: 0.0127

Epoch 6/50

 - 3s - loss: 0.0187 - val_loss: 0.0139

Epoch 7/50

 - 3s - loss: 0.0185 - val_loss: 0.0131

Epoch 8/50

 - 3s - loss: 0.0218 - val_loss: 0.0128

Epoch 9/50

 - 3s - loss: 0.0194 - val_loss: 0.0129

Epoch 10/50

 - 3s - loss: 0.0192 - val_loss: 0.0134

Epoch 11/50

 - 3s - loss: 0.0199 - val_loss: 0.0136

Epoch 12/50

 - 3s - loss: 0.0175 - val_loss: 0.0151

Epoch 13/50

 - 3s - loss: 0.0210 - val_loss: 0.0134

Epoch 14/50

 - 3s - loss: 0.0197 - val_loss: 0.0168

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 63949	action = 1	current_phase = 0	next_phase = 1	reward = -0.710370	array([[-2.8711228, -2.3740802]], dtype=float32)

time = 63957	action = 1	current_phase = 1	next_phase = 0	reward = -0.818118	array([[-3.4418607, -1.9843428]], dtype=float32)

time = 63965	action = 0	current_phase = 0	next_phase = 1	reward = 0.167289	array([[-1.3565352, -3.1557322]], dtype=float32)

time = 63970	action = 0	current_phase = 0	next_phase = 1	reward = -0.314841	array([[-2.0006433, -2.864501 ]], dtype=float32)

time = 63975	action = 0	current_phase = 0	next_phase = 1	reward = -0.232719	array([[-2.2275805, -3.2334092]], dtype=float32)

time = 63980	action = 1	current_phase = 0	next_phase = 1	reward = -0.743489	array([[-6.159093 , -2.6494472]], dtype=float32)

time = 63988	action = 1	current_phase = 1	next_phase = 0	reward = -0.702780	array([[-3.5639596, -2.0897777]], dtype=float32)

time = 63996	action = 0	current_phase = 0	next_phase = 1	reward = -0.077191	array([[-1.773032 , -3.0549614]], dtype=float32)

time = 64001	action = 0	current_phase = 0	next_phase = 1	reward = -0.008904	array([[-2.0928235, -3.1456354]], dtype=float32)

time = 64006	action = 0	current_phase = 0	next_phase = 1	reward = 0.049295	array([[-2.5516741, -3.4080586]], dtype=float32)

time = 64011	action = 1	current_phase = 0	next_phase = 1	reward = -1.504772	array([[-7.3606834, -3.3507864]], dtype=float32)

time = 64019	action = 1	current_phase = 1	next_phase = 0	reward = -0.868185	array([[-3.7827873, -2.2672317]], dtype=float32)

time = 64027	action = 0	current_phase = 0	next_phase = 1	reward = -0.062831	array([[-1.7991259, -2.9595764]], dtype=float32)

time = 64032	action = 0	current_phase = 0	next_phase = 1	reward = 0.012450	array([[-2.2614474, -3.0581553]], dtype=float32)

time = 64037	action = 0	current_phase = 0	next_phase = 1	reward = 0.070758	array([[-2.8077483, -3.5405078]], dtype=float32)

time = 64042	action = 1	current_phase = 0	next_phase = 1	reward = -1.574095	array([[-7.3328805, -3.5980868]], dtype=float32)

time = 64050	action = 1	current_phase = 1	next_phase = 0	reward = -1.013793	array([[-3.9063818, -2.4134007]], dtype=float32)

time = 64058	action = 0	current_phase = 0	next_phase = 1	reward = -0.050699	array([[-1.7410023, -3.0412378]], dtype=float32)

time = 64063	action = 0	current_phase = 0	next_phase = 1	reward = 0.027058	array([[-2.2117515, -3.0341475]], dtype=float32)

time = 64068	action = 0	current_phase = 0	next_phase = 1	reward = 0.079798	array([[-2.791298, -3.586129]], dtype=float32)

time = 64073	action = 1	current_phase = 0	next_phase = 1	reward = -1.927436	array([[-7.319164 , -3.5801492]], dtype=float32)

time = 64081	action = 1	current_phase = 1	next_phase = 0	reward = -1.447676	array([[-4.047363 , -2.3143182]], dtype=float32)

time = 64089	action = 0	current_phase = 0	next_phase = 1	reward = 0.254240	array([[-1.248351 , -2.3201056]], dtype=float32)

time = 64094	action = 0	current_phase = 0	next_phase = 1	reward = 0.026969	array([[-1.869936 , -3.0488198]], dtype=float32)

time = 64099	action = 1	current_phase = 0	next_phase = 1	reward = -0.713440	array([[-2.8955684, -2.3829935]], dtype=float32)

time = 64107	action = 1	current_phase = 1	next_phase = 0	reward = -1.209352	array([[-3.391505 , -1.9611521]], dtype=float32)

time = 64115	action = 0	current_phase = 0	next_phase = 1	reward = 0.182901	array([[-1.1361706, -3.151439 ]], dtype=float32)

time = 64120	action = 0	current_phase = 0	next_phase = 1	reward = -0.286681	array([[-1.7249199, -2.6949916]], dtype=float32)

time = 64125	action = 0	current_phase = 0	next_phase = 1	reward = 0.625483	array([[-2.0943809, -3.140119 ]], dtype=float32)

time = 64130	action = 1	current_phase = 0	next_phase = 1	reward = -1.429732	array([[-6.2763624, -3.1374457]], dtype=float32)

time = 64138	action = 1	current_phase = 1	next_phase = 0	reward = -0.697479	array([[-3.6282291, -2.11096  ]], dtype=float32)

time = 64146	action = 0	current_phase = 0	next_phase = 1	reward = -0.075425	array([[-1.8407433, -3.1043868]], dtype=float32)

time = 64151	action = 0	current_phase = 0	next_phase = 1	reward = -0.001617	array([[-2.135489 , -3.1118336]], dtype=float32)

time = 64156	action = 0	current_phase = 0	next_phase = 1	reward = 0.058022	array([[-2.6547418, -3.5256224]], dtype=float32)

time = 64161	action = 1	current_phase = 0	next_phase = 1	reward = -1.489712	array([[-7.335001 , -3.3707473]], dtype=float32)

time = 64169	action = 1	current_phase = 1	next_phase = 0	reward = -0.782570	array([[-3.7583003, -2.240374 ]], dtype=float32)

time = 64177	action = 0	current_phase = 0	next_phase = 1	reward = -0.072720	array([[-1.8012748, -2.9421322]], dtype=float32)

time = 64182	action = 0	current_phase = 0	next_phase = 1	reward = 0.003680	array([[-2.2576487, -3.061777 ]], dtype=float32)

time = 64187	action = 0	current_phase = 0	next_phase = 1	reward = 0.063109	array([[-2.8314247, -3.5722764]], dtype=float32)

time = 64192	action = 1	current_phase = 0	next_phase = 1	reward = -1.656971	array([[-7.3296056, -3.6222928]], dtype=float32)

time = 64200	action = 1	current_phase = 1	next_phase = 0	reward = -1.308104	array([[-3.8874695, -2.4039311]], dtype=float32)

time = 64208	action = 0	current_phase = 0	next_phase = 1	reward = 0.222359	array([[-1.6315649, -2.945872 ]], dtype=float32)

time = 64213	action = 0	current_phase = 0	next_phase = 1	reward = 0.005922	array([[-2.228709 , -3.0346067]], dtype=float32)

time = 64218	action = 0	current_phase = 0	next_phase = 1	reward = 0.075513	array([[-2.815301 , -3.6057937]], dtype=float32)

time = 64223	action = 1	current_phase = 0	next_phase = 1	reward = -1.841581	array([[-7.3171854, -3.6442318]], dtype=float32)

time = 64231	action = 1	current_phase = 1	next_phase = 0	reward = -1.087975	array([[-4.0152144, -2.3281639]], dtype=float32)

time = 64239	action = 0	current_phase = 0	next_phase = 1	reward = -0.030429	array([[-1.388258 , -2.3488355]], dtype=float32)

time = 64244	action = 0	current_phase = 0	next_phase = 1	reward = 0.037654	array([[-1.8492336, -3.0346894]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0263 - val_loss: 0.0079

Epoch 2/50

 - 3s - loss: 0.0298 - val_loss: 0.0086

Epoch 3/50

 - 3s - loss: 0.0202 - val_loss: 0.0082

Epoch 4/50

 - 3s - loss: 0.0226 - val_loss: 0.0083

Epoch 5/50

 - 3s - loss: 0.0324 - val_loss: 0.0111

Epoch 6/50

 - 3s - loss: 0.0293 - val_loss: 0.0100

Epoch 7/50

 - 3s - loss: 0.0220 - val_loss: 0.0093

Epoch 8/50

 - 3s - loss: 0.0283 - val_loss: 0.0097

Epoch 9/50

 - 3s - loss: 0.0205 - val_loss: 0.0096

Epoch 10/50

 - 3s - loss: 0.0230 - val_loss: 0.0086

Epoch 11/50

 - 3s - loss: 0.0243 - val_loss: 0.0097

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 64249	action = 1	current_phase = 0	next_phase = 1	reward = -0.647336	array([[-2.9163177, -2.4046924]], dtype=float32)

time = 64257	action = 1	current_phase = 1	next_phase = 0	reward = -1.204634	array([[-3.283536, -1.890455]], dtype=float32)

time = 64265	action = 0	current_phase = 0	next_phase = 1	reward = 0.461667	array([[-1.1061743, -3.1352837]], dtype=float32)

time = 64270	action = 0	current_phase = 0	next_phase = 1	reward = -0.300425	array([[-1.9316082, -3.0005014]], dtype=float32)

time = 64275	action = 0	current_phase = 0	next_phase = 1	reward = 0.327558	array([[-2.2518492, -3.2348475]], dtype=float32)

time = 64280	action = 1	current_phase = 0	next_phase = 1	reward = -1.371278	array([[-6.281913 , -3.1063378]], dtype=float32)

time = 64288	action = 1	current_phase = 1	next_phase = 0	reward = -0.696606	array([[-3.5976708, -2.1058233]], dtype=float32)

time = 64296	action = 0	current_phase = 0	next_phase = 1	reward = -0.083861	array([[-1.7798526, -3.030705 ]], dtype=float32)

time = 64301	action = 0	current_phase = 0	next_phase = 1	reward = -0.010267	array([[-2.100931, -3.079953]], dtype=float32)

time = 64306	action = 0	current_phase = 0	next_phase = 1	reward = 0.059685	array([[-2.6565268, -3.517406 ]], dtype=float32)

time = 64311	action = 1	current_phase = 0	next_phase = 1	reward = -1.437264	array([[-7.351505 , -3.3122177]], dtype=float32)

time = 64319	action = 1	current_phase = 1	next_phase = 0	reward = -0.842332	array([[-3.7326477, -2.222115 ]], dtype=float32)

time = 64327	action = 0	current_phase = 0	next_phase = 1	reward = -0.068603	array([[-1.862397 , -2.9679437]], dtype=float32)

time = 64332	action = 0	current_phase = 0	next_phase = 1	reward = 0.006028	array([[-2.2299526, -3.088944 ]], dtype=float32)

time = 64337	action = 0	current_phase = 0	next_phase = 1	reward = 0.068900	array([[-2.8643847, -3.6165001]], dtype=float32)

time = 64342	action = 1	current_phase = 0	next_phase = 1	reward = -1.699553	array([[-7.3255315, -3.5979776]], dtype=float32)

time = 64350	action = 1	current_phase = 1	next_phase = 0	reward = -0.959304	array([[-3.8410246, -2.390936 ]], dtype=float32)

time = 64358	action = 0	current_phase = 0	next_phase = 1	reward = -0.050595	array([[-1.6899284, -2.9698815]], dtype=float32)

time = 64363	action = 0	current_phase = 0	next_phase = 1	reward = 0.029661	array([[-2.2373822, -3.0470905]], dtype=float32)

time = 64368	action = 0	current_phase = 0	next_phase = 1	reward = 0.084815	array([[-2.8153422, -3.6103003]], dtype=float32)

time = 64373	action = 1	current_phase = 0	next_phase = 1	reward = -1.926189	array([[-7.322637 , -3.6665566]], dtype=float32)

time = 64381	action = 1	current_phase = 1	next_phase = 0	reward = -1.137372	array([[-3.9107356, -2.216623 ]], dtype=float32)

time = 64389	action = 0	current_phase = 0	next_phase = 1	reward = -0.039104	array([[-1.3809445, -2.3595526]], dtype=float32)

time = 64394	action = 0	current_phase = 0	next_phase = 1	reward = 0.022623	array([[-1.7948813, -3.0209825]], dtype=float32)

time = 64399	action = 1	current_phase = 0	next_phase = 1	reward = -0.754090	array([[-2.9021723, -2.3482563]], dtype=float32)

time = 64407	action = 1	current_phase = 1	next_phase = 0	reward = -1.197580	array([[-3.4192376, -1.8986751]], dtype=float32)

time = 64415	action = 0	current_phase = 0	next_phase = 1	reward = 0.464098	array([[-1.1204641, -3.1815581]], dtype=float32)

time = 64420	action = 0	current_phase = 0	next_phase = 1	reward = -0.018444	array([[-1.8564624, -2.9149034]], dtype=float32)

time = 64425	action = 0	current_phase = 0	next_phase = 1	reward = 0.054276	array([[-2.430398, -3.320883]], dtype=float32)

time = 64430	action = 1	current_phase = 0	next_phase = 1	reward = -1.362456	array([[-6.3176475, -3.128272 ]], dtype=float32)

time = 64438	action = 1	current_phase = 1	next_phase = 0	reward = -0.714563	array([[-3.6703217, -2.1538413]], dtype=float32)

time = 64446	action = 0	current_phase = 0	next_phase = 1	reward = -0.091026	array([[-1.7787154, -2.997883 ]], dtype=float32)

time = 64451	action = 0	current_phase = 0	next_phase = 1	reward = -0.010010	array([[-2.1021845, -3.101029 ]], dtype=float32)

time = 64456	action = 0	current_phase = 0	next_phase = 1	reward = 0.051270	array([[-2.694252, -3.541941]], dtype=float32)

time = 64461	action = 1	current_phase = 0	next_phase = 1	reward = -1.458800	array([[-7.349771, -3.338512]], dtype=float32)

time = 64469	action = 1	current_phase = 1	next_phase = 0	reward = -0.773029	array([[-3.703108, -2.199349]], dtype=float32)

time = 64477	action = 0	current_phase = 0	next_phase = 1	reward = -0.067616	array([[-1.8177338, -2.940909 ]], dtype=float32)

time = 64482	action = 0	current_phase = 0	next_phase = 1	reward = 0.001627	array([[-2.2313724, -3.077787 ]], dtype=float32)

time = 64487	action = 0	current_phase = 0	next_phase = 1	reward = 0.061530	array([[-2.836499, -3.569226]], dtype=float32)

time = 64492	action = 1	current_phase = 0	next_phase = 1	reward = -1.642056	array([[-7.335163, -3.589708]], dtype=float32)

time = 64500	action = 1	current_phase = 1	next_phase = 0	reward = -0.908144	array([[-3.8303232, -2.3270583]], dtype=float32)

time = 64508	action = 0	current_phase = 0	next_phase = 1	reward = -0.051102	array([[-1.6703424, -2.9722962]], dtype=float32)

time = 64513	action = 0	current_phase = 0	next_phase = 1	reward = 0.006072	array([[-2.2132885, -3.0323496]], dtype=float32)

time = 64518	action = 0	current_phase = 0	next_phase = 1	reward = 0.090257	array([[-2.727476 , -3.4837105]], dtype=float32)

time = 64523	action = 1	current_phase = 0	next_phase = 1	reward = -1.927954	array([[-7.32109  , -3.6439307]], dtype=float32)

time = 64531	action = 1	current_phase = 1	next_phase = 0	reward = -1.404373	array([[-3.925    , -2.2593665]], dtype=float32)

time = 64539	action = 0	current_phase = 0	next_phase = 1	reward = 0.256033	array([[-1.2439481, -2.3566334]], dtype=float32)

time = 64544	action = 0	current_phase = 0	next_phase = 1	reward = 0.022871	array([[-1.9104562, -3.0280855]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0171 - val_loss: 0.0086

Epoch 2/50

 - 3s - loss: 0.0222 - val_loss: 0.0111

Epoch 3/50

 - 3s - loss: 0.0155 - val_loss: 0.0117

Epoch 4/50

 - 3s - loss: 0.0170 - val_loss: 0.0099

Epoch 5/50

 - 3s - loss: 0.0202 - val_loss: 0.0115

Epoch 6/50

 - 3s - loss: 0.0186 - val_loss: 0.0114

Epoch 7/50

 - 3s - loss: 0.0158 - val_loss: 0.0092

Epoch 8/50

 - 3s - loss: 0.0222 - val_loss: 0.0114

Epoch 9/50

 - 3s - loss: 0.0176 - val_loss: 0.0091

Epoch 10/50

 - 3s - loss: 0.0160 - val_loss: 0.0110

Epoch 11/50

 - 3s - loss: 0.0169 - val_loss: 0.0108

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 64549	action = 1	current_phase = 0	next_phase = 1	reward = -0.724568	array([[-2.8298502, -2.346855 ]], dtype=float32)

time = 64557	action = 1	current_phase = 1	next_phase = 0	reward = -0.648967	array([[-3.385332 , -1.9193914]], dtype=float32)

time = 64565	action = 0	current_phase = 0	next_phase = 1	reward = -0.113922	array([[-1.5260017, -3.2311676]], dtype=float32)

time = 64570	action = 0	current_phase = 0	next_phase = 1	reward = -0.303398	array([[-1.7938147, -2.875097 ]], dtype=float32)

time = 64575	action = 0	current_phase = 0	next_phase = 1	reward = 0.062429	array([[-2.122575, -3.188203]], dtype=float32)

time = 64580	action = 1	current_phase = 0	next_phase = 1	reward = -1.079608	array([[-6.2367268, -2.8547075]], dtype=float32)

time = 64588	action = 1	current_phase = 1	next_phase = 0	reward = -0.713025	array([[-3.6279955, -2.1188178]], dtype=float32)

time = 64596	action = 0	current_phase = 0	next_phase = 1	reward = -0.078897	array([[-1.6602341, -3.0165403]], dtype=float32)

time = 64601	action = 0	current_phase = 0	next_phase = 1	reward = 0.009124	array([[-2.0067842, -3.0996215]], dtype=float32)

time = 64606	action = 0	current_phase = 0	next_phase = 1	reward = 0.063183	array([[-2.5851672, -3.4935834]], dtype=float32)

time = 64611	action = 1	current_phase = 0	next_phase = 1	reward = -1.455799	array([[-7.326167 , -3.3149862]], dtype=float32)

time = 64619	action = 1	current_phase = 1	next_phase = 0	reward = -0.838785	array([[-3.7279053, -2.2297244]], dtype=float32)

time = 64627	action = 0	current_phase = 0	next_phase = 1	reward = -0.056720	array([[-1.727042, -2.98846 ]], dtype=float32)

time = 64632	action = 0	current_phase = 0	next_phase = 1	reward = 0.014786	array([[-2.19637  , -3.0715842]], dtype=float32)

time = 64637	action = 0	current_phase = 0	next_phase = 1	reward = 0.073028	array([[-2.8123443, -3.6776762]], dtype=float32)

time = 64642	action = 1	current_phase = 0	next_phase = 1	reward = -1.560167	array([[-7.309857, -3.581221]], dtype=float32)

time = 64650	action = 1	current_phase = 1	next_phase = 0	reward = -0.994223	array([[-3.8319821, -2.3455193]], dtype=float32)

time = 64658	action = 0	current_phase = 0	next_phase = 1	reward = -0.051176	array([[-1.7083812, -3.023124 ]], dtype=float32)

time = 64663	action = 0	current_phase = 0	next_phase = 1	reward = 0.022044	array([[-2.1577492, -3.048184 ]], dtype=float32)

time = 64668	action = 0	current_phase = 0	next_phase = 1	reward = 0.075296	array([[-2.7049694, -3.5652647]], dtype=float32)

time = 64673	action = 1	current_phase = 0	next_phase = 1	reward = -1.336738	array([[-7.3361754, -3.3300748]], dtype=float32)

time = 64681	action = 1	current_phase = 1	next_phase = 0	reward = -1.387254	array([[-3.7861772, -2.362652 ]], dtype=float32)

time = 64689	action = 0	current_phase = 0	next_phase = 1	reward = 0.257082	array([[-1.2081964, -2.35772  ]], dtype=float32)

time = 64694	action = 0	current_phase = 0	next_phase = 1	reward = 0.036231	array([[-1.8201756, -3.0589647]], dtype=float32)

time = 64699	action = 1	current_phase = 0	next_phase = 1	reward = -0.681132	array([[-2.7383459, -2.3770995]], dtype=float32)

time = 64707	action = 1	current_phase = 1	next_phase = 0	reward = -0.879609	array([[-3.4424958, -2.0568795]], dtype=float32)

time = 64715	action = 0	current_phase = 0	next_phase = 1	reward = 0.165805	array([[-1.301467, -3.205429]], dtype=float32)

time = 64720	action = 0	current_phase = 0	next_phase = 1	reward = -0.301679	array([[-1.9268506, -3.0745165]], dtype=float32)

time = 64725	action = 0	current_phase = 0	next_phase = 1	reward = 0.058751	array([[-2.217646 , -3.2547936]], dtype=float32)

time = 64730	action = 1	current_phase = 0	next_phase = 1	reward = -1.023809	array([[-6.201282 , -2.8541694]], dtype=float32)

time = 64738	action = 1	current_phase = 1	next_phase = 0	reward = -0.719827	array([[-3.6032972, -2.1176977]], dtype=float32)

time = 64746	action = 0	current_phase = 0	next_phase = 1	reward = -0.085225	array([[-1.7177541, -3.0441263]], dtype=float32)

time = 64751	action = 0	current_phase = 0	next_phase = 1	reward = -0.011128	array([[-2.0955467, -3.101547 ]], dtype=float32)

time = 64756	action = 0	current_phase = 0	next_phase = 1	reward = 0.060402	array([[-2.5403924, -3.471777 ]], dtype=float32)

time = 64761	action = 1	current_phase = 0	next_phase = 1	reward = -1.377488	array([[-7.3539896, -3.328906 ]], dtype=float32)

time = 64769	action = 1	current_phase = 1	next_phase = 0	reward = -0.848223	array([[-3.7311683, -2.2481346]], dtype=float32)

time = 64777	action = 0	current_phase = 0	next_phase = 1	reward = -0.080499	array([[-1.7179564, -2.9798684]], dtype=float32)

time = 64782	action = 0	current_phase = 0	next_phase = 1	reward = 0.007450	array([[-2.1820977, -3.129358 ]], dtype=float32)

time = 64787	action = 0	current_phase = 0	next_phase = 1	reward = 0.073053	array([[-2.799653 , -3.6510792]], dtype=float32)

time = 64792	action = 1	current_phase = 0	next_phase = 1	reward = -1.639852	array([[-7.31639  , -3.6088223]], dtype=float32)

time = 64800	action = 1	current_phase = 1	next_phase = 0	reward = -0.995674	array([[-3.8417535, -2.3644228]], dtype=float32)

time = 64808	action = 0	current_phase = 0	next_phase = 1	reward = -0.058107	array([[-1.6783609, -3.0269704]], dtype=float32)

time = 64813	action = 0	current_phase = 0	next_phase = 1	reward = 0.007176	array([[-2.1943448, -3.0519576]], dtype=float32)

time = 64818	action = 0	current_phase = 0	next_phase = 1	reward = 0.081685	array([[-2.6665578, -3.4826274]], dtype=float32)

time = 64823	action = 1	current_phase = 0	next_phase = 1	reward = -1.288913	array([[-7.3105497, -3.4306674]], dtype=float32)

time = 64831	action = 1	current_phase = 1	next_phase = 0	reward = -1.370625	array([[-3.8036847, -2.334682 ]], dtype=float32)

time = 64839	action = 0	current_phase = 0	next_phase = 1	reward = 0.253174	array([[-1.227886 , -2.4350977]], dtype=float32)

time = 64844	action = 0	current_phase = 0	next_phase = 1	reward = 0.041411	array([[-1.8006775, -3.0361795]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0270 - val_loss: 0.0143

Epoch 2/50

 - 3s - loss: 0.0178 - val_loss: 0.0152

Epoch 3/50

 - 3s - loss: 0.0184 - val_loss: 0.0136

Epoch 4/50

 - 3s - loss: 0.0213 - val_loss: 0.0148

Epoch 5/50

 - 3s - loss: 0.0217 - val_loss: 0.0135

Epoch 6/50

 - 3s - loss: 0.0199 - val_loss: 0.0137

Epoch 7/50

 - 3s - loss: 0.0201 - val_loss: 0.0148

Epoch 8/50

 - 3s - loss: 0.0223 - val_loss: 0.0139

Epoch 9/50

 - 3s - loss: 0.0151 - val_loss: 0.0160

Epoch 10/50

 - 3s - loss: 0.0179 - val_loss: 0.0155

Epoch 11/50

 - 3s - loss: 0.0160 - val_loss: 0.0147

Epoch 12/50

 - 3s - loss: 0.0202 - val_loss: 0.0159

Epoch 13/50

 - 3s - loss: 0.0189 - val_loss: 0.0127

Epoch 14/50

 - 3s - loss: 0.0214 - val_loss: 0.0173

Epoch 15/50

 - 3s - loss: 0.0174 - val_loss: 0.0156

Epoch 16/50

 - 3s - loss: 0.0205 - val_loss: 0.0128

Epoch 17/50

 - 3s - loss: 0.0178 - val_loss: 0.0160

Epoch 18/50

 - 3s - loss: 0.0151 - val_loss: 0.0134

Epoch 19/50

 - 3s - loss: 0.0194 - val_loss: 0.0153

Epoch 20/50

 - 3s - loss: 0.0170 - val_loss: 0.0149

Epoch 21/50

 - 3s - loss: 0.0193 - val_loss: 0.0154

Epoch 22/50

 - 3s - loss: 0.0161 - val_loss: 0.0162

Epoch 23/50

 - 3s - loss: 0.0131 - val_loss: 0.0164

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 64849	action = 1	current_phase = 0	next_phase = 1	reward = -0.709551	array([[-2.8015425, -2.2878401]], dtype=float32)

time = 64857	action = 1	current_phase = 1	next_phase = 0	reward = -0.645095	array([[-3.4921935, -2.0061915]], dtype=float32)

time = 64865	action = 0	current_phase = 0	next_phase = 1	reward = -0.117949	array([[-1.5125062, -3.1447747]], dtype=float32)

time = 64870	action = 0	current_phase = 0	next_phase = 1	reward = -0.321157	array([[-1.9223542, -2.7995553]], dtype=float32)

time = 64875	action = 0	current_phase = 0	next_phase = 1	reward = -0.243011	array([[-2.2303073, -3.2044928]], dtype=float32)

time = 64880	action = 1	current_phase = 0	next_phase = 1	reward = -0.742373	array([[-6.158355 , -2.5602274]], dtype=float32)

time = 64888	action = 1	current_phase = 1	next_phase = 0	reward = -0.695622	array([[-3.6145408, -2.1206248]], dtype=float32)

time = 64896	action = 0	current_phase = 0	next_phase = 1	reward = -0.073736	array([[-1.7019464, -3.0317457]], dtype=float32)

time = 64901	action = 0	current_phase = 0	next_phase = 1	reward = 0.000244	array([[-2.066233 , -3.1688163]], dtype=float32)

time = 64906	action = 0	current_phase = 0	next_phase = 1	reward = 0.069494	array([[-2.5960531, -3.49666  ]], dtype=float32)

time = 64911	action = 1	current_phase = 0	next_phase = 1	reward = -1.500762	array([[-7.3295407, -3.2307456]], dtype=float32)

time = 64919	action = 1	current_phase = 1	next_phase = 0	reward = -0.807376	array([[-3.7724848, -2.3028235]], dtype=float32)

time = 64927	action = 0	current_phase = 0	next_phase = 1	reward = -0.082448	array([[-1.7677953, -2.9612052]], dtype=float32)

time = 64932	action = 0	current_phase = 0	next_phase = 1	reward = 0.012362	array([[-2.1843   , -3.0734174]], dtype=float32)

time = 64937	action = 0	current_phase = 0	next_phase = 1	reward = 0.078914	array([[-2.7702475, -3.5657632]], dtype=float32)

time = 64942	action = 1	current_phase = 0	next_phase = 1	reward = -1.702582	array([[-7.3020654, -3.4994175]], dtype=float32)

time = 64950	action = 1	current_phase = 1	next_phase = 0	reward = -0.946112	array([[-3.858365 , -2.4056933]], dtype=float32)

time = 64958	action = 0	current_phase = 0	next_phase = 1	reward = -0.063126	array([[-1.7470661, -3.0549214]], dtype=float32)

time = 64963	action = 0	current_phase = 0	next_phase = 1	reward = 0.003458	array([[-2.153911 , -3.0466433]], dtype=float32)

time = 64968	action = 0	current_phase = 0	next_phase = 1	reward = 0.075245	array([[-2.65977  , -3.4451666]], dtype=float32)

time = 64973	action = 1	current_phase = 0	next_phase = 1	reward = -1.919732	array([[-7.305743 , -3.5272293]], dtype=float32)

time = 64981	action = 1	current_phase = 1	next_phase = 0	reward = -1.022948	array([[-3.9516199, -2.2757077]], dtype=float32)

time = 64989	action = 0	current_phase = 0	next_phase = 1	reward = -0.020278	array([[-1.3934482, -2.4554422]], dtype=float32)

time = 64994	action = 0	current_phase = 0	next_phase = 1	reward = 0.064516	array([[-1.8726404, -3.0530365]], dtype=float32)

time = 64999	action = 1	current_phase = 0	next_phase = 1	reward = -0.758616	array([[-2.86092  , -2.2987905]], dtype=float32)

time = 65007	action = 1	current_phase = 1	next_phase = 0	reward = -0.918561	array([[-3.46168  , -2.0118544]], dtype=float32)

time = 65015	action = 0	current_phase = 0	next_phase = 1	reward = 0.172283	array([[-1.2763122, -3.148412 ]], dtype=float32)

time = 65020	action = 0	current_phase = 0	next_phase = 1	reward = -0.054834	array([[-1.9499255, -3.0918434]], dtype=float32)

time = 65025	action = 0	current_phase = 0	next_phase = 1	reward = 0.020018	array([[-2.408375, -3.318426]], dtype=float32)

time = 65030	action = 1	current_phase = 0	next_phase = 1	reward = -1.360093	array([[-6.2593145, -2.9769535]], dtype=float32)

time = 65038	action = 1	current_phase = 1	next_phase = 0	reward = -0.718775	array([[-3.6080244, -2.1341605]], dtype=float32)

time = 65046	action = 0	current_phase = 0	next_phase = 1	reward = -0.081940	array([[-1.7217948, -3.074703 ]], dtype=float32)

time = 65051	action = 0	current_phase = 0	next_phase = 1	reward = -0.014269	array([[-2.0841522, -3.1672008]], dtype=float32)

time = 65056	action = 0	current_phase = 0	next_phase = 1	reward = 0.071584	array([[-2.5578306, -3.3960824]], dtype=float32)

time = 65061	action = 1	current_phase = 0	next_phase = 1	reward = -1.494464	array([[-7.331742 , -3.2454584]], dtype=float32)

time = 65069	action = 1	current_phase = 1	next_phase = 0	reward = -0.841808	array([[-3.730707, -2.251132]], dtype=float32)

time = 65077	action = 0	current_phase = 0	next_phase = 1	reward = -0.074026	array([[-1.7457862, -2.9959757]], dtype=float32)

time = 65082	action = 0	current_phase = 0	next_phase = 1	reward = -0.007414	array([[-2.1663833, -3.0589883]], dtype=float32)

time = 65087	action = 0	current_phase = 0	next_phase = 1	reward = 0.068740	array([[-2.7558393, -3.5136888]], dtype=float32)

time = 65092	action = 1	current_phase = 0	next_phase = 1	reward = -1.531378	array([[-7.3093414, -3.5085967]], dtype=float32)

time = 65100	action = 1	current_phase = 1	next_phase = 0	reward = -1.242471	array([[-3.821879 , -2.3755867]], dtype=float32)

time = 65108	action = 0	current_phase = 0	next_phase = 1	reward = 0.253627	array([[-1.6195226, -2.9203787]], dtype=float32)

time = 65113	action = 0	current_phase = 0	next_phase = 1	reward = 0.024310	array([[-2.1854377, -3.0545928]], dtype=float32)

time = 65118	action = 0	current_phase = 0	next_phase = 1	reward = 0.070456	array([[-2.7425418, -3.558644 ]], dtype=float32)

time = 65123	action = 1	current_phase = 0	next_phase = 1	reward = -1.331657	array([[-7.303217 , -3.2681415]], dtype=float32)

time = 65131	action = 1	current_phase = 1	next_phase = 0	reward = -1.368446	array([[-3.8306398, -2.3318818]], dtype=float32)

time = 65139	action = 0	current_phase = 0	next_phase = 1	reward = 0.252657	array([[-1.2999904, -2.4556885]], dtype=float32)

time = 65144	action = 0	current_phase = 0	next_phase = 1	reward = 0.048074	array([[-1.9214973, -3.090986 ]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0188 - val_loss: 0.0109

Epoch 2/50

 - 3s - loss: 0.0237 - val_loss: 0.0101

Epoch 3/50

 - 3s - loss: 0.0227 - val_loss: 0.0097

Epoch 4/50

 - 3s - loss: 0.0148 - val_loss: 0.0103

Epoch 5/50

 - 3s - loss: 0.0171 - val_loss: 0.0139

Epoch 6/50

 - 3s - loss: 0.0153 - val_loss: 0.0109

Epoch 7/50

 - 3s - loss: 0.0157 - val_loss: 0.0084

Epoch 8/50

 - 3s - loss: 0.0204 - val_loss: 0.0112

Epoch 9/50

 - 3s - loss: 0.0188 - val_loss: 0.0086

Epoch 10/50

 - 3s - loss: 0.0166 - val_loss: 0.0089

Epoch 11/50

 - 3s - loss: 0.0160 - val_loss: 0.0078

Epoch 12/50

 - 3s - loss: 0.0193 - val_loss: 0.0100

Epoch 13/50

 - 3s - loss: 0.0152 - val_loss: 0.0122

Epoch 14/50

 - 3s - loss: 0.0164 - val_loss: 0.0106

Epoch 15/50

 - 3s - loss: 0.0174 - val_loss: 0.0088

Epoch 16/50

 - 3s - loss: 0.0185 - val_loss: 0.0096

Epoch 17/50

 - 3s - loss: 0.0160 - val_loss: 0.0101

Epoch 18/50

 - 3s - loss: 0.0161 - val_loss: 0.0084

Epoch 19/50

 - 3s - loss: 0.0191 - val_loss: 0.0090

Epoch 20/50

 - 3s - loss: 0.0160 - val_loss: 0.0088

Epoch 21/50

 - 3s - loss: 0.0138 - val_loss: 0.0093

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 65149	action = 1	current_phase = 0	next_phase = 1	reward = -0.628705	array([[-2.8689675, -2.379337 ]], dtype=float32)

time = 65157	action = 1	current_phase = 1	next_phase = 0	reward = -0.872176	array([[-3.336182 , -1.9200432]], dtype=float32)

time = 65165	action = 0	current_phase = 0	next_phase = 1	reward = -0.084855	array([[-1.4604036, -3.1890547]], dtype=float32)

time = 65170	action = 0	current_phase = 0	next_phase = 1	reward = 0.274271	array([[-1.6757028, -3.0978835]], dtype=float32)

time = 65175	action = 0	current_phase = 0	next_phase = 1	reward = 0.061135	array([[-2.5070138, -3.350658 ]], dtype=float32)

time = 65180	action = 1	current_phase = 0	next_phase = 1	reward = -1.308232	array([[-6.284236 , -3.0817091]], dtype=float32)

time = 65188	action = 1	current_phase = 1	next_phase = 0	reward = -0.712820	array([[-3.6255622, -2.1144257]], dtype=float32)

time = 65196	action = 0	current_phase = 0	next_phase = 1	reward = -0.098311	array([[-1.7833135, -3.077496 ]], dtype=float32)

time = 65201	action = 0	current_phase = 0	next_phase = 1	reward = -0.014414	array([[-2.0850036, -3.062102 ]], dtype=float32)

time = 65206	action = 0	current_phase = 0	next_phase = 1	reward = 0.056065	array([[-2.6376166, -3.4646313]], dtype=float32)

time = 65211	action = 1	current_phase = 0	next_phase = 1	reward = -1.441977	array([[-7.339067 , -3.3160913]], dtype=float32)

time = 65219	action = 1	current_phase = 1	next_phase = 0	reward = -0.768949	array([[-3.7375498, -2.2327013]], dtype=float32)

time = 65227	action = 0	current_phase = 0	next_phase = 1	reward = -0.080527	array([[-1.7624238, -2.9606235]], dtype=float32)

time = 65232	action = 0	current_phase = 0	next_phase = 1	reward = -0.002037	array([[-2.1929622, -3.0717194]], dtype=float32)

time = 65237	action = 0	current_phase = 0	next_phase = 1	reward = 0.069333	array([[-2.8122072, -3.5388343]], dtype=float32)

time = 65242	action = 1	current_phase = 0	next_phase = 1	reward = -1.484026	array([[-7.3113723, -3.5123613]], dtype=float32)

time = 65250	action = 1	current_phase = 1	next_phase = 0	reward = -1.004124	array([[-3.8186922, -2.3540044]], dtype=float32)

time = 65258	action = 0	current_phase = 0	next_phase = 1	reward = -0.043977	array([[-1.7914281, -3.0020742]], dtype=float32)

time = 65263	action = 0	current_phase = 0	next_phase = 1	reward = 0.032409	array([[-2.1852705, -3.043135 ]], dtype=float32)

time = 65268	action = 0	current_phase = 0	next_phase = 1	reward = 0.076249	array([[-2.9167109, -3.760638 ]], dtype=float32)

time = 65273	action = 1	current_phase = 0	next_phase = 1	reward = -1.819617	array([[-7.2932353, -3.620932 ]], dtype=float32)

time = 65281	action = 1	current_phase = 1	next_phase = 0	reward = -1.433692	array([[-3.9093328, -2.2028515]], dtype=float32)

time = 65289	action = 0	current_phase = 0	next_phase = 1	reward = 0.266240	array([[-1.2678881, -2.4088783]], dtype=float32)

time = 65294	action = 0	current_phase = 0	next_phase = 1	reward = 0.044505	array([[-1.8851519, -3.0439832]], dtype=float32)

time = 65299	action = 1	current_phase = 0	next_phase = 1	reward = -0.638801	array([[-2.8903482, -2.2543325]], dtype=float32)

time = 65307	action = 1	current_phase = 1	next_phase = 0	reward = -0.636591	array([[-3.323039 , -1.8932165]], dtype=float32)

time = 65315	action = 0	current_phase = 0	next_phase = 1	reward = -0.107758	array([[-1.6301816, -3.1719456]], dtype=float32)

time = 65320	action = 0	current_phase = 0	next_phase = 1	reward = -0.024877	array([[-1.8949792, -2.9467685]], dtype=float32)

time = 65325	action = 0	current_phase = 0	next_phase = 1	reward = -0.245140	array([[-2.3766642, -3.267811 ]], dtype=float32)

time = 65330	action = 1	current_phase = 0	next_phase = 1	reward = -1.029687	array([[-6.2161207, -2.8219855]], dtype=float32)

time = 65338	action = 1	current_phase = 1	next_phase = 0	reward = -0.709529	array([[-3.5552263, -2.0723538]], dtype=float32)

time = 65346	action = 0	current_phase = 0	next_phase = 1	reward = -0.087446	array([[-1.7936027, -3.0548732]], dtype=float32)

time = 65351	action = 0	current_phase = 0	next_phase = 1	reward = -0.009872	array([[-2.081379, -3.0886  ]], dtype=float32)

time = 65356	action = 0	current_phase = 0	next_phase = 1	reward = 0.056355	array([[-2.7004433, -3.508385 ]], dtype=float32)

time = 65361	action = 1	current_phase = 0	next_phase = 1	reward = -1.442528	array([[-7.339595 , -3.2639418]], dtype=float32)

time = 65369	action = 1	current_phase = 1	next_phase = 0	reward = -0.779591	array([[-3.7262068, -2.2243671]], dtype=float32)

time = 65377	action = 0	current_phase = 0	next_phase = 1	reward = -0.081191	array([[-1.7635672, -2.9501429]], dtype=float32)

time = 65382	action = 0	current_phase = 0	next_phase = 1	reward = 0.003315	array([[-2.1958628, -3.070587 ]], dtype=float32)

time = 65387	action = 0	current_phase = 0	next_phase = 1	reward = 0.081503	array([[-2.8435009, -3.5718248]], dtype=float32)

time = 65392	action = 1	current_phase = 0	next_phase = 1	reward = -1.536783	array([[-7.308556 , -3.5489626]], dtype=float32)

time = 65400	action = 1	current_phase = 1	next_phase = 0	reward = -1.248895	array([[-3.8173213, -2.3363538]], dtype=float32)

time = 65408	action = 0	current_phase = 0	next_phase = 1	reward = 0.249998	array([[-1.62746  , -2.9803164]], dtype=float32)

time = 65413	action = 0	current_phase = 0	next_phase = 1	reward = 0.022526	array([[-2.1896622, -3.0484512]], dtype=float32)

time = 65418	action = 0	current_phase = 0	next_phase = 1	reward = 0.081206	array([[-2.7786422, -3.5309722]], dtype=float32)

time = 65423	action = 1	current_phase = 0	next_phase = 1	reward = -1.897388	array([[-7.303698 , -3.5846877]], dtype=float32)

time = 65431	action = 1	current_phase = 1	next_phase = 0	reward = -1.081631	array([[-3.9231443, -2.1927147]], dtype=float32)

time = 65439	action = 0	current_phase = 0	next_phase = 1	reward = -0.036925	array([[-1.3826278, -2.4881961]], dtype=float32)

time = 65444	action = 0	current_phase = 0	next_phase = 1	reward = 0.027789	array([[-1.8818948, -3.0776775]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0219 - val_loss: 0.0099

Epoch 2/50

 - 3s - loss: 0.0223 - val_loss: 0.0138

Epoch 3/50

 - 3s - loss: 0.0221 - val_loss: 0.0091

Epoch 4/50

 - 3s - loss: 0.0180 - val_loss: 0.0108

Epoch 5/50

 - 3s - loss: 0.0270 - val_loss: 0.0085

Epoch 6/50

 - 3s - loss: 0.0195 - val_loss: 0.0108

Epoch 7/50

 - 3s - loss: 0.0189 - val_loss: 0.0148

Epoch 8/50

 - 3s - loss: 0.0173 - val_loss: 0.0119

Epoch 9/50

 - 3s - loss: 0.0184 - val_loss: 0.0111

Epoch 10/50

 - 3s - loss: 0.0198 - val_loss: 0.0106

Epoch 11/50

 - 3s - loss: 0.0193 - val_loss: 0.0152

Epoch 12/50

 - 3s - loss: 0.0181 - val_loss: 0.0104

Epoch 13/50

 - 3s - loss: 0.0197 - val_loss: 0.0091

Epoch 14/50

 - 3s - loss: 0.0175 - val_loss: 0.0102

Epoch 15/50

 - 3s - loss: 0.0172 - val_loss: 0.0130

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 65449	action = 1	current_phase = 0	next_phase = 1	reward = -0.693446	array([[-2.8956685, -2.2885594]], dtype=float32)

time = 65457	action = 1	current_phase = 1	next_phase = 0	reward = -0.875009	array([[-3.3437386, -1.946554 ]], dtype=float32)

time = 65465	action = 0	current_phase = 0	next_phase = 1	reward = -0.099060	array([[-1.2624066, -3.1820064]], dtype=float32)

time = 65470	action = 0	current_phase = 0	next_phase = 1	reward = 0.250381	array([[-1.573014, -3.092381]], dtype=float32)

time = 65475	action = 0	current_phase = 0	next_phase = 1	reward = 0.038697	array([[-2.3792238, -3.3031518]], dtype=float32)

time = 65480	action = 1	current_phase = 0	next_phase = 1	reward = -1.316580	array([[-6.2498107, -2.9188077]], dtype=float32)

time = 65488	action = 1	current_phase = 1	next_phase = 0	reward = -0.698636	array([[-3.586927 , -2.1614494]], dtype=float32)

time = 65496	action = 0	current_phase = 0	next_phase = 1	reward = -0.090606	array([[-1.6578612, -3.1263366]], dtype=float32)

time = 65501	action = 0	current_phase = 0	next_phase = 1	reward = -0.011626	array([[-1.9848441, -3.1551137]], dtype=float32)

time = 65506	action = 0	current_phase = 0	next_phase = 1	reward = 0.066388	array([[-2.5250068, -3.421021 ]], dtype=float32)

time = 65511	action = 1	current_phase = 0	next_phase = 1	reward = -1.366754	array([[-7.3019986, -3.213262 ]], dtype=float32)

time = 65519	action = 1	current_phase = 1	next_phase = 0	reward = -0.831261	array([[-3.7195954, -2.288907 ]], dtype=float32)

time = 65527	action = 0	current_phase = 0	next_phase = 1	reward = -0.079977	array([[-1.7356725, -2.970462 ]], dtype=float32)

time = 65532	action = 0	current_phase = 0	next_phase = 1	reward = 0.006560	array([[-2.15738  , -3.0721216]], dtype=float32)

time = 65537	action = 0	current_phase = 0	next_phase = 1	reward = 0.068422	array([[-2.7451315, -3.5006418]], dtype=float32)

time = 65542	action = 1	current_phase = 0	next_phase = 1	reward = -1.658373	array([[-7.298172 , -3.5122724]], dtype=float32)

time = 65550	action = 1	current_phase = 1	next_phase = 0	reward = -0.896190	array([[-3.841817, -2.43871 ]], dtype=float32)

time = 65558	action = 0	current_phase = 0	next_phase = 1	reward = -0.076430	array([[-1.6468675, -3.0372546]], dtype=float32)

time = 65563	action = 0	current_phase = 0	next_phase = 1	reward = 0.005427	array([[-2.14814  , -3.0479054]], dtype=float32)

time = 65568	action = 0	current_phase = 0	next_phase = 1	reward = 0.079047	array([[-2.762576, -3.537754]], dtype=float32)

time = 65573	action = 1	current_phase = 0	next_phase = 1	reward = -1.785027	array([[-7.285524, -3.549287]], dtype=float32)

time = 65581	action = 1	current_phase = 1	next_phase = 0	reward = -1.044214	array([[-3.899057 , -2.2565794]], dtype=float32)

time = 65589	action = 0	current_phase = 0	next_phase = 1	reward = -0.047559	array([[-1.3081585, -2.4249246]], dtype=float32)

time = 65594	action = 0	current_phase = 0	next_phase = 1	reward = 0.025571	array([[-1.9684966, -3.0144784]], dtype=float32)

time = 65599	action = 1	current_phase = 0	next_phase = 1	reward = -0.699945	array([[-2.8973908, -2.241983 ]], dtype=float32)

time = 65607	action = 1	current_phase = 1	next_phase = 0	reward = -0.639052	array([[-3.30686 , -1.924388]], dtype=float32)

time = 65615	action = 0	current_phase = 0	next_phase = 1	reward = -0.084421	array([[-1.6042879, -3.1694922]], dtype=float32)

time = 65620	action = 0	current_phase = 0	next_phase = 1	reward = -0.008322	array([[-1.7042373, -3.0781584]], dtype=float32)

time = 65625	action = 0	current_phase = 0	next_phase = 1	reward = 0.056100	array([[-2.4574687, -3.3022335]], dtype=float32)

time = 65630	action = 1	current_phase = 0	next_phase = 1	reward = -1.326702	array([[-6.290417, -2.955994]], dtype=float32)

time = 65638	action = 1	current_phase = 1	next_phase = 0	reward = -0.692736	array([[-3.5466738, -2.1251516]], dtype=float32)

time = 65646	action = 0	current_phase = 0	next_phase = 1	reward = -0.088360	array([[-1.7014395, -3.0150192]], dtype=float32)

time = 65651	action = 0	current_phase = 0	next_phase = 1	reward = -0.007649	array([[-2.044208 , -3.0855742]], dtype=float32)

time = 65656	action = 0	current_phase = 0	next_phase = 1	reward = 0.061230	array([[-2.616879 , -3.4916446]], dtype=float32)

time = 65661	action = 1	current_phase = 0	next_phase = 1	reward = -1.560751	array([[-7.3328567, -3.200732 ]], dtype=float32)

time = 65669	action = 1	current_phase = 1	next_phase = 0	reward = -1.067508	array([[-3.6999745, -2.2513924]], dtype=float32)

time = 65677	action = 0	current_phase = 0	next_phase = 1	reward = 0.224002	array([[-1.6043228, -2.9632282]], dtype=float32)

time = 65682	action = 0	current_phase = 0	next_phase = 1	reward = 0.001915	array([[-2.1416183, -3.0533853]], dtype=float32)

time = 65687	action = 0	current_phase = 0	next_phase = 1	reward = 0.064464	array([[-2.7876499, -3.5558672]], dtype=float32)

time = 65692	action = 1	current_phase = 0	next_phase = 1	reward = -1.685289	array([[-7.3002934, -3.456968 ]], dtype=float32)

time = 65700	action = 1	current_phase = 1	next_phase = 0	reward = -0.898276	array([[-3.8068857, -2.407432 ]], dtype=float32)

time = 65708	action = 0	current_phase = 0	next_phase = 1	reward = -0.060673	array([[-1.6818886, -2.9812338]], dtype=float32)

time = 65713	action = 0	current_phase = 0	next_phase = 1	reward = 0.009154	array([[-2.14185  , -3.0396867]], dtype=float32)

time = 65718	action = 0	current_phase = 0	next_phase = 1	reward = 0.075205	array([[-2.734217 , -3.5167432]], dtype=float32)

time = 65723	action = 1	current_phase = 0	next_phase = 1	reward = -1.849816	array([[-7.278577 , -3.5669441]], dtype=float32)

time = 65731	action = 1	current_phase = 1	next_phase = 0	reward = -1.028917	array([[-3.8784356, -2.2702243]], dtype=float32)

time = 65739	action = 0	current_phase = 0	next_phase = 1	reward = -0.019649	array([[-1.2752681, -2.4418216]], dtype=float32)

time = 65744	action = 0	current_phase = 0	next_phase = 1	reward = 0.035225	array([[-1.8018575, -3.0522404]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0148 - val_loss: 0.0109

Epoch 2/50

 - 3s - loss: 0.0226 - val_loss: 0.0101

Epoch 3/50

 - 3s - loss: 0.0176 - val_loss: 0.0106

Epoch 4/50

 - 3s - loss: 0.0149 - val_loss: 0.0117

Epoch 5/50

 - 3s - loss: 0.0152 - val_loss: 0.0115

Epoch 6/50

 - 3s - loss: 0.0164 - val_loss: 0.0106

Epoch 7/50

 - 3s - loss: 0.0198 - val_loss: 0.0111

Epoch 8/50

 - 3s - loss: 0.0146 - val_loss: 0.0111

Epoch 9/50

 - 3s - loss: 0.0156 - val_loss: 0.0123

Epoch 10/50

 - 3s - loss: 0.0138 - val_loss: 0.0115

Epoch 11/50

 - 3s - loss: 0.0135 - val_loss: 0.0109

Epoch 12/50

 - 3s - loss: 0.0147 - val_loss: 0.0106

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 65749	action = 1	current_phase = 0	next_phase = 1	reward = -0.710869	array([[-2.8884292, -2.3271847]], dtype=float32)

time = 65757	action = 1	current_phase = 1	next_phase = 0	reward = -1.194128	array([[-3.319913 , -1.9012125]], dtype=float32)

time = 65765	action = 0	current_phase = 0	next_phase = 1	reward = 0.209423	array([[-1.1065505, -3.236494 ]], dtype=float32)

time = 65770	action = 0	current_phase = 0	next_phase = 1	reward = 0.016163	array([[-1.6238152, -3.073914 ]], dtype=float32)

time = 65775	action = 0	current_phase = 0	next_phase = 1	reward = 0.353827	array([[-2.3176067, -3.2646914]], dtype=float32)

time = 65780	action = 1	current_phase = 0	next_phase = 1	reward = -1.400315	array([[-6.242851 , -2.9257293]], dtype=float32)

time = 65788	action = 1	current_phase = 1	next_phase = 0	reward = -0.713573	array([[-3.6388931, -2.1304111]], dtype=float32)

time = 65796	action = 0	current_phase = 0	next_phase = 1	reward = -0.084494	array([[-1.7278736, -3.0569077]], dtype=float32)

time = 65801	action = 0	current_phase = 0	next_phase = 1	reward = -0.016023	array([[-2.1148052, -3.0858796]], dtype=float32)

time = 65806	action = 0	current_phase = 0	next_phase = 1	reward = 0.053903	array([[-2.7011762, -3.5647547]], dtype=float32)

time = 65811	action = 1	current_phase = 0	next_phase = 1	reward = -1.512922	array([[-7.3414783, -3.3171499]], dtype=float32)

time = 65819	action = 1	current_phase = 1	next_phase = 0	reward = -0.772822	array([[-3.6485896, -2.1743221]], dtype=float32)

time = 65827	action = 0	current_phase = 0	next_phase = 1	reward = -0.063374	array([[-1.827585 , -3.0300035]], dtype=float32)

time = 65832	action = 0	current_phase = 0	next_phase = 1	reward = 0.008384	array([[-2.2025177, -3.104922 ]], dtype=float32)

time = 65837	action = 0	current_phase = 0	next_phase = 1	reward = 0.059648	array([[-2.799872 , -3.6031396]], dtype=float32)

time = 65842	action = 1	current_phase = 0	next_phase = 1	reward = -1.610901	array([[-7.313747, -3.535906]], dtype=float32)

time = 65850	action = 1	current_phase = 1	next_phase = 0	reward = -1.065416	array([[-3.8389049, -2.3444455]], dtype=float32)

time = 65858	action = 0	current_phase = 0	next_phase = 1	reward = -0.053986	array([[-1.7760348, -3.0046477]], dtype=float32)

time = 65863	action = 0	current_phase = 0	next_phase = 1	reward = 0.007204	array([[-2.1786783, -3.076308 ]], dtype=float32)

time = 65868	action = 0	current_phase = 0	next_phase = 1	reward = 0.071966	array([[-2.7576313, -3.5281107]], dtype=float32)

time = 65873	action = 1	current_phase = 0	next_phase = 1	reward = -1.230180	array([[-7.3136044, -3.4647696]], dtype=float32)

time = 65881	action = 1	current_phase = 1	next_phase = 0	reward = -1.384989	array([[-3.7384582, -2.3428192]], dtype=float32)

time = 65889	action = 0	current_phase = 0	next_phase = 1	reward = 0.240521	array([[-1.2418199, -2.458654 ]], dtype=float32)

time = 65894	action = 0	current_phase = 0	next_phase = 1	reward = 0.017530	array([[-1.8832998, -3.1428583]], dtype=float32)

time = 65899	action = 1	current_phase = 0	next_phase = 1	reward = -0.668967	array([[-2.8108249, -2.2985582]], dtype=float32)

time = 65907	action = 1	current_phase = 1	next_phase = 0	reward = -0.815848	array([[-3.2176418, -1.8727635]], dtype=float32)

time = 65915	action = 0	current_phase = 0	next_phase = 1	reward = -0.094345	array([[-1.4214149, -3.267612 ]], dtype=float32)

time = 65920	action = 0	current_phase = 0	next_phase = 1	reward = 0.253111	array([[-1.6675138, -2.9905207]], dtype=float32)

time = 65925	action = 0	current_phase = 0	next_phase = 1	reward = -0.518202	array([[-2.3721845, -3.2228744]], dtype=float32)

time = 65930	action = 1	current_phase = 0	next_phase = 1	reward = -0.766983	array([[-6.210702 , -2.6926517]], dtype=float32)

time = 65938	action = 1	current_phase = 1	next_phase = 0	reward = -0.704275	array([[-3.5895238, -2.0832767]], dtype=float32)

time = 65946	action = 0	current_phase = 0	next_phase = 1	reward = -0.083757	array([[-1.7199085, -3.1430433]], dtype=float32)

time = 65951	action = 0	current_phase = 0	next_phase = 1	reward = 0.005724	array([[-2.0595365, -3.2218475]], dtype=float32)

time = 65956	action = 0	current_phase = 0	next_phase = 1	reward = 0.062021	array([[-2.6439676, -3.4961662]], dtype=float32)

time = 65961	action = 1	current_phase = 0	next_phase = 1	reward = -1.507288	array([[-7.325606, -3.329612]], dtype=float32)

time = 65969	action = 1	current_phase = 1	next_phase = 0	reward = -1.135277	array([[-3.722868 , -2.2469485]], dtype=float32)

time = 65977	action = 0	current_phase = 0	next_phase = 1	reward = 0.209995	array([[-1.6917434, -3.0307617]], dtype=float32)

time = 65982	action = 0	current_phase = 0	next_phase = 1	reward = 0.019688	array([[-2.162583 , -3.0922801]], dtype=float32)

time = 65987	action = 0	current_phase = 0	next_phase = 1	reward = 0.071699	array([[-2.8506198, -3.6720843]], dtype=float32)

time = 65992	action = 1	current_phase = 0	next_phase = 1	reward = -1.590003	array([[-7.3118668, -3.537357 ]], dtype=float32)

time = 66000	action = 1	current_phase = 1	next_phase = 0	reward = -1.351495	array([[-3.817103 , -2.3367558]], dtype=float32)

time = 66008	action = 0	current_phase = 0	next_phase = 1	reward = 0.233411	array([[-1.6231222, -3.0544217]], dtype=float32)

time = 66013	action = 0	current_phase = 0	next_phase = 1	reward = 0.023417	array([[-2.1839545, -3.0875633]], dtype=float32)

time = 66018	action = 0	current_phase = 0	next_phase = 1	reward = 0.080621	array([[-2.754397 , -3.5342748]], dtype=float32)

time = 66023	action = 1	current_phase = 0	next_phase = 1	reward = -1.938704	array([[-7.288378 , -3.6727092]], dtype=float32)

time = 66031	action = 1	current_phase = 1	next_phase = 0	reward = -1.097987	array([[-3.919417 , -2.2290783]], dtype=float32)

time = 66039	action = 0	current_phase = 0	next_phase = 1	reward = -0.041865	array([[-1.3676937, -2.4586277]], dtype=float32)

time = 66044	action = 0	current_phase = 0	next_phase = 1	reward = 0.035882	array([[-1.8942363, -3.1417923]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0214 - val_loss: 0.0118

Epoch 2/50

 - 3s - loss: 0.0258 - val_loss: 0.0095

Epoch 3/50

 - 3s - loss: 0.0189 - val_loss: 0.0095

Epoch 4/50

 - 3s - loss: 0.0213 - val_loss: 0.0111

Epoch 5/50

 - 3s - loss: 0.0194 - val_loss: 0.0093

Epoch 6/50

 - 3s - loss: 0.0219 - val_loss: 0.0113

Epoch 7/50

 - 3s - loss: 0.0182 - val_loss: 0.0076

Epoch 8/50

 - 3s - loss: 0.0221 - val_loss: 0.0083

Epoch 9/50

 - 3s - loss: 0.0158 - val_loss: 0.0088

Epoch 10/50

 - 3s - loss: 0.0177 - val_loss: 0.0087

Epoch 11/50

 - 3s - loss: 0.0174 - val_loss: 0.0103

Epoch 12/50

 - 3s - loss: 0.0152 - val_loss: 0.0079

Epoch 13/50

 - 3s - loss: 0.0193 - val_loss: 0.0090

Epoch 14/50

 - 3s - loss: 0.0197 - val_loss: 0.0105

Epoch 15/50

 - 3s - loss: 0.0155 - val_loss: 0.0069

Epoch 16/50

 - 3s - loss: 0.0184 - val_loss: 0.0108

Epoch 17/50

 - 3s - loss: 0.0198 - val_loss: 0.0100

Epoch 18/50

 - 3s - loss: 0.0196 - val_loss: 0.0111

Epoch 19/50

 - 3s - loss: 0.0168 - val_loss: 0.0099

Epoch 20/50

 - 3s - loss: 0.0156 - val_loss: 0.0084

Epoch 21/50

 - 3s - loss: 0.0232 - val_loss: 0.0084

Epoch 22/50

 - 3s - loss: 0.0185 - val_loss: 0.0126

Epoch 23/50

 - 3s - loss: 0.0162 - val_loss: 0.0126

Epoch 24/50

 - 3s - loss: 0.0178 - val_loss: 0.0125

Epoch 25/50

 - 3s - loss: 0.0164 - val_loss: 0.0127

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 66049	action = 1	current_phase = 0	next_phase = 1	reward = -0.638511	array([[-2.9054134, -2.258442 ]], dtype=float32)

time = 66057	action = 1	current_phase = 1	next_phase = 0	reward = -0.602006	array([[-3.2159681, -1.8810451]], dtype=float32)

time = 66065	action = 0	current_phase = 0	next_phase = 1	reward = -0.383665	array([[-1.5994294, -3.2182186]], dtype=float32)

time = 66070	action = 0	current_phase = 0	next_phase = 1	reward = 0.258658	array([[-1.7643893, -3.0993466]], dtype=float32)

time = 66075	action = 0	current_phase = 0	next_phase = 1	reward = -0.242964	array([[-2.5793438, -3.346416 ]], dtype=float32)

time = 66080	action = 1	current_phase = 0	next_phase = 1	reward = -1.051874	array([[-6.2984824, -2.873945 ]], dtype=float32)

time = 66088	action = 1	current_phase = 1	next_phase = 0	reward = -0.707158	array([[-3.6032772, -2.113812 ]], dtype=float32)

time = 66096	action = 0	current_phase = 0	next_phase = 1	reward = -0.093712	array([[-1.7549107, -3.087332 ]], dtype=float32)

time = 66101	action = 0	current_phase = 0	next_phase = 1	reward = -0.003304	array([[-2.0813055, -3.2019749]], dtype=float32)

time = 66106	action = 0	current_phase = 0	next_phase = 1	reward = 0.061507	array([[-2.65162  , -3.4930153]], dtype=float32)

time = 66111	action = 1	current_phase = 0	next_phase = 1	reward = -1.503088	array([[-7.368953 , -3.2500114]], dtype=float32)

time = 66119	action = 1	current_phase = 1	next_phase = 0	reward = -0.826107	array([[-3.7210417, -2.2162452]], dtype=float32)

time = 66127	action = 0	current_phase = 0	next_phase = 1	reward = -0.058236	array([[-1.822196 , -3.0210023]], dtype=float32)

time = 66132	action = 0	current_phase = 0	next_phase = 1	reward = 0.020347	array([[-2.1786788, -3.1031966]], dtype=float32)

time = 66137	action = 0	current_phase = 0	next_phase = 1	reward = 0.071009	array([[-2.8653262, -3.6474445]], dtype=float32)

time = 66142	action = 1	current_phase = 0	next_phase = 1	reward = -1.613072	array([[-7.32043 , -3.541549]], dtype=float32)

time = 66150	action = 1	current_phase = 1	next_phase = 0	reward = -1.260067	array([[-3.7949743, -2.3592992]], dtype=float32)

time = 66158	action = 0	current_phase = 0	next_phase = 1	reward = 0.233617	array([[-1.64324  , -3.0134356]], dtype=float32)

time = 66163	action = 0	current_phase = 0	next_phase = 1	reward = 0.002201	array([[-2.054556 , -3.0938501]], dtype=float32)

time = 66168	action = 0	current_phase = 0	next_phase = 1	reward = 0.073178	array([[-2.7344139, -3.4433303]], dtype=float32)

time = 66173	action = 1	current_phase = 0	next_phase = 1	reward = -1.265446	array([[-7.3475585, -3.3496857]], dtype=float32)

time = 66181	action = 1	current_phase = 1	next_phase = 0	reward = -1.069299	array([[-3.7487445, -2.2864735]], dtype=float32)

time = 66189	action = 0	current_phase = 0	next_phase = 1	reward = -0.038976	array([[-1.3935273, -2.4294863]], dtype=float32)

time = 66194	action = 0	current_phase = 0	next_phase = 1	reward = 0.023791	array([[-2.0032053, -3.1008077]], dtype=float32)

time = 66199	action = 1	current_phase = 0	next_phase = 1	reward = -0.707923	array([[-2.8702917, -2.2660432]], dtype=float32)

time = 66207	action = 1	current_phase = 1	next_phase = 0	reward = -0.645386	array([[-3.310536 , -1.9470005]], dtype=float32)

time = 66215	action = 0	current_phase = 0	next_phase = 1	reward = -0.105894	array([[-1.5820837, -3.1725504]], dtype=float32)

time = 66220	action = 0	current_phase = 0	next_phase = 1	reward = -0.032601	array([[-1.9276662, -3.349046 ]], dtype=float32)

time = 66225	action = 0	current_phase = 0	next_phase = 1	reward = 0.038587	array([[-2.5135622, -3.3016994]], dtype=float32)

time = 66230	action = 1	current_phase = 0	next_phase = 1	reward = -1.261933	array([[-6.270955, -2.999438]], dtype=float32)

time = 66238	action = 1	current_phase = 1	next_phase = 0	reward = -0.716576	array([[-3.5134587, -2.076748 ]], dtype=float32)

time = 66246	action = 0	current_phase = 0	next_phase = 1	reward = -0.090954	array([[-1.6858156, -3.1162512]], dtype=float32)

time = 66251	action = 0	current_phase = 0	next_phase = 1	reward = 0.010365	array([[-2.103545 , -3.1182954]], dtype=float32)

time = 66256	action = 0	current_phase = 0	next_phase = 1	reward = 0.074546	array([[-2.664412 , -3.5116682]], dtype=float32)

time = 66261	action = 1	current_phase = 0	next_phase = 1	reward = -1.443757	array([[-7.3495636, -3.2566237]], dtype=float32)

time = 66269	action = 1	current_phase = 1	next_phase = 0	reward = -0.903668	array([[-3.756732, -2.274926]], dtype=float32)

time = 66277	action = 0	current_phase = 0	next_phase = 1	reward = -0.059097	array([[-1.8127508, -3.0272253]], dtype=float32)

time = 66282	action = 0	current_phase = 0	next_phase = 1	reward = 0.005021	array([[-2.169669 , -3.1006331]], dtype=float32)

time = 66287	action = 0	current_phase = 0	next_phase = 1	reward = 0.062901	array([[-2.8350985, -3.6182194]], dtype=float32)

time = 66292	action = 1	current_phase = 0	next_phase = 1	reward = -1.587772	array([[-7.33091  , -3.4862607]], dtype=float32)

time = 66300	action = 1	current_phase = 1	next_phase = 0	reward = -0.903308	array([[-3.802136, -2.346131]], dtype=float32)

time = 66308	action = 0	current_phase = 0	next_phase = 1	reward = -0.068505	array([[-1.8276975, -2.9893746]], dtype=float32)

time = 66313	action = 0	current_phase = 0	next_phase = 1	reward = 0.008310	array([[-2.1682396, -3.0877752]], dtype=float32)

time = 66318	action = 0	current_phase = 0	next_phase = 1	reward = 0.076841	array([[-2.812612 , -3.6092641]], dtype=float32)

time = 66323	action = 1	current_phase = 0	next_phase = 1	reward = -1.799050	array([[-7.3152113, -3.5701303]], dtype=float32)

time = 66331	action = 1	current_phase = 1	next_phase = 0	reward = -1.128168	array([[-3.9523015, -2.251483 ]], dtype=float32)

time = 66339	action = 0	current_phase = 0	next_phase = 1	reward = -0.031031	array([[-1.3925918, -2.4291482]], dtype=float32)

time = 66344	action = 0	current_phase = 0	next_phase = 1	reward = 0.034465	array([[-1.875423 , -3.1218724]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0181 - val_loss: 0.0074

Epoch 2/50

 - 3s - loss: 0.0227 - val_loss: 0.0072

Epoch 3/50

 - 3s - loss: 0.0151 - val_loss: 0.0066

Epoch 4/50

 - 3s - loss: 0.0159 - val_loss: 0.0065

Epoch 5/50

 - 3s - loss: 0.0172 - val_loss: 0.0079

Epoch 6/50

 - 3s - loss: 0.0204 - val_loss: 0.0078

Epoch 7/50

 - 3s - loss: 0.0215 - val_loss: 0.0086

Epoch 8/50

 - 3s - loss: 0.0159 - val_loss: 0.0069

Epoch 9/50

 - 3s - loss: 0.0166 - val_loss: 0.0059

Epoch 10/50

 - 3s - loss: 0.0159 - val_loss: 0.0069

Epoch 11/50

 - 3s - loss: 0.0150 - val_loss: 0.0063

Epoch 12/50

 - 3s - loss: 0.0133 - val_loss: 0.0098

Epoch 13/50

 - 3s - loss: 0.0217 - val_loss: 0.0068

Epoch 14/50

 - 3s - loss: 0.0227 - val_loss: 0.0058

Epoch 15/50

 - 3s - loss: 0.0154 - val_loss: 0.0076

Epoch 16/50

 - 3s - loss: 0.0202 - val_loss: 0.0090

Epoch 17/50

 - 3s - loss: 0.0201 - val_loss: 0.0076

Epoch 18/50

 - 3s - loss: 0.0186 - val_loss: 0.0087

Epoch 19/50

 - 3s - loss: 0.0152 - val_loss: 0.0078

Epoch 20/50

 - 3s - loss: 0.0132 - val_loss: 0.0080

Epoch 21/50

 - 3s - loss: 0.0152 - val_loss: 0.0077

Epoch 22/50

 - 3s - loss: 0.0166 - val_loss: 0.0082

Epoch 23/50

 - 3s - loss: 0.0205 - val_loss: 0.0084

Epoch 24/50

 - 3s - loss: 0.0140 - val_loss: 0.0072

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 66349	action = 1	current_phase = 0	next_phase = 1	reward = -0.749267	array([[-2.9325438, -2.2615142]], dtype=float32)

time = 66357	action = 1	current_phase = 1	next_phase = 0	reward = -0.646428	array([[-3.4122958, -2.021054 ]], dtype=float32)

time = 66365	action = 0	current_phase = 0	next_phase = 1	reward = -0.099053	array([[-1.6198175, -3.2231574]], dtype=float32)

time = 66370	action = 0	current_phase = 0	next_phase = 1	reward = -0.026700	array([[-2.0685852, -3.1453788]], dtype=float32)

time = 66375	action = 0	current_phase = 0	next_phase = 1	reward = -0.233631	array([[-2.4153175, -3.344979 ]], dtype=float32)

time = 66380	action = 1	current_phase = 0	next_phase = 1	reward = -0.907911	array([[-6.2665462, -2.7202888]], dtype=float32)

time = 66388	action = 1	current_phase = 1	next_phase = 0	reward = -0.713910	array([[-3.3794703, -2.0722299]], dtype=float32)

time = 66396	action = 0	current_phase = 0	next_phase = 1	reward = -0.097325	array([[-1.696352 , -3.0438607]], dtype=float32)

time = 66401	action = 0	current_phase = 0	next_phase = 1	reward = -0.012708	array([[-2.1479306, -3.1217854]], dtype=float32)

time = 66406	action = 0	current_phase = 0	next_phase = 1	reward = 0.070093	array([[-2.5841777, -3.5511196]], dtype=float32)

time = 66411	action = 1	current_phase = 0	next_phase = 1	reward = -1.501408	array([[-7.3695765, -3.366781 ]], dtype=float32)

time = 66419	action = 1	current_phase = 1	next_phase = 0	reward = -1.479493	array([[-3.7061043, -2.2617965]], dtype=float32)

time = 66427	action = 0	current_phase = 0	next_phase = 1	reward = 0.497675	array([[-1.6368067, -3.0056338]], dtype=float32)

time = 66432	action = 0	current_phase = 0	next_phase = 1	reward = -0.007763	array([[-2.2244456, -3.107959 ]], dtype=float32)

time = 66437	action = 0	current_phase = 0	next_phase = 1	reward = 0.057351	array([[-2.72502  , -3.6107073]], dtype=float32)

time = 66442	action = 1	current_phase = 0	next_phase = 1	reward = -1.707621	array([[-7.302175 , -3.5473883]], dtype=float32)

time = 66450	action = 1	current_phase = 1	next_phase = 0	reward = -1.239602	array([[-3.7712226, -2.3598907]], dtype=float32)

time = 66458	action = 0	current_phase = 0	next_phase = 1	reward = 0.239656	array([[-1.5960312, -3.0087276]], dtype=float32)

time = 66463	action = 0	current_phase = 0	next_phase = 1	reward = 0.017188	array([[-2.191999 , -3.1104848]], dtype=float32)

time = 66468	action = 0	current_phase = 0	next_phase = 1	reward = 0.074564	array([[-2.7509031, -3.6021006]], dtype=float32)

time = 66473	action = 1	current_phase = 0	next_phase = 1	reward = -1.271965	array([[-7.3686543, -3.2784822]], dtype=float32)

time = 66481	action = 1	current_phase = 1	next_phase = 0	reward = -1.435170	array([[-3.7184343, -2.3885794]], dtype=float32)

time = 66489	action = 0	current_phase = 0	next_phase = 1	reward = 0.249966	array([[-1.3270051, -2.4933386]], dtype=float32)

time = 66494	action = 0	current_phase = 0	next_phase = 1	reward = 0.045213	array([[-1.7951214, -3.142864 ]], dtype=float32)

time = 66499	action = 1	current_phase = 0	next_phase = 1	reward = -0.634464	array([[-2.8884678, -2.433796 ]], dtype=float32)

time = 66507	action = 1	current_phase = 1	next_phase = 0	reward = -0.912697	array([[-3.2194104, -2.0010602]], dtype=float32)

time = 66515	action = 0	current_phase = 0	next_phase = 1	reward = 0.192408	array([[-1.4808481, -3.2388244]], dtype=float32)

time = 66520	action = 0	current_phase = 0	next_phase = 1	reward = -0.010418	array([[-2.1152666, -3.153373 ]], dtype=float32)

time = 66525	action = 0	current_phase = 0	next_phase = 1	reward = 0.048859	array([[-2.402073 , -3.3256743]], dtype=float32)

time = 66530	action = 1	current_phase = 0	next_phase = 1	reward = -1.385431	array([[-6.321456 , -3.0437064]], dtype=float32)

time = 66538	action = 1	current_phase = 1	next_phase = 0	reward = -0.705342	array([[-3.6218443, -2.1807866]], dtype=float32)

time = 66546	action = 0	current_phase = 0	next_phase = 1	reward = -0.077962	array([[-1.7455728, -3.0781999]], dtype=float32)

time = 66551	action = 0	current_phase = 0	next_phase = 1	reward = -0.006578	array([[-2.0972152, -3.1282585]], dtype=float32)

time = 66556	action = 0	current_phase = 0	next_phase = 1	reward = 0.065581	array([[-2.5898616, -3.4859145]], dtype=float32)

time = 66561	action = 1	current_phase = 0	next_phase = 1	reward = -1.434186	array([[-7.357543 , -3.3249166]], dtype=float32)

time = 66569	action = 1	current_phase = 1	next_phase = 0	reward = -0.825483	array([[-3.705244 , -2.2692542]], dtype=float32)

time = 66577	action = 0	current_phase = 0	next_phase = 1	reward = -0.072374	array([[-1.744461 , -3.0212831]], dtype=float32)

time = 66582	action = 0	current_phase = 0	next_phase = 1	reward = 0.011985	array([[-2.242454 , -3.1022162]], dtype=float32)

time = 66587	action = 0	current_phase = 0	next_phase = 1	reward = 0.070887	array([[-2.7456355, -3.6372983]], dtype=float32)

time = 66592	action = 1	current_phase = 0	next_phase = 1	reward = -1.706462	array([[-7.327163, -3.549851]], dtype=float32)

time = 66600	action = 1	current_phase = 1	next_phase = 0	reward = -1.016152	array([[-3.849113 , -2.4207096]], dtype=float32)

time = 66608	action = 0	current_phase = 0	next_phase = 1	reward = -0.055261	array([[-1.6352901, -3.0800917]], dtype=float32)

time = 66613	action = 0	current_phase = 0	next_phase = 1	reward = 0.017145	array([[-2.166982, -3.092695]], dtype=float32)

time = 66618	action = 0	current_phase = 0	next_phase = 1	reward = 0.073328	array([[-2.7405405, -3.5844808]], dtype=float32)

time = 66623	action = 1	current_phase = 0	next_phase = 1	reward = -1.888156	array([[-7.3093853, -3.679955 ]], dtype=float32)

time = 66631	action = 1	current_phase = 1	next_phase = 0	reward = -0.926410	array([[-3.9667096, -2.2410011]], dtype=float32)

time = 66639	action = 0	current_phase = 0	next_phase = 1	reward = -0.027767	array([[-1.4720898, -2.512181 ]], dtype=float32)

time = 66644	action = 0	current_phase = 0	next_phase = 1	reward = 0.047512	array([[-1.7319541, -3.1271682]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0254 - val_loss: 0.0105

Epoch 2/50

 - 3s - loss: 0.0180 - val_loss: 0.0115

Epoch 3/50

 - 3s - loss: 0.0191 - val_loss: 0.0112

Epoch 4/50

 - 3s - loss: 0.0194 - val_loss: 0.0111

Epoch 5/50

 - 3s - loss: 0.0182 - val_loss: 0.0106

Epoch 6/50

 - 3s - loss: 0.0187 - val_loss: 0.0107

Epoch 7/50

 - 3s - loss: 0.0164 - val_loss: 0.0109

Epoch 8/50

 - 3s - loss: 0.0179 - val_loss: 0.0109

Epoch 9/50

 - 3s - loss: 0.0172 - val_loss: 0.0124

Epoch 10/50

 - 3s - loss: 0.0150 - val_loss: 0.0119

Epoch 11/50

 - 3s - loss: 0.0180 - val_loss: 0.0114

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 66649	action = 1	current_phase = 0	next_phase = 1	reward = -0.638006	array([[-2.9350107, -2.1894598]], dtype=float32)

time = 66657	action = 1	current_phase = 1	next_phase = 0	reward = -0.924216	array([[-3.2085018, -1.9360265]], dtype=float32)

time = 66665	action = 0	current_phase = 0	next_phase = 1	reward = 0.183583	array([[-1.2306886, -3.2601147]], dtype=float32)

time = 66670	action = 0	current_phase = 0	next_phase = 1	reward = -0.297689	array([[-2.093368 , -3.4240394]], dtype=float32)

time = 66675	action = 0	current_phase = 0	next_phase = 1	reward = 0.336890	array([[-2.241074 , -3.2784111]], dtype=float32)

time = 66680	action = 1	current_phase = 0	next_phase = 1	reward = -1.436516	array([[-6.3683157, -2.895707 ]], dtype=float32)

time = 66688	action = 1	current_phase = 1	next_phase = 0	reward = -0.714157	array([[-3.5806804, -2.1550546]], dtype=float32)

time = 66696	action = 0	current_phase = 0	next_phase = 1	reward = -0.094407	array([[-1.7213209, -3.121842 ]], dtype=float32)

time = 66701	action = 0	current_phase = 0	next_phase = 1	reward = -0.003480	array([[-2.1831408, -3.1312923]], dtype=float32)

time = 66706	action = 0	current_phase = 0	next_phase = 1	reward = 0.067386	array([[-2.6278245, -3.5785518]], dtype=float32)

time = 66711	action = 1	current_phase = 0	next_phase = 1	reward = -1.398927	array([[-7.4278297, -3.2790442]], dtype=float32)

time = 66719	action = 1	current_phase = 1	next_phase = 0	reward = -0.774976	array([[-3.7116032, -2.293427 ]], dtype=float32)

time = 66727	action = 0	current_phase = 0	next_phase = 1	reward = -0.046562	array([[-1.750853 , -3.0592563]], dtype=float32)

time = 66732	action = 0	current_phase = 0	next_phase = 1	reward = 0.013510	array([[-2.2347488, -3.144231 ]], dtype=float32)

time = 66737	action = 0	current_phase = 0	next_phase = 1	reward = 0.081876	array([[-2.795812 , -3.6411827]], dtype=float32)

time = 66742	action = 1	current_phase = 0	next_phase = 1	reward = -1.669864	array([[-7.3617043, -3.5551443]], dtype=float32)

time = 66750	action = 1	current_phase = 1	next_phase = 0	reward = -1.362294	array([[-3.78503  , -2.4028642]], dtype=float32)

time = 66758	action = 0	current_phase = 0	next_phase = 1	reward = 0.222279	array([[-1.5919902, -3.0454965]], dtype=float32)

time = 66763	action = 0	current_phase = 0	next_phase = 1	reward = 0.005046	array([[-2.20051  , -3.1044164]], dtype=float32)

time = 66768	action = 0	current_phase = 0	next_phase = 1	reward = 0.080105	array([[-2.7484913, -3.6110723]], dtype=float32)

time = 66773	action = 1	current_phase = 0	next_phase = 1	reward = -1.800922	array([[-7.345055, -3.607732]], dtype=float32)

time = 66781	action = 1	current_phase = 1	next_phase = 0	reward = -1.436078	array([[-3.9805856, -2.2564898]], dtype=float32)

time = 66789	action = 0	current_phase = 0	next_phase = 1	reward = 0.268702	array([[-1.319782 , -2.4727795]], dtype=float32)

time = 66794	action = 0	current_phase = 0	next_phase = 1	reward = 0.045389	array([[-1.8448126, -3.1050568]], dtype=float32)

time = 66799	action = 1	current_phase = 0	next_phase = 1	reward = -0.746726	array([[-2.9554584, -2.204129 ]], dtype=float32)

time = 66807	action = 1	current_phase = 1	next_phase = 0	reward = -0.645700	array([[-3.3040543, -1.9575858]], dtype=float32)

time = 66815	action = 0	current_phase = 0	next_phase = 1	reward = -0.109153	array([[-1.6203504, -3.2162461]], dtype=float32)

time = 66820	action = 0	current_phase = 0	next_phase = 1	reward = -0.038996	array([[-2.0531266, -2.8352218]], dtype=float32)

time = 66825	action = 0	current_phase = 0	next_phase = 1	reward = -0.245922	array([[-2.3301508, -3.3282778]], dtype=float32)

time = 66830	action = 1	current_phase = 0	next_phase = 1	reward = -0.978252	array([[-6.3390965, -2.666617 ]], dtype=float32)

time = 66838	action = 1	current_phase = 1	next_phase = 0	reward = -0.705222	array([[-3.4424572, -2.0738807]], dtype=float32)

time = 66846	action = 0	current_phase = 0	next_phase = 1	reward = -0.077440	array([[-1.7172563, -3.1513572]], dtype=float32)

time = 66851	action = 0	current_phase = 0	next_phase = 1	reward = -0.006811	array([[-2.1916497, -3.166506 ]], dtype=float32)

time = 66856	action = 0	current_phase = 0	next_phase = 1	reward = 0.054443	array([[-2.589331, -3.57249 ]], dtype=float32)

time = 66861	action = 1	current_phase = 0	next_phase = 1	reward = -1.503625	array([[-7.4198966, -3.188551 ]], dtype=float32)

time = 66869	action = 1	current_phase = 1	next_phase = 0	reward = -0.832764	array([[-3.6750684, -2.2675624]], dtype=float32)

time = 66877	action = 0	current_phase = 0	next_phase = 1	reward = -0.061724	array([[-1.7999482, -3.0292358]], dtype=float32)

time = 66882	action = 0	current_phase = 0	next_phase = 1	reward = 0.014310	array([[-2.2802072, -3.1212666]], dtype=float32)

time = 66887	action = 0	current_phase = 0	next_phase = 1	reward = 0.079940	array([[-2.8128011, -3.684456 ]], dtype=float32)

time = 66892	action = 1	current_phase = 0	next_phase = 1	reward = -1.602053	array([[-7.352108 , -3.5609782]], dtype=float32)

time = 66900	action = 1	current_phase = 1	next_phase = 0	reward = -1.250295	array([[-3.8372474, -2.4101593]], dtype=float32)

time = 66908	action = 0	current_phase = 0	next_phase = 1	reward = 0.237730	array([[-1.585458 , -3.0537815]], dtype=float32)

time = 66913	action = 0	current_phase = 0	next_phase = 1	reward = 0.006441	array([[-2.2458332, -3.0899067]], dtype=float32)

time = 66918	action = 0	current_phase = 0	next_phase = 1	reward = 0.071438	array([[-2.762478 , -3.6000245]], dtype=float32)

time = 66923	action = 1	current_phase = 0	next_phase = 1	reward = -1.973771	array([[-7.3667216, -3.5330224]], dtype=float32)

time = 66931	action = 1	current_phase = 1	next_phase = 0	reward = -1.083094	array([[-3.9569645, -2.2875488]], dtype=float32)

time = 66939	action = 0	current_phase = 0	next_phase = 1	reward = -0.029793	array([[-1.51404  , -2.4555323]], dtype=float32)

time = 66944	action = 0	current_phase = 0	next_phase = 1	reward = 0.037859	array([[-1.8769963, -3.0882418]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0204 - val_loss: 0.0090

Epoch 2/50

 - 3s - loss: 0.0165 - val_loss: 0.0086

Epoch 3/50

 - 3s - loss: 0.0163 - val_loss: 0.0089

Epoch 4/50

 - 3s - loss: 0.0189 - val_loss: 0.0086

Epoch 5/50

 - 3s - loss: 0.0204 - val_loss: 0.0082

Epoch 6/50

 - 3s - loss: 0.0180 - val_loss: 0.0081

Epoch 7/50

 - 3s - loss: 0.0169 - val_loss: 0.0092

Epoch 8/50

 - 3s - loss: 0.0188 - val_loss: 0.0098

Epoch 9/50

 - 3s - loss: 0.0182 - val_loss: 0.0088

Epoch 10/50

 - 3s - loss: 0.0153 - val_loss: 0.0097

Epoch 11/50

 - 3s - loss: 0.0172 - val_loss: 0.0080

Epoch 12/50

 - 3s - loss: 0.0128 - val_loss: 0.0092

Epoch 13/50

 - 3s - loss: 0.0147 - val_loss: 0.0100

Epoch 14/50

 - 3s - loss: 0.0174 - val_loss: 0.0089

Epoch 15/50

 - 3s - loss: 0.0171 - val_loss: 0.0094

Epoch 16/50

 - 3s - loss: 0.0137 - val_loss: 0.0105

Epoch 17/50

 - 3s - loss: 0.0135 - val_loss: 0.0093

Epoch 18/50

 - 3s - loss: 0.0196 - val_loss: 0.0105

Epoch 19/50

 - 3s - loss: 0.0146 - val_loss: 0.0103

Epoch 20/50

 - 3s - loss: 0.0151 - val_loss: 0.0091

Epoch 21/50

 - 3s - loss: 0.0141 - val_loss: 0.0109

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 66949	action = 1	current_phase = 0	next_phase = 1	reward = -0.662875	array([[-2.949599, -2.166629]], dtype=float32)

time = 66957	action = 1	current_phase = 1	next_phase = 0	reward = -0.937290	array([[-3.2232459, -1.9621985]], dtype=float32)

time = 66965	action = 0	current_phase = 0	next_phase = 1	reward = -0.095573	array([[-1.3581824, -3.2893438]], dtype=float32)

time = 66970	action = 0	current_phase = 0	next_phase = 1	reward = -0.012493	array([[-1.764023, -2.847593]], dtype=float32)

time = 66975	action = 0	current_phase = 0	next_phase = 1	reward = 0.061231	array([[-2.1811304, -3.2824898]], dtype=float32)

time = 66980	action = 1	current_phase = 0	next_phase = 1	reward = -0.928832	array([[-6.314561, -2.691534]], dtype=float32)

time = 66988	action = 1	current_phase = 1	next_phase = 0	reward = -0.697442	array([[-3.4180527, -2.0918403]], dtype=float32)

time = 66996	action = 0	current_phase = 0	next_phase = 1	reward = -0.104999	array([[-1.6357346, -3.0696273]], dtype=float32)

time = 67001	action = 0	current_phase = 0	next_phase = 1	reward = -0.024173	array([[-2.1495528, -3.1188083]], dtype=float32)

time = 67006	action = 0	current_phase = 0	next_phase = 1	reward = 0.046328	array([[-2.511611, -3.519402]], dtype=float32)

time = 67011	action = 1	current_phase = 0	next_phase = 1	reward = -1.441358	array([[-7.4377418, -3.2086568]], dtype=float32)

time = 67019	action = 1	current_phase = 1	next_phase = 0	reward = -0.769065	array([[-3.7406135, -2.287713 ]], dtype=float32)

time = 67027	action = 0	current_phase = 0	next_phase = 1	reward = -0.065749	array([[-1.6790894, -3.0573225]], dtype=float32)

time = 67032	action = 0	current_phase = 0	next_phase = 1	reward = 0.007764	array([[-2.257769, -3.141131]], dtype=float32)

time = 67037	action = 0	current_phase = 0	next_phase = 1	reward = 0.068085	array([[-2.754722 , -3.6736898]], dtype=float32)

time = 67042	action = 1	current_phase = 0	next_phase = 1	reward = -1.590284	array([[-7.383904 , -3.4509153]], dtype=float32)

time = 67050	action = 1	current_phase = 1	next_phase = 0	reward = -0.994215	array([[-3.8900313, -2.407804 ]], dtype=float32)

time = 67058	action = 0	current_phase = 0	next_phase = 1	reward = -0.047318	array([[-1.664161 , -3.0928617]], dtype=float32)

time = 67063	action = 0	current_phase = 0	next_phase = 1	reward = 0.026206	array([[-2.1821845, -3.0748744]], dtype=float32)

time = 67068	action = 0	current_phase = 0	next_phase = 1	reward = 0.074101	array([[-2.641767 , -3.5721798]], dtype=float32)

time = 67073	action = 1	current_phase = 0	next_phase = 1	reward = -0.787897	array([[-7.4318504, -3.1095881]], dtype=float32)

time = 67081	action = 1	current_phase = 1	next_phase = 0	reward = -1.260671	array([[-3.6778512, -2.4817574]], dtype=float32)

time = 67089	action = 0	current_phase = 0	next_phase = 1	reward = -0.041900	array([[-1.3720667, -2.5102527]], dtype=float32)

time = 67094	action = 0	current_phase = 0	next_phase = 1	reward = 0.035625	array([[-1.8132584, -3.1774526]], dtype=float32)

time = 67099	action = 1	current_phase = 0	next_phase = 1	reward = -0.650684	array([[-2.826271 , -2.2774596]], dtype=float32)

time = 67107	action = 1	current_phase = 1	next_phase = 0	reward = -0.601955	array([[-3.2269266, -1.970731 ]], dtype=float32)

time = 67115	action = 0	current_phase = 0	next_phase = 1	reward = -0.103592	array([[-1.6196971, -3.2531276]], dtype=float32)

time = 67120	action = 0	current_phase = 0	next_phase = 1	reward = -0.296697	array([[-2.074829, -3.039721]], dtype=float32)

time = 67125	action = 0	current_phase = 0	next_phase = 1	reward = 0.327742	array([[-2.1965199, -3.2878327]], dtype=float32)

time = 67130	action = 1	current_phase = 0	next_phase = 1	reward = -1.275840	array([[-6.3720517, -2.8778872]], dtype=float32)

time = 67138	action = 1	current_phase = 1	next_phase = 0	reward = -0.687689	array([[-3.4773967, -2.1173081]], dtype=float32)

time = 67146	action = 0	current_phase = 0	next_phase = 1	reward = -0.080358	array([[-1.6133368, -3.092535 ]], dtype=float32)

time = 67151	action = 0	current_phase = 0	next_phase = 1	reward = -0.014000	array([[-2.0175474, -3.130499 ]], dtype=float32)

time = 67156	action = 0	current_phase = 0	next_phase = 1	reward = 0.058760	array([[-2.550849 , -3.4920998]], dtype=float32)

time = 67161	action = 1	current_phase = 0	next_phase = 1	reward = -1.500779	array([[-7.435902 , -3.2902722]], dtype=float32)

time = 67169	action = 1	current_phase = 1	next_phase = 0	reward = -1.136991	array([[-3.764309 , -2.3410892]], dtype=float32)

time = 67177	action = 0	current_phase = 0	next_phase = 1	reward = 0.226426	array([[-1.4974254, -3.0436254]], dtype=float32)

time = 67182	action = 0	current_phase = 0	next_phase = 1	reward = 0.000646	array([[-2.2155724, -3.090629 ]], dtype=float32)

time = 67187	action = 0	current_phase = 0	next_phase = 1	reward = 0.074221	array([[-2.7503119, -3.6312585]], dtype=float32)

time = 67192	action = 1	current_phase = 0	next_phase = 1	reward = -1.671738	array([[-7.398377, -3.463553]], dtype=float32)

time = 67200	action = 1	current_phase = 1	next_phase = 0	reward = -1.014052	array([[-3.8495097, -2.4228578]], dtype=float32)

time = 67208	action = 0	current_phase = 0	next_phase = 1	reward = -0.059017	array([[-1.5960603, -3.0829277]], dtype=float32)

time = 67213	action = 0	current_phase = 0	next_phase = 1	reward = 0.008841	array([[-2.192353 , -3.0895438]], dtype=float32)

time = 67218	action = 0	current_phase = 0	next_phase = 1	reward = 0.064996	array([[-2.7127736, -3.6412716]], dtype=float32)

time = 67223	action = 1	current_phase = 0	next_phase = 1	reward = -1.785108	array([[-7.3571334, -3.5945134]], dtype=float32)

time = 67231	action = 1	current_phase = 1	next_phase = 0	reward = -1.096067	array([[-4.0781927, -2.2979121]], dtype=float32)

time = 67239	action = 0	current_phase = 0	next_phase = 1	reward = -0.034768	array([[-1.3971465, -2.4870348]], dtype=float32)

time = 67244	action = 0	current_phase = 0	next_phase = 1	reward = 0.033091	array([[-1.8045106, -3.0751715]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0184 - val_loss: 0.0098

Epoch 2/50

 - 3s - loss: 0.0160 - val_loss: 0.0115

Epoch 3/50

 - 3s - loss: 0.0151 - val_loss: 0.0096

Epoch 4/50

 - 3s - loss: 0.0169 - val_loss: 0.0083

Epoch 5/50

 - 3s - loss: 0.0168 - val_loss: 0.0093

Epoch 6/50

 - 3s - loss: 0.0181 - val_loss: 0.0067

Epoch 7/50

 - 3s - loss: 0.0164 - val_loss: 0.0126

Epoch 8/50

 - 3s - loss: 0.0148 - val_loss: 0.0092

Epoch 9/50

 - 3s - loss: 0.0156 - val_loss: 0.0121

Epoch 10/50

 - 3s - loss: 0.0146 - val_loss: 0.0090

Epoch 11/50

 - 3s - loss: 0.0117 - val_loss: 0.0089

Epoch 12/50

 - 3s - loss: 0.0151 - val_loss: 0.0092

Epoch 13/50

 - 3s - loss: 0.0135 - val_loss: 0.0104

Epoch 14/50

 - 3s - loss: 0.0156 - val_loss: 0.0091

Epoch 15/50

 - 3s - loss: 0.0137 - val_loss: 0.0098

Epoch 16/50

 - 3s - loss: 0.0151 - val_loss: 0.0094

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 67249	action = 1	current_phase = 0	next_phase = 1	reward = -0.649075	array([[-2.990559, -2.21359 ]], dtype=float32)

time = 67257	action = 1	current_phase = 1	next_phase = 0	reward = -0.883259	array([[-3.190703 , -1.9511876]], dtype=float32)

time = 67265	action = 0	current_phase = 0	next_phase = 1	reward = 0.178059	array([[-1.4493349, -3.2442265]], dtype=float32)

time = 67270	action = 0	current_phase = 0	next_phase = 1	reward = -0.572674	array([[-2.1420898, -3.0682116]], dtype=float32)

time = 67275	action = 0	current_phase = 0	next_phase = 1	reward = 0.619621	array([[-2.0645316, -3.208344 ]], dtype=float32)

time = 67280	action = 1	current_phase = 0	next_phase = 1	reward = -1.294790	array([[-6.3927245, -2.9646187]], dtype=float32)

time = 67288	action = 1	current_phase = 1	next_phase = 0	reward = -0.735926	array([[-3.605371 , -2.1710143]], dtype=float32)

time = 67296	action = 0	current_phase = 0	next_phase = 1	reward = -0.085730	array([[-1.7397882, -3.2115846]], dtype=float32)

time = 67301	action = 0	current_phase = 0	next_phase = 1	reward = -0.020799	array([[-2.1756086, -3.1193461]], dtype=float32)

time = 67306	action = 0	current_phase = 0	next_phase = 1	reward = 0.049564	array([[-2.5573463, -3.5210147]], dtype=float32)

time = 67311	action = 1	current_phase = 0	next_phase = 1	reward = -1.503049	array([[-7.4375834, -3.2826014]], dtype=float32)

time = 67319	action = 1	current_phase = 1	next_phase = 0	reward = -0.768326	array([[-3.726163 , -2.3031855]], dtype=float32)

time = 67327	action = 0	current_phase = 0	next_phase = 1	reward = -0.081426	array([[-1.8617232, -3.0403948]], dtype=float32)

time = 67332	action = 0	current_phase = 0	next_phase = 1	reward = 0.000793	array([[-2.2711885, -3.1122108]], dtype=float32)

time = 67337	action = 0	current_phase = 0	next_phase = 1	reward = 0.066527	array([[-2.7255945, -3.6027021]], dtype=float32)

time = 67342	action = 1	current_phase = 0	next_phase = 1	reward = -1.640588	array([[-7.406962, -3.488677]], dtype=float32)

time = 67350	action = 1	current_phase = 1	next_phase = 0	reward = -1.013369	array([[-3.8720732, -2.4687288]], dtype=float32)

time = 67358	action = 0	current_phase = 0	next_phase = 1	reward = -0.059293	array([[-1.6856421, -3.0882306]], dtype=float32)

time = 67363	action = 0	current_phase = 0	next_phase = 1	reward = 0.004615	array([[-2.244772 , -3.0735087]], dtype=float32)

time = 67368	action = 0	current_phase = 0	next_phase = 1	reward = 0.069144	array([[-2.6994314, -3.598031 ]], dtype=float32)

time = 67373	action = 1	current_phase = 0	next_phase = 1	reward = -1.742827	array([[-7.3661685, -3.5925221]], dtype=float32)

time = 67381	action = 1	current_phase = 1	next_phase = 0	reward = -1.368096	array([[-3.9723225, -2.3214927]], dtype=float32)

time = 67389	action = 0	current_phase = 0	next_phase = 1	reward = 0.262561	array([[-1.287427 , -2.4815884]], dtype=float32)

time = 67394	action = 0	current_phase = 0	next_phase = 1	reward = 0.020988	array([[-1.8228812, -3.1503482]], dtype=float32)

time = 67399	action = 1	current_phase = 0	next_phase = 1	reward = -0.662028	array([[-2.9518785, -2.202886 ]], dtype=float32)

time = 67407	action = 1	current_phase = 1	next_phase = 0	reward = -1.142764	array([[-3.1923752, -1.9438223]], dtype=float32)

time = 67415	action = 0	current_phase = 0	next_phase = 1	reward = 0.181629	array([[-1.203253 , -3.2534194]], dtype=float32)

time = 67420	action = 0	current_phase = 0	next_phase = 1	reward = -0.024756	array([[-1.9509168, -3.0606523]], dtype=float32)

time = 67425	action = 0	current_phase = 0	next_phase = 1	reward = 0.321082	array([[-2.1227756, -3.209381 ]], dtype=float32)

time = 67430	action = 1	current_phase = 0	next_phase = 1	reward = -1.374854	array([[-6.3924356, -2.9841504]], dtype=float32)

time = 67438	action = 1	current_phase = 1	next_phase = 0	reward = -0.707542	array([[-3.56385  , -2.1517754]], dtype=float32)

time = 67446	action = 0	current_phase = 0	next_phase = 1	reward = -0.082747	array([[-1.7203445, -3.1214175]], dtype=float32)

time = 67451	action = 0	current_phase = 0	next_phase = 1	reward = -0.001505	array([[-2.1691134, -3.175826 ]], dtype=float32)

time = 67456	action = 0	current_phase = 0	next_phase = 1	reward = 0.070482	array([[-2.6369917, -3.5721898]], dtype=float32)

time = 67461	action = 1	current_phase = 0	next_phase = 1	reward = -1.494905	array([[-7.4375067, -3.286563 ]], dtype=float32)

time = 67469	action = 1	current_phase = 1	next_phase = 0	reward = -0.823692	array([[-3.769404, -2.354744]], dtype=float32)

time = 67477	action = 0	current_phase = 0	next_phase = 1	reward = -0.088246	array([[-1.7821686, -3.0716534]], dtype=float32)

time = 67482	action = 0	current_phase = 0	next_phase = 1	reward = -0.022297	array([[-2.2728634, -3.11936  ]], dtype=float32)

time = 67487	action = 0	current_phase = 0	next_phase = 1	reward = 0.060805	array([[-2.715817, -3.568232]], dtype=float32)

time = 67492	action = 1	current_phase = 0	next_phase = 1	reward = -1.518707	array([[-7.409055 , -3.4830399]], dtype=float32)

time = 67500	action = 1	current_phase = 1	next_phase = 0	reward = -0.882713	array([[-3.8444324, -2.4221363]], dtype=float32)

time = 67508	action = 0	current_phase = 0	next_phase = 1	reward = -0.054744	array([[-1.760836 , -3.0880337]], dtype=float32)

time = 67513	action = 0	current_phase = 0	next_phase = 1	reward = 0.017431	array([[-2.2449553, -3.0707293]], dtype=float32)

time = 67518	action = 0	current_phase = 0	next_phase = 1	reward = 0.070911	array([[-2.6531794, -3.5439315]], dtype=float32)

time = 67523	action = 1	current_phase = 0	next_phase = 1	reward = -0.794440	array([[-7.4440126, -3.1108952]], dtype=float32)

time = 67531	action = 1	current_phase = 1	next_phase = 0	reward = -1.241621	array([[-3.7166781, -2.4893687]], dtype=float32)

time = 67539	action = 0	current_phase = 0	next_phase = 1	reward = -0.028811	array([[-1.4684299, -2.600388 ]], dtype=float32)

time = 67544	action = 0	current_phase = 0	next_phase = 1	reward = 0.044785	array([[-1.7508434, -3.1397686]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0166 - val_loss: 0.0066

Epoch 2/50

 - 3s - loss: 0.0211 - val_loss: 0.0072

Epoch 3/50

 - 3s - loss: 0.0186 - val_loss: 0.0060

Epoch 4/50

 - 3s - loss: 0.0170 - val_loss: 0.0063

Epoch 5/50

 - 3s - loss: 0.0150 - val_loss: 0.0095

Epoch 6/50

 - 3s - loss: 0.0186 - val_loss: 0.0068

Epoch 7/50

 - 3s - loss: 0.0153 - val_loss: 0.0067

Epoch 8/50

 - 3s - loss: 0.0216 - val_loss: 0.0074

Epoch 9/50

 - 3s - loss: 0.0164 - val_loss: 0.0070

Epoch 10/50

 - 3s - loss: 0.0148 - val_loss: 0.0066

Epoch 11/50

 - 3s - loss: 0.0129 - val_loss: 0.0088

Epoch 12/50

 - 3s - loss: 0.0206 - val_loss: 0.0107

Epoch 13/50

 - 3s - loss: 0.0124 - val_loss: 0.0066

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 67549	action = 1	current_phase = 0	next_phase = 1	reward = -0.709581	array([[-2.8997874, -2.358056 ]], dtype=float32)

time = 67557	action = 1	current_phase = 1	next_phase = 0	reward = -0.866937	array([[-3.3573995, -2.0447257]], dtype=float32)

time = 67565	action = 0	current_phase = 0	next_phase = 1	reward = 0.177766	array([[-1.349582 , -3.2456975]], dtype=float32)

time = 67570	action = 0	current_phase = 0	next_phase = 1	reward = -0.034985	array([[-2.0098476, -3.2063615]], dtype=float32)

time = 67575	action = 0	current_phase = 0	next_phase = 1	reward = 0.028768	array([[-2.382307 , -3.3540707]], dtype=float32)

time = 67580	action = 1	current_phase = 0	next_phase = 1	reward = -1.433297	array([[-6.3968167, -3.033229 ]], dtype=float32)

time = 67588	action = 1	current_phase = 1	next_phase = 0	reward = -0.699028	array([[-3.66538  , -2.1558387]], dtype=float32)

time = 67596	action = 0	current_phase = 0	next_phase = 1	reward = -0.075034	array([[-1.7040068, -3.1606517]], dtype=float32)

time = 67601	action = 0	current_phase = 0	next_phase = 1	reward = 0.011588	array([[-2.1608336, -3.2572272]], dtype=float32)

time = 67606	action = 0	current_phase = 0	next_phase = 1	reward = 0.069811	array([[-2.631133 , -3.5728755]], dtype=float32)

time = 67611	action = 1	current_phase = 0	next_phase = 1	reward = -1.495076	array([[-7.45065 , -3.271115]], dtype=float32)

time = 67619	action = 1	current_phase = 1	next_phase = 0	reward = -0.802303	array([[-3.7993426, -2.3599827]], dtype=float32)

time = 67627	action = 0	current_phase = 0	next_phase = 1	reward = -0.068980	array([[-1.7072515, -3.061496 ]], dtype=float32)

time = 67632	action = 0	current_phase = 0	next_phase = 1	reward = 0.009180	array([[-2.26971  , -3.0984204]], dtype=float32)

time = 67637	action = 0	current_phase = 0	next_phase = 1	reward = 0.064804	array([[-2.7663035, -3.6538062]], dtype=float32)

time = 67642	action = 1	current_phase = 0	next_phase = 1	reward = -1.605457	array([[-7.378487 , -3.5767245]], dtype=float32)

time = 67650	action = 1	current_phase = 1	next_phase = 0	reward = -0.887622	array([[-3.8887649, -2.4290006]], dtype=float32)

time = 67658	action = 0	current_phase = 0	next_phase = 1	reward = -0.053476	array([[-1.7076283, -3.1062331]], dtype=float32)

time = 67663	action = 0	current_phase = 0	next_phase = 1	reward = 0.015992	array([[-2.2541728, -3.0907605]], dtype=float32)

time = 67668	action = 0	current_phase = 0	next_phase = 1	reward = 0.070983	array([[-2.7424622, -3.6467597]], dtype=float32)

time = 67673	action = 1	current_phase = 0	next_phase = 1	reward = -1.914331	array([[-7.388157 , -3.5672896]], dtype=float32)

time = 67681	action = 1	current_phase = 1	next_phase = 0	reward = -1.080846	array([[-4.052285 , -2.4029381]], dtype=float32)

time = 67689	action = 0	current_phase = 0	next_phase = 1	reward = -0.031556	array([[-1.4971988, -2.5282373]], dtype=float32)

time = 67694	action = 0	current_phase = 0	next_phase = 1	reward = 0.045070	array([[-1.9002396, -3.0981047]], dtype=float32)

time = 67699	action = 1	current_phase = 0	next_phase = 1	reward = -0.785452	array([[-2.978478 , -2.2389483]], dtype=float32)

time = 67707	action = 1	current_phase = 1	next_phase = 0	reward = -1.214132	array([[-3.3894343, -2.025674 ]], dtype=float32)

time = 67715	action = 0	current_phase = 0	next_phase = 1	reward = 0.171255	array([[-1.1585183, -3.2634184]], dtype=float32)

time = 67720	action = 0	current_phase = 0	next_phase = 1	reward = -0.019302	array([[-1.8149239, -3.2848341]], dtype=float32)

time = 67725	action = 0	current_phase = 0	next_phase = 1	reward = 0.053067	array([[-2.1466346, -3.235913 ]], dtype=float32)

time = 67730	action = 1	current_phase = 0	next_phase = 1	reward = -1.095918	array([[-6.360103 , -2.7091494]], dtype=float32)

time = 67738	action = 1	current_phase = 1	next_phase = 0	reward = -0.699957	array([[-3.6396184, -2.169503 ]], dtype=float32)

time = 67746	action = 0	current_phase = 0	next_phase = 1	reward = -0.076326	array([[-1.7324412, -3.2100558]], dtype=float32)

time = 67751	action = 0	current_phase = 0	next_phase = 1	reward = -0.013678	array([[-2.1668363, -3.130999 ]], dtype=float32)

time = 67756	action = 0	current_phase = 0	next_phase = 1	reward = 0.067298	array([[-2.574315 , -3.4957864]], dtype=float32)

time = 67761	action = 1	current_phase = 0	next_phase = 1	reward = -1.441553	array([[-7.409362 , -3.2848923]], dtype=float32)

time = 67769	action = 1	current_phase = 1	next_phase = 0	reward = -0.791740	array([[-3.795679 , -2.3386936]], dtype=float32)

time = 67777	action = 0	current_phase = 0	next_phase = 1	reward = -0.058281	array([[-1.7343392, -3.0651321]], dtype=float32)

time = 67782	action = 0	current_phase = 0	next_phase = 1	reward = 0.028982	array([[-2.2501516, -3.0930912]], dtype=float32)

time = 67787	action = 0	current_phase = 0	next_phase = 1	reward = 0.075395	array([[-2.7773595, -3.6272714]], dtype=float32)

time = 67792	action = 1	current_phase = 0	next_phase = 1	reward = -1.674170	array([[-7.378147 , -3.5881648]], dtype=float32)

time = 67800	action = 1	current_phase = 1	next_phase = 0	reward = -0.913273	array([[-3.8749495, -2.4637904]], dtype=float32)

time = 67808	action = 0	current_phase = 0	next_phase = 1	reward = -0.060086	array([[-1.7678934, -3.1094887]], dtype=float32)

time = 67813	action = 0	current_phase = 0	next_phase = 1	reward = 0.023936	array([[-2.2217245, -3.0929368]], dtype=float32)

time = 67818	action = 0	current_phase = 0	next_phase = 1	reward = 0.072436	array([[-2.7313833, -3.6094606]], dtype=float32)

time = 67823	action = 1	current_phase = 0	next_phase = 1	reward = -1.860862	array([[-7.3662   , -3.6186361]], dtype=float32)

time = 67831	action = 1	current_phase = 1	next_phase = 0	reward = -1.379266	array([[-4.052601 , -2.3559842]], dtype=float32)

time = 67839	action = 0	current_phase = 0	next_phase = 1	reward = 0.262525	array([[-1.2871741, -2.45575  ]], dtype=float32)

time = 67844	action = 0	current_phase = 0	next_phase = 1	reward = 0.040907	array([[-1.800695, -3.100137]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0162 - val_loss: 0.0100

Epoch 2/50

 - 3s - loss: 0.0201 - val_loss: 0.0098

Epoch 3/50

 - 3s - loss: 0.0142 - val_loss: 0.0100

Epoch 4/50

 - 3s - loss: 0.0184 - val_loss: 0.0101

Epoch 5/50

 - 3s - loss: 0.0168 - val_loss: 0.0098

Epoch 6/50

 - 3s - loss: 0.0147 - val_loss: 0.0107

Epoch 7/50

 - 3s - loss: 0.0131 - val_loss: 0.0098

Epoch 8/50

 - 3s - loss: 0.0203 - val_loss: 0.0152

Epoch 9/50

 - 3s - loss: 0.0184 - val_loss: 0.0114

Epoch 10/50

 - 3s - loss: 0.0141 - val_loss: 0.0084

Epoch 11/50

 - 3s - loss: 0.0148 - val_loss: 0.0107

Epoch 12/50

 - 3s - loss: 0.0178 - val_loss: 0.0113

Epoch 13/50

 - 3s - loss: 0.0150 - val_loss: 0.0116

Epoch 14/50

 - 3s - loss: 0.0185 - val_loss: 0.0102

Epoch 15/50

 - 3s - loss: 0.0156 - val_loss: 0.0107

Epoch 16/50

 - 3s - loss: 0.0140 - val_loss: 0.0117

Epoch 17/50

 - 3s - loss: 0.0160 - val_loss: 0.0105

Epoch 18/50

 - 3s - loss: 0.0163 - val_loss: 0.0115

Epoch 19/50

 - 3s - loss: 0.0180 - val_loss: 0.0137

Epoch 20/50

 - 3s - loss: 0.0138 - val_loss: 0.0121

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 67849	action = 1	current_phase = 0	next_phase = 1	reward = -0.689574	array([[-3.0136428, -2.272898 ]], dtype=float32)

time = 67857	action = 1	current_phase = 1	next_phase = 0	reward = -0.923204	array([[-3.296897 , -1.9849524]], dtype=float32)

time = 67865	action = 0	current_phase = 0	next_phase = 1	reward = 0.194767	array([[-1.5492547, -3.2432215]], dtype=float32)

time = 67870	action = 0	current_phase = 0	next_phase = 1	reward = -0.022345	array([[-2.0175858, -3.3826566]], dtype=float32)

time = 67875	action = 0	current_phase = 0	next_phase = 1	reward = 0.045957	array([[-2.4100883, -3.3857064]], dtype=float32)

time = 67880	action = 1	current_phase = 0	next_phase = 1	reward = -1.211295	array([[-6.4723706, -3.0081174]], dtype=float32)

time = 67888	action = 1	current_phase = 1	next_phase = 0	reward = -0.639277	array([[-3.4183612, -2.0515969]], dtype=float32)

time = 67896	action = 0	current_phase = 0	next_phase = 1	reward = -0.095535	array([[-1.8147879, -3.1004858]], dtype=float32)

time = 67901	action = 0	current_phase = 0	next_phase = 1	reward = -0.010250	array([[-2.163526 , -3.1237507]], dtype=float32)

time = 67906	action = 0	current_phase = 0	next_phase = 1	reward = 0.056531	array([[-2.6018176, -3.5149841]], dtype=float32)

time = 67911	action = 1	current_phase = 0	next_phase = 1	reward = -1.390042	array([[-7.4741917, -3.332302 ]], dtype=float32)

time = 67919	action = 1	current_phase = 1	next_phase = 0	reward = -0.882972	array([[-3.79102  , -2.2800546]], dtype=float32)

time = 67927	action = 0	current_phase = 0	next_phase = 1	reward = -0.076709	array([[-1.8305614, -3.0539885]], dtype=float32)

time = 67932	action = 0	current_phase = 0	next_phase = 1	reward = 0.021058	array([[-2.241057 , -3.0884542]], dtype=float32)

time = 67937	action = 0	current_phase = 0	next_phase = 1	reward = 0.071082	array([[-2.8254352, -3.696011 ]], dtype=float32)

time = 67942	action = 1	current_phase = 0	next_phase = 1	reward = -1.609313	array([[-7.4255176, -3.577063 ]], dtype=float32)

time = 67950	action = 1	current_phase = 1	next_phase = 0	reward = -1.154812	array([[-3.914558 , -2.4278224]], dtype=float32)

time = 67958	action = 0	current_phase = 0	next_phase = 1	reward = 0.235447	array([[-1.5952572, -3.07773  ]], dtype=float32)

time = 67963	action = 0	current_phase = 0	next_phase = 1	reward = 0.012662	array([[-2.2024126, -3.0853398]], dtype=float32)

time = 67968	action = 0	current_phase = 0	next_phase = 1	reward = 0.083791	array([[-2.7881107, -3.6083038]], dtype=float32)

time = 67973	action = 1	current_phase = 0	next_phase = 1	reward = -1.804083	array([[-7.405888, -3.652295]], dtype=float32)

time = 67981	action = 1	current_phase = 1	next_phase = 0	reward = -1.033066	array([[-4.047566 , -2.2889853]], dtype=float32)

time = 67989	action = 0	current_phase = 0	next_phase = 1	reward = -0.032925	array([[-1.4626974, -2.5467672]], dtype=float32)

time = 67994	action = 0	current_phase = 0	next_phase = 1	reward = 0.034548	array([[-1.7440917, -3.1086724]], dtype=float32)

time = 67999	action = 1	current_phase = 0	next_phase = 1	reward = -0.688244	array([[-3.001162 , -2.2992702]], dtype=float32)

time = 68007	action = 1	current_phase = 1	next_phase = 0	reward = -1.198156	array([[-3.3922014, -2.0230768]], dtype=float32)

time = 68015	action = 0	current_phase = 0	next_phase = 1	reward = 0.472997	array([[-1.3539433, -3.336716 ]], dtype=float32)

time = 68020	action = 0	current_phase = 0	next_phase = 1	reward = -0.016562	array([[-2.0788245, -3.2735329]], dtype=float32)

time = 68025	action = 0	current_phase = 0	next_phase = 1	reward = 0.058087	array([[-2.4476295, -3.4300861]], dtype=float32)

time = 68030	action = 1	current_phase = 0	next_phase = 1	reward = -1.441303	array([[-6.445997 , -3.0823698]], dtype=float32)

time = 68038	action = 1	current_phase = 1	next_phase = 0	reward = -0.728146	array([[-3.679099 , -2.1337025]], dtype=float32)

time = 68046	action = 0	current_phase = 0	next_phase = 1	reward = -0.090337	array([[-1.8165033, -3.0853996]], dtype=float32)

time = 68051	action = 0	current_phase = 0	next_phase = 1	reward = -0.019466	array([[-2.0519803, -3.1198049]], dtype=float32)

time = 68056	action = 0	current_phase = 0	next_phase = 1	reward = 0.061973	array([[-2.494205 , -3.4714978]], dtype=float32)

time = 68061	action = 1	current_phase = 0	next_phase = 1	reward = -1.545427	array([[-7.4863486, -3.3202171]], dtype=float32)

time = 68069	action = 1	current_phase = 1	next_phase = 0	reward = -0.853458	array([[-3.8142576, -2.3005521]], dtype=float32)

time = 68077	action = 0	current_phase = 0	next_phase = 1	reward = -0.075342	array([[-1.7765274, -3.1046548]], dtype=float32)

time = 68082	action = 0	current_phase = 0	next_phase = 1	reward = -0.013692	array([[-2.2415164, -3.1094172]], dtype=float32)

time = 68087	action = 0	current_phase = 0	next_phase = 1	reward = 0.047935	array([[-2.766663 , -3.6176395]], dtype=float32)

time = 68092	action = 1	current_phase = 0	next_phase = 1	reward = -1.569018	array([[-7.4345765, -3.564222 ]], dtype=float32)

time = 68100	action = 1	current_phase = 1	next_phase = 0	reward = -0.892647	array([[-3.8784513, -2.392686 ]], dtype=float32)

time = 68108	action = 0	current_phase = 0	next_phase = 1	reward = -0.064827	array([[-1.7238973, -3.0046303]], dtype=float32)

time = 68113	action = 0	current_phase = 0	next_phase = 1	reward = -0.000129	array([[-2.1934514, -3.0770729]], dtype=float32)

time = 68118	action = 0	current_phase = 0	next_phase = 1	reward = 0.062291	array([[-2.8045425, -3.6389632]], dtype=float32)

time = 68123	action = 1	current_phase = 0	next_phase = 1	reward = -1.863317	array([[-7.394077 , -3.6915135]], dtype=float32)

time = 68131	action = 1	current_phase = 1	next_phase = 0	reward = -0.979246	array([[-4.0611672, -2.2982693]], dtype=float32)

time = 68139	action = 0	current_phase = 0	next_phase = 1	reward = -0.032663	array([[-1.435801 , -2.5422535]], dtype=float32)

time = 68144	action = 0	current_phase = 0	next_phase = 1	reward = 0.032119	array([[-1.8442256, -3.0940373]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0141 - val_loss: 0.0069

Epoch 2/50

 - 3s - loss: 0.0226 - val_loss: 0.0063

Epoch 3/50

 - 3s - loss: 0.0172 - val_loss: 0.0069

Epoch 4/50

 - 3s - loss: 0.0159 - val_loss: 0.0068

Epoch 5/50

 - 3s - loss: 0.0190 - val_loss: 0.0068

Epoch 6/50

 - 3s - loss: 0.0140 - val_loss: 0.0068

Epoch 7/50

 - 3s - loss: 0.0187 - val_loss: 0.0067

Epoch 8/50

 - 3s - loss: 0.0156 - val_loss: 0.0086

Epoch 9/50

 - 3s - loss: 0.0157 - val_loss: 0.0075

Epoch 10/50

 - 3s - loss: 0.0162 - val_loss: 0.0071

Epoch 11/50

 - 3s - loss: 0.0181 - val_loss: 0.0079

Epoch 12/50

 - 3s - loss: 0.0158 - val_loss: 0.0099

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 68149	action = 1	current_phase = 0	next_phase = 1	reward = -0.701552	array([[-3.0223687, -2.25018  ]], dtype=float32)

time = 68157	action = 1	current_phase = 1	next_phase = 0	reward = -0.937047	array([[-3.4173317, -2.0237322]], dtype=float32)

time = 68165	action = 0	current_phase = 0	next_phase = 1	reward = -0.103444	array([[-1.3421472, -3.1766531]], dtype=float32)

time = 68170	action = 0	current_phase = 0	next_phase = 1	reward = 0.260862	array([[-1.693177, -3.061738]], dtype=float32)

time = 68175	action = 0	current_phase = 0	next_phase = 1	reward = 0.050968	array([[-2.192637 , -3.2442331]], dtype=float32)

time = 68180	action = 1	current_phase = 0	next_phase = 1	reward = -1.304396	array([[-6.4287906, -2.969658 ]], dtype=float32)

time = 68188	action = 1	current_phase = 1	next_phase = 0	reward = -0.704458	array([[-3.60174 , -2.128841]], dtype=float32)

time = 68196	action = 0	current_phase = 0	next_phase = 1	reward = -0.070129	array([[-1.7598104, -3.0914598]], dtype=float32)

time = 68201	action = 0	current_phase = 0	next_phase = 1	reward = -0.022283	array([[-2.0576167, -3.081591 ]], dtype=float32)

time = 68206	action = 0	current_phase = 0	next_phase = 1	reward = 0.031361	array([[-2.4306889, -3.4836597]], dtype=float32)

time = 68211	action = 1	current_phase = 0	next_phase = 1	reward = -1.389468	array([[-7.4639683, -3.257996 ]], dtype=float32)

time = 68219	action = 1	current_phase = 1	next_phase = 0	reward = -0.779232	array([[-3.8070407, -2.2593405]], dtype=float32)

time = 68227	action = 0	current_phase = 0	next_phase = 1	reward = -0.086174	array([[-1.7910585, -3.0007277]], dtype=float32)

time = 68232	action = 0	current_phase = 0	next_phase = 1	reward = -0.006648	array([[-2.1254673, -3.0687299]], dtype=float32)

time = 68237	action = 0	current_phase = 0	next_phase = 1	reward = 0.059673	array([[-2.7625136, -3.6286762]], dtype=float32)

time = 68242	action = 1	current_phase = 0	next_phase = 1	reward = -1.510543	array([[-7.4227004, -3.4313102]], dtype=float32)

time = 68250	action = 1	current_phase = 1	next_phase = 0	reward = -0.951013	array([[-3.9507008, -2.3695297]], dtype=float32)

time = 68258	action = 0	current_phase = 0	next_phase = 1	reward = -0.043275	array([[-1.7163882, -3.058389 ]], dtype=float32)

time = 68263	action = 0	current_phase = 0	next_phase = 1	reward = 0.037357	array([[-2.1857061, -3.055634 ]], dtype=float32)

time = 68268	action = 0	current_phase = 0	next_phase = 1	reward = 0.076955	array([[-2.7857087, -3.648739 ]], dtype=float32)

time = 68273	action = 1	current_phase = 0	next_phase = 1	reward = -1.956172	array([[-7.3716908, -3.702027 ]], dtype=float32)

time = 68281	action = 1	current_phase = 1	next_phase = 0	reward = -1.292436	array([[-4.10515 , -2.323318]], dtype=float32)

time = 68289	action = 0	current_phase = 0	next_phase = 1	reward = 0.273138	array([[-1.1727169, -2.4893706]], dtype=float32)

time = 68294	action = 0	current_phase = 0	next_phase = 1	reward = 0.052628	array([[-1.7255859, -3.0750425]], dtype=float32)

time = 68299	action = 1	current_phase = 0	next_phase = 1	reward = -0.764706	array([[-2.990116 , -2.2794418]], dtype=float32)

time = 68307	action = 1	current_phase = 1	next_phase = 0	reward = -1.215591	array([[-3.4528995, -2.0355225]], dtype=float32)

time = 68315	action = 0	current_phase = 0	next_phase = 1	reward = 0.445617	array([[-1.235464 , -3.1833858]], dtype=float32)

time = 68320	action = 0	current_phase = 0	next_phase = 1	reward = -0.319843	array([[-1.869532 , -3.1798468]], dtype=float32)

time = 68325	action = 0	current_phase = 0	next_phase = 1	reward = -0.230450	array([[-1.7719717, -3.0958555]], dtype=float32)

time = 68330	action = 1	current_phase = 0	next_phase = 1	reward = -0.856959	array([[-6.301468, -2.718271]], dtype=float32)

time = 68338	action = 1	current_phase = 1	next_phase = 0	reward = -0.708256	array([[-3.7115107, -2.1583576]], dtype=float32)

time = 68346	action = 0	current_phase = 0	next_phase = 1	reward = -0.083164	array([[-1.6751722, -3.1077685]], dtype=float32)

time = 68351	action = 0	current_phase = 0	next_phase = 1	reward = -0.008460	array([[-2.0129247, -3.137795 ]], dtype=float32)

time = 68356	action = 0	current_phase = 0	next_phase = 1	reward = 0.059057	array([[-2.4805658, -3.5035276]], dtype=float32)

time = 68361	action = 1	current_phase = 0	next_phase = 1	reward = -1.488040	array([[-7.4636254, -3.2248695]], dtype=float32)

time = 68369	action = 1	current_phase = 1	next_phase = 0	reward = -0.846636	array([[-3.8175268, -2.2765274]], dtype=float32)

time = 68377	action = 0	current_phase = 0	next_phase = 1	reward = -0.058637	array([[-1.7161752, -3.0060437]], dtype=float32)

time = 68382	action = 0	current_phase = 0	next_phase = 1	reward = 0.033635	array([[-2.192034, -3.0621  ]], dtype=float32)

time = 68387	action = 0	current_phase = 0	next_phase = 1	reward = 0.072002	array([[-2.7372785, -3.6336856]], dtype=float32)

time = 68392	action = 1	current_phase = 0	next_phase = 1	reward = -1.667742	array([[-7.3976893, -3.4482796]], dtype=float32)

time = 68400	action = 1	current_phase = 1	next_phase = 0	reward = -1.003567	array([[-3.961256, -2.417195]], dtype=float32)

time = 68408	action = 0	current_phase = 0	next_phase = 1	reward = -0.066536	array([[-1.6008637, -3.060748 ]], dtype=float32)

time = 68413	action = 0	current_phase = 0	next_phase = 1	reward = -0.001122	array([[-2.136027, -3.038262]], dtype=float32)

time = 68418	action = 0	current_phase = 0	next_phase = 1	reward = 0.079514	array([[-2.7251644, -3.573802 ]], dtype=float32)

time = 68423	action = 1	current_phase = 0	next_phase = 1	reward = -1.848034	array([[-7.3689775, -3.595106 ]], dtype=float32)

time = 68431	action = 1	current_phase = 1	next_phase = 0	reward = -1.035440	array([[-4.117742 , -2.3274999]], dtype=float32)

time = 68439	action = 0	current_phase = 0	next_phase = 1	reward = -0.041294	array([[-1.2408893, -2.533013 ]], dtype=float32)

time = 68444	action = 0	current_phase = 0	next_phase = 1	reward = 0.026867	array([[-1.802124 , -3.0338352]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0180 - val_loss: 0.0082

Epoch 2/50

 - 3s - loss: 0.0172 - val_loss: 0.0095

Epoch 3/50

 - 3s - loss: 0.0208 - val_loss: 0.0086

Epoch 4/50

 - 3s - loss: 0.0177 - val_loss: 0.0100

Epoch 5/50

 - 3s - loss: 0.0154 - val_loss: 0.0080

Epoch 6/50

 - 3s - loss: 0.0195 - val_loss: 0.0104

Epoch 7/50

 - 3s - loss: 0.0152 - val_loss: 0.0094

Epoch 8/50

 - 3s - loss: 0.0141 - val_loss: 0.0148

Epoch 9/50

 - 3s - loss: 0.0149 - val_loss: 0.0110

Epoch 10/50

 - 3s - loss: 0.0141 - val_loss: 0.0080

Epoch 11/50

 - 3s - loss: 0.0145 - val_loss: 0.0077

Epoch 12/50

 - 3s - loss: 0.0156 - val_loss: 0.0077

Epoch 13/50

 - 3s - loss: 0.0158 - val_loss: 0.0087

Epoch 14/50

 - 3s - loss: 0.0226 - val_loss: 0.0094

Epoch 15/50

 - 3s - loss: 0.0174 - val_loss: 0.0102

Epoch 16/50

 - 3s - loss: 0.0171 - val_loss: 0.0096

Epoch 17/50

 - 3s - loss: 0.0149 - val_loss: 0.0091

Epoch 18/50

 - 3s - loss: 0.0147 - val_loss: 0.0086

Epoch 19/50

 - 3s - loss: 0.0159 - val_loss: 0.0081

Epoch 20/50

 - 3s - loss: 0.0155 - val_loss: 0.0083

Epoch 21/50

 - 3s - loss: 0.0153 - val_loss: 0.0110

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 68449	action = 1	current_phase = 0	next_phase = 1	reward = -0.654635	array([[-3.0073783, -2.280499 ]], dtype=float32)

time = 68457	action = 1	current_phase = 1	next_phase = 0	reward = -0.922930	array([[-3.2635784, -2.0320764]], dtype=float32)

time = 68465	action = 0	current_phase = 0	next_phase = 1	reward = 0.169996	array([[-1.481571 , -3.1769722]], dtype=float32)

time = 68470	action = 0	current_phase = 0	next_phase = 1	reward = -0.024372	array([[-1.8552374, -3.0441031]], dtype=float32)

time = 68475	action = 0	current_phase = 0	next_phase = 1	reward = 0.042346	array([[-2.3772562, -3.3528507]], dtype=float32)

time = 68480	action = 1	current_phase = 0	next_phase = 1	reward = -1.308917	array([[-6.4007893, -2.9376109]], dtype=float32)

time = 68488	action = 1	current_phase = 1	next_phase = 0	reward = -0.715198	array([[-3.6792595, -2.1867983]], dtype=float32)

time = 68496	action = 0	current_phase = 0	next_phase = 1	reward = -0.079433	array([[-1.8056788, -3.040429 ]], dtype=float32)

time = 68501	action = 0	current_phase = 0	next_phase = 1	reward = -0.003719	array([[-2.0672605, -3.1557982]], dtype=float32)

time = 68506	action = 0	current_phase = 0	next_phase = 1	reward = 0.062057	array([[-2.5538054, -3.5314178]], dtype=float32)

time = 68511	action = 1	current_phase = 0	next_phase = 1	reward = -1.439392	array([[-7.4293227, -3.3368754]], dtype=float32)

time = 68519	action = 1	current_phase = 1	next_phase = 0	reward = -0.796136	array([[-3.792335 , -2.2991042]], dtype=float32)

time = 68527	action = 0	current_phase = 0	next_phase = 1	reward = -0.074979	array([[-1.8493681, -2.9963164]], dtype=float32)

time = 68532	action = 0	current_phase = 0	next_phase = 1	reward = -0.012059	array([[-2.1886306, -3.07139  ]], dtype=float32)

time = 68537	action = 0	current_phase = 0	next_phase = 1	reward = 0.059680	array([[-2.7039628, -3.5706604]], dtype=float32)

time = 68542	action = 1	current_phase = 0	next_phase = 1	reward = -1.635665	array([[-7.3952456, -3.5391574]], dtype=float32)

time = 68550	action = 1	current_phase = 1	next_phase = 0	reward = -0.958366	array([[-3.8974223, -2.3985424]], dtype=float32)

time = 68558	action = 0	current_phase = 0	next_phase = 1	reward = -0.053842	array([[-1.7210405, -3.0086894]], dtype=float32)

time = 68563	action = 0	current_phase = 0	next_phase = 1	reward = 0.016160	array([[-2.1718507, -3.0559676]], dtype=float32)

time = 68568	action = 0	current_phase = 0	next_phase = 1	reward = 0.079342	array([[-2.7063668, -3.5732114]], dtype=float32)

time = 68573	action = 1	current_phase = 0	next_phase = 1	reward = -1.254134	array([[-7.3501334, -3.7284803]], dtype=float32)

time = 68581	action = 1	current_phase = 1	next_phase = 0	reward = -1.497143	array([[-3.9107249, -2.4369793]], dtype=float32)

time = 68589	action = 0	current_phase = 0	next_phase = 1	reward = 0.233563	array([[-1.1555715, -2.4986358]], dtype=float32)

time = 68594	action = 0	current_phase = 0	next_phase = 1	reward = 0.026541	array([[-1.7272522, -3.0733466]], dtype=float32)

time = 68599	action = 1	current_phase = 0	next_phase = 1	reward = -0.632316	array([[-2.9641433, -2.3249156]], dtype=float32)

time = 68607	action = 1	current_phase = 1	next_phase = 0	reward = -0.635967	array([[-3.2965746, -2.0582285]], dtype=float32)

time = 68615	action = 0	current_phase = 0	next_phase = 1	reward = -0.369486	array([[-1.8715434, -3.1994827]], dtype=float32)

time = 68620	action = 0	current_phase = 0	next_phase = 1	reward = 0.275915	array([[-1.7206184, -3.2060437]], dtype=float32)

time = 68625	action = 0	current_phase = 0	next_phase = 1	reward = 0.062397	array([[-2.341151, -3.361439]], dtype=float32)

time = 68630	action = 1	current_phase = 0	next_phase = 1	reward = -1.362522	array([[-6.4012327, -2.9691064]], dtype=float32)

time = 68638	action = 1	current_phase = 1	next_phase = 0	reward = -0.726530	array([[-3.7067957, -2.2007117]], dtype=float32)

time = 68646	action = 0	current_phase = 0	next_phase = 1	reward = -0.096762	array([[-1.8200905, -3.0512474]], dtype=float32)

time = 68651	action = 0	current_phase = 0	next_phase = 1	reward = -0.018106	array([[-2.1443326, -3.1144037]], dtype=float32)

time = 68656	action = 0	current_phase = 0	next_phase = 1	reward = 0.042437	array([[-2.5574489, -3.5297341]], dtype=float32)

time = 68661	action = 1	current_phase = 0	next_phase = 1	reward = -1.388251	array([[-7.46165  , -3.3463764]], dtype=float32)

time = 68669	action = 1	current_phase = 1	next_phase = 0	reward = -0.844894	array([[-3.7575173, -2.292353 ]], dtype=float32)

time = 68677	action = 0	current_phase = 0	next_phase = 1	reward = -0.067624	array([[-1.7745945, -3.0153265]], dtype=float32)

time = 68682	action = 0	current_phase = 0	next_phase = 1	reward = 0.015824	array([[-2.1934485, -3.1049247]], dtype=float32)

time = 68687	action = 0	current_phase = 0	next_phase = 1	reward = 0.074658	array([[-2.7665858, -3.636922 ]], dtype=float32)

time = 68692	action = 1	current_phase = 0	next_phase = 1	reward = -1.667361	array([[-7.376269 , -3.5441875]], dtype=float32)

time = 68700	action = 1	current_phase = 1	next_phase = 0	reward = -0.999397	array([[-3.923688, -2.437419]], dtype=float32)

time = 68708	action = 0	current_phase = 0	next_phase = 1	reward = -0.034693	array([[-1.7467725, -3.0344627]], dtype=float32)

time = 68713	action = 0	current_phase = 0	next_phase = 1	reward = 0.029771	array([[-2.177693 , -3.0546136]], dtype=float32)

time = 68718	action = 0	current_phase = 0	next_phase = 1	reward = 0.074496	array([[-2.7178166, -3.572895 ]], dtype=float32)

time = 68723	action = 1	current_phase = 0	next_phase = 1	reward = -1.955269	array([[-7.355024 , -3.7477577]], dtype=float32)

time = 68731	action = 1	current_phase = 1	next_phase = 0	reward = -1.146262	array([[-4.056699 , -2.3514476]], dtype=float32)

time = 68739	action = 0	current_phase = 0	next_phase = 1	reward = -0.031161	array([[-1.336726 , -2.5227525]], dtype=float32)

time = 68744	action = 0	current_phase = 0	next_phase = 1	reward = 0.026019	array([[-1.757154 , -3.0528605]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0205 - val_loss: 0.0058

Epoch 2/50

 - 3s - loss: 0.0168 - val_loss: 0.0065

Epoch 3/50

 - 3s - loss: 0.0196 - val_loss: 0.0062

Epoch 4/50

 - 3s - loss: 0.0170 - val_loss: 0.0064

Epoch 5/50

 - 3s - loss: 0.0182 - val_loss: 0.0070

Epoch 6/50

 - 3s - loss: 0.0173 - val_loss: 0.0063

Epoch 7/50

 - 3s - loss: 0.0154 - val_loss: 0.0071

Epoch 8/50

 - 3s - loss: 0.0177 - val_loss: 0.0071

Epoch 9/50

 - 3s - loss: 0.0161 - val_loss: 0.0071

Epoch 10/50

 - 3s - loss: 0.0151 - val_loss: 0.0071

Epoch 11/50

 - 3s - loss: 0.0153 - val_loss: 0.0059

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 68749	action = 1	current_phase = 0	next_phase = 1	reward = -0.772991	array([[-3.026739, -2.290873]], dtype=float32)

time = 68757	action = 1	current_phase = 1	next_phase = 0	reward = -1.203849	array([[-3.5193348, -2.0711899]], dtype=float32)

time = 68765	action = 0	current_phase = 0	next_phase = 1	reward = 0.453718	array([[-1.3166046, -3.1968005]], dtype=float32)

time = 68770	action = 0	current_phase = 0	next_phase = 1	reward = -0.310509	array([[-1.8703382, -3.1420243]], dtype=float32)

time = 68775	action = 0	current_phase = 0	next_phase = 1	reward = 0.313468	array([[-2.1801496, -3.2212534]], dtype=float32)

time = 68780	action = 1	current_phase = 0	next_phase = 1	reward = -1.255555	array([[-6.421467 , -2.9699168]], dtype=float32)

time = 68788	action = 1	current_phase = 1	next_phase = 0	reward = -0.974778	array([[-3.6389055, -2.159264 ]], dtype=float32)

time = 68796	action = 0	current_phase = 0	next_phase = 1	reward = 0.208234	array([[-1.6648154, -3.137074 ]], dtype=float32)

time = 68801	action = 0	current_phase = 0	next_phase = 1	reward = -0.006067	array([[-2.1185818, -3.085207 ]], dtype=float32)

time = 68806	action = 0	current_phase = 0	next_phase = 1	reward = 0.070875	array([[-2.6645198, -3.5657377]], dtype=float32)

time = 68811	action = 1	current_phase = 0	next_phase = 1	reward = -1.489734	array([[-7.45853 , -3.346984]], dtype=float32)

time = 68819	action = 1	current_phase = 1	next_phase = 0	reward = -0.882729	array([[-3.7637033, -2.2434092]], dtype=float32)

time = 68827	action = 0	current_phase = 0	next_phase = 1	reward = -0.074443	array([[-1.8057078, -3.0098367]], dtype=float32)

time = 68832	action = 0	current_phase = 0	next_phase = 1	reward = -0.012627	array([[-2.203099, -3.079603]], dtype=float32)

time = 68837	action = 0	current_phase = 0	next_phase = 1	reward = 0.071085	array([[-2.7391572, -3.5902452]], dtype=float32)

time = 68842	action = 1	current_phase = 0	next_phase = 1	reward = -1.696365	array([[-7.4098783, -3.572402 ]], dtype=float32)

time = 68850	action = 1	current_phase = 1	next_phase = 0	reward = -1.263446	array([[-3.881947 , -2.3567383]], dtype=float32)

time = 68858	action = 0	current_phase = 0	next_phase = 1	reward = 0.224145	array([[-1.5334513, -2.983122 ]], dtype=float32)

time = 68863	action = 0	current_phase = 0	next_phase = 1	reward = 0.002575	array([[-2.1901793, -3.0631166]], dtype=float32)

time = 68868	action = 0	current_phase = 0	next_phase = 1	reward = 0.075013	array([[-2.762325 , -3.6278157]], dtype=float32)

time = 68873	action = 1	current_phase = 0	next_phase = 1	reward = -1.799771	array([[-7.396955 , -3.6292412]], dtype=float32)

time = 68881	action = 1	current_phase = 1	next_phase = 0	reward = -0.946357	array([[-4.017811 , -2.3110309]], dtype=float32)

time = 68889	action = 0	current_phase = 0	next_phase = 1	reward = -0.038119	array([[-1.3416897, -2.518348 ]], dtype=float32)

time = 68894	action = 0	current_phase = 0	next_phase = 1	reward = 0.049552	array([[-1.7710276, -3.056937 ]], dtype=float32)

time = 68899	action = 1	current_phase = 0	next_phase = 1	reward = -0.646559	array([[-3.0388055, -2.2917075]], dtype=float32)

time = 68907	action = 1	current_phase = 1	next_phase = 0	reward = -0.665874	array([[-3.2929177, -1.9978805]], dtype=float32)

time = 68915	action = 0	current_phase = 0	next_phase = 1	reward = -0.411556	array([[-1.7435751, -3.233959 ]], dtype=float32)

time = 68920	action = 0	current_phase = 0	next_phase = 1	reward = -0.028405	array([[-1.6596215, -2.8324537]], dtype=float32)

time = 68925	action = 0	current_phase = 0	next_phase = 1	reward = 0.045941	array([[-2.2145283, -3.2049122]], dtype=float32)

time = 68930	action = 1	current_phase = 0	next_phase = 1	reward = -1.040718	array([[-6.388903 , -2.8652437]], dtype=float32)

time = 68938	action = 1	current_phase = 1	next_phase = 0	reward = -0.708304	array([[-3.5785446, -2.1087065]], dtype=float32)

time = 68946	action = 0	current_phase = 0	next_phase = 1	reward = -0.087372	array([[-1.7988851, -3.1196766]], dtype=float32)

time = 68951	action = 0	current_phase = 0	next_phase = 1	reward = 0.001448	array([[-2.1225126, -3.0975351]], dtype=float32)

time = 68956	action = 0	current_phase = 0	next_phase = 1	reward = 0.052810	array([[-2.6437495, -3.5810447]], dtype=float32)

time = 68961	action = 1	current_phase = 0	next_phase = 1	reward = -1.462151	array([[-7.4368925, -3.3329744]], dtype=float32)

time = 68969	action = 1	current_phase = 1	next_phase = 0	reward = -1.050915	array([[-3.749251 , -2.2151022]], dtype=float32)

time = 68977	action = 0	current_phase = 0	next_phase = 1	reward = 0.219183	array([[-1.6820323, -2.9970117]], dtype=float32)

time = 68982	action = 0	current_phase = 0	next_phase = 1	reward = 0.016581	array([[-2.1980221, -3.0917563]], dtype=float32)

time = 68987	action = 0	current_phase = 0	next_phase = 1	reward = 0.071691	array([[-2.8156464, -3.6785133]], dtype=float32)

time = 68992	action = 1	current_phase = 0	next_phase = 1	reward = -1.660510	array([[-7.422263 , -3.5452933]], dtype=float32)

time = 69000	action = 1	current_phase = 1	next_phase = 0	reward = -0.948477	array([[-3.8981028, -2.3695862]], dtype=float32)

time = 69008	action = 0	current_phase = 0	next_phase = 1	reward = -0.056538	array([[-1.7165   , -3.0540931]], dtype=float32)

time = 69013	action = 0	current_phase = 0	next_phase = 1	reward = 0.032091	array([[-2.1609702, -3.0592692]], dtype=float32)

time = 69018	action = 0	current_phase = 0	next_phase = 1	reward = 0.081988	array([[-2.7369754, -3.647202 ]], dtype=float32)

time = 69023	action = 1	current_phase = 0	next_phase = 1	reward = -1.353634	array([[-7.4645157, -3.2091794]], dtype=float32)

time = 69031	action = 1	current_phase = 1	next_phase = 0	reward = -1.564810	array([[-3.7732882, -2.4328482]], dtype=float32)

time = 69039	action = 0	current_phase = 0	next_phase = 1	reward = 0.231121	array([[-1.251803 , -2.6234558]], dtype=float32)

time = 69044	action = 0	current_phase = 0	next_phase = 1	reward = 0.026545	array([[-1.7722328, -3.1758056]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0166 - val_loss: 0.0089

Epoch 2/50

 - 3s - loss: 0.0150 - val_loss: 0.0096

Epoch 3/50

 - 3s - loss: 0.0165 - val_loss: 0.0090

Epoch 4/50

 - 3s - loss: 0.0203 - val_loss: 0.0107

Epoch 5/50

 - 3s - loss: 0.0143 - val_loss: 0.0077

Epoch 6/50

 - 3s - loss: 0.0139 - val_loss: 0.0146

Epoch 7/50

 - 3s - loss: 0.0151 - val_loss: 0.0086

Epoch 8/50

 - 3s - loss: 0.0176 - val_loss: 0.0105

Epoch 9/50

 - 3s - loss: 0.0164 - val_loss: 0.0094

Epoch 10/50

 - 3s - loss: 0.0154 - val_loss: 0.0105

Epoch 11/50

 - 3s - loss: 0.0151 - val_loss: 0.0081

Epoch 12/50

 - 3s - loss: 0.0145 - val_loss: 0.0120

Epoch 13/50

 - 3s - loss: 0.0131 - val_loss: 0.0100

Epoch 14/50

 - 3s - loss: 0.0139 - val_loss: 0.0082

Epoch 15/50

 - 3s - loss: 0.0126 - val_loss: 0.0093

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 69049	action = 1	current_phase = 0	next_phase = 1	reward = -1.246459	array([[-2.9047563, -2.2914498]], dtype=float32)

time = 69057	action = 1	current_phase = 1	next_phase = 0	reward = -0.833233	array([[-3.4092693, -2.0689163]], dtype=float32)

time = 69065	action = 0	current_phase = 0	next_phase = 1	reward = 0.158076	array([[-1.5076182, -3.130767 ]], dtype=float32)

time = 69070	action = 0	current_phase = 0	next_phase = 1	reward = -0.594060	array([[-1.9436765, -2.9662864]], dtype=float32)

time = 69075	action = 0	current_phase = 0	next_phase = 1	reward = 0.327366	array([[-1.9832451, -3.1813197]], dtype=float32)

time = 69080	action = 1	current_phase = 0	next_phase = 1	reward = -0.969536	array([[-6.324445 , -2.7520723]], dtype=float32)

time = 69088	action = 1	current_phase = 1	next_phase = 0	reward = -0.705979	array([[-3.5852065, -2.1164997]], dtype=float32)

time = 69096	action = 0	current_phase = 0	next_phase = 1	reward = -0.079578	array([[-1.7805879, -3.0922396]], dtype=float32)

time = 69101	action = 0	current_phase = 0	next_phase = 1	reward = 0.009099	array([[-2.0336733, -3.0848358]], dtype=float32)

time = 69106	action = 0	current_phase = 0	next_phase = 1	reward = 0.069067	array([[-2.5573251, -3.5511272]], dtype=float32)

time = 69111	action = 1	current_phase = 0	next_phase = 1	reward = -1.428553	array([[-7.4213424, -3.3116271]], dtype=float32)

time = 69119	action = 1	current_phase = 1	next_phase = 0	reward = -0.824333	array([[-3.7870655, -2.263768 ]], dtype=float32)

time = 69127	action = 0	current_phase = 0	next_phase = 1	reward = -0.070024	array([[-1.739068 , -3.0273035]], dtype=float32)

time = 69132	action = 0	current_phase = 0	next_phase = 1	reward = 0.005410	array([[-2.1506448, -3.0834816]], dtype=float32)

time = 69137	action = 0	current_phase = 0	next_phase = 1	reward = 0.062084	array([[-2.7602665, -3.6858697]], dtype=float32)

time = 69142	action = 1	current_phase = 0	next_phase = 1	reward = -1.704424	array([[-7.376172 , -3.5801759]], dtype=float32)

time = 69150	action = 1	current_phase = 1	next_phase = 0	reward = -0.964104	array([[-3.9188943, -2.3796048]], dtype=float32)

time = 69158	action = 0	current_phase = 0	next_phase = 1	reward = -0.060519	array([[-1.7388482, -3.067651 ]], dtype=float32)

time = 69163	action = 0	current_phase = 0	next_phase = 1	reward = 0.010465	array([[-2.0992737, -3.0588024]], dtype=float32)

time = 69168	action = 0	current_phase = 0	next_phase = 1	reward = 0.065563	array([[-2.6493711, -3.6028364]], dtype=float32)

time = 69173	action = 1	current_phase = 0	next_phase = 1	reward = -1.307181	array([[-7.3699765, -3.382949 ]], dtype=float32)

time = 69181	action = 1	current_phase = 1	next_phase = 0	reward = -1.386712	array([[-3.9182348, -2.4273803]], dtype=float32)

time = 69189	action = 0	current_phase = 0	next_phase = 1	reward = 0.243508	array([[-1.1198589, -2.5132825]], dtype=float32)

time = 69194	action = 0	current_phase = 0	next_phase = 1	reward = 0.049877	array([[-1.6673468, -3.1240456]], dtype=float32)

time = 69199	action = 1	current_phase = 0	next_phase = 1	reward = -0.654457	array([[-2.9677765, -2.392969 ]], dtype=float32)

time = 69207	action = 1	current_phase = 1	next_phase = 0	reward = -0.631496	array([[-3.3977861, -2.1211522]], dtype=float32)

time = 69215	action = 0	current_phase = 0	next_phase = 1	reward = -0.095355	array([[-1.8116375, -3.192696 ]], dtype=float32)

time = 69220	action = 0	current_phase = 0	next_phase = 1	reward = -0.025574	array([[-1.966704 , -3.1811635]], dtype=float32)

time = 69225	action = 0	current_phase = 0	next_phase = 1	reward = 0.053418	array([[-2.3246217, -3.3355522]], dtype=float32)

time = 69230	action = 1	current_phase = 0	next_phase = 1	reward = -1.308690	array([[-6.410098, -3.000225]], dtype=float32)

time = 69238	action = 1	current_phase = 1	next_phase = 0	reward = -0.701379	array([[-3.6218352, -2.1338098]], dtype=float32)

time = 69246	action = 0	current_phase = 0	next_phase = 1	reward = -0.081986	array([[-1.8308785, -3.1096764]], dtype=float32)

time = 69251	action = 0	current_phase = 0	next_phase = 1	reward = -0.004171	array([[-2.0783086, -3.0737708]], dtype=float32)

time = 69256	action = 0	current_phase = 0	next_phase = 1	reward = 0.062791	array([[-2.5861313, -3.5565598]], dtype=float32)

time = 69261	action = 1	current_phase = 0	next_phase = 1	reward = -1.442980	array([[-7.4263945, -3.2894306]], dtype=float32)

time = 69269	action = 1	current_phase = 1	next_phase = 0	reward = -0.792110	array([[-3.767838 , -2.2508864]], dtype=float32)

time = 69277	action = 0	current_phase = 0	next_phase = 1	reward = -0.074313	array([[-1.7351885, -3.0410118]], dtype=float32)

time = 69282	action = 0	current_phase = 0	next_phase = 1	reward = 0.004490	array([[-2.1383805, -3.0635064]], dtype=float32)

time = 69287	action = 0	current_phase = 0	next_phase = 1	reward = 0.073608	array([[-2.7134438, -3.6449015]], dtype=float32)

time = 69292	action = 1	current_phase = 0	next_phase = 1	reward = -1.590748	array([[-7.3814397, -3.5729423]], dtype=float32)

time = 69300	action = 1	current_phase = 1	next_phase = 0	reward = -0.996770	array([[-3.863227, -2.347971]], dtype=float32)

time = 69308	action = 0	current_phase = 0	next_phase = 1	reward = -0.059827	array([[-1.6806581, -3.016469 ]], dtype=float32)

time = 69313	action = 0	current_phase = 0	next_phase = 1	reward = 0.046808	array([[-2.1293507, -3.0717485]], dtype=float32)

time = 69318	action = 0	current_phase = 0	next_phase = 1	reward = 0.077320	array([[-2.7479906, -3.672718 ]], dtype=float32)

time = 69323	action = 1	current_phase = 0	next_phase = 1	reward = -1.999899	array([[-7.3598423, -3.6854465]], dtype=float32)

time = 69331	action = 1	current_phase = 1	next_phase = 0	reward = -1.095257	array([[-4.031196 , -2.3535995]], dtype=float32)

time = 69339	action = 0	current_phase = 0	next_phase = 1	reward = -0.036787	array([[-1.2933877, -2.4840286]], dtype=float32)

time = 69344	action = 0	current_phase = 0	next_phase = 1	reward = 0.034939	array([[-1.7120017, -3.1278846]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0196 - val_loss: 0.0083

Epoch 2/50

 - 3s - loss: 0.0146 - val_loss: 0.0092

Epoch 3/50

 - 3s - loss: 0.0164 - val_loss: 0.0083

Epoch 4/50

 - 3s - loss: 0.0173 - val_loss: 0.0086

Epoch 5/50

 - 3s - loss: 0.0156 - val_loss: 0.0083

Epoch 6/50

 - 3s - loss: 0.0223 - val_loss: 0.0107

Epoch 7/50

 - 3s - loss: 0.0149 - val_loss: 0.0089

Epoch 8/50

 - 3s - loss: 0.0166 - val_loss: 0.0081

Epoch 9/50

 - 3s - loss: 0.0163 - val_loss: 0.0084

Epoch 10/50

 - 3s - loss: 0.0145 - val_loss: 0.0085

Epoch 11/50

 - 3s - loss: 0.0183 - val_loss: 0.0095

Epoch 12/50

 - 3s - loss: 0.0157 - val_loss: 0.0084

Epoch 13/50

 - 3s - loss: 0.0191 - val_loss: 0.0096

Epoch 14/50

 - 3s - loss: 0.0158 - val_loss: 0.0105

Epoch 15/50

 - 3s - loss: 0.0144 - val_loss: 0.0101

Epoch 16/50

 - 3s - loss: 0.0123 - val_loss: 0.0095

Epoch 17/50

 - 3s - loss: 0.0177 - val_loss: 0.0108

Epoch 18/50

 - 3s - loss: 0.0136 - val_loss: 0.0105

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 69349	action = 1	current_phase = 0	next_phase = 1	reward = -0.640367	array([[-3.0271218, -2.351849 ]], dtype=float32)

time = 69357	action = 1	current_phase = 1	next_phase = 0	reward = -0.633752	array([[-3.3081117, -2.0275173]], dtype=float32)

time = 69365	action = 0	current_phase = 0	next_phase = 1	reward = -0.099438	array([[-1.6112237, -3.3442628]], dtype=float32)

time = 69370	action = 0	current_phase = 0	next_phase = 1	reward = -0.042628	array([[-1.8157272, -3.2603917]], dtype=float32)

time = 69375	action = 0	current_phase = 0	next_phase = 1	reward = -0.244823	array([[-2.3323035, -3.355567 ]], dtype=float32)

time = 69380	action = 1	current_phase = 0	next_phase = 1	reward = -1.025184	array([[-6.378644, -2.875012]], dtype=float32)

time = 69388	action = 1	current_phase = 1	next_phase = 0	reward = -0.703736	array([[-3.8140836, -2.156575 ]], dtype=float32)

time = 69396	action = 0	current_phase = 0	next_phase = 1	reward = -0.105397	array([[-1.7367462, -3.140008 ]], dtype=float32)

time = 69401	action = 0	current_phase = 0	next_phase = 1	reward = -0.030726	array([[-1.96362  , -3.1475804]], dtype=float32)

time = 69406	action = 0	current_phase = 0	next_phase = 1	reward = 0.040403	array([[-2.5078123, -3.5232244]], dtype=float32)

time = 69411	action = 1	current_phase = 0	next_phase = 1	reward = -1.422826	array([[-7.4573064, -3.2550266]], dtype=float32)

time = 69419	action = 1	current_phase = 1	next_phase = 0	reward = -0.754780	array([[-3.8325086, -2.2232099]], dtype=float32)

time = 69427	action = 0	current_phase = 0	next_phase = 1	reward = -0.062242	array([[-1.7478409, -3.1172585]], dtype=float32)

time = 69432	action = 0	current_phase = 0	next_phase = 1	reward = 0.003124	array([[-2.124951, -3.100255]], dtype=float32)

time = 69437	action = 0	current_phase = 0	next_phase = 1	reward = 0.065387	array([[-2.7609632, -3.6261628]], dtype=float32)

time = 69442	action = 1	current_phase = 0	next_phase = 1	reward = -1.591916	array([[-7.4164076, -3.4845989]], dtype=float32)

time = 69450	action = 1	current_phase = 1	next_phase = 0	reward = -0.937130	array([[-3.9337292, -2.338945 ]], dtype=float32)

time = 69458	action = 0	current_phase = 0	next_phase = 1	reward = -0.042732	array([[-1.687311 , -3.0706348]], dtype=float32)

time = 69463	action = 0	current_phase = 0	next_phase = 1	reward = 0.037186	array([[-2.1166487, -3.0856683]], dtype=float32)

time = 69468	action = 0	current_phase = 0	next_phase = 1	reward = 0.073386	array([[-2.737462 , -3.6407864]], dtype=float32)

time = 69473	action = 1	current_phase = 0	next_phase = 1	reward = -1.405700	array([[-7.4065166, -3.3048656]], dtype=float32)

time = 69481	action = 1	current_phase = 1	next_phase = 0	reward = -1.192085	array([[-3.8979645, -2.2747679]], dtype=float32)

time = 69489	action = 0	current_phase = 0	next_phase = 1	reward = -0.049062	array([[-1.2902787, -2.6292264]], dtype=float32)

time = 69494	action = 0	current_phase = 0	next_phase = 1	reward = 0.019065	array([[-1.7893289, -3.1328936]], dtype=float32)

time = 69499	action = 1	current_phase = 0	next_phase = 1	reward = -0.668343	array([[-3.0161319, -2.3834572]], dtype=float32)

time = 69507	action = 1	current_phase = 1	next_phase = 0	reward = -0.871311	array([[-3.352634, -2.042338]], dtype=float32)

time = 69515	action = 0	current_phase = 0	next_phase = 1	reward = 0.174029	array([[-1.34906 , -3.284404]], dtype=float32)

time = 69520	action = 0	current_phase = 0	next_phase = 1	reward = -0.029345	array([[-1.835113 , -3.0478604]], dtype=float32)

time = 69525	action = 0	current_phase = 0	next_phase = 1	reward = 0.036699	array([[-2.2242393, -3.2517962]], dtype=float32)

time = 69530	action = 1	current_phase = 0	next_phase = 1	reward = -1.365115	array([[-6.415169 , -3.0724885]], dtype=float32)

time = 69538	action = 1	current_phase = 1	next_phase = 0	reward = -0.701231	array([[-3.789301 , -2.1774569]], dtype=float32)

time = 69546	action = 0	current_phase = 0	next_phase = 1	reward = -0.077853	array([[-1.7207494, -3.1676128]], dtype=float32)

time = 69551	action = 0	current_phase = 0	next_phase = 1	reward = -0.011317	array([[-1.9683359, -3.1815493]], dtype=float32)

time = 69556	action = 0	current_phase = 0	next_phase = 1	reward = 0.055227	array([[-2.604878 , -3.5565975]], dtype=float32)

time = 69561	action = 1	current_phase = 0	next_phase = 1	reward = -1.558272	array([[-7.4291863, -3.3248105]], dtype=float32)

time = 69569	action = 1	current_phase = 1	next_phase = 0	reward = -0.828871	array([[-3.832868 , -2.2263193]], dtype=float32)

time = 69577	action = 0	current_phase = 0	next_phase = 1	reward = -0.074584	array([[-1.7821612, -3.0581484]], dtype=float32)

time = 69582	action = 0	current_phase = 0	next_phase = 1	reward = 0.004630	array([[-2.1704886, -3.0929432]], dtype=float32)

time = 69587	action = 0	current_phase = 0	next_phase = 1	reward = 0.081258	array([[-2.7894635, -3.6495268]], dtype=float32)

time = 69592	action = 1	current_phase = 0	next_phase = 1	reward = -1.582904	array([[-7.39258  , -3.5320706]], dtype=float32)

time = 69600	action = 1	current_phase = 1	next_phase = 0	reward = -0.957219	array([[-3.941184, -2.306315]], dtype=float32)

time = 69608	action = 0	current_phase = 0	next_phase = 1	reward = -0.059658	array([[-1.5938601, -3.0726495]], dtype=float32)

time = 69613	action = 0	current_phase = 0	next_phase = 1	reward = 0.004965	array([[-2.073011 , -3.0813363]], dtype=float32)

time = 69618	action = 0	current_phase = 0	next_phase = 1	reward = 0.081266	array([[-2.7360857, -3.5854967]], dtype=float32)

time = 69623	action = 1	current_phase = 0	next_phase = 1	reward = -1.254091	array([[-7.3696194, -3.2263238]], dtype=float32)

time = 69631	action = 1	current_phase = 1	next_phase = 0	reward = -1.553119	array([[-3.9626803, -2.317247 ]], dtype=float32)

time = 69639	action = 0	current_phase = 0	next_phase = 1	reward = 0.245328	array([[-1.1425253, -2.5909865]], dtype=float32)

time = 69644	action = 0	current_phase = 0	next_phase = 1	reward = 0.031003	array([[-1.7277689, -3.1240723]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0125 - val_loss: 0.0073

Epoch 2/50

 - 3s - loss: 0.0138 - val_loss: 0.0070

Epoch 3/50

 - 3s - loss: 0.0137 - val_loss: 0.0084

Epoch 4/50

 - 3s - loss: 0.0139 - val_loss: 0.0085

Epoch 5/50

 - 3s - loss: 0.0174 - val_loss: 0.0081

Epoch 6/50

 - 3s - loss: 0.0136 - val_loss: 0.0073

Epoch 7/50

 - 3s - loss: 0.0135 - val_loss: 0.0070

Epoch 8/50

 - 3s - loss: 0.0131 - val_loss: 0.0062

Epoch 9/50

 - 3s - loss: 0.0156 - val_loss: 0.0070

Epoch 10/50

 - 3s - loss: 0.0125 - val_loss: 0.0082

Epoch 11/50

 - 3s - loss: 0.0138 - val_loss: 0.0070

Epoch 12/50

 - 3s - loss: 0.0117 - val_loss: 0.0070

Epoch 13/50

 - 3s - loss: 0.0118 - val_loss: 0.0073

Epoch 14/50

 - 3s - loss: 0.0134 - val_loss: 0.0075

Epoch 15/50

 - 4s - loss: 0.0103 - val_loss: 0.0082

Epoch 16/50

 - 4s - loss: 0.0133 - val_loss: 0.0079

Epoch 17/50

 - 4s - loss: 0.0127 - val_loss: 0.0084

Epoch 18/50

 - 3s - loss: 0.0117 - val_loss: 0.0080

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 69649	action = 1	current_phase = 0	next_phase = 1	reward = -0.715661	array([[-2.9643247, -2.387334 ]], dtype=float32)

time = 69657	action = 1	current_phase = 1	next_phase = 0	reward = -1.200515	array([[-3.521853 , -2.1468418]], dtype=float32)

time = 69665	action = 0	current_phase = 0	next_phase = 1	reward = 0.180532	array([[-1.1490703, -3.2625124]], dtype=float32)

time = 69670	action = 0	current_phase = 0	next_phase = 1	reward = 0.245724	array([[-1.6032317, -3.248912 ]], dtype=float32)

time = 69675	action = 0	current_phase = 0	next_phase = 1	reward = -0.248500	array([[-2.3779511, -3.3463755]], dtype=float32)

time = 69680	action = 1	current_phase = 0	next_phase = 1	reward = -1.103719	array([[-6.37938 , -2.792914]], dtype=float32)

time = 69688	action = 1	current_phase = 1	next_phase = 0	reward = -0.703701	array([[-3.796013 , -2.1867485]], dtype=float32)

time = 69696	action = 0	current_phase = 0	next_phase = 1	reward = -0.089027	array([[-1.6403023, -3.1965928]], dtype=float32)

time = 69701	action = 0	current_phase = 0	next_phase = 1	reward = -0.014022	array([[-1.8921305, -3.238988 ]], dtype=float32)

time = 69706	action = 0	current_phase = 0	next_phase = 1	reward = 0.053589	array([[-2.5712578, -3.5216806]], dtype=float32)

time = 69711	action = 1	current_phase = 0	next_phase = 1	reward = -1.389763	array([[-7.431461 , -3.2773304]], dtype=float32)

time = 69719	action = 1	current_phase = 1	next_phase = 0	reward = -0.826966	array([[-3.8334572, -2.2263808]], dtype=float32)

time = 69727	action = 0	current_phase = 0	next_phase = 1	reward = -0.074139	array([[-1.7054973, -3.0633192]], dtype=float32)

time = 69732	action = 0	current_phase = 0	next_phase = 1	reward = 0.010647	array([[-2.1355057, -3.0862534]], dtype=float32)

time = 69737	action = 0	current_phase = 0	next_phase = 1	reward = 0.068648	array([[-2.7823613, -3.6744444]], dtype=float32)

time = 69742	action = 1	current_phase = 0	next_phase = 1	reward = -1.645527	array([[-7.3927274, -3.456967 ]], dtype=float32)

time = 69750	action = 1	current_phase = 1	next_phase = 0	reward = -1.001368	array([[-3.9742932, -2.362313 ]], dtype=float32)

time = 69758	action = 0	current_phase = 0	next_phase = 1	reward = -0.053797	array([[-1.6697617, -3.0319514]], dtype=float32)

time = 69763	action = 0	current_phase = 0	next_phase = 1	reward = 0.030471	array([[-2.146051 , -3.0746205]], dtype=float32)

time = 69768	action = 0	current_phase = 0	next_phase = 1	reward = 0.073592	array([[-2.7684064, -3.6583045]], dtype=float32)

time = 69773	action = 1	current_phase = 0	next_phase = 1	reward = -1.809536	array([[-7.3574424, -3.6096768]], dtype=float32)

time = 69781	action = 1	current_phase = 1	next_phase = 0	reward = -1.387433	array([[-4.079662, -2.163641]], dtype=float32)

time = 69789	action = 0	current_phase = 0	next_phase = 1	reward = 0.262866	array([[-1.1440853, -2.5796316]], dtype=float32)

time = 69794	action = 0	current_phase = 0	next_phase = 1	reward = 0.042312	array([[-1.7856131, -3.151121 ]], dtype=float32)

time = 69799	action = 1	current_phase = 0	next_phase = 1	reward = -0.693656	array([[-3.0519679, -2.2794359]], dtype=float32)

time = 69807	action = 1	current_phase = 1	next_phase = 0	reward = -0.868175	array([[-3.4305317, -2.05463  ]], dtype=float32)

time = 69815	action = 0	current_phase = 0	next_phase = 1	reward = 0.174775	array([[-1.3209348, -3.3007596]], dtype=float32)

time = 69820	action = 0	current_phase = 0	next_phase = 1	reward = -0.298963	array([[-1.8282809, -3.424068 ]], dtype=float32)

time = 69825	action = 0	current_phase = 0	next_phase = 1	reward = 0.337325	array([[-2.2867203, -3.3440201]], dtype=float32)

time = 69830	action = 1	current_phase = 0	next_phase = 1	reward = -1.325271	array([[-6.412957 , -2.9875786]], dtype=float32)

time = 69838	action = 1	current_phase = 1	next_phase = 0	reward = -0.701635	array([[-3.63507  , -2.1473932]], dtype=float32)

time = 69846	action = 0	current_phase = 0	next_phase = 1	reward = -0.081938	array([[-1.6889687, -3.06529  ]], dtype=float32)

time = 69851	action = 0	current_phase = 0	next_phase = 1	reward = 0.000629	array([[-2.0131972, -3.1233578]], dtype=float32)

time = 69856	action = 0	current_phase = 0	next_phase = 1	reward = 0.078418	array([[-2.6261635, -3.5220752]], dtype=float32)

time = 69861	action = 1	current_phase = 0	next_phase = 1	reward = -1.447899	array([[-7.4164433, -3.2372608]], dtype=float32)

time = 69869	action = 1	current_phase = 1	next_phase = 0	reward = -0.837912	array([[-3.79575  , -2.1967497]], dtype=float32)

time = 69877	action = 0	current_phase = 0	next_phase = 1	reward = -0.081976	array([[-1.6893673, -3.074839 ]], dtype=float32)

time = 69882	action = 0	current_phase = 0	next_phase = 1	reward = 0.001524	array([[-2.138433 , -3.0984612]], dtype=float32)

time = 69887	action = 0	current_phase = 0	next_phase = 1	reward = 0.063787	array([[-2.7832818, -3.6820219]], dtype=float32)

time = 69892	action = 1	current_phase = 0	next_phase = 1	reward = -1.611856	array([[-7.3825545, -3.4561055]], dtype=float32)

time = 69900	action = 1	current_phase = 1	next_phase = 0	reward = -0.942324	array([[-3.9550595, -2.350205 ]], dtype=float32)

time = 69908	action = 0	current_phase = 0	next_phase = 1	reward = -0.044742	array([[-1.5767953, -3.1013563]], dtype=float32)

time = 69913	action = 0	current_phase = 0	next_phase = 1	reward = 0.017688	array([[-2.0949445, -3.1004512]], dtype=float32)

time = 69918	action = 0	current_phase = 0	next_phase = 1	reward = 0.081440	array([[-2.7548203, -3.6410491]], dtype=float32)

time = 69923	action = 1	current_phase = 0	next_phase = 1	reward = -1.871957	array([[-7.3628173, -3.6211002]], dtype=float32)

time = 69931	action = 1	current_phase = 1	next_phase = 0	reward = -1.078653	array([[-4.0659294, -2.147625 ]], dtype=float32)

time = 69939	action = 0	current_phase = 0	next_phase = 1	reward = -0.030734	array([[-1.2556031, -2.6083565]], dtype=float32)

time = 69944	action = 0	current_phase = 0	next_phase = 1	reward = 0.028350	array([[-1.7942685, -3.1373718]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0175 - val_loss: 0.0122

Epoch 2/50

 - 3s - loss: 0.0182 - val_loss: 0.0117

Epoch 3/50

 - 3s - loss: 0.0165 - val_loss: 0.0136

Epoch 4/50

 - 3s - loss: 0.0147 - val_loss: 0.0130

Epoch 5/50

 - 3s - loss: 0.0192 - val_loss: 0.0141

Epoch 6/50

 - 3s - loss: 0.0157 - val_loss: 0.0122

Epoch 7/50

 - 3s - loss: 0.0142 - val_loss: 0.0136

Epoch 8/50

 - 3s - loss: 0.0130 - val_loss: 0.0129

Epoch 9/50

 - 3s - loss: 0.0133 - val_loss: 0.0160

Epoch 10/50

 - 3s - loss: 0.0139 - val_loss: 0.0110

Epoch 11/50

 - 3s - loss: 0.0152 - val_loss: 0.0113

Epoch 12/50

 - 3s - loss: 0.0151 - val_loss: 0.0117

Epoch 13/50

 - 3s - loss: 0.0128 - val_loss: 0.0124

Epoch 14/50

 - 3s - loss: 0.0129 - val_loss: 0.0125

Epoch 15/50

 - 3s - loss: 0.0173 - val_loss: 0.0129

Epoch 16/50

 - 3s - loss: 0.0116 - val_loss: 0.0152

Epoch 17/50

 - 3s - loss: 0.0122 - val_loss: 0.0129

Epoch 18/50

 - 3s - loss: 0.0113 - val_loss: 0.0111

Epoch 19/50

 - 3s - loss: 0.0124 - val_loss: 0.0137

Epoch 20/50

 - 3s - loss: 0.0146 - val_loss: 0.0125

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 69949	action = 1	current_phase = 0	next_phase = 1	reward = -0.632617	array([[-3.0656023, -2.311631 ]], dtype=float32)

time = 69957	action = 1	current_phase = 1	next_phase = 0	reward = -0.539455	array([[-3.325562, -2.05797 ]], dtype=float32)

time = 69965	action = 0	current_phase = 0	next_phase = 1	reward = -0.649887	array([[-1.6912804, -3.3064086]], dtype=float32)

time = 69970	action = 0	current_phase = 0	next_phase = 1	reward = 0.544877	array([[-1.5060992, -3.0735826]], dtype=float32)

time = 69975	action = 0	current_phase = 0	next_phase = 1	reward = 0.040067	array([[-2.4132  , -3.366928]], dtype=float32)

time = 69980	action = 1	current_phase = 0	next_phase = 1	reward = -1.318001	array([[-6.4214277, -3.0044894]], dtype=float32)

time = 69988	action = 1	current_phase = 1	next_phase = 0	reward = -0.703067	array([[-3.6280026, -2.1443708]], dtype=float32)

time = 69996	action = 0	current_phase = 0	next_phase = 1	reward = -0.063341	array([[-1.6412914, -3.0795038]], dtype=float32)

time = 70001	action = 0	current_phase = 0	next_phase = 1	reward = 0.030440	array([[-2.0426593, -3.1317332]], dtype=float32)

time = 70006	action = 0	current_phase = 0	next_phase = 1	reward = 0.078681	array([[-2.5846431, -3.5470061]], dtype=float32)

time = 70011	action = 1	current_phase = 0	next_phase = 1	reward = -1.524086	array([[-7.4103236, -3.3263671]], dtype=float32)

time = 70019	action = 1	current_phase = 1	next_phase = 0	reward = -0.848816	array([[-3.8243346, -2.2321804]], dtype=float32)

time = 70027	action = 0	current_phase = 0	next_phase = 1	reward = -0.066915	array([[-1.6780132, -3.0976865]], dtype=float32)

time = 70032	action = 0	current_phase = 0	next_phase = 1	reward = 0.004188	array([[-2.158548 , -3.1113913]], dtype=float32)

time = 70037	action = 0	current_phase = 0	next_phase = 1	reward = 0.064239	array([[-2.7778387, -3.6666186]], dtype=float32)

time = 70042	action = 1	current_phase = 0	next_phase = 1	reward = -1.537791	array([[-7.391509 , -3.4422328]], dtype=float32)

time = 70050	action = 1	current_phase = 1	next_phase = 0	reward = -0.985340	array([[-3.9285703, -2.3126829]], dtype=float32)

time = 70058	action = 0	current_phase = 0	next_phase = 1	reward = -0.055803	array([[-1.6627996, -3.0988507]], dtype=float32)

time = 70063	action = 0	current_phase = 0	next_phase = 1	reward = 0.017463	array([[-2.1453626, -3.0815182]], dtype=float32)

time = 70068	action = 0	current_phase = 0	next_phase = 1	reward = 0.062980	array([[-2.7007365, -3.6179636]], dtype=float32)

time = 70073	action = 1	current_phase = 0	next_phase = 1	reward = -1.374512	array([[-7.3954015, -3.319986 ]], dtype=float32)

time = 70081	action = 1	current_phase = 1	next_phase = 0	reward = -1.342061	array([[-3.848065 , -2.3713899]], dtype=float32)

time = 70089	action = 0	current_phase = 0	next_phase = 1	reward = 0.247441	array([[-1.1062491, -2.6058836]], dtype=float32)

time = 70094	action = 0	current_phase = 0	next_phase = 1	reward = 0.028669	array([[-1.7656308, -3.268262 ]], dtype=float32)

time = 70099	action = 1	current_phase = 0	next_phase = 1	reward = -0.705101	array([[-3.0149941, -2.3109844]], dtype=float32)

time = 70107	action = 1	current_phase = 1	next_phase = 0	reward = -0.933451	array([[-3.3816366, -2.0879285]], dtype=float32)

time = 70115	action = 0	current_phase = 0	next_phase = 1	reward = -0.369560	array([[-1.3730791, -3.2279823]], dtype=float32)

time = 70120	action = 0	current_phase = 0	next_phase = 1	reward = 0.541264	array([[-1.5509145, -3.431333 ]], dtype=float32)

time = 70125	action = 0	current_phase = 0	next_phase = 1	reward = 0.047072	array([[-2.3433862, -3.3223948]], dtype=float32)

time = 70130	action = 1	current_phase = 0	next_phase = 1	reward = -1.311233	array([[-6.434979 , -2.9358885]], dtype=float32)

time = 70138	action = 1	current_phase = 1	next_phase = 0	reward = -0.708510	array([[-3.7781482, -2.158625 ]], dtype=float32)

time = 70146	action = 0	current_phase = 0	next_phase = 1	reward = -0.084912	array([[-1.6872075, -3.175244 ]], dtype=float32)

time = 70151	action = 0	current_phase = 0	next_phase = 1	reward = -0.001590	array([[-2.0470266, -3.1275284]], dtype=float32)

time = 70156	action = 0	current_phase = 0	next_phase = 1	reward = 0.065673	array([[-2.5805845, -3.539629 ]], dtype=float32)

time = 70161	action = 1	current_phase = 0	next_phase = 1	reward = -1.386153	array([[-7.423808 , -3.3076904]], dtype=float32)

time = 70169	action = 1	current_phase = 1	next_phase = 0	reward = -0.844729	array([[-3.8672009, -2.2500935]], dtype=float32)

time = 70177	action = 0	current_phase = 0	next_phase = 1	reward = -0.072978	array([[-1.7809285, -3.0742323]], dtype=float32)

time = 70182	action = 0	current_phase = 0	next_phase = 1	reward = 0.004865	array([[-2.1489773, -3.095978 ]], dtype=float32)

time = 70187	action = 0	current_phase = 0	next_phase = 1	reward = 0.068808	array([[-2.7651854, -3.6727664]], dtype=float32)

time = 70192	action = 1	current_phase = 0	next_phase = 1	reward = -1.481489	array([[-7.396032 , -3.5151598]], dtype=float32)

time = 70200	action = 1	current_phase = 1	next_phase = 0	reward = -0.946930	array([[-3.8949413, -2.2971497]], dtype=float32)

time = 70208	action = 0	current_phase = 0	next_phase = 1	reward = -0.045496	array([[-1.6455423, -3.146968 ]], dtype=float32)

time = 70213	action = 0	current_phase = 0	next_phase = 1	reward = 0.026005	array([[-2.1582375, -3.0819476]], dtype=float32)

time = 70218	action = 0	current_phase = 0	next_phase = 1	reward = 0.076787	array([[-2.7690935, -3.6748195]], dtype=float32)

time = 70223	action = 1	current_phase = 0	next_phase = 1	reward = -1.817597	array([[-7.352362 , -3.6801646]], dtype=float32)

time = 70231	action = 1	current_phase = 1	next_phase = 0	reward = -1.077020	array([[-4.0661216, -2.206635 ]], dtype=float32)

time = 70239	action = 0	current_phase = 0	next_phase = 1	reward = -0.032943	array([[-1.303055 , -2.7015994]], dtype=float32)

time = 70244	action = 0	current_phase = 0	next_phase = 1	reward = 0.028431	array([[-1.7702101, -3.1256778]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0149 - val_loss: 0.0090

Epoch 2/50

 - 3s - loss: 0.0153 - val_loss: 0.0135

Epoch 3/50

 - 3s - loss: 0.0125 - val_loss: 0.0079

Epoch 4/50

 - 3s - loss: 0.0163 - val_loss: 0.0104

Epoch 5/50

 - 3s - loss: 0.0133 - val_loss: 0.0113

Epoch 6/50

 - 3s - loss: 0.0145 - val_loss: 0.0105

Epoch 7/50

 - 3s - loss: 0.0120 - val_loss: 0.0103

Epoch 8/50

 - 3s - loss: 0.0168 - val_loss: 0.0088

Epoch 9/50

 - 3s - loss: 0.0145 - val_loss: 0.0107

Epoch 10/50

 - 3s - loss: 0.0150 - val_loss: 0.0090

Epoch 11/50

 - 3s - loss: 0.0136 - val_loss: 0.0116

Epoch 12/50

 - 3s - loss: 0.0134 - val_loss: 0.0123

Epoch 13/50

 - 3s - loss: 0.0137 - val_loss: 0.0123

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 70249	action = 1	current_phase = 0	next_phase = 1	reward = -0.696427	array([[-3.0845313, -2.2857447]], dtype=float32)

time = 70257	action = 1	current_phase = 1	next_phase = 0	reward = -0.662365	array([[-3.3920155, -2.0948987]], dtype=float32)

time = 70265	action = 0	current_phase = 0	next_phase = 1	reward = -0.386173	array([[-1.7341394, -3.2245486]], dtype=float32)

time = 70270	action = 0	current_phase = 0	next_phase = 1	reward = -0.286065	array([[-1.6024709, -3.2220435]], dtype=float32)

time = 70275	action = 0	current_phase = 0	next_phase = 1	reward = 0.620020	array([[-2.0107512, -3.1576104]], dtype=float32)

time = 70280	action = 1	current_phase = 0	next_phase = 1	reward = -1.371203	array([[-6.4437275, -2.9356356]], dtype=float32)

time = 70288	action = 1	current_phase = 1	next_phase = 0	reward = -0.718557	array([[-3.6800508, -2.1411312]], dtype=float32)

time = 70296	action = 0	current_phase = 0	next_phase = 1	reward = -0.071988	array([[-1.8076682, -3.1737516]], dtype=float32)

time = 70301	action = 0	current_phase = 0	next_phase = 1	reward = -0.013763	array([[-2.0725765, -3.0699878]], dtype=float32)

time = 70306	action = 0	current_phase = 0	next_phase = 1	reward = 0.050726	array([[-2.6102757, -3.5146213]], dtype=float32)

time = 70311	action = 1	current_phase = 0	next_phase = 1	reward = -1.457143	array([[-7.4430633, -3.2698412]], dtype=float32)

time = 70319	action = 1	current_phase = 1	next_phase = 0	reward = -0.837467	array([[-3.76864  , -2.2189894]], dtype=float32)

time = 70327	action = 0	current_phase = 0	next_phase = 1	reward = -0.077022	array([[-1.8004746, -3.0489502]], dtype=float32)

time = 70332	action = 0	current_phase = 0	next_phase = 1	reward = -0.011730	array([[-2.145976 , -3.0526202]], dtype=float32)

time = 70337	action = 0	current_phase = 0	next_phase = 1	reward = 0.057848	array([[-2.7665076, -3.5789866]], dtype=float32)

time = 70342	action = 1	current_phase = 0	next_phase = 1	reward = -1.645574	array([[-7.3975883, -3.5259793]], dtype=float32)

time = 70350	action = 1	current_phase = 1	next_phase = 0	reward = -0.971938	array([[-3.8866887, -2.3038518]], dtype=float32)

time = 70358	action = 0	current_phase = 0	next_phase = 1	reward = -0.051491	array([[-1.6611711, -3.02849  ]], dtype=float32)

time = 70363	action = 0	current_phase = 0	next_phase = 1	reward = 0.010125	array([[-2.1524491, -3.0494263]], dtype=float32)

time = 70368	action = 0	current_phase = 0	next_phase = 1	reward = 0.076766	array([[-2.7713394, -3.5854354]], dtype=float32)

time = 70373	action = 1	current_phase = 0	next_phase = 1	reward = -1.807935	array([[-7.370459 , -3.6216035]], dtype=float32)

time = 70381	action = 1	current_phase = 1	next_phase = 0	reward = -1.026713	array([[-4.0163608, -2.1995044]], dtype=float32)

time = 70389	action = 0	current_phase = 0	next_phase = 1	reward = -0.027321	array([[-1.3185837, -2.5970738]], dtype=float32)

time = 70394	action = 0	current_phase = 0	next_phase = 1	reward = 0.040660	array([[-1.9591103, -3.033513 ]], dtype=float32)

time = 70399	action = 1	current_phase = 0	next_phase = 1	reward = -0.691302	array([[-3.0839195, -2.325243 ]], dtype=float32)

time = 70407	action = 1	current_phase = 1	next_phase = 0	reward = -0.870649	array([[-3.3297539, -2.087575 ]], dtype=float32)

time = 70415	action = 0	current_phase = 0	next_phase = 1	reward = -0.115179	array([[-1.4198023, -3.2541857]], dtype=float32)

time = 70420	action = 0	current_phase = 0	next_phase = 1	reward = -0.024492	array([[-1.6535604, -2.8908048]], dtype=float32)

time = 70425	action = 0	current_phase = 0	next_phase = 1	reward = 0.327977	array([[-2.2114925, -3.175916 ]], dtype=float32)

time = 70430	action = 1	current_phase = 0	next_phase = 1	reward = -1.350992	array([[-6.4420295, -2.9675767]], dtype=float32)

time = 70438	action = 1	current_phase = 1	next_phase = 0	reward = -0.711466	array([[-3.688939 , -2.1598797]], dtype=float32)

time = 70446	action = 0	current_phase = 0	next_phase = 1	reward = -0.068902	array([[-1.7064989, -3.1416082]], dtype=float32)

time = 70451	action = 0	current_phase = 0	next_phase = 1	reward = -0.005301	array([[-2.0261989, -3.108211 ]], dtype=float32)

time = 70456	action = 0	current_phase = 0	next_phase = 1	reward = 0.061325	array([[-2.6584244, -3.4834301]], dtype=float32)

time = 70461	action = 1	current_phase = 0	next_phase = 1	reward = -1.506933	array([[-7.4422913, -3.274854 ]], dtype=float32)

time = 70469	action = 1	current_phase = 1	next_phase = 0	reward = -0.852160	array([[-3.7437148, -2.203489 ]], dtype=float32)

time = 70477	action = 0	current_phase = 0	next_phase = 1	reward = -0.075652	array([[-1.6954592, -3.025112 ]], dtype=float32)

time = 70482	action = 0	current_phase = 0	next_phase = 1	reward = 0.000236	array([[-2.1664681, -3.057362 ]], dtype=float32)

time = 70487	action = 0	current_phase = 0	next_phase = 1	reward = 0.072619	array([[-2.7877278, -3.6031308]], dtype=float32)

time = 70492	action = 1	current_phase = 0	next_phase = 1	reward = -1.564048	array([[-7.3852696, -3.5450234]], dtype=float32)

time = 70500	action = 1	current_phase = 1	next_phase = 0	reward = -0.960353	array([[-3.8622236, -2.3005772]], dtype=float32)

time = 70508	action = 0	current_phase = 0	next_phase = 1	reward = -0.050752	array([[-1.72491 , -3.030823]], dtype=float32)

time = 70513	action = 0	current_phase = 0	next_phase = 1	reward = 0.019796	array([[-2.1473765, -3.044469 ]], dtype=float32)

time = 70518	action = 0	current_phase = 0	next_phase = 1	reward = 0.086420	array([[-2.7404294, -3.5615242]], dtype=float32)

time = 70523	action = 1	current_phase = 0	next_phase = 1	reward = -0.766673	array([[-7.393926, -3.057617]], dtype=float32)

time = 70531	action = 1	current_phase = 1	next_phase = 0	reward = -1.268486	array([[-3.6906853, -2.35845  ]], dtype=float32)

time = 70539	action = 0	current_phase = 0	next_phase = 1	reward = -0.037224	array([[-1.413945, -2.669674]], dtype=float32)

time = 70544	action = 0	current_phase = 0	next_phase = 1	reward = 0.053442	array([[-1.808595 , -3.1414373]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0143 - val_loss: 0.0063

Epoch 2/50

 - 3s - loss: 0.0149 - val_loss: 0.0060

Epoch 3/50

 - 3s - loss: 0.0154 - val_loss: 0.0076

Epoch 4/50

 - 3s - loss: 0.0122 - val_loss: 0.0075

Epoch 5/50

 - 3s - loss: 0.0129 - val_loss: 0.0069

Epoch 6/50

 - 3s - loss: 0.0149 - val_loss: 0.0062

Epoch 7/50

 - 3s - loss: 0.0119 - val_loss: 0.0060

Epoch 8/50

 - 3s - loss: 0.0110 - val_loss: 0.0063

Epoch 9/50

 - 3s - loss: 0.0158 - val_loss: 0.0072

Epoch 10/50

 - 3s - loss: 0.0116 - val_loss: 0.0063

Epoch 11/50

 - 3s - loss: 0.0132 - val_loss: 0.0057

Epoch 12/50

 - 3s - loss: 0.0124 - val_loss: 0.0061

Epoch 13/50

 - 3s - loss: 0.0112 - val_loss: 0.0075

Epoch 14/50

 - 3s - loss: 0.0115 - val_loss: 0.0058

Epoch 15/50

 - 3s - loss: 0.0120 - val_loss: 0.0058

Epoch 16/50

 - 3s - loss: 0.0152 - val_loss: 0.0054

Epoch 17/50

 - 3s - loss: 0.0115 - val_loss: 0.0071

Epoch 18/50

 - 3s - loss: 0.0117 - val_loss: 0.0054

Epoch 19/50

 - 3s - loss: 0.0138 - val_loss: 0.0070

Epoch 20/50

 - 3s - loss: 0.0108 - val_loss: 0.0082

Epoch 21/50

 - 3s - loss: 0.0100 - val_loss: 0.0060

Epoch 22/50

 - 3s - loss: 0.0100 - val_loss: 0.0079

Epoch 23/50

 - 3s - loss: 0.0110 - val_loss: 0.0092

Epoch 24/50

 - 3s - loss: 0.0138 - val_loss: 0.0070

Epoch 25/50

 - 3s - loss: 0.0113 - val_loss: 0.0085

Epoch 26/50

 - 3s - loss: 0.0112 - val_loss: 0.0081

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 70549	action = 1	current_phase = 0	next_phase = 1	reward = -0.638271	array([[-3.0300608, -2.4869723]], dtype=float32)

time = 70557	action = 1	current_phase = 1	next_phase = 0	reward = -0.935498	array([[-3.31334  , -2.0675745]], dtype=float32)

time = 70565	action = 0	current_phase = 0	next_phase = 1	reward = -0.105508	array([[-1.3854026, -3.136744 ]], dtype=float32)

time = 70570	action = 0	current_phase = 0	next_phase = 1	reward = -0.016608	array([[-1.5419421, -3.2238164]], dtype=float32)

time = 70575	action = 0	current_phase = 0	next_phase = 1	reward = 0.331763	array([[-2.1849694, -3.252409 ]], dtype=float32)

time = 70580	action = 1	current_phase = 0	next_phase = 1	reward = -1.409071	array([[-6.5332303, -3.1190553]], dtype=float32)

time = 70588	action = 1	current_phase = 1	next_phase = 0	reward = -0.725109	array([[-3.774951 , -2.1524103]], dtype=float32)

time = 70596	action = 0	current_phase = 0	next_phase = 1	reward = -0.078819	array([[-1.6616063, -3.0929446]], dtype=float32)

time = 70601	action = 0	current_phase = 0	next_phase = 1	reward = -0.001072	array([[-2.1107054, -3.0680544]], dtype=float32)

time = 70606	action = 0	current_phase = 0	next_phase = 1	reward = 0.066263	array([[-2.6329226, -3.5474195]], dtype=float32)

time = 70611	action = 1	current_phase = 0	next_phase = 1	reward = -1.377644	array([[-7.4815965, -3.2226374]], dtype=float32)

time = 70619	action = 1	current_phase = 1	next_phase = 0	reward = -0.844932	array([[-3.803472, -2.240071]], dtype=float32)

time = 70627	action = 0	current_phase = 0	next_phase = 1	reward = -0.062430	array([[-1.6960927, -3.0366802]], dtype=float32)

time = 70632	action = 0	current_phase = 0	next_phase = 1	reward = -0.002737	array([[-2.203484 , -3.0679328]], dtype=float32)

time = 70637	action = 0	current_phase = 0	next_phase = 1	reward = 0.059457	array([[-2.8370585, -3.6426234]], dtype=float32)

time = 70642	action = 1	current_phase = 0	next_phase = 1	reward = -1.595936	array([[-7.4421206, -3.499484 ]], dtype=float32)

time = 70650	action = 1	current_phase = 1	next_phase = 0	reward = -0.993359	array([[-3.9241686, -2.3117824]], dtype=float32)

time = 70658	action = 0	current_phase = 0	next_phase = 1	reward = -0.037243	array([[-1.6815971, -3.0452986]], dtype=float32)

time = 70663	action = 0	current_phase = 0	next_phase = 1	reward = 0.053197	array([[-2.1644597, -3.043073 ]], dtype=float32)

time = 70668	action = 0	current_phase = 0	next_phase = 1	reward = 0.092216	array([[-2.8069715, -3.622323 ]], dtype=float32)

time = 70673	action = 1	current_phase = 0	next_phase = 1	reward = -1.421927	array([[-7.4523363, -3.2606409]], dtype=float32)

time = 70681	action = 1	current_phase = 1	next_phase = 0	reward = -1.581933	array([[-3.868596 , -2.3272123]], dtype=float32)

time = 70689	action = 0	current_phase = 0	next_phase = 1	reward = 0.252383	array([[-1.2341423, -2.6785283]], dtype=float32)

time = 70694	action = 0	current_phase = 0	next_phase = 1	reward = 0.048130	array([[-1.8792646, -3.1802502]], dtype=float32)

time = 70699	action = 1	current_phase = 0	next_phase = 1	reward = -0.696194	array([[-3.075964, -2.344808]], dtype=float32)

time = 70707	action = 1	current_phase = 1	next_phase = 0	reward = -0.650615	array([[-3.2808185, -2.0733442]], dtype=float32)

time = 70715	action = 0	current_phase = 0	next_phase = 1	reward = -0.375087	array([[-1.6346362, -3.190363 ]], dtype=float32)

time = 70720	action = 0	current_phase = 0	next_phase = 1	reward = 0.266826	array([[-1.7159854, -3.3875356]], dtype=float32)

time = 70725	action = 0	current_phase = 0	next_phase = 1	reward = 0.063389	array([[-2.3055282, -3.2675982]], dtype=float32)

time = 70730	action = 1	current_phase = 0	next_phase = 1	reward = -1.429457	array([[-6.4991937, -3.0576055]], dtype=float32)

time = 70738	action = 1	current_phase = 1	next_phase = 0	reward = -0.710176	array([[-3.7449756, -2.1635835]], dtype=float32)

time = 70746	action = 0	current_phase = 0	next_phase = 1	reward = -0.073462	array([[-1.7294643, -3.132911 ]], dtype=float32)

time = 70751	action = 0	current_phase = 0	next_phase = 1	reward = -0.006554	array([[-2.1218634, -3.0598006]], dtype=float32)

time = 70756	action = 0	current_phase = 0	next_phase = 1	reward = 0.056930	array([[-2.6732013, -3.5741231]], dtype=float32)

time = 70761	action = 1	current_phase = 0	next_phase = 1	reward = -1.446638	array([[-7.4767265, -3.3066165]], dtype=float32)

time = 70769	action = 1	current_phase = 1	next_phase = 0	reward = -0.841720	array([[-3.7798958, -2.221399 ]], dtype=float32)

time = 70777	action = 0	current_phase = 0	next_phase = 1	reward = -0.096792	array([[-1.7045069, -3.0194383]], dtype=float32)

time = 70782	action = 0	current_phase = 0	next_phase = 1	reward = 0.000162	array([[-2.1656241, -3.0630689]], dtype=float32)

time = 70787	action = 0	current_phase = 0	next_phase = 1	reward = 0.078846	array([[-2.81501  , -3.6249661]], dtype=float32)

time = 70792	action = 1	current_phase = 0	next_phase = 1	reward = -1.636450	array([[-7.4347134, -3.536886 ]], dtype=float32)

time = 70800	action = 1	current_phase = 1	next_phase = 0	reward = -0.942165	array([[-3.872601 , -2.2933245]], dtype=float32)

time = 70808	action = 0	current_phase = 0	next_phase = 1	reward = -0.045579	array([[-1.7667396, -3.0081131]], dtype=float32)

time = 70813	action = 0	current_phase = 0	next_phase = 1	reward = 0.033890	array([[-2.176273 , -3.0537133]], dtype=float32)

time = 70818	action = 0	current_phase = 0	next_phase = 1	reward = 0.066316	array([[-2.81153  , -3.6370785]], dtype=float32)

time = 70823	action = 1	current_phase = 0	next_phase = 1	reward = -1.340257	array([[-7.4522977, -3.3194218]], dtype=float32)

time = 70831	action = 1	current_phase = 1	next_phase = 0	reward = -1.498829	array([[-3.833561 , -2.3255916]], dtype=float32)

time = 70839	action = 0	current_phase = 0	next_phase = 1	reward = 0.247545	array([[-1.1035187, -2.5635633]], dtype=float32)

time = 70844	action = 0	current_phase = 0	next_phase = 1	reward = 0.030675	array([[-1.7821668, -3.1653636]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0128 - val_loss: 0.0085

Epoch 2/50

 - 3s - loss: 0.0158 - val_loss: 0.0093

Epoch 3/50

 - 3s - loss: 0.0145 - val_loss: 0.0078

Epoch 4/50

 - 3s - loss: 0.0136 - val_loss: 0.0090

Epoch 5/50

 - 3s - loss: 0.0143 - val_loss: 0.0081

Epoch 6/50

 - 3s - loss: 0.0129 - val_loss: 0.0089

Epoch 7/50

 - 3s - loss: 0.0151 - val_loss: 0.0089

Epoch 8/50

 - 3s - loss: 0.0136 - val_loss: 0.0092

Epoch 9/50

 - 3s - loss: 0.0135 - val_loss: 0.0088

Epoch 10/50

 - 3s - loss: 0.0139 - val_loss: 0.0082

Epoch 11/50

 - 3s - loss: 0.0184 - val_loss: 0.0076

Epoch 12/50

 - 3s - loss: 0.0156 - val_loss: 0.0083

Epoch 13/50

 - 3s - loss: 0.0146 - val_loss: 0.0090

Epoch 14/50

 - 3s - loss: 0.0149 - val_loss: 0.0081

Epoch 15/50

 - 3s - loss: 0.0128 - val_loss: 0.0083

Epoch 16/50

 - 3s - loss: 0.0114 - val_loss: 0.0077

Epoch 17/50

 - 3s - loss: 0.0133 - val_loss: 0.0078

Epoch 18/50

 - 3s - loss: 0.0113 - val_loss: 0.0081

Epoch 19/50

 - 3s - loss: 0.0103 - val_loss: 0.0078

Epoch 20/50

 - 3s - loss: 0.0136 - val_loss: 0.0090

Epoch 21/50

 - 3s - loss: 0.0112 - val_loss: 0.0101

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 70849	action = 1	current_phase = 0	next_phase = 1	reward = -0.653969	array([[-3.0855157, -2.3791955]], dtype=float32)

time = 70857	action = 1	current_phase = 1	next_phase = 0	reward = -0.922451	array([[-3.3465695, -2.0063586]], dtype=float32)

time = 70865	action = 0	current_phase = 0	next_phase = 1	reward = -0.091565	array([[-1.3729141, -3.1876168]], dtype=float32)

time = 70870	action = 0	current_phase = 0	next_phase = 1	reward = -0.012138	array([[-1.7335379, -3.208119 ]], dtype=float32)

time = 70875	action = 0	current_phase = 0	next_phase = 1	reward = 0.328959	array([[-2.1126444, -3.1947165]], dtype=float32)

time = 70880	action = 1	current_phase = 0	next_phase = 1	reward = -1.390439	array([[-6.5084496, -3.0026994]], dtype=float32)

time = 70888	action = 1	current_phase = 1	next_phase = 0	reward = -0.703729	array([[-3.8218875, -2.1469328]], dtype=float32)

time = 70896	action = 0	current_phase = 0	next_phase = 1	reward = -0.074701	array([[-1.7428495, -3.1411533]], dtype=float32)

time = 70901	action = 0	current_phase = 0	next_phase = 1	reward = -0.000769	array([[-2.097458 , -3.1704416]], dtype=float32)

time = 70906	action = 0	current_phase = 0	next_phase = 1	reward = 0.064815	array([[-2.6121948, -3.537417 ]], dtype=float32)

time = 70911	action = 1	current_phase = 0	next_phase = 1	reward = -1.548528	array([[-7.4483023, -3.2952147]], dtype=float32)

time = 70919	action = 1	current_phase = 1	next_phase = 0	reward = -0.838647	array([[-3.8595374, -2.2015219]], dtype=float32)

time = 70927	action = 0	current_phase = 0	next_phase = 1	reward = -0.066254	array([[-1.8125668, -3.0350645]], dtype=float32)

time = 70932	action = 0	current_phase = 0	next_phase = 1	reward = 0.020510	array([[-2.2614024, -3.0576775]], dtype=float32)

time = 70937	action = 0	current_phase = 0	next_phase = 1	reward = 0.078298	array([[-2.8267374, -3.6747332]], dtype=float32)

time = 70942	action = 1	current_phase = 0	next_phase = 1	reward = -1.508728	array([[-7.4318466, -3.5003405]], dtype=float32)

time = 70950	action = 1	current_phase = 1	next_phase = 0	reward = -1.002311	array([[-3.950577 , -2.4109979]], dtype=float32)

time = 70958	action = 0	current_phase = 0	next_phase = 1	reward = -0.047346	array([[-1.7088739, -3.0836048]], dtype=float32)

time = 70963	action = 0	current_phase = 0	next_phase = 1	reward = 0.033934	array([[-2.2387886, -3.0583076]], dtype=float32)

time = 70968	action = 0	current_phase = 0	next_phase = 1	reward = 0.078203	array([[-2.761396, -3.605101]], dtype=float32)

time = 70973	action = 1	current_phase = 0	next_phase = 1	reward = -1.303308	array([[-7.4501753, -3.2437367]], dtype=float32)

time = 70981	action = 1	current_phase = 1	next_phase = 0	reward = -1.338481	array([[-3.8945003, -2.2743132]], dtype=float32)

time = 70989	action = 0	current_phase = 0	next_phase = 1	reward = 0.232770	array([[-1.1756704, -2.5831184]], dtype=float32)

time = 70994	action = 0	current_phase = 0	next_phase = 1	reward = 0.025395	array([[-1.8082914, -3.148645 ]], dtype=float32)

time = 70999	action = 1	current_phase = 0	next_phase = 1	reward = -0.691303	array([[-3.0631742, -2.3574982]], dtype=float32)

time = 71007	action = 1	current_phase = 1	next_phase = 0	reward = -0.935179	array([[-3.4186785, -2.0322099]], dtype=float32)

time = 71015	action = 0	current_phase = 0	next_phase = 1	reward = -0.105403	array([[-1.3133211, -3.1890354]], dtype=float32)

time = 71020	action = 0	current_phase = 0	next_phase = 1	reward = -0.018635	array([[-1.7250552, -3.3274922]], dtype=float32)

time = 71025	action = 0	current_phase = 0	next_phase = 1	reward = 0.328323	array([[-2.1568017, -3.202038 ]], dtype=float32)

time = 71030	action = 1	current_phase = 0	next_phase = 1	reward = -1.210580	array([[-6.499708 , -2.9798748]], dtype=float32)

time = 71038	action = 1	current_phase = 1	next_phase = 0	reward = -0.702628	array([[-3.4939983, -2.0578885]], dtype=float32)

time = 71046	action = 0	current_phase = 0	next_phase = 1	reward = -0.082837	array([[-1.6751268, -3.1351626]], dtype=float32)

time = 71051	action = 0	current_phase = 0	next_phase = 1	reward = 0.003118	array([[-2.1384723, -3.144824 ]], dtype=float32)

time = 71056	action = 0	current_phase = 0	next_phase = 1	reward = 0.055752	array([[-2.641413, -3.53363 ]], dtype=float32)

time = 71061	action = 1	current_phase = 0	next_phase = 1	reward = -1.548114	array([[-7.48699  , -3.2731726]], dtype=float32)

time = 71069	action = 1	current_phase = 1	next_phase = 0	reward = -0.840688	array([[-3.8579512, -2.2064993]], dtype=float32)

time = 71077	action = 0	current_phase = 0	next_phase = 1	reward = -0.062642	array([[-1.789623 , -3.0346913]], dtype=float32)

time = 71082	action = 0	current_phase = 0	next_phase = 1	reward = 0.010822	array([[-2.2429903, -3.0648742]], dtype=float32)

time = 71087	action = 0	current_phase = 0	next_phase = 1	reward = 0.067234	array([[-2.7721822, -3.6091115]], dtype=float32)

time = 71092	action = 1	current_phase = 0	next_phase = 1	reward = -1.555525	array([[-7.4253693, -3.5595002]], dtype=float32)

time = 71100	action = 1	current_phase = 1	next_phase = 0	reward = -0.949098	array([[-3.9943902, -2.3617718]], dtype=float32)

time = 71108	action = 0	current_phase = 0	next_phase = 1	reward = -0.039774	array([[-1.7428768, -3.0812101]], dtype=float32)

time = 71113	action = 0	current_phase = 0	next_phase = 1	reward = 0.034496	array([[-2.2050664, -3.0488403]], dtype=float32)

time = 71118	action = 0	current_phase = 0	next_phase = 1	reward = 0.084874	array([[-2.7220936, -3.5647757]], dtype=float32)

time = 71123	action = 1	current_phase = 0	next_phase = 1	reward = -0.791616	array([[-7.438855 , -3.0125847]], dtype=float32)

time = 71131	action = 1	current_phase = 1	next_phase = 0	reward = -1.352056	array([[-3.747551 , -2.3657649]], dtype=float32)

time = 71139	action = 0	current_phase = 0	next_phase = 1	reward = -0.051147	array([[-1.3868978, -2.6694593]], dtype=float32)

time = 71144	action = 0	current_phase = 0	next_phase = 1	reward = 0.023875	array([[-1.837084 , -3.1668453]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0173 - val_loss: 0.0072

Epoch 2/50

 - 3s - loss: 0.0126 - val_loss: 0.0065

Epoch 3/50

 - 3s - loss: 0.0126 - val_loss: 0.0072

Epoch 4/50

 - 3s - loss: 0.0140 - val_loss: 0.0063

Epoch 5/50

 - 3s - loss: 0.0141 - val_loss: 0.0092

Epoch 6/50

 - 3s - loss: 0.0162 - val_loss: 0.0076

Epoch 7/50

 - 3s - loss: 0.0119 - val_loss: 0.0084

Epoch 8/50

 - 3s - loss: 0.0117 - val_loss: 0.0084

Epoch 9/50

 - 3s - loss: 0.0141 - val_loss: 0.0086

Epoch 10/50

 - 3s - loss: 0.0141 - val_loss: 0.0077

Epoch 11/50

 - 3s - loss: 0.0108 - val_loss: 0.0083

Epoch 12/50

 - 3s - loss: 0.0145 - val_loss: 0.0093

Epoch 13/50

 - 3s - loss: 0.0123 - val_loss: 0.0069

Epoch 14/50

 - 3s - loss: 0.0108 - val_loss: 0.0098

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 71149	action = 1	current_phase = 0	next_phase = 1	reward = -0.711678	array([[-3.0435503, -2.377249 ]], dtype=float32)

time = 71157	action = 1	current_phase = 1	next_phase = 0	reward = -0.933654	array([[-3.401372, -2.011581]], dtype=float32)

time = 71165	action = 0	current_phase = 0	next_phase = 1	reward = 0.158083	array([[-1.3838534, -3.1670907]], dtype=float32)

time = 71170	action = 0	current_phase = 0	next_phase = 1	reward = -0.319518	array([[-1.8773398, -3.1188345]], dtype=float32)

time = 71175	action = 0	current_phase = 0	next_phase = 1	reward = 0.045778	array([[-2.1510274, -3.1506896]], dtype=float32)

time = 71180	action = 1	current_phase = 0	next_phase = 1	reward = -1.028023	array([[-6.4484344, -2.884548 ]], dtype=float32)

time = 71188	action = 1	current_phase = 1	next_phase = 0	reward = -0.692335	array([[-3.776569, -2.094721]], dtype=float32)

time = 71196	action = 0	current_phase = 0	next_phase = 1	reward = -0.093105	array([[-1.7030008, -3.086585 ]], dtype=float32)

time = 71201	action = 0	current_phase = 0	next_phase = 1	reward = 0.005605	array([[-2.037636, -3.175922]], dtype=float32)

time = 71206	action = 0	current_phase = 0	next_phase = 1	reward = 0.057132	array([[-2.6272004, -3.5227754]], dtype=float32)

time = 71211	action = 1	current_phase = 0	next_phase = 1	reward = -1.443418	array([[-7.488098 , -3.2556422]], dtype=float32)

time = 71219	action = 1	current_phase = 1	next_phase = 0	reward = -1.109398	array([[-3.839531 , -2.1969492]], dtype=float32)

time = 71227	action = 0	current_phase = 0	next_phase = 1	reward = 0.224792	array([[-1.6412547, -2.967809 ]], dtype=float32)

time = 71232	action = 0	current_phase = 0	next_phase = 1	reward = 0.007950	array([[-2.2208488, -3.0707927]], dtype=float32)

time = 71237	action = 0	current_phase = 0	next_phase = 1	reward = 0.080016	array([[-2.7583373, -3.6292472]], dtype=float32)

time = 71242	action = 1	current_phase = 0	next_phase = 1	reward = -1.616289	array([[-7.430943 , -3.4724598]], dtype=float32)

time = 71250	action = 1	current_phase = 1	next_phase = 0	reward = -0.953076	array([[-3.9473948, -2.3406901]], dtype=float32)

time = 71258	action = 0	current_phase = 0	next_phase = 1	reward = -0.046711	array([[-1.7019435, -3.0536125]], dtype=float32)

time = 71263	action = 0	current_phase = 0	next_phase = 1	reward = 0.031387	array([[-2.2046516, -3.052383 ]], dtype=float32)

time = 71268	action = 0	current_phase = 0	next_phase = 1	reward = 0.080058	array([[-2.7280374, -3.5517125]], dtype=float32)

time = 71273	action = 1	current_phase = 0	next_phase = 1	reward = -1.382971	array([[-7.4581776, -3.194236 ]], dtype=float32)

time = 71281	action = 1	current_phase = 1	next_phase = 0	reward = -1.232814	array([[-3.9004254, -2.2399771]], dtype=float32)

time = 71289	action = 0	current_phase = 0	next_phase = 1	reward = -0.036287	array([[-1.3213928, -2.669372 ]], dtype=float32)

time = 71294	action = 0	current_phase = 0	next_phase = 1	reward = 0.044917	array([[-1.7792757, -3.1246438]], dtype=float32)

time = 71299	action = 1	current_phase = 0	next_phase = 1	reward = -0.703150	array([[-3.1142526, -2.3494732]], dtype=float32)

time = 71307	action = 1	current_phase = 1	next_phase = 0	reward = -0.929083	array([[-3.3707542, -1.9932346]], dtype=float32)

time = 71315	action = 0	current_phase = 0	next_phase = 1	reward = -0.108300	array([[-1.2921703, -3.2550113]], dtype=float32)

time = 71320	action = 0	current_phase = 0	next_phase = 1	reward = 0.253590	array([[-1.7168448, -3.079411 ]], dtype=float32)

time = 71325	action = 0	current_phase = 0	next_phase = 1	reward = -0.229741	array([[-2.378273, -3.297375]], dtype=float32)

time = 71330	action = 1	current_phase = 0	next_phase = 1	reward = -1.032155	array([[-6.4690905, -2.7988715]], dtype=float32)

time = 71338	action = 1	current_phase = 1	next_phase = 0	reward = -0.716950	array([[-3.7961483, -2.1156287]], dtype=float32)

time = 71346	action = 0	current_phase = 0	next_phase = 1	reward = -0.090849	array([[-1.7159355, -3.1144898]], dtype=float32)

time = 71351	action = 0	current_phase = 0	next_phase = 1	reward = -0.002232	array([[-2.0857856, -3.102699 ]], dtype=float32)

time = 71356	action = 0	current_phase = 0	next_phase = 1	reward = 0.065572	array([[-2.5446038, -3.5325544]], dtype=float32)

time = 71361	action = 1	current_phase = 0	next_phase = 1	reward = -1.500136	array([[-7.474178 , -3.2345386]], dtype=float32)

time = 71369	action = 1	current_phase = 1	next_phase = 0	reward = -1.470460	array([[-3.8158383, -2.1820753]], dtype=float32)

time = 71377	action = 0	current_phase = 0	next_phase = 1	reward = 0.525553	array([[-1.5555644, -2.9849808]], dtype=float32)

time = 71382	action = 0	current_phase = 0	next_phase = 1	reward = 0.041702	array([[-2.2461462, -3.0582688]], dtype=float32)

time = 71387	action = 0	current_phase = 0	next_phase = 1	reward = 0.090832	array([[-2.770648 , -3.6784647]], dtype=float32)

time = 71392	action = 1	current_phase = 0	next_phase = 1	reward = -1.658383	array([[-7.4391494, -3.4897015]], dtype=float32)

time = 71400	action = 1	current_phase = 1	next_phase = 0	reward = -0.957832	array([[-3.9549842, -2.3276892]], dtype=float32)

time = 71408	action = 0	current_phase = 0	next_phase = 1	reward = -0.058753	array([[-1.786202 , -3.0791216]], dtype=float32)

time = 71413	action = 0	current_phase = 0	next_phase = 1	reward = 0.044673	array([[-2.163342 , -3.0520241]], dtype=float32)

time = 71418	action = 0	current_phase = 0	next_phase = 1	reward = 0.083324	array([[-2.7555013, -3.5942786]], dtype=float32)

time = 71423	action = 1	current_phase = 0	next_phase = 1	reward = -0.779701	array([[-7.4575725, -3.0593054]], dtype=float32)

time = 71431	action = 1	current_phase = 1	next_phase = 0	reward = -1.258514	array([[-3.8010292, -2.2905593]], dtype=float32)

time = 71439	action = 0	current_phase = 0	next_phase = 1	reward = -0.048077	array([[-1.3328462, -2.6781797]], dtype=float32)

time = 71444	action = 0	current_phase = 0	next_phase = 1	reward = 0.024520	array([[-1.8387125, -3.1231046]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0143 - val_loss: 0.0075

Epoch 2/50

 - 3s - loss: 0.0115 - val_loss: 0.0073

Epoch 3/50

 - 3s - loss: 0.0123 - val_loss: 0.0082

Epoch 4/50

 - 3s - loss: 0.0115 - val_loss: 0.0081

Epoch 5/50

 - 3s - loss: 0.0138 - val_loss: 0.0088

Epoch 6/50

 - 3s - loss: 0.0120 - val_loss: 0.0094

Epoch 7/50

 - 3s - loss: 0.0133 - val_loss: 0.0086

Epoch 8/50

 - 3s - loss: 0.0108 - val_loss: 0.0086

Epoch 9/50

 - 3s - loss: 0.0102 - val_loss: 0.0095

Epoch 10/50

 - 3s - loss: 0.0101 - val_loss: 0.0097

Epoch 11/50

 - 3s - loss: 0.0109 - val_loss: 0.0087

Epoch 12/50

 - 3s - loss: 0.0114 - val_loss: 0.0088

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 71449	action = 1	current_phase = 0	next_phase = 1	reward = -0.648218	array([[-3.042038 , -2.4441063]], dtype=float32)

time = 71457	action = 1	current_phase = 1	next_phase = 0	reward = -0.863887	array([[-3.3013635, -1.9811075]], dtype=float32)

time = 71465	action = 0	current_phase = 0	next_phase = 1	reward = 0.183873	array([[-1.3991778, -3.1510968]], dtype=float32)

time = 71470	action = 0	current_phase = 0	next_phase = 1	reward = -0.017758	array([[-1.8905938, -3.2232237]], dtype=float32)

time = 71475	action = 0	current_phase = 0	next_phase = 1	reward = 0.049695	array([[-2.4156353, -3.2212043]], dtype=float32)

time = 71480	action = 1	current_phase = 0	next_phase = 1	reward = -1.373384	array([[-6.531585 , -3.0205464]], dtype=float32)

time = 71488	action = 1	current_phase = 1	next_phase = 0	reward = -0.702043	array([[-3.8383594, -2.1422985]], dtype=float32)

time = 71496	action = 0	current_phase = 0	next_phase = 1	reward = -0.084063	array([[-1.708627 , -3.0695968]], dtype=float32)

time = 71501	action = 0	current_phase = 0	next_phase = 1	reward = -0.011066	array([[-2.0864463, -3.0868325]], dtype=float32)

time = 71506	action = 0	current_phase = 0	next_phase = 1	reward = 0.059691	array([[-2.589972 , -3.5553517]], dtype=float32)

time = 71511	action = 1	current_phase = 0	next_phase = 1	reward = -1.390327	array([[-7.492347 , -3.2395022]], dtype=float32)

time = 71519	action = 1	current_phase = 1	next_phase = 0	reward = -0.807552	array([[-3.8360538, -2.193037 ]], dtype=float32)

time = 71527	action = 0	current_phase = 0	next_phase = 1	reward = -0.059596	array([[-1.7428721, -3.0294256]], dtype=float32)

time = 71532	action = 0	current_phase = 0	next_phase = 1	reward = 0.021780	array([[-2.257413 , -3.0652897]], dtype=float32)

time = 71537	action = 0	current_phase = 0	next_phase = 1	reward = 0.061918	array([[-2.8013058, -3.6633058]], dtype=float32)

time = 71542	action = 1	current_phase = 0	next_phase = 1	reward = -1.686858	array([[-7.4468675, -3.4625962]], dtype=float32)

time = 71550	action = 1	current_phase = 1	next_phase = 0	reward = -0.945038	array([[-3.967112 , -2.3430495]], dtype=float32)

time = 71558	action = 0	current_phase = 0	next_phase = 1	reward = -0.058228	array([[-1.7652811, -3.0182993]], dtype=float32)

time = 71563	action = 0	current_phase = 0	next_phase = 1	reward = 0.022667	array([[-2.2129335, -3.0632038]], dtype=float32)

time = 71568	action = 0	current_phase = 0	next_phase = 1	reward = 0.071044	array([[-2.8155885, -3.6401308]], dtype=float32)

time = 71573	action = 1	current_phase = 0	next_phase = 1	reward = -1.803147	array([[-7.435234 , -3.5412505]], dtype=float32)

time = 71581	action = 1	current_phase = 1	next_phase = 0	reward = -1.089411	array([[-4.067542 , -2.1568098]], dtype=float32)

time = 71589	action = 0	current_phase = 0	next_phase = 1	reward = -0.048681	array([[-1.4361807, -2.6548214]], dtype=float32)

time = 71594	action = 0	current_phase = 0	next_phase = 1	reward = 0.016912	array([[-1.883625 , -3.0628662]], dtype=float32)

time = 71599	action = 1	current_phase = 0	next_phase = 1	reward = -0.641406	array([[-3.2107735, -2.3148496]], dtype=float32)

time = 71607	action = 1	current_phase = 1	next_phase = 0	reward = -0.938171	array([[-3.3353672, -1.9605484]], dtype=float32)

time = 71615	action = 0	current_phase = 0	next_phase = 1	reward = -0.104497	array([[-1.2762527, -3.2124631]], dtype=float32)

time = 71620	action = 0	current_phase = 0	next_phase = 1	reward = 0.248411	array([[-1.6649044, -3.0025458]], dtype=float32)

time = 71625	action = 0	current_phase = 0	next_phase = 1	reward = -0.240400	array([[-2.3472624, -3.2724247]], dtype=float32)

time = 71630	action = 1	current_phase = 0	next_phase = 1	reward = -1.147184	array([[-6.486204 , -2.7862587]], dtype=float32)

time = 71638	action = 1	current_phase = 1	next_phase = 0	reward = -0.704537	array([[-3.7722406, -2.088976 ]], dtype=float32)

time = 71646	action = 0	current_phase = 0	next_phase = 1	reward = -0.086421	array([[-1.7063916, -3.1516464]], dtype=float32)

time = 71651	action = 0	current_phase = 0	next_phase = 1	reward = -0.021100	array([[-2.113276 , -3.0846603]], dtype=float32)

time = 71656	action = 0	current_phase = 0	next_phase = 1	reward = 0.057931	array([[-2.5173678, -3.4274952]], dtype=float32)

time = 71661	action = 1	current_phase = 0	next_phase = 1	reward = -1.466159	array([[-7.4941564, -3.2098362]], dtype=float32)

time = 71669	action = 1	current_phase = 1	next_phase = 0	reward = -1.068387	array([[-3.8162107, -2.1465921]], dtype=float32)

time = 71677	action = 0	current_phase = 0	next_phase = 1	reward = 0.223367	array([[-1.6147323, -3.0378149]], dtype=float32)

time = 71682	action = 0	current_phase = 0	next_phase = 1	reward = 0.004686	array([[-2.2544425, -3.0771675]], dtype=float32)

time = 71687	action = 0	current_phase = 0	next_phase = 1	reward = 0.079951	array([[-2.824157 , -3.6713014]], dtype=float32)

time = 71692	action = 1	current_phase = 0	next_phase = 1	reward = -1.615021	array([[-7.4565263, -3.4638703]], dtype=float32)

time = 71700	action = 1	current_phase = 1	next_phase = 0	reward = -1.319397	array([[-3.9784532, -2.3768024]], dtype=float32)

time = 71708	action = 0	current_phase = 0	next_phase = 1	reward = 0.248223	array([[-1.5752487, -2.9734597]], dtype=float32)

time = 71713	action = 0	current_phase = 0	next_phase = 1	reward = 0.009890	array([[-2.233121 , -3.0576315]], dtype=float32)

time = 71718	action = 0	current_phase = 0	next_phase = 1	reward = 0.086162	array([[-2.816615, -3.656951]], dtype=float32)

time = 71723	action = 1	current_phase = 0	next_phase = 1	reward = -1.757667	array([[-7.4605427, -3.4065564]], dtype=float32)

time = 71731	action = 1	current_phase = 1	next_phase = 0	reward = -1.744858	array([[-4.0600424, -2.1812127]], dtype=float32)

time = 71739	action = 0	current_phase = 0	next_phase = 1	reward = 0.554222	array([[-1.2564229, -2.5689373]], dtype=float32)

time = 71744	action = 0	current_phase = 0	next_phase = 1	reward = 0.024574	array([[-1.8417567, -3.108611 ]], dtype=float32)

Train on 840 samples, validate on 360 samples

Epoch 1/50

 - 3s - loss: 0.0139 - val_loss: 0.0068

Epoch 2/50

 - 3s - loss: 0.0117 - val_loss: 0.0063

Epoch 3/50

 - 3s - loss: 0.0117 - val_loss: 0.0091

Epoch 4/50

 - 3s - loss: 0.0129 - val_loss: 0.0074

Epoch 5/50

 - 3s - loss: 0.0113 - val_loss: 0.0085

Epoch 6/50

 - 3s - loss: 0.0109 - val_loss: 0.0069

Epoch 7/50

 - 3s - loss: 0.0112 - val_loss: 0.0069

Epoch 8/50

 - 3s - loss: 0.0129 - val_loss: 0.0088

Epoch 9/50

 - 3s - loss: 0.0126 - val_loss: 0.0086

Epoch 10/50

 - 3s - loss: 0.0130 - val_loss: 0.0078

Epoch 11/50

 - 3s - loss: 0.0147 - val_loss: 0.0094

Epoch 12/50

 - 3s - loss: 0.0117 - val_loss: 0.0081

length of memory (state 0, action 0): 1028, before forget

length of memory (state 0, action 0): 1000, after forget

length of memory (state 0, action 1): 1010, before forget

length of memory (state 0, action 1): 1000, after forget

length of memory (state 1, action 0): 495, after forget

length of memory (state 1, action 1): 1010, before forget

length of memory (state 1, action 1): 1000, after forget

time = 71749	action = 1	current_phase = 0	next_phase = 1	reward = -0.650598	array([[-3.1605606, -2.3164456]], dtype=float32)

time = 71757	action = 1	current_phase = 1	next_phase = 0	reward = -0.591907	array([[-3.2997055, -1.9543912]], dtype=float32)

time = 71765	action = 0	current_phase = 0	next_phase = 1	reward = -0.094663	array([[-1.4456543, -3.2579527]], dtype=float32)

time = 71770	action = 0	current_phase = 0	next_phase = 1	reward = -0.019716	array([[-1.772115, -3.070417]], dtype=float32)

time = 71775	action = 0	current_phase = 0	next_phase = 1	reward = -0.235633	array([[-2.442275 , -3.2737083]], dtype=float32)

time = 71780	action = 1	current_phase = 0	next_phase = 1	reward = -0.997337	array([[-6.4827485, -2.8110046]], dtype=float32)

time = 71788	action = 1	current_phase = 1	next_phase = 0	reward = -0.985602	array([[-3.6183028, -2.0554786]], dtype=float32)

time = 71796	action = 0	current_phase = 0	next_phase = 1	reward = 0.205853	array([[-1.571887 , -2.9936266]], dtype=float32)

time = 71801	action = 0	current_phase = 0	next_phase = 1	reward = 0.008543	array([[-2.0733812, -3.0705276]], dtype=float32)

time = 71806	action = 0	current_phase = 0	next_phase = 1	reward = 0.067514	array([[-2.497515 , -3.4773855]], dtype=float32)

time = 71811	action = 1	current_phase = 0	next_phase = 1	reward = -1.459084	array([[-7.4734416, -3.3014073]], dtype=float32)

time = 71819	action = 1	current_phase = 1	next_phase = 0	reward = -0.830982	array([[-3.8742752, -2.2234697]], dtype=float32)

time = 71827	action = 0	current_phase = 0	next_phase = 1	reward = -0.064857	array([[-1.7797544, -2.9973664]], dtype=float32)

time = 71832	action = 0	current_phase = 0	next_phase = 1	reward = 0.004489	array([[-2.2123542, -3.0625463]], dtype=float32)

time = 71837	action = 0	current_phase = 0	next_phase = 1	reward = 0.062566	array([[-2.7712965, -3.6428552]], dtype=float32)

time = 71842	action = 1	current_phase = 0	next_phase = 1	reward = -1.643258	array([[-7.452342 , -3.4632998]], dtype=float32)

time = 71850	action = 1	current_phase = 1	next_phase = 0	reward = -0.953132	array([[-3.9811785, -2.3938448]], dtype=float32)

time = 71858	action = 0	current_phase = 0	next_phase = 1	reward = -0.053293	array([[-1.6669047, -3.018416 ]], dtype=float32)

time = 71863	action = 0	current_phase = 0	next_phase = 1	reward = 0.012453	array([[-2.1903002, -3.048427 ]], dtype=float32)

time = 71868	action = 0	current_phase = 0	next_phase = 1	reward = 0.077795	array([[-2.7171826, -3.6114678]], dtype=float32)

time = 71873	action = 1	current_phase = 0	next_phase = 1	reward = -0.774438	array([[-7.4619384, -2.9683537]], dtype=float32)

time = 71881	action = 1	current_phase = 1	next_phase = 0	reward = -1.232115	array([[-3.7820015, -2.2562494]], dtype=float32)

time = 71889	action = 0	current_phase = 0	next_phase = 1	reward = -0.049336	array([[-1.3734001, -2.6621776]], dtype=float32)

time = 71894	action = 0	current_phase = 0	next_phase = 1	reward = 0.025103	array([[-1.8189554, -3.1868186]], dtype=float32)

time = 71899	action = 1	current_phase = 0	next_phase = 1	reward = -0.728873	array([[-3.1318526, -2.327887 ]], dtype=float32)

time = 71907	action = 1	current_phase = 1	next_phase = 0	reward = -0.913399	array([[-3.4657874, -2.0565782]], dtype=float32)

time = 71915	action = 0	current_phase = 0	next_phase = 1	reward = 0.180335	array([[-1.2002083, -3.2089677]], dtype=float32)

time = 71920	action = 0	current_phase = 0	next_phase = 1	reward = -0.027465	array([[-1.8044661, -3.1366563]], dtype=float32)

time = 71925	action = 0	current_phase = 0	next_phase = 1	reward = -0.237274	array([[-2.4453714, -3.3032303]], dtype=float32)

time = 71930	action = 1	current_phase = 0	next_phase = 1	reward = -1.056029	array([[-6.4955373, -2.8023095]], dtype=float32)

time = 71938	action = 1	current_phase = 1	next_phase = 0	reward = -0.710835	array([[-3.7511337, -2.098185 ]], dtype=float32)

time = 71946	action = 0	current_phase = 0	next_phase = 1	reward = -0.082125	array([[-1.6460084, -3.1241508]], dtype=float32)

time = 71951	action = 0	current_phase = 0	next_phase = 1	reward = -0.022202	array([[-2.1338372, -3.0734153]], dtype=float32)

time = 71956	action = 0	current_phase = 0	next_phase = 1	reward = 0.057680	array([[-2.5587635, -3.5621972]], dtype=float32)

time = 71961	action = 1	current_phase = 0	next_phase = 1	reward = -1.486446	array([[-7.501233 , -3.2096353]], dtype=float32)

time = 71969	action = 1	current_phase = 1	next_phase = 0	reward = -0.845697	array([[-3.8148816, -2.1744683]], dtype=float32)

time = 71977	action = 0	current_phase = 0	next_phase = 1	reward = -0.076213	array([[-1.7254398, -3.0110574]], dtype=float32)

time = 71982	action = 0	current_phase = 0	next_phase = 1	reward = -0.001446	array([[-2.2290664, -3.0681987]], dtype=float32)

time = 71987	action = 0	current_phase = 0	next_phase = 1	reward = 0.066159	array([[-2.7333512, -3.6566467]], dtype=float32)

time = 71992	action = 1	current_phase = 0	next_phase = 1	reward = -1.525629	array([[-7.438412, -3.493124]], dtype=float32)

END

finished ['cross.2phases_rou01_unequal_5_300s.xml']

finished Deeplight

Error: tcpip::Socket::recvAndCheck @ recv: peer shutdown

Quitting (on error).

(venv3.6) ]0;soup@soup-virtual-machine: ~/IntelliLight[01;32msoup@soup-virtual-machine[00m:[01;34m~/IntelliLight[00m$ exiy[Kt

exit



Script done on 2021-07-09 10:02:36+10:00 [COMMAND_EXIT_CODE="0"]

